<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Graph Retrieval-Augmented Generation: A Survey</title>
<!--Generated on Tue Sep 10 15:35:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models,  Graph Retrieval-Augmented Generation,  Knowledge Graphs,  Graph Neural Networks" lang="en" name="keywords"/>
<base href="/html/2408.08921v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S1" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S2" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Comparison with Related Techniques and Surveys</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S2.SS1" title="In 2. Comparison with Related Techniques and Surveys ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S2.SS2" title="In 2. Comparison with Related Techniques and Surveys ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>LLMs on Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S2.SS3" title="In 2. Comparison with Related Techniques and Surveys ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>KBQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S3" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S3.SS1" title="In 3. Preliminaries ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Text-Attributed Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S3.SS2" title="In 3. Preliminaries ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Graph Neural Networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S3.SS3" title="In 3. Preliminaries ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S4" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Overview of GraphRAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S4.SS0.SSS0.Px1" title="In 4. Overview of GraphRAG ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">Graph-Based Indexing (G-Indexing)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S4.SS0.SSS0.Px2" title="In 4. Overview of GraphRAG ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">Graph-Guided Retrieval (G-Retrieval)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S4.SS0.SSS0.Px3" title="In 4. Overview of GraphRAG ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">Graph-Enhanced Generation (G-Generation)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Graph-Based Indexing</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS1" title="In 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Graph Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS1.SSS1" title="In 5.1. Graph Data ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Open Knowledge Graphs</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS1.SSS1.Px1" title="In 5.1.1. Open Knowledge Graphs ‣ 5.1. Graph Data ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) General Knowledge Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS1.SSS1.Px2" title="In 5.1.1. Open Knowledge Graphs ‣ 5.1. Graph Data ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Domain Knowledge Graphs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS1.SSS2" title="In 5.1. Graph Data ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Self-Constructed Graph Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS2" title="In 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Indexing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS2.SSS1" title="In 5.2. Indexing ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Graph Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS2.SSS2" title="In 5.2. Indexing ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Text Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS2.SSS3" title="In 5.2. Indexing ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Vector Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.SS2.SSS4" title="In 5.2. Indexing ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.4 </span>Hybrid Indexing</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Graph-Guided Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS1" title="In 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Retriever</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS1.SSS1" title="In 6.1. Retriever ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Non-parametric Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS1.SSS2" title="In 6.1. Retriever ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>LM-based Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS1.SSS3" title="In 6.1. Retriever ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>GNN-based Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS1.SSS4" title="In 6.1. Retriever ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2" title="In 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Retrieval Paradigm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS1" title="In 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Once Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS2" title="In 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Iterative Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS2.Px1" title="In 6.2.2. Iterative Retrieval ‣ 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Non-Adaptive Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS2.Px2" title="In 6.2.2. Iterative Retrieval ‣ 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Adaptive Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS3" title="In 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Multi-Stage Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS2.SSS4" title="In 6.2. Retrieval Paradigm ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3" title="In 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Retrieval Granularity</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS1" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Nodes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS2" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Triplets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS3" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.3 </span>Paths</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS4" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.4 </span>Subgraphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS5" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.5 </span>Hybrid Granularties</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS3.SSS6" title="In 6.3. Retrieval Granularity ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.6 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4" title="In 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Retrieval Enhancement</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS1" title="In 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.1 </span>Query Enhancement</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS1.Px1" title="In 6.4.1. Query Enhancement ‣ 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Query Expansion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS1.Px2" title="In 6.4.1. Query Enhancement ‣ 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Query Decomposition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS2" title="In 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.2 </span>Knowledge Enhancement</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS2.Px1" title="In 6.4.2. Knowledge Enhancement ‣ 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Knowledge Merging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.SS4.SSS2.Px2" title="In 6.4.2. Knowledge Enhancement ‣ 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Knowledge Pruning</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Graph-Enhanced Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1" title="In 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Generators</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS1" title="In 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.1 </span>GNNs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS2" title="In 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.2 </span>LMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS3" title="In 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.3 </span>Hybrid Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS3.Px1" title="In 7.1.3. Hybrid Models ‣ 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Cascaded Paradigm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS3.Px2" title="In 7.1.3. Hybrid Models ‣ 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Parallel Paradigm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS1.SSS3.Px3" title="In 7.1.3. Hybrid Models ‣ 7.1. Generators ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">Discussion</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2" title="In 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Graph Formats</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1" title="In 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Graph Languages</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px1" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Adjacency / Edge Table</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px2" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Natural Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px3" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(3) Code-Like Forms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px4" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(4) Syntax Tree</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px5" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(5) Node Sequence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS1.Px6" title="In 7.2.1. Graph Languages ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2.SSS2" title="In 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Graph Embeddings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS3" title="In 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Generation Enhancement</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS3.SSS1" title="In 7.3. Generation Enhancement ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span>Pre-Generation Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS3.SSS2" title="In 7.3. Generation Enhancement ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.2 </span>Mid-Generation Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS3.SSS3" title="In 7.3. Generation Enhancement ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.3 </span>Post-Generation Enhancement</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS1" title="In 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Training Strategies of Retriever</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS1.SSS1" title="In 8.1. Training Strategies of Retriever ‣ 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.1 </span>Training-Free</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS1.SSS2" title="In 8.1. Training Strategies of Retriever ‣ 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.2 </span>Training-Based</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS2" title="In 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Training of Generator</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS2.SSS1" title="In 8.2. Training of Generator ‣ 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.1 </span>Training-Free</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS2.SSS2" title="In 8.2. Training of Generator ‣ 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.2 </span>Training-Based</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8.SS3" title="In 8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Joint Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Applications and Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1" title="In 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Downstream Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS1" title="In 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.1 </span>Question Answering</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS1.Px1" title="In 9.1.1. Question Answering ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) KBQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS1.Px2" title="In 9.1.1. Question Answering ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) CSQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS2" title="In 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.2 </span>Information Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS2.Px1" title="In 9.1.2. Information Retrieval ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Entity Linking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS2.Px2" title="In 9.1.2. Information Retrieval ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Relation Extraction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS3" title="In 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.3 </span>Others</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS3.Px1" title="In 9.1.3. Others ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Fact Verification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS3.Px2" title="In 9.1.3. Others ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Link Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS3.Px3" title="In 9.1.3. Others ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(3) Dialogue Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1.SSS3.Px4" title="In 9.1.3. Others ‣ 9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(4) Recommendation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2" title="In 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Application Domains</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS1" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.1 </span>E-Commerce</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS2" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.2 </span>Biomedical</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS3" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.3 </span>Academic</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS4" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.4 </span>Literature</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS5" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.5 </span>Legal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS2.SSS6" title="In 9.2. Application Domains ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.6 </span>Others</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS3" title="In 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>Benchmarks and Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS3.SSS1" title="In 9.3. Benchmarks and Metrics ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3.1 </span>Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS3.SSS2" title="In 9.3. Benchmarks and Metrics ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3.2 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS3.SSS2.Px1" title="In 9.3.2. Metrics ‣ 9.3. Benchmarks and Metrics ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(1) Downstream Task Evaluation (Generation Quality)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS3.SSS2.Px2" title="In 9.3.2. Metrics ‣ 9.3. Benchmarks and Metrics ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title">(2) Retrieval Quality Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS4" title="In 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4 </span>GraphRAG in Industry</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Future Prospects</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS1" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span>Dynamic and Adaptive Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS2" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span>Multi-Modality Information Integration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS3" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3 </span>Scalable and Efficient Retrieval Mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS4" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.4 </span>Combination with Graph Foundation Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS5" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.5 </span>Lossless Compression of Retrieved Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS6" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.6 </span>Standard Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10.SS7" title="In 10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.7 </span>Broader Applications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S11" title="In Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\pdfcolInitStack</span>
<p class="ltx_p" id="p1.2">tcb@breakable






































































</p>
</div>
<h1 class="ltx_title ltx_title_document">Graph Retrieval-Augmented Generation: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Boci Peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">School of Intelligence Science and
Technology, Peking University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:bcpeng@stu.pku.edu.cn">bcpeng@stu.pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yun Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">College of Computer Science and Technology, Zhejiang University</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhuyun%CB%99dcd@zju.edu.cn">zhuyun˙dcd@zju.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yongchao Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yongchao.ly@antgroup.com">yongchao.ly@antgroup.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaohe Bo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Gaoling School of Artificial Intelligence, Renmin University of China</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:bellebxh@gmail.com">bellebxh@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haizhou Shi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Rutgers University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">New Brunswick</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">New Jersey</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">US</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:haizhou.shi@rutgers.edu">haizhou.shi@rutgers.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuntao Hong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id19.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chuntao.hct@antgroup.com">chuntao.hct@antgroup.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yan Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id20.1.id1">School of Intelligence Science and
Technology, Peking University</span><span class="ltx_text ltx_affiliation_city" id="id21.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id22.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhyzhy001@pku.edu.cn">zhyzhy001@pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siliang Tang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id23.1.id1">College of Computer Science and Technology, Zhejiang University</span><span class="ltx_text ltx_affiliation_city" id="id24.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id25.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:siliang@zju.edu.cn">siliang@zju.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id26.id1">Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as “hallucination”, lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses.
Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pengboci/GraphRAG-Survey" title="">https://github.com/pengboci/GraphRAG-Survey</a>.</p>
</div>
<div class="ltx_keywords">Large Language Models, Graph Retrieval-Augmented Generation, Knowledge Graphs, Graph Neural Networks
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>JACM</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalvolume" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalnumber" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_article" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>9</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Knowledge representation and reasoning</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Information retrieval</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id11"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Data mining</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The development of Large Language Models like GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib128" title="">2024</a>)</cite>, Qwen2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib185" title="">2024c</a>)</cite>, and LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib32" title="">2024</a>)</cite> has sparked a revolution in the field of artificial intelligence, fundamentally altering the landscape of natural language processing. These models, built on Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib162" title="">2017</a>)</cite> architectures and trained on diverse and extensive datasets, have demonstrated unprecedented capabilities in understanding, interpreting, and generating human language. The impact of these advancements is profound, stretching across various sectors including healthcare <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib167" title="">2024c</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib104" title="">2024c</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib204" title="">2024</a>)</cite>, finance <cite class="ltx_cite ltx_citemacro_citep">(Nie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib126" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib94" title="">2024d</a>)</cite>, and education <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib170" title="">2024d</a>; Ghimire et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib47" title="">2024</a>)</cite>, where they facilitate more nuanced and efficient interactions between humans and machines.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their remarkable language comprehension and text generation capabilities, LLMs may exhibit limitations due to a lack of domain-specific knowledge, real-time updated information, and proprietary knowledge, which are outside LLMs’ pre-training corpus. These gaps can lead to a phenomenon known as “hallucination” <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib62" title="">2023b</a>)</cite> where the model generates inaccurate or even fabricated information. Consequently, it is imperative to supplement LLMs with external knowledge to mitigate this problem. Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib35" title="">2024a</a>; Hu and Lu, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib60" title="">2024</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib63" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib203" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib46" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib196" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib179" title="">2024b</a>)</cite> emerged as a significant evolution, which aims to enhance the quality and relevance of generated content by integrating a retrieval component within the generation process. The essence of RAG lies in its ability to dynamically query a large text corpus to incorporate relevant factual knowledge into the responses generated by the underlying language models. This integration not only enriches the contextual depth of the responses but also ensures a higher degree of factual accuracy and specificity. RAG has gained widespread attention due to its exceptional performance and broad applications, becoming a key focus within the field.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S1.F1.g1" src="x1.png" width="523"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Comparision between Direct LLM, RAG, and GraphRAG. Given a user query, direct answering by LLMs may suffer from shallow responses or lack of specificity. RAG addresses this by retrieving relevant textual information, somewhat alleviating the issue. However, due to the text’s length and flexible natural language expressions of entity relationships, RAG struggles to emphasize “influence” relations, which is the core of the question. While, GraphRAG methods leverage explicit entity and relationship representations in graph data, enabling precise answers by retrieving relevant structured information.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Although RAG has achieved impressive results and has been widely applied across various domains, it faces limitations in real-world scenarios: (1) <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">Neglecting Relationships</em>: In practice, textual content is not isolated but interconnected. Traditional RAG fails to capture significant structured relational knowledge that cannot be represented through semantic similarity alone. For instance, in a citation network where papers are linked by citation relationships, traditional RAG methods focus on finding the relevant papers based on the query but overlook important citation relationships between papers. (2) <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">Redundant Information</em>: RAG often recounts content in the form of textual snippets when concatenated as prompts. This makes context become excessively lengthy, leading to the “lost in the middle” dilemma <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib105" title="">2024a</a>)</cite>. (3) <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">Lacking Global Information</em>: RAG can only retrieve a subset of documents and fails to grasp global information comprehensively, and hence struggles with tasks such as Query-Focused Summarization (QFS).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Graph Retrieval-Augmented Generation (GraphRAG) <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>; Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> emerges as an innovative solution to address these challenges. Unlike traditional RAG, GraphRAG retrieves graph elements containing relational knowledge pertinent to a given query from a pre-constructed graph database, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>. These elements may include nodes, triples, paths, or subgraphs, which are utilized to generate responses. GraphRAG considers the interconnections between texts, enabling a more accurate and comprehensive retrieval of relational information. Additionally, graph data, such as knowledge graphs, offer abstraction and summarization of textual data, thereby significantly shortening the length of the input text and mitigating concerns of verbosity. By retrieving subgraphs or graph communities, we can access comprehensive information to effectively address the QFS challenge by capturing the broader context and interconnections within the graph structure.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we are the first to provide a systematic survey of GraphRAG. Specifically, we begin by introducing the GraphRAG workflow, along with the foundational background knowledge that underpins the field. Then, we categorize the literature according to the primary stages of the GraphRAG process: Graph-Based Indexing (G-Indexing), Graph-Guided Retrieval (G-Retrieval), and Graph-Enhanced Generation (G-Generation) in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5" title="5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6" title="6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7" title="7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a> respectively, detailing the core technologies and training methods within each phase.
Furthermore, we investigate downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. This exploration elucidates how GraphRAG is being utilized in practical settings and reflects its versatility and adaptability across various sectors. Finally, acknowledging that research in GraphRAG is still in its early stages, we delve into potential future research directions. This prognostic discussion aims to pave the way for forthcoming studies, inspire new lines of inquiry, and catalyze progress within the field, ultimately propelling GraphRAG toward more mature and innovative horizons.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a comprehensive and systematic review of existing state-of-the-art GraphRAG methodologies. We offer a formal definition of GraphRAG, outlining its universal workflow which includes G-Indexing, G-Retrieval, and G-Generation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We discuss the core technologies underpinning existing GraphRAG systems, including G-Indexing, G-Retrieval, and G-Generation. For each component, we analyze the spectrum of model selection, methodological design, and enhancement strategies currently being explored. Additionally, we contrast the diverse training methodologies employed across these modules.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We delineate the downstream tasks, benchmarks, application domains, evaluation metrics, current challenges, and future research directions pertinent to GraphRAG, discussing both the progress and prospects of this field. Furthermore, we compile an inventory of existing industry GraphRAG systems, providing insights into the translation of academic research into real-world industry solutions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Organization.</span> The rest of the survey is organized as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S2" title="2. Comparison with Related Techniques and Surveys ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a> compares related techniques, while Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S3" title="3. Preliminaries ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a> outlines the general process of GraphRAG. Sections <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5" title="5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a> to <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7" title="7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a> categorize the techniques associated with GraphRAG’s three stages: G-Indexing, G-Retrieval, and G-Generation. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S8" title="8. Training ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">8</span></a> introduces the training strategies of retrievers and generators. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9" title="9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">9</span></a> summarizes GraphRAG’s downstream tasks, corresponding benchmarks, application domains, evaluation metrics, and industrial GraphRAG systems. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S10" title="10. Future Prospects ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">10</span></a> provides an outlook on future directions. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S11" title="11. Conclusion ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">11</span></a> concludes the content of this survey.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The overview of the GraphRAG framework for question answering task. In this survey, we divide GraphRAG into three stages: G-Indexing, G-Retrieval, and G-Generation. We categorize the retrieval sources into open-source knowledge graphs and self-constructed graph data. Various enhancing techniques like query enhancement and knowledge enhancement may be adopted to boost the relevance of the results. Unlike RAG, which uses retrieved text directly for generation, GraphRAG requires converting the retrieved graph information into patterns acceptable to generators to enhance the task performance.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Comparison with Related Techniques and Surveys</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we compare Graph Retrieval-Augmented Generation (GraphRAG) with related techniques and corresponding surveys, including RAG, LLMs on graphs, and Knowledge Base Question Answering (KBQA).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>RAG</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">RAG combines external knowledge with LLMs for improved task performance, integrating domain-specific information to ensure factuality and credibility. In the past two years, researchers have written many comprehensive surveys about RAG <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib35" title="">2024a</a>; Hu and Lu, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib60" title="">2024</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib63" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib203" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib46" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib196" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib179" title="">2024b</a>)</cite>. For example, 



<cite class="ltx_cite ltx_citemacro_citet">Fan et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib35" title="">2024a</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Gao et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib46" title="">2024b</a>)</cite>
 categorize RAG methods from the perspectives of retrieval, generation, and augmentation. 

<cite class="ltx_cite ltx_citemacro_citet">Zhao et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib203" title="">2024</a>)</cite>
 review RAG methods for databases with different modalities. 

<cite class="ltx_cite ltx_citemacro_citet">Yu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib196" title="">2024</a>)</cite>
 systematically summarize the evaluation of RAG methods. These works provide a structured synthesis of current RAG methodologies, fostering a deeper understanding and suggesting future directions of the area.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">From a broad perspective, GraphRAG can be seen as a branch of RAG, which retrieves relevant relational knowledge from graph databases instead of text corpus. However, compared to text-based RAG, GraphRAG takes into account the relationships between texts and incorporates the structural information as additional knowledge beyond text. Furthermore, during the construction of graph data, raw text data may undergo filtering and summarization processes, enhancing the refinement of information within the graph data. Although previous surveys on RAG have touched upon GraphRAG, they predominantly center on textual data integration. This paper diverges by placing a primary emphasis on the indexing, retrieval, and utilization of structured graph data, which represents a substantial departure from handling purely textual information and spurs the emergence of many new techniques.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>LLMs on Graphs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">LLMs are revolutionizing natural language processing due to their excellent text understanding, reasoning, and generation capabilities, along with their generalization and zero-shot transfer abilities.
Although LLMs are primarily designed to process pure text and struggle with non-Euclidean data containing complex structural information, such as graphs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib166" title="">2023b</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib50" title="">2023</a>)</cite>, numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib36" title="">2024b</a>; Mao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib117" title="">2024c</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib103" title="">2024d</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib93" title="">2024c</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib131" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib132" title="">2024</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib75" title="">2024a</a>; Chen, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib18" title="">2024</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib205" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib174" title="">2024e</a>)</cite> have been conducted in these fields. These papers primarily integrate LLMs with GNNs to enhance modeling capabilities for graph data, thereby improving performance on downstream tasks such as node classification, edge prediction, graph classification, and others. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Zhu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib205" title="">2024</a>)</cite>
 propose an efficient fine-tuning method named ENGINE, which combines LLMs and GNNs through a side structure for enhancing graph representation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Different from these methods, GraphRAG focuses on retrieving relevant graph elements using queries from an external graph-structured database. In this paper, we provide a detailed introduction to the relevant technologies and applications of GraphRAG, which are not included in previous surveys of LLMs on Graphs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>KBQA</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">KBQA is a significant task in natural language processing, aiming to respond to user queries based on external knowledge bases <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib42" title="">2020</a>; Lan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib87" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib86" title="">2021</a>; Yani and Krisnadhi, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib189" title="">2021</a>)</cite>, thereby achieving goals such as fact verification, passage retrieval enhancement, and text understanding. Previous surveys typically categorize existing KBQA approaches into two main types: Information Retrieval (IR)-based methods and Semantic Parsing (SP)-based methods. Specifically, IR-based methods <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib168" title="">2023a</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite> retrieve information related to the query from the knowledge graph (KG) and use it to enhance the generation process. While SP-based methods <cite class="ltx_cite ltx_citemacro_citep">(Chakraborty, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib17" title="">2024</a>; Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib37" title="">2024b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib20" title="">2021</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib192" title="">2021</a>; Gu and Su, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib49" title="">2022</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib154" title="">2023</a>)</cite> generate a logical form (LF) for each query and execute it against knowledge bases to obtain the answer.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">GraphRAG and KBQA are closely related, with IR-based KBQA methods representing a subset of GraphRAG approaches focused on downstream applications. In this work, we extend the discussion beyond KBQA to include GraphRAG’s applications across various downstream tasks. Our survey provides a thorough and detailed exploration of GraphRAG technology, offering a comprehensive understanding of existing methods and potential improvements.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Preliminaries</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce background knowledge of GraphRAG for easier comprehension of our survey. First, we introduce Text-Attributed Graphs which is a universal and general format of graph data used in GraphRAG. Then, we provide formal definitions for two types of models that can be used in the retrieval and generation stages: Graph Neural Networks and Language Models.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Text-Attributed Graphs</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">The graph data used in Graph RAG can be represented uniformly as Text-Attributed Graphs (TAGs), where nodes and edges possess textual attributes. Formally, a text-attributed graph can be denoted as <math alttext="\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{A},\{\mathbf{x}_{v}\}_{v\in%
\mathcal{V}},\{\mathbf{e}_{i,j}\}_{i,j\in\mathcal{E}})" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.9"><semantics id="S3.SS1.p1.1.m1.9a"><mrow id="S3.SS1.p1.1.m1.9.9" xref="S3.SS1.p1.1.m1.9.9.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.9.9.4" xref="S3.SS1.p1.1.m1.9.9.4.cmml">𝒢</mi><mo id="S3.SS1.p1.1.m1.9.9.3" xref="S3.SS1.p1.1.m1.9.9.3.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.9.9.2.2" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml"><mo id="S3.SS1.p1.1.m1.9.9.2.2.3" stretchy="false" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.5.5" xref="S3.SS1.p1.1.m1.5.5.cmml">𝒱</mi><mo id="S3.SS1.p1.1.m1.9.9.2.2.4" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.6.6" xref="S3.SS1.p1.1.m1.6.6.cmml">ℰ</mi><mo id="S3.SS1.p1.1.m1.9.9.2.2.5" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.7.7" xref="S3.SS1.p1.1.m1.7.7.cmml">𝒜</mi><mo id="S3.SS1.p1.1.m1.9.9.2.2.6" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.8.8.1.1.1" xref="S3.SS1.p1.1.m1.8.8.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.1.m1.8.8.1.1.1.3" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.8.8.1.1.1.3.2" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.2.cmml">v</mi><mo id="S3.SS1.p1.1.m1.8.8.1.1.1.3.1" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.8.8.1.1.1.3.3" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.3.cmml">𝒱</mi></mrow></msub><mo id="S3.SS1.p1.1.m1.9.9.2.2.7" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.9.9.2.2.2" xref="S3.SS1.p1.1.m1.9.9.2.2.2.cmml"><mrow id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.2.cmml"><mo id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.2.cmml">{</mo><msub id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.2.cmml">𝐞</mi><mrow id="S3.SS1.p1.1.m1.2.2.2.4" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.1.m1.2.2.2.4.1" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.1.m1.4.4.2" xref="S3.SS1.p1.1.m1.4.4.2.cmml"><mrow id="S3.SS1.p1.1.m1.4.4.2.4.2" xref="S3.SS1.p1.1.m1.4.4.2.4.1.cmml"><mi id="S3.SS1.p1.1.m1.3.3.1.1" xref="S3.SS1.p1.1.m1.3.3.1.1.cmml">i</mi><mo id="S3.SS1.p1.1.m1.4.4.2.4.2.1" xref="S3.SS1.p1.1.m1.4.4.2.4.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.4.4.2.2" xref="S3.SS1.p1.1.m1.4.4.2.2.cmml">j</mi></mrow><mo id="S3.SS1.p1.1.m1.4.4.2.3" xref="S3.SS1.p1.1.m1.4.4.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.4.4.2.5" xref="S3.SS1.p1.1.m1.4.4.2.5.cmml">ℰ</mi></mrow></msub><mo id="S3.SS1.p1.1.m1.9.9.2.2.8" stretchy="false" xref="S3.SS1.p1.1.m1.9.9.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.9b"><apply id="S3.SS1.p1.1.m1.9.9.cmml" xref="S3.SS1.p1.1.m1.9.9"><eq id="S3.SS1.p1.1.m1.9.9.3.cmml" xref="S3.SS1.p1.1.m1.9.9.3"></eq><ci id="S3.SS1.p1.1.m1.9.9.4.cmml" xref="S3.SS1.p1.1.m1.9.9.4">𝒢</ci><vector id="S3.SS1.p1.1.m1.9.9.2.3.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2"><ci id="S3.SS1.p1.1.m1.5.5.cmml" xref="S3.SS1.p1.1.m1.5.5">𝒱</ci><ci id="S3.SS1.p1.1.m1.6.6.cmml" xref="S3.SS1.p1.1.m1.6.6">ℰ</ci><ci id="S3.SS1.p1.1.m1.7.7.cmml" xref="S3.SS1.p1.1.m1.7.7">𝒜</ci><apply id="S3.SS1.p1.1.m1.8.8.1.1.1.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.8.8.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1">subscript</csymbol><set id="S3.SS1.p1.1.m1.8.8.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1"><apply id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.1.1.1.3">𝑣</ci></apply></set><apply id="S3.SS1.p1.1.m1.8.8.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3"><in id="S3.SS1.p1.1.m1.8.8.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.1"></in><ci id="S3.SS1.p1.1.m1.8.8.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.2">𝑣</ci><ci id="S3.SS1.p1.1.m1.8.8.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.8.8.1.1.1.3.3">𝒱</ci></apply></apply><apply id="S3.SS1.p1.1.m1.9.9.2.2.2.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.9.9.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2">subscript</csymbol><set id="S3.SS1.p1.1.m1.9.9.2.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1"><apply id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.9.9.2.2.2.1.1.1.2">𝐞</ci><list id="S3.SS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.4"><ci id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">𝑗</ci></list></apply></set><apply id="S3.SS1.p1.1.m1.4.4.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2"><in id="S3.SS1.p1.1.m1.4.4.2.3.cmml" xref="S3.SS1.p1.1.m1.4.4.2.3"></in><list id="S3.SS1.p1.1.m1.4.4.2.4.1.cmml" xref="S3.SS1.p1.1.m1.4.4.2.4.2"><ci id="S3.SS1.p1.1.m1.3.3.1.1.cmml" xref="S3.SS1.p1.1.m1.3.3.1.1">𝑖</ci><ci id="S3.SS1.p1.1.m1.4.4.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2.2">𝑗</ci></list><ci id="S3.SS1.p1.1.m1.4.4.2.5.cmml" xref="S3.SS1.p1.1.m1.4.4.2.5">ℰ</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.9c">\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{A},\{\mathbf{x}_{v}\}_{v\in%
\mathcal{V}},\{\mathbf{e}_{i,j}\}_{i,j\in\mathcal{E}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.9d">caligraphic_G = ( caligraphic_V , caligraphic_E , caligraphic_A , { bold_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v ∈ caligraphic_V end_POSTSUBSCRIPT , { bold_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i , italic_j ∈ caligraphic_E end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">caligraphic_V</annotation></semantics></math> is the set of nodes, <math alttext="\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">ℰ</mi><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">⊆</mo><mrow id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">𝒱</mi><mo id="S3.SS1.p1.3.m3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">𝒱</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><subset id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></subset><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ℰ</ci><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><times id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">𝒱</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">𝒱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">caligraphic_E ⊆ caligraphic_V × caligraphic_V</annotation></semantics></math> is the set of edges, <math alttext="\mathcal{A}\in\{0,1\}^{\lvert\mathcal{V}\rvert\times\lvert\mathcal{V}\rvert}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.4"><semantics id="S3.SS1.p1.4.m4.4a"><mrow id="S3.SS1.p1.4.m4.4.5" xref="S3.SS1.p1.4.m4.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.4.5.2" xref="S3.SS1.p1.4.m4.4.5.2.cmml">𝒜</mi><mo id="S3.SS1.p1.4.m4.4.5.1" xref="S3.SS1.p1.4.m4.4.5.1.cmml">∈</mo><msup id="S3.SS1.p1.4.m4.4.5.3" xref="S3.SS1.p1.4.m4.4.5.3.cmml"><mrow id="S3.SS1.p1.4.m4.4.5.3.2.2" xref="S3.SS1.p1.4.m4.4.5.3.2.1.cmml"><mo id="S3.SS1.p1.4.m4.4.5.3.2.2.1" stretchy="false" xref="S3.SS1.p1.4.m4.4.5.3.2.1.cmml">{</mo><mn id="S3.SS1.p1.4.m4.3.3" xref="S3.SS1.p1.4.m4.3.3.cmml">0</mn><mo id="S3.SS1.p1.4.m4.4.5.3.2.2.2" xref="S3.SS1.p1.4.m4.4.5.3.2.1.cmml">,</mo><mn id="S3.SS1.p1.4.m4.4.4" xref="S3.SS1.p1.4.m4.4.4.cmml">1</mn><mo id="S3.SS1.p1.4.m4.4.5.3.2.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.4.5.3.2.1.cmml">}</mo></mrow><mrow id="S3.SS1.p1.4.m4.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.cmml"><mrow id="S3.SS1.p1.4.m4.2.2.2.4.2" xref="S3.SS1.p1.4.m4.2.2.2.4.1.cmml"><mo id="S3.SS1.p1.4.m4.2.2.2.4.2.1" stretchy="false" xref="S3.SS1.p1.4.m4.2.2.2.4.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.cmml">𝒱</mi><mo id="S3.SS1.p1.4.m4.2.2.2.4.2.2" rspace="0.055em" stretchy="false" xref="S3.SS1.p1.4.m4.2.2.2.4.1.1.cmml">|</mo></mrow><mo id="S3.SS1.p1.4.m4.2.2.2.3" rspace="0.222em" xref="S3.SS1.p1.4.m4.2.2.2.3.cmml">×</mo><mrow id="S3.SS1.p1.4.m4.2.2.2.5.2" xref="S3.SS1.p1.4.m4.2.2.2.5.1.cmml"><mo id="S3.SS1.p1.4.m4.2.2.2.5.2.1" stretchy="false" xref="S3.SS1.p1.4.m4.2.2.2.5.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.2.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.2.cmml">𝒱</mi><mo id="S3.SS1.p1.4.m4.2.2.2.5.2.2" stretchy="false" xref="S3.SS1.p1.4.m4.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.4b"><apply id="S3.SS1.p1.4.m4.4.5.cmml" xref="S3.SS1.p1.4.m4.4.5"><in id="S3.SS1.p1.4.m4.4.5.1.cmml" xref="S3.SS1.p1.4.m4.4.5.1"></in><ci id="S3.SS1.p1.4.m4.4.5.2.cmml" xref="S3.SS1.p1.4.m4.4.5.2">𝒜</ci><apply id="S3.SS1.p1.4.m4.4.5.3.cmml" xref="S3.SS1.p1.4.m4.4.5.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.4.5.3.1.cmml" xref="S3.SS1.p1.4.m4.4.5.3">superscript</csymbol><set id="S3.SS1.p1.4.m4.4.5.3.2.1.cmml" xref="S3.SS1.p1.4.m4.4.5.3.2.2"><cn id="S3.SS1.p1.4.m4.3.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.3.3">0</cn><cn id="S3.SS1.p1.4.m4.4.4.cmml" type="integer" xref="S3.SS1.p1.4.m4.4.4">1</cn></set><apply id="S3.SS1.p1.4.m4.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2"><times id="S3.SS1.p1.4.m4.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.2.3"></times><apply id="S3.SS1.p1.4.m4.2.2.2.4.1.cmml" xref="S3.SS1.p1.4.m4.2.2.2.4.2"><abs id="S3.SS1.p1.4.m4.2.2.2.4.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.2.4.2.1"></abs><ci id="S3.SS1.p1.4.m4.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1">𝒱</ci></apply><apply id="S3.SS1.p1.4.m4.2.2.2.5.1.cmml" xref="S3.SS1.p1.4.m4.2.2.2.5.2"><abs id="S3.SS1.p1.4.m4.2.2.2.5.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.2.5.2.1"></abs><ci id="S3.SS1.p1.4.m4.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2">𝒱</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.4c">\mathcal{A}\in\{0,1\}^{\lvert\mathcal{V}\rvert\times\lvert\mathcal{V}\rvert}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.4d">caligraphic_A ∈ { 0 , 1 } start_POSTSUPERSCRIPT | caligraphic_V | × | caligraphic_V | end_POSTSUPERSCRIPT</annotation></semantics></math> is the adjacent matrix. Additionally, <math alttext="\{\mathbf{x}_{v}\}_{v\in\mathcal{V}}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mrow id="S3.SS1.p1.5.m5.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.2.cmml"><mo id="S3.SS1.p1.5.m5.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.5.m5.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.5.m5.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.p1.5.m5.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.5.m5.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">v</mi><mo id="S3.SS1.p1.5.m5.1.1.3.1" xref="S3.SS1.p1.5.m5.1.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml">𝒱</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><set id="S3.SS1.p1.5.m5.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1"><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2">𝐱</ci><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3">𝑣</ci></apply></set><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><in id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.1"></in><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">𝑣</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3">𝒱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\{\mathbf{x}_{v}\}_{v\in\mathcal{V}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">{ bold_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v ∈ caligraphic_V end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\{\mathbf{e}_{i,j}\}_{i,j\in\mathcal{E}}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.5"><semantics id="S3.SS1.p1.6.m6.5a"><msub id="S3.SS1.p1.6.m6.5.5" xref="S3.SS1.p1.6.m6.5.5.cmml"><mrow id="S3.SS1.p1.6.m6.5.5.1.1" xref="S3.SS1.p1.6.m6.5.5.1.2.cmml"><mo id="S3.SS1.p1.6.m6.5.5.1.1.2" stretchy="false" xref="S3.SS1.p1.6.m6.5.5.1.2.cmml">{</mo><msub id="S3.SS1.p1.6.m6.5.5.1.1.1" xref="S3.SS1.p1.6.m6.5.5.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.5.5.1.1.1.2" xref="S3.SS1.p1.6.m6.5.5.1.1.1.2.cmml">𝐞</mi><mrow id="S3.SS1.p1.6.m6.2.2.2.4" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.6.m6.2.2.2.4.1" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.6.m6.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS1.p1.6.m6.5.5.1.1.3" stretchy="false" xref="S3.SS1.p1.6.m6.5.5.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.6.m6.4.4.2" xref="S3.SS1.p1.6.m6.4.4.2.cmml"><mrow id="S3.SS1.p1.6.m6.4.4.2.4.2" xref="S3.SS1.p1.6.m6.4.4.2.4.1.cmml"><mi id="S3.SS1.p1.6.m6.3.3.1.1" xref="S3.SS1.p1.6.m6.3.3.1.1.cmml">i</mi><mo id="S3.SS1.p1.6.m6.4.4.2.4.2.1" xref="S3.SS1.p1.6.m6.4.4.2.4.1.cmml">,</mo><mi id="S3.SS1.p1.6.m6.4.4.2.2" xref="S3.SS1.p1.6.m6.4.4.2.2.cmml">j</mi></mrow><mo id="S3.SS1.p1.6.m6.4.4.2.3" xref="S3.SS1.p1.6.m6.4.4.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m6.4.4.2.5" xref="S3.SS1.p1.6.m6.4.4.2.5.cmml">ℰ</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.5b"><apply id="S3.SS1.p1.6.m6.5.5.cmml" xref="S3.SS1.p1.6.m6.5.5"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.5.5.2.cmml" xref="S3.SS1.p1.6.m6.5.5">subscript</csymbol><set id="S3.SS1.p1.6.m6.5.5.1.2.cmml" xref="S3.SS1.p1.6.m6.5.5.1.1"><apply id="S3.SS1.p1.6.m6.5.5.1.1.1.cmml" xref="S3.SS1.p1.6.m6.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.5.5.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.5.5.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.5.5.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.5.5.1.1.1.2">𝐞</ci><list id="S3.SS1.p1.6.m6.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.4"><ci id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1">𝑖</ci><ci id="S3.SS1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2">𝑗</ci></list></apply></set><apply id="S3.SS1.p1.6.m6.4.4.2.cmml" xref="S3.SS1.p1.6.m6.4.4.2"><in id="S3.SS1.p1.6.m6.4.4.2.3.cmml" xref="S3.SS1.p1.6.m6.4.4.2.3"></in><list id="S3.SS1.p1.6.m6.4.4.2.4.1.cmml" xref="S3.SS1.p1.6.m6.4.4.2.4.2"><ci id="S3.SS1.p1.6.m6.3.3.1.1.cmml" xref="S3.SS1.p1.6.m6.3.3.1.1">𝑖</ci><ci id="S3.SS1.p1.6.m6.4.4.2.2.cmml" xref="S3.SS1.p1.6.m6.4.4.2.2">𝑗</ci></list><ci id="S3.SS1.p1.6.m6.4.4.2.5.cmml" xref="S3.SS1.p1.6.m6.4.4.2.5">ℰ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.5c">\{\mathbf{e}_{i,j}\}_{i,j\in\mathcal{E}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.5d">{ bold_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i , italic_j ∈ caligraphic_E end_POSTSUBSCRIPT</annotation></semantics></math> are textual attributes of nodes and edges, respectively. One typical kind of TAGs is Knowledge Graphs (KGs), where nodes are entities, edges are relations among entities, and text attributes are the names of entities and relations.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Graph Neural Networks</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">Graph Neural Networks (GNNs) are a kind of deep learning framework to model the graph data. Classical GNNs, e.g., GCN <cite class="ltx_cite ltx_citemacro_citep">(Kipf and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib84" title="">2017</a>)</cite>, GAT <cite class="ltx_cite ltx_citemacro_citep">(Veličković et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib163" title="">2018</a>)</cite>, GraphSAGE <cite class="ltx_cite ltx_citemacro_citep">(Hamilton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib53" title="">2017</a>)</cite>, adopt a message-passing manner to obtain node representations. Formally, each node representation <math alttext="\mathbf{h}^{(l-1)}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msubsup id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.2.2.2" xref="S3.SS2.p1.1.m1.1.2.2.2.cmml">𝐡</mi><mi id="S3.SS2.p1.1.m1.1.2.3" xref="S3.SS2.p1.1.m1.1.2.3.cmml">i</mi><mrow id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.1.m1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.cmml">l</mi><mo id="S3.SS2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS2.p1.1.m1.1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2">subscript</csymbol><apply id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2">𝐡</ci><apply id="S3.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1"><minus id="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1"></minus><ci id="S3.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2">𝑙</ci><cn id="S3.SS2.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.SS2.p1.1.m1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.2.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathbf{h}^{(l-1)}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">bold_h start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in the <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_l</annotation></semantics></math>-th layer is updated by aggregating the information from neighboring nodes and edges:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{h}^{(l)}_{i}=\textbf{UPD}(\mathbf{h}^{(l-1)}_{i},\textbf{AGG}_{j\in%
\mathcal{N}(i)}\textbf{MSG}(\mathbf{h}^{(l-1)}_{i},\mathbf{h}^{(l-1)}_{j},%
\mathbf{e}^{(l-1)}_{i,j}))," class="ltx_Math" display="block" id="S3.E1.m1.9"><semantics id="S3.E1.m1.9a"><mrow id="S3.E1.m1.9.9.1" xref="S3.E1.m1.9.9.1.1.cmml"><mrow id="S3.E1.m1.9.9.1.1" xref="S3.E1.m1.9.9.1.1.cmml"><msubsup id="S3.E1.m1.9.9.1.1.4" xref="S3.E1.m1.9.9.1.1.4.cmml"><mi id="S3.E1.m1.9.9.1.1.4.2.2" xref="S3.E1.m1.9.9.1.1.4.2.2.cmml">𝐡</mi><mi id="S3.E1.m1.9.9.1.1.4.3" xref="S3.E1.m1.9.9.1.1.4.3.cmml">i</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.4.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.9.9.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">l</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.9.9.1.1.4.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.9.9.1.1.3" xref="S3.E1.m1.9.9.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.9.9.1.1.2" xref="S3.E1.m1.9.9.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.4" xref="S3.E1.m1.9.9.1.1.2.4a.cmml">UPD</mtext><mo id="S3.E1.m1.9.9.1.1.2.3" xref="S3.E1.m1.9.9.1.1.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.9.9.1.1.2.2.2" xref="S3.E1.m1.9.9.1.1.2.2.3.cmml"><mo id="S3.E1.m1.9.9.1.1.2.2.2.3" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E1.m1.9.9.1.1.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.1.1.2.2" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mi id="S3.E1.m1.9.9.1.1.1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.1.1.1.1.3.cmml">i</mi><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">l</mi><mo id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.2.2.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.9.9.1.1.2.2.2.4" xref="S3.E1.m1.9.9.1.1.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.9.9.1.1.2.2.2.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.cmml"><msub id="S3.E1.m1.9.9.1.1.2.2.2.2.5" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.2.2.2.5.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5.2a.cmml">AGG</mtext><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">j</mi><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">∈</mo><mrow id="S3.E1.m1.3.3.1.4" xref="S3.E1.m1.3.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.4.2" xref="S3.E1.m1.3.3.1.4.2.cmml">𝒩</mi><mo id="S3.E1.m1.3.3.1.4.1" xref="S3.E1.m1.3.3.1.4.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.4.3.2" xref="S3.E1.m1.3.3.1.4.cmml"><mo id="S3.E1.m1.3.3.1.4.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.4.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo id="S3.E1.m1.3.3.1.4.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.4" xref="S3.E1.m1.9.9.1.1.2.2.2.2.4.cmml">⁢</mo><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.2.2.2.6" xref="S3.E1.m1.9.9.1.1.2.2.2.2.6a.cmml">MSG</mtext><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.4a" xref="S3.E1.m1.9.9.1.1.2.2.2.2.4.cmml">⁢</mo><mrow id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml"><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.4" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml">(</mo><msubsup id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.2.cmml">𝐡</mi><mi id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.3.cmml">i</mi><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mo id="S3.E1.m1.4.4.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">l</mi><mo id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.4.4.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.5" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml">,</mo><msubsup id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.2.cmml">𝐡</mi><mi id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.3" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.3.cmml">j</mi><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">l</mi><mo id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.5.5.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.6" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml">,</mo><msubsup id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.cmml"><mi id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.2" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.2.cmml">𝐞</mi><mrow id="S3.E1.m1.8.8.2.4" xref="S3.E1.m1.8.8.2.3.cmml"><mi id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml">i</mi><mo id="S3.E1.m1.8.8.2.4.1" xref="S3.E1.m1.8.8.2.3.cmml">,</mo><mi id="S3.E1.m1.8.8.2.2" xref="S3.E1.m1.8.8.2.2.cmml">j</mi></mrow><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><mo id="S3.E1.m1.6.6.1.1.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.2.cmml">l</mi><mo id="S3.E1.m1.6.6.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.6.6.1.1.3" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.7" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.9.9.1.1.2.2.2.5" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.9.9.1.2" xref="S3.E1.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.9b"><apply id="S3.E1.m1.9.9.1.1.cmml" xref="S3.E1.m1.9.9.1"><eq id="S3.E1.m1.9.9.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.3"></eq><apply id="S3.E1.m1.9.9.1.1.4.cmml" xref="S3.E1.m1.9.9.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.4.1.cmml" xref="S3.E1.m1.9.9.1.1.4">subscript</csymbol><apply id="S3.E1.m1.9.9.1.1.4.2.cmml" xref="S3.E1.m1.9.9.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.4.2.1.cmml" xref="S3.E1.m1.9.9.1.1.4">superscript</csymbol><ci id="S3.E1.m1.9.9.1.1.4.2.2.cmml" xref="S3.E1.m1.9.9.1.1.4.2.2">𝐡</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑙</ci></apply><ci id="S3.E1.m1.9.9.1.1.4.3.cmml" xref="S3.E1.m1.9.9.1.1.4.3">𝑖</ci></apply><apply id="S3.E1.m1.9.9.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.2"><times id="S3.E1.m1.9.9.1.1.2.3.cmml" xref="S3.E1.m1.9.9.1.1.2.3"></times><ci id="S3.E1.m1.9.9.1.1.2.4a.cmml" xref="S3.E1.m1.9.9.1.1.2.4"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.4.cmml" xref="S3.E1.m1.9.9.1.1.2.4">UPD</mtext></ci><interval closure="open" id="S3.E1.m1.9.9.1.1.2.2.3.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2"><apply id="S3.E1.m1.9.9.1.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.9.9.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.2">𝐡</ci><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><minus id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"></minus><ci id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2">𝑙</ci><cn id="S3.E1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.1.1.1.3">1</cn></apply></apply><ci id="S3.E1.m1.9.9.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2"><times id="S3.E1.m1.9.9.1.1.2.2.2.2.4.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.4"></times><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.5.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.5.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5">subscript</csymbol><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.5.2a.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.2.2.2.5.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.5.2">AGG</mtext></ci><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><in id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></in><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">𝑗</ci><apply id="S3.E1.m1.3.3.1.4.cmml" xref="S3.E1.m1.3.3.1.4"><times id="S3.E1.m1.3.3.1.4.1.cmml" xref="S3.E1.m1.3.3.1.4.1"></times><ci id="S3.E1.m1.3.3.1.4.2.cmml" xref="S3.E1.m1.3.3.1.4.2">𝒩</ci><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">𝑖</ci></apply></apply></apply><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.6a.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.6"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.9.9.1.1.2.2.2.2.6.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.6">MSG</mtext></ci><vector id="S3.E1.m1.9.9.1.1.2.2.2.2.3.4.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3"><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1">subscript</csymbol><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1">superscript</csymbol><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.2.2">𝐡</ci><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1"><minus id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1"></minus><ci id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2">𝑙</ci><cn id="S3.E1.m1.4.4.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.4.4.1.1.1.3">1</cn></apply></apply><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.2.2">𝐡</ci><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1"><minus id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"></minus><ci id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2">𝑙</ci><cn id="S3.E1.m1.5.5.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.1.3">1</cn></apply></apply><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.2.2.2.3">𝑗</ci></apply><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3">subscript</csymbol><apply id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.1.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3">superscript</csymbol><ci id="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2.2.2.3.3.3.2.2">𝐞</ci><apply id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1"><minus id="S3.E1.m1.6.6.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1"></minus><ci id="S3.E1.m1.6.6.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.2">𝑙</ci><cn id="S3.E1.m1.6.6.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.6.6.1.1.1.3">1</cn></apply></apply><list id="S3.E1.m1.8.8.2.3.cmml" xref="S3.E1.m1.8.8.2.4"><ci id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1.1">𝑖</ci><ci id="S3.E1.m1.8.8.2.2.cmml" xref="S3.E1.m1.8.8.2.2">𝑗</ci></list></apply></vector></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.9c">\mathbf{h}^{(l)}_{i}=\textbf{UPD}(\mathbf{h}^{(l-1)}_{i},\textbf{AGG}_{j\in%
\mathcal{N}(i)}\textbf{MSG}(\mathbf{h}^{(l-1)}_{i},\mathbf{h}^{(l-1)}_{j},%
\mathbf{e}^{(l-1)}_{i,j})),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.9d">bold_h start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = UPD ( bold_h start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , AGG start_POSTSUBSCRIPT italic_j ∈ caligraphic_N ( italic_i ) end_POSTSUBSCRIPT MSG ( bold_h start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_h start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , bold_e start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.4">where <math alttext="\mathcal{N}_{(i)}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m1.1"><semantics id="S3.SS2.p1.3.m1.1a"><msub id="S3.SS2.p1.3.m1.1.2" xref="S3.SS2.p1.3.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m1.1.2.2" xref="S3.SS2.p1.3.m1.1.2.2.cmml">𝒩</mi><mrow id="S3.SS2.p1.3.m1.1.1.1.3" xref="S3.SS2.p1.3.m1.1.2.cmml"><mo id="S3.SS2.p1.3.m1.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.3.m1.1.2.cmml">(</mo><mi id="S3.SS2.p1.3.m1.1.1.1.1" xref="S3.SS2.p1.3.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.3.m1.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.3.m1.1.2.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m1.1b"><apply id="S3.SS2.p1.3.m1.1.2.cmml" xref="S3.SS2.p1.3.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.1.2.1.cmml" xref="S3.SS2.p1.3.m1.1.2">subscript</csymbol><ci id="S3.SS2.p1.3.m1.1.2.2.cmml" xref="S3.SS2.p1.3.m1.1.2.2">𝒩</ci><ci id="S3.SS2.p1.3.m1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m1.1c">\mathcal{N}_{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m1.1d">caligraphic_N start_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> represents the neighbors of node <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m2.1"><semantics id="S3.SS2.p1.4.m2.1a"><mi id="S3.SS2.p1.4.m2.1.1" xref="S3.SS2.p1.4.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m2.1b"><ci id="S3.SS2.p1.4.m2.1.1.cmml" xref="S3.SS2.p1.4.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m2.1d">italic_i</annotation></semantics></math>. <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.4.1">MSG</span> denotes the message function, which computes the message based on the node, its neighbor, and the edge between them. <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.4.2">AGG</span> refers to the aggregation function that combines the received messages using a permutation-invariant method, such as mean, sum, or max. <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.4.3">UPD</span> represents the update function, which updates each node’s attributes with the aggregated messages.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Subsequently, a readout function, e.g., mean, sum, or max pooling, can be applied to obtain the global-level representation:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{h}_{G}=\textbf{READOUT}_{i\in\mathcal{V}_{G}}(\mathbf{h}_{i}^{(L)})." class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">𝐡</mi><mi id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml">G</mi></msub><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.3.2a.cmml">READOUT</mtext><mrow id="S3.E2.m1.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.1.3.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.1.3.3.1.cmml">∈</mo><msub id="S3.E2.m1.2.2.1.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.1.3.3.3.2" xref="S3.E2.m1.2.2.1.1.1.3.3.3.2.cmml">𝒱</mi><mi id="S3.E2.m1.2.2.1.1.1.3.3.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.3.3.cmml">G</mi></msub></mrow></msub><mo id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">i</mi><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">L</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" lspace="0em" xref="S3.E2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">𝐡</ci><ci id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3">𝐺</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.3.2a.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.E2.m1.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2">READOUT</mtext></ci><apply id="S3.E2.m1.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3"><in id="S3.E2.m1.2.2.1.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.1"></in><ci id="S3.E2.m1.2.2.1.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.2">𝑖</ci><apply id="S3.E2.m1.2.2.1.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.3.2">𝒱</ci><ci id="S3.E2.m1.2.2.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3.3.3">𝐺</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2">𝐡</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝐿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathbf{h}_{G}=\textbf{READOUT}_{i\in\mathcal{V}_{G}}(\mathbf{h}_{i}^{(L)}).</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = READOUT start_POSTSUBSCRIPT italic_i ∈ caligraphic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In GraphRAG, GNNs can be utilized to obtain representations of graph data for the retrieval phase, as well as to model the retrieved graph structures.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Language Models</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Language models (LMs) excel in language understanding and are mainly classified into two types: discriminative and generative. Discriminative models, like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib29" title="">2019</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib108" title="">2019</a>)</cite> and SentenceBERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib141" title="">2019</a>)</cite>, focus on estimating the conditional probability <math alttext="P(\mathbf{y}|\mathbf{x})" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">P</mi><mo id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.2.cmml">𝐲</mi><mo fence="false" id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.3.cmml">𝐱</mi></mrow><mo id="S3.SS3.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"></times><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑃</ci><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.2">𝐲</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.3">𝐱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">P(\mathbf{y}|\mathbf{x})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_P ( bold_y | bold_x )</annotation></semantics></math> and are effective in tasks such as text classification and sentiment analysis. In contrast, generative models, including GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib15" title="">2020</a>)</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib128" title="">2024</a>)</cite>, aim to model the joint probability <math alttext="P(\mathbf{x},\mathbf{y})" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.2"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.3" xref="S3.SS3.p1.2.m2.2.3.cmml"><mi id="S3.SS3.p1.2.m2.2.3.2" xref="S3.SS3.p1.2.m2.2.3.2.cmml">P</mi><mo id="S3.SS3.p1.2.m2.2.3.1" xref="S3.SS3.p1.2.m2.2.3.1.cmml">⁢</mo><mrow id="S3.SS3.p1.2.m2.2.3.3.2" xref="S3.SS3.p1.2.m2.2.3.3.1.cmml"><mo id="S3.SS3.p1.2.m2.2.3.3.2.1" stretchy="false" xref="S3.SS3.p1.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">𝐱</mi><mo id="S3.SS3.p1.2.m2.2.3.3.2.2" xref="S3.SS3.p1.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">𝐲</mi><mo id="S3.SS3.p1.2.m2.2.3.3.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><apply id="S3.SS3.p1.2.m2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.3"><times id="S3.SS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.1"></times><ci id="S3.SS3.p1.2.m2.2.3.2.cmml" xref="S3.SS3.p1.2.m2.2.3.2">𝑃</ci><interval closure="open" id="S3.SS3.p1.2.m2.2.3.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.3.2"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐱</ci><ci id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">𝐲</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">P(\mathbf{x},\mathbf{y})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.2d">italic_P ( bold_x , bold_y )</annotation></semantics></math> for tasks like machine translation and text generation. These generative pre-trained models have significantly advanced the field of natural language processing (NLP) by leveraging massive datasets and billions of parameters, contributing to the rise of Large Language Models (LLMs) with outstanding performance across various tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">In the early stages, RAG and GraphRAG focused on improving pre-training techniques for discriminative language models <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib29" title="">2019</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib108" title="">2019</a>; Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib141" title="">2019</a>)</cite>. Recently, LLMs such as ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib129" title="">2022</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib32" title="">2024</a>)</cite>, and Qwen2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib185" title="">2024c</a>)</cite> have shown great potential in language understanding, demonstrating powerful in-context learning capabilities. Subsequently, research on RAG and GraphRAG shifted towards enhancing information retrieval for language models, addressing increasingly complex tasks and mitigating hallucinations, thereby driving rapid advancements in the field.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Overview of GraphRAG</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.12">GraphRAG is a framework that leverages external structured knowledge graphs to improve contextual understanding of LMs and generate more informed responses, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>. The goal of GraphRAG is to retrieve the most relevant knowledge from databases, thereby enhancing the answers of downstream tasks. The process can be defined as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle a^{*}=\arg\max_{a\in A}p(a|q,\mathcal{G})," class="ltx_Math" display="inline" id="S4.E3X.2.1.1.m1.3"><semantics id="S4.E3X.2.1.1.m1.3a"><mrow id="S4.E3X.2.1.1.m1.3.3.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S4.E3X.2.1.1.m1.3.3.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml"><msup id="S4.E3X.2.1.1.m1.3.3.1.1.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.3.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.2.cmml">a</mi><mo id="S4.E3X.2.1.1.m1.3.3.1.1.3.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.3.cmml">∗</mo></msup><mo id="S4.E3X.2.1.1.m1.3.3.1.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.cmml"><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.1.cmml">arg</mi><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.3a" lspace="0.167em" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.cmml">⁡</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.cmml"><munder id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.2.cmml">max</mi><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.2.cmml">a</mi><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.1.cmml">∈</mo><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.3.cmml">A</mi></mrow></munder><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2a" lspace="0.167em" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.cmml">⁡</mo><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.2.cmml">p</mi></mrow></mrow><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1.cmml"><mi id="S4.E3X.2.1.1.m1.1.1" xref="S4.E3X.2.1.1.m1.1.1.cmml">q</mi><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3X.2.1.1.m1.2.2" xref="S4.E3X.2.1.1.m1.2.2.cmml">𝒢</mi></mrow></mrow><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3X.2.1.1.m1.3.3.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3X.2.1.1.m1.3b"><apply id="S4.E3X.2.1.1.m1.3.3.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1"><eq id="S4.E3X.2.1.1.m1.3.3.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.2"></eq><apply id="S4.E3X.2.1.1.m1.3.3.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3">superscript</csymbol><ci id="S4.E3X.2.1.1.m1.3.3.1.1.3.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.2">𝑎</ci><times id="S4.E3X.2.1.1.m1.3.3.1.1.3.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.3"></times></apply><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1"><times id="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2"></times><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3"><arg id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.1"></arg><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2"><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1">subscript</csymbol><max id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.2"></max><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3"><in id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.1"></in><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.2">𝑎</ci><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.1.3.3">𝐴</ci></apply></apply><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.3.2.2">𝑝</ci></apply></apply><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2">𝑎</ci><list id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2"><ci id="S4.E3X.2.1.1.m1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1">𝑞</ci><ci id="S4.E3X.2.1.1.m1.2.2.cmml" xref="S4.E3X.2.1.1.m1.2.2">𝒢</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3X.2.1.1.m1.3c">\displaystyle a^{*}=\arg\max_{a\in A}p(a|q,\mathcal{G}),</annotation><annotation encoding="application/x-llamapun" id="S4.E3X.2.1.1.m1.3d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_a ∈ italic_A end_POSTSUBSCRIPT italic_p ( italic_a | italic_q , caligraphic_G ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.p1.9">where <math alttext="a^{*}" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><msup id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">a</mi><mo id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">superscript</csymbol><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑎</ci><times id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">a^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is the optimal answer of the query <math alttext="q" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_q</annotation></semantics></math> given the TAG <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">caligraphic_G</annotation></semantics></math>, and <math alttext="A" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_A</annotation></semantics></math> is the set of possible responses. After that, we jointly model the target distribution <math alttext="p(a|q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.p1.5.m5.3"><semantics id="S4.p1.5.m5.3a"><mrow id="S4.p1.5.m5.3.3" xref="S4.p1.5.m5.3.3.cmml"><mi id="S4.p1.5.m5.3.3.3" xref="S4.p1.5.m5.3.3.3.cmml">p</mi><mo id="S4.p1.5.m5.3.3.2" xref="S4.p1.5.m5.3.3.2.cmml">⁢</mo><mrow id="S4.p1.5.m5.3.3.1.1" xref="S4.p1.5.m5.3.3.1.1.1.cmml"><mo id="S4.p1.5.m5.3.3.1.1.2" stretchy="false" xref="S4.p1.5.m5.3.3.1.1.1.cmml">(</mo><mrow id="S4.p1.5.m5.3.3.1.1.1" xref="S4.p1.5.m5.3.3.1.1.1.cmml"><mi id="S4.p1.5.m5.3.3.1.1.1.2" xref="S4.p1.5.m5.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.p1.5.m5.3.3.1.1.1.1" xref="S4.p1.5.m5.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.p1.5.m5.3.3.1.1.1.3.2" xref="S4.p1.5.m5.3.3.1.1.1.3.1.cmml"><mi id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">q</mi><mo id="S4.p1.5.m5.3.3.1.1.1.3.2.1" xref="S4.p1.5.m5.3.3.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.p1.5.m5.2.2" xref="S4.p1.5.m5.2.2.cmml">𝒢</mi></mrow></mrow><mo id="S4.p1.5.m5.3.3.1.1.3" stretchy="false" xref="S4.p1.5.m5.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.3b"><apply id="S4.p1.5.m5.3.3.cmml" xref="S4.p1.5.m5.3.3"><times id="S4.p1.5.m5.3.3.2.cmml" xref="S4.p1.5.m5.3.3.2"></times><ci id="S4.p1.5.m5.3.3.3.cmml" xref="S4.p1.5.m5.3.3.3">𝑝</ci><apply id="S4.p1.5.m5.3.3.1.1.1.cmml" xref="S4.p1.5.m5.3.3.1.1"><csymbol cd="latexml" id="S4.p1.5.m5.3.3.1.1.1.1.cmml" xref="S4.p1.5.m5.3.3.1.1.1.1">conditional</csymbol><ci id="S4.p1.5.m5.3.3.1.1.1.2.cmml" xref="S4.p1.5.m5.3.3.1.1.1.2">𝑎</ci><list id="S4.p1.5.m5.3.3.1.1.1.3.1.cmml" xref="S4.p1.5.m5.3.3.1.1.1.3.2"><ci id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">𝑞</ci><ci id="S4.p1.5.m5.2.2.cmml" xref="S4.p1.5.m5.2.2">𝒢</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.3c">p(a|q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.p1.5.m5.3d">italic_p ( italic_a | italic_q , caligraphic_G )</annotation></semantics></math> with a graph retriever <math alttext="p_{\theta}(G|q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.p1.6.m6.3"><semantics id="S4.p1.6.m6.3a"><mrow id="S4.p1.6.m6.3.3" xref="S4.p1.6.m6.3.3.cmml"><msub id="S4.p1.6.m6.3.3.3" xref="S4.p1.6.m6.3.3.3.cmml"><mi id="S4.p1.6.m6.3.3.3.2" xref="S4.p1.6.m6.3.3.3.2.cmml">p</mi><mi id="S4.p1.6.m6.3.3.3.3" xref="S4.p1.6.m6.3.3.3.3.cmml">θ</mi></msub><mo id="S4.p1.6.m6.3.3.2" xref="S4.p1.6.m6.3.3.2.cmml">⁢</mo><mrow id="S4.p1.6.m6.3.3.1.1" xref="S4.p1.6.m6.3.3.1.1.1.cmml"><mo id="S4.p1.6.m6.3.3.1.1.2" stretchy="false" xref="S4.p1.6.m6.3.3.1.1.1.cmml">(</mo><mrow id="S4.p1.6.m6.3.3.1.1.1" xref="S4.p1.6.m6.3.3.1.1.1.cmml"><mi id="S4.p1.6.m6.3.3.1.1.1.2" xref="S4.p1.6.m6.3.3.1.1.1.2.cmml">G</mi><mo fence="false" id="S4.p1.6.m6.3.3.1.1.1.1" xref="S4.p1.6.m6.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.p1.6.m6.3.3.1.1.1.3.2" xref="S4.p1.6.m6.3.3.1.1.1.3.1.cmml"><mi id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">q</mi><mo id="S4.p1.6.m6.3.3.1.1.1.3.2.1" xref="S4.p1.6.m6.3.3.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.p1.6.m6.2.2" xref="S4.p1.6.m6.2.2.cmml">𝒢</mi></mrow></mrow><mo id="S4.p1.6.m6.3.3.1.1.3" stretchy="false" xref="S4.p1.6.m6.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.3b"><apply id="S4.p1.6.m6.3.3.cmml" xref="S4.p1.6.m6.3.3"><times id="S4.p1.6.m6.3.3.2.cmml" xref="S4.p1.6.m6.3.3.2"></times><apply id="S4.p1.6.m6.3.3.3.cmml" xref="S4.p1.6.m6.3.3.3"><csymbol cd="ambiguous" id="S4.p1.6.m6.3.3.3.1.cmml" xref="S4.p1.6.m6.3.3.3">subscript</csymbol><ci id="S4.p1.6.m6.3.3.3.2.cmml" xref="S4.p1.6.m6.3.3.3.2">𝑝</ci><ci id="S4.p1.6.m6.3.3.3.3.cmml" xref="S4.p1.6.m6.3.3.3.3">𝜃</ci></apply><apply id="S4.p1.6.m6.3.3.1.1.1.cmml" xref="S4.p1.6.m6.3.3.1.1"><csymbol cd="latexml" id="S4.p1.6.m6.3.3.1.1.1.1.cmml" xref="S4.p1.6.m6.3.3.1.1.1.1">conditional</csymbol><ci id="S4.p1.6.m6.3.3.1.1.1.2.cmml" xref="S4.p1.6.m6.3.3.1.1.1.2">𝐺</ci><list id="S4.p1.6.m6.3.3.1.1.1.3.1.cmml" xref="S4.p1.6.m6.3.3.1.1.1.3.2"><ci id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">𝑞</ci><ci id="S4.p1.6.m6.2.2.cmml" xref="S4.p1.6.m6.2.2">𝒢</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.3c">p_{\theta}(G|q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.p1.6.m6.3d">italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_G | italic_q , caligraphic_G )</annotation></semantics></math> and an answer generator <math alttext="p_{\phi}(a|q,G)" class="ltx_Math" display="inline" id="S4.p1.7.m7.3"><semantics id="S4.p1.7.m7.3a"><mrow id="S4.p1.7.m7.3.3" xref="S4.p1.7.m7.3.3.cmml"><msub id="S4.p1.7.m7.3.3.3" xref="S4.p1.7.m7.3.3.3.cmml"><mi id="S4.p1.7.m7.3.3.3.2" xref="S4.p1.7.m7.3.3.3.2.cmml">p</mi><mi id="S4.p1.7.m7.3.3.3.3" xref="S4.p1.7.m7.3.3.3.3.cmml">ϕ</mi></msub><mo id="S4.p1.7.m7.3.3.2" xref="S4.p1.7.m7.3.3.2.cmml">⁢</mo><mrow id="S4.p1.7.m7.3.3.1.1" xref="S4.p1.7.m7.3.3.1.1.1.cmml"><mo id="S4.p1.7.m7.3.3.1.1.2" stretchy="false" xref="S4.p1.7.m7.3.3.1.1.1.cmml">(</mo><mrow id="S4.p1.7.m7.3.3.1.1.1" xref="S4.p1.7.m7.3.3.1.1.1.cmml"><mi id="S4.p1.7.m7.3.3.1.1.1.2" xref="S4.p1.7.m7.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.p1.7.m7.3.3.1.1.1.1" xref="S4.p1.7.m7.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.p1.7.m7.3.3.1.1.1.3.2" xref="S4.p1.7.m7.3.3.1.1.1.3.1.cmml"><mi id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">q</mi><mo id="S4.p1.7.m7.3.3.1.1.1.3.2.1" xref="S4.p1.7.m7.3.3.1.1.1.3.1.cmml">,</mo><mi id="S4.p1.7.m7.2.2" xref="S4.p1.7.m7.2.2.cmml">G</mi></mrow></mrow><mo id="S4.p1.7.m7.3.3.1.1.3" stretchy="false" xref="S4.p1.7.m7.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.3b"><apply id="S4.p1.7.m7.3.3.cmml" xref="S4.p1.7.m7.3.3"><times id="S4.p1.7.m7.3.3.2.cmml" xref="S4.p1.7.m7.3.3.2"></times><apply id="S4.p1.7.m7.3.3.3.cmml" xref="S4.p1.7.m7.3.3.3"><csymbol cd="ambiguous" id="S4.p1.7.m7.3.3.3.1.cmml" xref="S4.p1.7.m7.3.3.3">subscript</csymbol><ci id="S4.p1.7.m7.3.3.3.2.cmml" xref="S4.p1.7.m7.3.3.3.2">𝑝</ci><ci id="S4.p1.7.m7.3.3.3.3.cmml" xref="S4.p1.7.m7.3.3.3.3">italic-ϕ</ci></apply><apply id="S4.p1.7.m7.3.3.1.1.1.cmml" xref="S4.p1.7.m7.3.3.1.1"><csymbol cd="latexml" id="S4.p1.7.m7.3.3.1.1.1.1.cmml" xref="S4.p1.7.m7.3.3.1.1.1.1">conditional</csymbol><ci id="S4.p1.7.m7.3.3.1.1.1.2.cmml" xref="S4.p1.7.m7.3.3.1.1.1.2">𝑎</ci><list id="S4.p1.7.m7.3.3.1.1.1.3.1.cmml" xref="S4.p1.7.m7.3.3.1.1.1.3.2"><ci id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">𝑞</ci><ci id="S4.p1.7.m7.2.2.cmml" xref="S4.p1.7.m7.2.2">𝐺</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.3c">p_{\phi}(a|q,G)</annotation><annotation encoding="application/x-llamapun" id="S4.p1.7.m7.3d">italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_a | italic_q , italic_G )</annotation></semantics></math> where <math alttext="\theta,\phi" class="ltx_Math" display="inline" id="S4.p1.8.m8.2"><semantics id="S4.p1.8.m8.2a"><mrow id="S4.p1.8.m8.2.3.2" xref="S4.p1.8.m8.2.3.1.cmml"><mi id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml">θ</mi><mo id="S4.p1.8.m8.2.3.2.1" xref="S4.p1.8.m8.2.3.1.cmml">,</mo><mi id="S4.p1.8.m8.2.2" xref="S4.p1.8.m8.2.2.cmml">ϕ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.2b"><list id="S4.p1.8.m8.2.3.1.cmml" xref="S4.p1.8.m8.2.3.2"><ci id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1">𝜃</ci><ci id="S4.p1.8.m8.2.2.cmml" xref="S4.p1.8.m8.2.2">italic-ϕ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.2c">\theta,\phi</annotation><annotation encoding="application/x-llamapun" id="S4.p1.8.m8.2d">italic_θ , italic_ϕ</annotation></semantics></math> are learnable parameters, and utilize the total probability formula to decompose <math alttext="p(a|q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.p1.9.m9.3"><semantics id="S4.p1.9.m9.3a"><mrow id="S4.p1.9.m9.3.3" xref="S4.p1.9.m9.3.3.cmml"><mi id="S4.p1.9.m9.3.3.3" xref="S4.p1.9.m9.3.3.3.cmml">p</mi><mo id="S4.p1.9.m9.3.3.2" xref="S4.p1.9.m9.3.3.2.cmml">⁢</mo><mrow id="S4.p1.9.m9.3.3.1.1" xref="S4.p1.9.m9.3.3.1.1.1.cmml"><mo id="S4.p1.9.m9.3.3.1.1.2" stretchy="false" xref="S4.p1.9.m9.3.3.1.1.1.cmml">(</mo><mrow id="S4.p1.9.m9.3.3.1.1.1" xref="S4.p1.9.m9.3.3.1.1.1.cmml"><mi id="S4.p1.9.m9.3.3.1.1.1.2" xref="S4.p1.9.m9.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.p1.9.m9.3.3.1.1.1.1" xref="S4.p1.9.m9.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.p1.9.m9.3.3.1.1.1.3.2" xref="S4.p1.9.m9.3.3.1.1.1.3.1.cmml"><mi id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml">q</mi><mo id="S4.p1.9.m9.3.3.1.1.1.3.2.1" xref="S4.p1.9.m9.3.3.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.p1.9.m9.2.2" xref="S4.p1.9.m9.2.2.cmml">𝒢</mi></mrow></mrow><mo id="S4.p1.9.m9.3.3.1.1.3" stretchy="false" xref="S4.p1.9.m9.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.3b"><apply id="S4.p1.9.m9.3.3.cmml" xref="S4.p1.9.m9.3.3"><times id="S4.p1.9.m9.3.3.2.cmml" xref="S4.p1.9.m9.3.3.2"></times><ci id="S4.p1.9.m9.3.3.3.cmml" xref="S4.p1.9.m9.3.3.3">𝑝</ci><apply id="S4.p1.9.m9.3.3.1.1.1.cmml" xref="S4.p1.9.m9.3.3.1.1"><csymbol cd="latexml" id="S4.p1.9.m9.3.3.1.1.1.1.cmml" xref="S4.p1.9.m9.3.3.1.1.1.1">conditional</csymbol><ci id="S4.p1.9.m9.3.3.1.1.1.2.cmml" xref="S4.p1.9.m9.3.3.1.1.1.2">𝑎</ci><list id="S4.p1.9.m9.3.3.1.1.1.3.1.cmml" xref="S4.p1.9.m9.3.3.1.1.1.3.2"><ci id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1">𝑞</ci><ci id="S4.p1.9.m9.2.2.cmml" xref="S4.p1.9.m9.2.2">𝒢</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.3c">p(a|q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.p1.9.m9.3d">italic_p ( italic_a | italic_q , caligraphic_G )</annotation></semantics></math>, which can be formulated as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(a|q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.E4X.2.1.1.m1.3"><semantics id="S4.E4X.2.1.1.m1.3a"><mrow id="S4.E4X.2.1.1.m1.3.3" xref="S4.E4X.2.1.1.m1.3.3.cmml"><mi id="S4.E4X.2.1.1.m1.3.3.3" xref="S4.E4X.2.1.1.m1.3.3.3.cmml">p</mi><mo id="S4.E4X.2.1.1.m1.3.3.2" xref="S4.E4X.2.1.1.m1.3.3.2.cmml">⁢</mo><mrow id="S4.E4X.2.1.1.m1.3.3.1.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml"><mo id="S4.E4X.2.1.1.m1.3.3.1.1.2" stretchy="false" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S4.E4X.2.1.1.m1.3.3.1.1.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml"><mi id="S4.E4X.2.1.1.m1.3.3.1.1.1.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.E4X.2.1.1.m1.3.3.1.1.1.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.E4X.2.1.1.m1.3.3.1.1.1.3.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S4.E4X.2.1.1.m1.1.1" xref="S4.E4X.2.1.1.m1.1.1.cmml">q</mi><mo id="S4.E4X.2.1.1.m1.3.3.1.1.1.3.2.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4X.2.1.1.m1.2.2" xref="S4.E4X.2.1.1.m1.2.2.cmml">𝒢</mi></mrow></mrow><mo id="S4.E4X.2.1.1.m1.3.3.1.1.3" stretchy="false" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4X.2.1.1.m1.3b"><apply id="S4.E4X.2.1.1.m1.3.3.cmml" xref="S4.E4X.2.1.1.m1.3.3"><times id="S4.E4X.2.1.1.m1.3.3.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.2"></times><ci id="S4.E4X.2.1.1.m1.3.3.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.3">𝑝</ci><apply id="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S4.E4X.2.1.1.m1.3.3.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S4.E4X.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.2">𝑎</ci><list id="S4.E4X.2.1.1.m1.3.3.1.1.1.3.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.3.2"><ci id="S4.E4X.2.1.1.m1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1">𝑞</ci><ci id="S4.E4X.2.1.1.m1.2.2.cmml" xref="S4.E4X.2.1.1.m1.2.2">𝒢</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.2.1.1.m1.3c">\displaystyle p(a|q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.2.1.1.m1.3d">italic_p ( italic_a | italic_q , caligraphic_G )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum\limits_{G\subseteq\mathcal{G}}p_{\phi}(a|q,G)p_{\theta}(G|q%
,\mathcal{G})" class="ltx_Math" display="inline" id="S4.E4X.3.2.2.m1.6"><semantics id="S4.E4X.3.2.2.m1.6a"><mrow id="S4.E4X.3.2.2.m1.6.6" xref="S4.E4X.3.2.2.m1.6.6.cmml"><mi id="S4.E4X.3.2.2.m1.6.6.4" xref="S4.E4X.3.2.2.m1.6.6.4.cmml"></mi><mo id="S4.E4X.3.2.2.m1.6.6.3" xref="S4.E4X.3.2.2.m1.6.6.3.cmml">=</mo><mrow id="S4.E4X.3.2.2.m1.6.6.2" xref="S4.E4X.3.2.2.m1.6.6.2.cmml"><mstyle displaystyle="true" id="S4.E4X.3.2.2.m1.6.6.2.3" xref="S4.E4X.3.2.2.m1.6.6.2.3.cmml"><munder id="S4.E4X.3.2.2.m1.6.6.2.3a" xref="S4.E4X.3.2.2.m1.6.6.2.3.cmml"><mo id="S4.E4X.3.2.2.m1.6.6.2.3.2" movablelimits="false" xref="S4.E4X.3.2.2.m1.6.6.2.3.2.cmml">∑</mo><mrow id="S4.E4X.3.2.2.m1.6.6.2.3.3" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.cmml"><mi id="S4.E4X.3.2.2.m1.6.6.2.3.3.2" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.2.cmml">G</mi><mo id="S4.E4X.3.2.2.m1.6.6.2.3.3.1" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.1.cmml">⊆</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4X.3.2.2.m1.6.6.2.3.3.3" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.3.cmml">𝒢</mi></mrow></munder></mstyle><mrow id="S4.E4X.3.2.2.m1.6.6.2.2" xref="S4.E4X.3.2.2.m1.6.6.2.2.cmml"><msub id="S4.E4X.3.2.2.m1.6.6.2.2.4" xref="S4.E4X.3.2.2.m1.6.6.2.2.4.cmml"><mi id="S4.E4X.3.2.2.m1.6.6.2.2.4.2" xref="S4.E4X.3.2.2.m1.6.6.2.2.4.2.cmml">p</mi><mi id="S4.E4X.3.2.2.m1.6.6.2.2.4.3" xref="S4.E4X.3.2.2.m1.6.6.2.2.4.3.cmml">ϕ</mi></msub><mo id="S4.E4X.3.2.2.m1.6.6.2.2.3" xref="S4.E4X.3.2.2.m1.6.6.2.2.3.cmml">⁢</mo><mrow id="S4.E4X.3.2.2.m1.5.5.1.1.1.1" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.cmml"><mo id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.2" stretchy="false" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.cmml"><mi id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.2" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.1" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.1.cmml">|</mo><mrow id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.2" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.1.cmml"><mi id="S4.E4X.3.2.2.m1.1.1" xref="S4.E4X.3.2.2.m1.1.1.cmml">q</mi><mo id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.2.1" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E4X.3.2.2.m1.2.2" xref="S4.E4X.3.2.2.m1.2.2.cmml">G</mi></mrow></mrow><mo id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.3" stretchy="false" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E4X.3.2.2.m1.6.6.2.2.3a" xref="S4.E4X.3.2.2.m1.6.6.2.2.3.cmml">⁢</mo><msub id="S4.E4X.3.2.2.m1.6.6.2.2.5" xref="S4.E4X.3.2.2.m1.6.6.2.2.5.cmml"><mi id="S4.E4X.3.2.2.m1.6.6.2.2.5.2" xref="S4.E4X.3.2.2.m1.6.6.2.2.5.2.cmml">p</mi><mi id="S4.E4X.3.2.2.m1.6.6.2.2.5.3" xref="S4.E4X.3.2.2.m1.6.6.2.2.5.3.cmml">θ</mi></msub><mo id="S4.E4X.3.2.2.m1.6.6.2.2.3b" xref="S4.E4X.3.2.2.m1.6.6.2.2.3.cmml">⁢</mo><mrow id="S4.E4X.3.2.2.m1.6.6.2.2.2.1" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.cmml"><mo id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.2" stretchy="false" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.cmml">(</mo><mrow id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.cmml"><mi id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.2" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.2.cmml">G</mi><mo fence="false" id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.1" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.1.cmml">|</mo><mrow id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.2" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.1.cmml"><mi id="S4.E4X.3.2.2.m1.3.3" xref="S4.E4X.3.2.2.m1.3.3.cmml">q</mi><mo id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.2.1" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4X.3.2.2.m1.4.4" xref="S4.E4X.3.2.2.m1.4.4.cmml">𝒢</mi></mrow></mrow><mo id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.3" stretchy="false" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4X.3.2.2.m1.6b"><apply id="S4.E4X.3.2.2.m1.6.6.cmml" xref="S4.E4X.3.2.2.m1.6.6"><eq id="S4.E4X.3.2.2.m1.6.6.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.3"></eq><csymbol cd="latexml" id="S4.E4X.3.2.2.m1.6.6.4.cmml" xref="S4.E4X.3.2.2.m1.6.6.4">absent</csymbol><apply id="S4.E4X.3.2.2.m1.6.6.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2"><apply id="S4.E4X.3.2.2.m1.6.6.2.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3"><csymbol cd="ambiguous" id="S4.E4X.3.2.2.m1.6.6.2.3.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3">subscript</csymbol><sum id="S4.E4X.3.2.2.m1.6.6.2.3.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3.2"></sum><apply id="S4.E4X.3.2.2.m1.6.6.2.3.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3.3"><subset id="S4.E4X.3.2.2.m1.6.6.2.3.3.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.1"></subset><ci id="S4.E4X.3.2.2.m1.6.6.2.3.3.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.2">𝐺</ci><ci id="S4.E4X.3.2.2.m1.6.6.2.3.3.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.3.3.3">𝒢</ci></apply></apply><apply id="S4.E4X.3.2.2.m1.6.6.2.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2"><times id="S4.E4X.3.2.2.m1.6.6.2.2.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.3"></times><apply id="S4.E4X.3.2.2.m1.6.6.2.2.4.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.4"><csymbol cd="ambiguous" id="S4.E4X.3.2.2.m1.6.6.2.2.4.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.4">subscript</csymbol><ci id="S4.E4X.3.2.2.m1.6.6.2.2.4.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.4.2">𝑝</ci><ci id="S4.E4X.3.2.2.m1.6.6.2.2.4.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.4.3">italic-ϕ</ci></apply><apply id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.2.cmml" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.2">𝑎</ci><list id="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S4.E4X.3.2.2.m1.5.5.1.1.1.1.1.3.2"><ci id="S4.E4X.3.2.2.m1.1.1.cmml" xref="S4.E4X.3.2.2.m1.1.1">𝑞</ci><ci id="S4.E4X.3.2.2.m1.2.2.cmml" xref="S4.E4X.3.2.2.m1.2.2">𝐺</ci></list></apply><apply id="S4.E4X.3.2.2.m1.6.6.2.2.5.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.5"><csymbol cd="ambiguous" id="S4.E4X.3.2.2.m1.6.6.2.2.5.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.5">subscript</csymbol><ci id="S4.E4X.3.2.2.m1.6.6.2.2.5.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.5.2">𝑝</ci><ci id="S4.E4X.3.2.2.m1.6.6.2.2.5.3.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.5.3">𝜃</ci></apply><apply id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1"><csymbol cd="latexml" id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.1">conditional</csymbol><ci id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.2.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.2">𝐺</ci><list id="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.1.cmml" xref="S4.E4X.3.2.2.m1.6.6.2.2.2.1.1.3.2"><ci id="S4.E4X.3.2.2.m1.3.3.cmml" xref="S4.E4X.3.2.2.m1.3.3">𝑞</ci><ci id="S4.E4X.3.2.2.m1.4.4.cmml" xref="S4.E4X.3.2.2.m1.4.4">𝒢</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.3.2.2.m1.6c">\displaystyle=\sum\limits_{G\subseteq\mathcal{G}}p_{\phi}(a|q,G)p_{\theta}(G|q%
,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.3.2.2.m1.6d">= ∑ start_POSTSUBSCRIPT italic_G ⊆ caligraphic_G end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_a | italic_q , italic_G ) italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_G | italic_q , caligraphic_G )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\approx p_{\phi}(a|q,G^{*})p_{\theta}(G^{*}|q,\mathcal{G})," class="ltx_Math" display="inline" id="S4.E4Xa.2.1.1.m1.4"><semantics id="S4.E4Xa.2.1.1.m1.4a"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.4" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.4.cmml"></mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.cmml">≈</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml"><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.2.cmml">p</mi><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.3.cmml">ϕ</mi></msub><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3.cmml">⁢</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml">a</mi><mo fence="false" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4Xa.2.1.1.m1.1.1" xref="S4.E4Xa.2.1.1.m1.1.1.cmml">q</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">∗</mo></msup></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3a" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3.cmml">⁢</mo><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.2.cmml">p</mi><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.3.cmml">θ</mi></msub><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3b" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3.cmml">⁢</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.cmml">(</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.cmml"><msup id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.2.cmml">G</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.3.cmml">∗</mo></msup><mo fence="false" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.1.cmml">|</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.2.2" xref="S4.E4Xa.2.1.1.m1.2.2.cmml">q</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.2.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4Xa.2.1.1.m1.3.3" xref="S4.E4Xa.2.1.1.m1.3.3.cmml">𝒢</mi></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.3" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4Xa.2.1.1.m1.4b"><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1"><approx id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3"></approx><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.4.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.4">absent</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2"><times id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.3"></times><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4">subscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.2">𝑝</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.4.3">italic-ϕ</ci></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2">conditional</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3">𝑎</ci><list id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1"><ci id="S4.E4Xa.2.1.1.m1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.1.1">𝑞</ci><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝐺</ci><times id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.3"></times></apply></list></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5">subscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.2">𝑝</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.5.3">𝜃</ci></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1"><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.1">conditional</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2">superscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.2">𝐺</ci><times id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.2.3"></times></apply><list id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.1.1.3.2"><ci id="S4.E4Xa.2.1.1.m1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.2.2">𝑞</ci><ci id="S4.E4Xa.2.1.1.m1.3.3.cmml" xref="S4.E4Xa.2.1.1.m1.3.3">𝒢</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4Xa.2.1.1.m1.4c">\displaystyle\approx p_{\phi}(a|q,G^{*})p_{\theta}(G^{*}|q,\mathcal{G}),</annotation><annotation encoding="application/x-llamapun" id="S4.E4Xa.2.1.1.m1.4d">≈ italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_a | italic_q , italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT | italic_q , caligraphic_G ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.p1.11">where <math alttext="G^{*}" class="ltx_Math" display="inline" id="S4.p1.10.m1.1"><semantics id="S4.p1.10.m1.1a"><msup id="S4.p1.10.m1.1.1" xref="S4.p1.10.m1.1.1.cmml"><mi id="S4.p1.10.m1.1.1.2" xref="S4.p1.10.m1.1.1.2.cmml">G</mi><mo id="S4.p1.10.m1.1.1.3" xref="S4.p1.10.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p1.10.m1.1b"><apply id="S4.p1.10.m1.1.1.cmml" xref="S4.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.10.m1.1.1.1.cmml" xref="S4.p1.10.m1.1.1">superscript</csymbol><ci id="S4.p1.10.m1.1.1.2.cmml" xref="S4.p1.10.m1.1.1.2">𝐺</ci><times id="S4.p1.10.m1.1.1.3.cmml" xref="S4.p1.10.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m1.1c">G^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.10.m1.1d">italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is the optimal subgraph. Because the number of candidate subgraphs can grow exponentially with the size of the graph, efficient approximation methods are necessary. The first line of Equation <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S4.E4" title="In 4. Overview of GraphRAG ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a> is thus approximated by the second line. Specifically, a graph retriever is employed to extract the optimal subgraph <math alttext="G^{*}" class="ltx_Math" display="inline" id="S4.p1.11.m2.1"><semantics id="S4.p1.11.m2.1a"><msup id="S4.p1.11.m2.1.1" xref="S4.p1.11.m2.1.1.cmml"><mi id="S4.p1.11.m2.1.1.2" xref="S4.p1.11.m2.1.1.2.cmml">G</mi><mo id="S4.p1.11.m2.1.1.3" xref="S4.p1.11.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p1.11.m2.1b"><apply id="S4.p1.11.m2.1.1.cmml" xref="S4.p1.11.m2.1.1"><csymbol cd="ambiguous" id="S4.p1.11.m2.1.1.1.cmml" xref="S4.p1.11.m2.1.1">superscript</csymbol><ci id="S4.p1.11.m2.1.1.2.cmml" xref="S4.p1.11.m2.1.1.2">𝐺</ci><times id="S4.p1.11.m2.1.1.3.cmml" xref="S4.p1.11.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.11.m2.1c">G^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.11.m2.1d">italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, after which the generator produces the answer based on the retrieved subgraph.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Therefore, in this survey, we decompose the entire process of GraphRAG into three main stages: Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. The overall workflow of GraphRAG is illustrated in Figure 2 and detailed introductions of each stage are as follows.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Graph-Based Indexing (G-Indexing)</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">Graph-Based Indexing constitutes the initial phase of GraphRAG, aimed at identifying or constructing a graph database <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.1.m1.1d">caligraphic_G</annotation></semantics></math> that aligns with downstream tasks and establishing indices on it. The graph database can originate from public knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Vrandečić and Krötzsch, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib164" title="">2014</a>; Bollacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib11" title="">2008a</a>; Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib5" title="">2007</a>; Suchanek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib151" title="">2007</a>; Liu and Singh, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib101" title="">2004</a>; Sap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib143" title="">2019a</a>)</cite>, graph data <cite class="ltx_cite ltx_citemacro_citep">(Morris et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib124" title="">2020</a>)</cite>, or be constructed based on proprietary data sources such as textual <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib173" title="">2024b</a>; Gutiérrez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>)</cite> or other forms of data <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib184" title="">2024</a>)</cite>. The indexing process typically includes mapping node and edge properties, establishing pointers between connected nodes, and organizing data to support fast traversal and retrieval operations. Indexing determines the granularity of the subsequent retrieval stage, playing a crucial role in enhancing query efficiency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Graph-Guided Retrieval (G-Retrieval)</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Following graph-based indexing, the graph-guided retrieval phase focuses on extracting pertinent information from the graph database in response to user queries or input. Specifically, given a user query <math alttext="q" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">italic_q</annotation></semantics></math> which is expressed in natural language, the retrieval stage aims to extract the most relevant elements (e.g., entities, triplets, paths, subgraphs) from knowledge graphs, which can be formulated as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle G^{*}" class="ltx_Math" display="inline" id="S4.E5X.2.1.1.m1.1"><semantics id="S4.E5X.2.1.1.m1.1a"><msup id="S4.E5X.2.1.1.m1.1.1" xref="S4.E5X.2.1.1.m1.1.1.cmml"><mi id="S4.E5X.2.1.1.m1.1.1.2" xref="S4.E5X.2.1.1.m1.1.1.2.cmml">G</mi><mo id="S4.E5X.2.1.1.m1.1.1.3" xref="S4.E5X.2.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.E5X.2.1.1.m1.1b"><apply id="S4.E5X.2.1.1.m1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1">superscript</csymbol><ci id="S4.E5X.2.1.1.m1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.2">𝐺</ci><times id="S4.E5X.2.1.1.m1.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.2.1.1.m1.1c">\displaystyle G^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.2.1.1.m1.1d">italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\textbf{G-Retriever}(q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.E5X.3.2.2.m1.2"><semantics id="S4.E5X.3.2.2.m1.2a"><mrow id="S4.E5X.3.2.2.m1.2.3" xref="S4.E5X.3.2.2.m1.2.3.cmml"><mi id="S4.E5X.3.2.2.m1.2.3.2" xref="S4.E5X.3.2.2.m1.2.3.2.cmml"></mi><mo id="S4.E5X.3.2.2.m1.2.3.1" xref="S4.E5X.3.2.2.m1.2.3.1.cmml">=</mo><mrow id="S4.E5X.3.2.2.m1.2.3.3" xref="S4.E5X.3.2.2.m1.2.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E5X.3.2.2.m1.2.3.3.2" xref="S4.E5X.3.2.2.m1.2.3.3.2a.cmml">G-Retriever</mtext><mo id="S4.E5X.3.2.2.m1.2.3.3.1" xref="S4.E5X.3.2.2.m1.2.3.3.1.cmml">⁢</mo><mrow id="S4.E5X.3.2.2.m1.2.3.3.3.2" xref="S4.E5X.3.2.2.m1.2.3.3.3.1.cmml"><mo id="S4.E5X.3.2.2.m1.2.3.3.3.2.1" stretchy="false" xref="S4.E5X.3.2.2.m1.2.3.3.3.1.cmml">(</mo><mi id="S4.E5X.3.2.2.m1.1.1" xref="S4.E5X.3.2.2.m1.1.1.cmml">q</mi><mo id="S4.E5X.3.2.2.m1.2.3.3.3.2.2" xref="S4.E5X.3.2.2.m1.2.3.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E5X.3.2.2.m1.2.2" xref="S4.E5X.3.2.2.m1.2.2.cmml">𝒢</mi><mo id="S4.E5X.3.2.2.m1.2.3.3.3.2.3" stretchy="false" xref="S4.E5X.3.2.2.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5X.3.2.2.m1.2b"><apply id="S4.E5X.3.2.2.m1.2.3.cmml" xref="S4.E5X.3.2.2.m1.2.3"><eq id="S4.E5X.3.2.2.m1.2.3.1.cmml" xref="S4.E5X.3.2.2.m1.2.3.1"></eq><csymbol cd="latexml" id="S4.E5X.3.2.2.m1.2.3.2.cmml" xref="S4.E5X.3.2.2.m1.2.3.2">absent</csymbol><apply id="S4.E5X.3.2.2.m1.2.3.3.cmml" xref="S4.E5X.3.2.2.m1.2.3.3"><times id="S4.E5X.3.2.2.m1.2.3.3.1.cmml" xref="S4.E5X.3.2.2.m1.2.3.3.1"></times><ci id="S4.E5X.3.2.2.m1.2.3.3.2a.cmml" xref="S4.E5X.3.2.2.m1.2.3.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E5X.3.2.2.m1.2.3.3.2.cmml" xref="S4.E5X.3.2.2.m1.2.3.3.2">G-Retriever</mtext></ci><interval closure="open" id="S4.E5X.3.2.2.m1.2.3.3.3.1.cmml" xref="S4.E5X.3.2.2.m1.2.3.3.3.2"><ci id="S4.E5X.3.2.2.m1.1.1.cmml" xref="S4.E5X.3.2.2.m1.1.1">𝑞</ci><ci id="S4.E5X.3.2.2.m1.2.2.cmml" xref="S4.E5X.3.2.2.m1.2.2">𝒢</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.3.2.2.m1.2c">\displaystyle=\textbf{G-Retriever}(q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.3.2.2.m1.2d">= G-Retriever ( italic_q , caligraphic_G )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\,max}_{G\subseteq\mathcal{R}(\mathcal{G})}\,p%
_{\theta}(G|q,\mathcal{G})" class="ltx_Math" display="inline" id="S4.E5Xa.2.1.1.m1.4"><semantics id="S4.E5Xa.2.1.1.m1.4a"><mrow id="S4.E5Xa.2.1.1.m1.4.4" xref="S4.E5Xa.2.1.1.m1.4.4.cmml"><mi id="S4.E5Xa.2.1.1.m1.4.4.3" xref="S4.E5Xa.2.1.1.m1.4.4.3.cmml"></mi><mo id="S4.E5Xa.2.1.1.m1.4.4.2" xref="S4.E5Xa.2.1.1.m1.4.4.2.cmml">=</mo><mrow id="S4.E5Xa.2.1.1.m1.4.4.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.cmml"><mrow id="S4.E5Xa.2.1.1.m1.4.4.1.3" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.cmml"><munder id="S4.E5Xa.2.1.1.m1.4.4.1.3.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.cmml"><mrow id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.cmml"><mi id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.2.cmml">arg</mi><mo id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.1" lspace="0.170em" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.1.cmml">⁢</mo><mi id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.3" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S4.E5Xa.2.1.1.m1.1.1.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.cmml"><mi id="S4.E5Xa.2.1.1.m1.1.1.1.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.3.cmml">G</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.2.cmml">⊆</mo><mrow id="S4.E5Xa.2.1.1.m1.1.1.1.4" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5Xa.2.1.1.m1.1.1.1.4.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.2.cmml">ℛ</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.4.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.1.cmml">⁢</mo><mrow id="S4.E5Xa.2.1.1.m1.1.1.1.4.3.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.cmml"><mo id="S4.E5Xa.2.1.1.m1.1.1.1.4.3.2.1" stretchy="false" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.E5Xa.2.1.1.m1.1.1.1.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml">𝒢</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.4.3.2.2" stretchy="false" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mo id="S4.E5Xa.2.1.1.m1.4.4.1.3a" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.cmml">⁡</mo><msub id="S4.E5Xa.2.1.1.m1.4.4.1.3.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2.cmml"><mi id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2.2.cmml">p</mi><mi id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.3" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2.3.cmml">θ</mi></msub></mrow><mo id="S4.E5Xa.2.1.1.m1.4.4.1.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.2.cmml">⁢</mo><mrow id="S4.E5Xa.2.1.1.m1.4.4.1.1.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.cmml"><mo id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.2" stretchy="false" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.cmml"><mi id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.2.cmml">G</mi><mo fence="false" id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.1.cmml">|</mo><mrow id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.2" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.1.cmml"><mi id="S4.E5Xa.2.1.1.m1.2.2" xref="S4.E5Xa.2.1.1.m1.2.2.cmml">q</mi><mo id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.2.1" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.E5Xa.2.1.1.m1.3.3" xref="S4.E5Xa.2.1.1.m1.3.3.cmml">𝒢</mi></mrow></mrow><mo id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.3" stretchy="false" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5Xa.2.1.1.m1.4b"><apply id="S4.E5Xa.2.1.1.m1.4.4.cmml" xref="S4.E5Xa.2.1.1.m1.4.4"><eq id="S4.E5Xa.2.1.1.m1.4.4.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.2"></eq><csymbol cd="latexml" id="S4.E5Xa.2.1.1.m1.4.4.3.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.3">absent</csymbol><apply id="S4.E5Xa.2.1.1.m1.4.4.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1"><times id="S4.E5Xa.2.1.1.m1.4.4.1.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.2"></times><apply id="S4.E5Xa.2.1.1.m1.4.4.1.3.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3"><apply id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1">subscript</csymbol><apply id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2"><times id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.1"></times><ci id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.2">arg</ci><ci id="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.3.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.1.2.3">max</ci></apply><apply id="S4.E5Xa.2.1.1.m1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1"><subset id="S4.E5Xa.2.1.1.m1.1.1.1.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.2"></subset><ci id="S4.E5Xa.2.1.1.m1.1.1.1.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.3">𝐺</ci><apply id="S4.E5Xa.2.1.1.m1.1.1.1.4.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.4"><times id="S4.E5Xa.2.1.1.m1.1.1.1.4.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.1"></times><ci id="S4.E5Xa.2.1.1.m1.1.1.1.4.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.4.2">ℛ</ci><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1">𝒢</ci></apply></apply></apply><apply id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2">subscript</csymbol><ci id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2.2">𝑝</ci><ci id="S4.E5Xa.2.1.1.m1.4.4.1.3.2.3.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.3.2.3">𝜃</ci></apply></apply><apply id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.2">𝐺</ci><list id="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.1.cmml" xref="S4.E5Xa.2.1.1.m1.4.4.1.1.1.1.3.2"><ci id="S4.E5Xa.2.1.1.m1.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.2.2">𝑞</ci><ci id="S4.E5Xa.2.1.1.m1.3.3.cmml" xref="S4.E5Xa.2.1.1.m1.3.3">𝒢</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5Xa.2.1.1.m1.4c">\displaystyle=\operatorname*{arg\,max}_{G\subseteq\mathcal{R}(\mathcal{G})}\,p%
_{\theta}(G|q,\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S4.E5Xa.2.1.1.m1.4d">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_G ⊆ caligraphic_R ( caligraphic_G ) end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_G | italic_q , caligraphic_G )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\,max}_{G\subseteq\mathcal{R}(\mathcal{G})}\,%
\textbf{Sim}(q,G)," class="ltx_Math" display="inline" id="S4.E5Xb.2.1.1.m1.4"><semantics id="S4.E5Xb.2.1.1.m1.4a"><mrow id="S4.E5Xb.2.1.1.m1.4.4.1" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E5Xb.2.1.1.m1.4.4.1.1" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.cmml"><mi id="S4.E5Xb.2.1.1.m1.4.4.1.1.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.2.cmml"></mi><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.1" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E5Xb.2.1.1.m1.4.4.1.1.3" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.cmml"><mrow id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.cmml"><munder id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.cmml"><mrow id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.cmml"><mi id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.2.cmml">arg</mi><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.1" lspace="0.170em" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.1.cmml">⁢</mo><mi id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.3" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.3.cmml">max</mi></mrow><mrow id="S4.E5Xb.2.1.1.m1.1.1.1" xref="S4.E5Xb.2.1.1.m1.1.1.1.cmml"><mi id="S4.E5Xb.2.1.1.m1.1.1.1.3" xref="S4.E5Xb.2.1.1.m1.1.1.1.3.cmml">G</mi><mo id="S4.E5Xb.2.1.1.m1.1.1.1.2" xref="S4.E5Xb.2.1.1.m1.1.1.1.2.cmml">⊆</mo><mrow id="S4.E5Xb.2.1.1.m1.1.1.1.4" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5Xb.2.1.1.m1.1.1.1.4.2" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.2.cmml">ℛ</mi><mo id="S4.E5Xb.2.1.1.m1.1.1.1.4.1" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.1.cmml">⁢</mo><mrow id="S4.E5Xb.2.1.1.m1.1.1.1.4.3.2" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.cmml"><mo id="S4.E5Xb.2.1.1.m1.1.1.1.4.3.2.1" stretchy="false" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.E5Xb.2.1.1.m1.1.1.1.1" xref="S4.E5Xb.2.1.1.m1.1.1.1.1.cmml">𝒢</mi><mo id="S4.E5Xb.2.1.1.m1.1.1.1.4.3.2.2" stretchy="false" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2a" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.cmml">⁡</mo><mtext class="ltx_mathvariant_bold" id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2a.cmml">Sim</mtext></mrow><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.1" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.1.cmml">⁢</mo><mrow id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.1.cmml"><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.2.1" stretchy="false" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S4.E5Xb.2.1.1.m1.2.2" xref="S4.E5Xb.2.1.1.m1.2.2.cmml">q</mi><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.2.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.1.cmml">,</mo><mi id="S4.E5Xb.2.1.1.m1.3.3" xref="S4.E5Xb.2.1.1.m1.3.3.cmml">G</mi><mo id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.2.3" stretchy="false" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E5Xb.2.1.1.m1.4.4.1.2" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5Xb.2.1.1.m1.4b"><apply id="S4.E5Xb.2.1.1.m1.4.4.1.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1"><eq id="S4.E5Xb.2.1.1.m1.4.4.1.1.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.1"></eq><csymbol cd="latexml" id="S4.E5Xb.2.1.1.m1.4.4.1.1.2.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.2">absent</csymbol><apply id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3"><times id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.1"></times><apply id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2"><apply id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1">subscript</csymbol><apply id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2"><times id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.1"></times><ci id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.2.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.2">arg</ci><ci id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.3.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.1.2.3">max</ci></apply><apply id="S4.E5Xb.2.1.1.m1.1.1.1.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1"><subset id="S4.E5Xb.2.1.1.m1.1.1.1.2.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.2"></subset><ci id="S4.E5Xb.2.1.1.m1.1.1.1.3.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.3">𝐺</ci><apply id="S4.E5Xb.2.1.1.m1.1.1.1.4.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.4"><times id="S4.E5Xb.2.1.1.m1.1.1.1.4.1.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.1"></times><ci id="S4.E5Xb.2.1.1.m1.1.1.1.4.2.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.4.2">ℛ</ci><ci id="S4.E5Xb.2.1.1.m1.1.1.1.1.cmml" xref="S4.E5Xb.2.1.1.m1.1.1.1.1">𝒢</ci></apply></apply></apply><ci id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2a.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.2.2">Sim</mtext></ci></apply><interval closure="open" id="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.1.cmml" xref="S4.E5Xb.2.1.1.m1.4.4.1.1.3.3.2"><ci id="S4.E5Xb.2.1.1.m1.2.2.cmml" xref="S4.E5Xb.2.1.1.m1.2.2">𝑞</ci><ci id="S4.E5Xb.2.1.1.m1.3.3.cmml" xref="S4.E5Xb.2.1.1.m1.3.3">𝐺</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5Xb.2.1.1.m1.4c">\displaystyle=\operatorname*{arg\,max}_{G\subseteq\mathcal{R}(\mathcal{G})}\,%
\textbf{Sim}(q,G),</annotation><annotation encoding="application/x-llamapun" id="S4.E5Xb.2.1.1.m1.4d">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_G ⊆ caligraphic_R ( caligraphic_G ) end_POSTSUBSCRIPT Sim ( italic_q , italic_G ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.4">where <math alttext="G^{*}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m1.1a"><msup id="S4.SS0.SSS0.Px2.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1.2.cmml">G</mi><mo id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m1.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1">superscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1.2">𝐺</ci><times id="S4.SS0.SSS0.Px2.p1.2.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m1.1c">G^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m1.1d">italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is the optimal retrieved graph elements and <math alttext="\textbf{Sim}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m2.2"><semantics id="S4.SS0.SSS0.Px2.p1.3.m2.2a"><mrow id="S4.SS0.SSS0.Px2.p1.3.m2.2.3" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2a.cmml">Sim</mtext><mo id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.1" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.1.cmml">⁢</mo><mrow id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.2" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.2.1" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.1.cmml">(</mo><mo id="S4.SS0.SSS0.Px2.p1.3.m2.1.1" lspace="0em" rspace="0em" xref="S4.SS0.SSS0.Px2.p1.3.m2.1.1.cmml">⋅</mo><mo id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.2.2" rspace="0em" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.1.cmml">,</mo><mo id="S4.SS0.SSS0.Px2.p1.3.m2.2.2" lspace="0em" rspace="0em" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.2.cmml">⋅</mo><mo id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.2.3" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m2.2b"><apply id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3"><times id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.1"></times><ci id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2a.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2"><mtext class="ltx_mathvariant_bold" id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.2">Sim</mtext></ci><interval closure="open" id="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.3.3.2"><ci id="S4.SS0.SSS0.Px2.p1.3.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.1.1">⋅</ci><ci id="S4.SS0.SSS0.Px2.p1.3.m2.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m2.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m2.2c">\textbf{Sim}(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m2.2d">Sim ( ⋅ , ⋅ )</annotation></semantics></math> is a function that measures the semantic similarity between user queries and the graph data. <math alttext="\mathcal{R(\cdot)}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m3.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m3.1a"><mrow id="S4.SS0.SSS0.Px2.p1.4.m3.1.2" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.2" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.2.cmml">ℛ</mi><mo id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.1" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.1.cmml">⁢</mo><mrow id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.3.2" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.cmml"><mo id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.3.2.1" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.cmml">(</mo><mo id="S4.SS0.SSS0.Px2.p1.4.m3.1.1" lspace="0em" rspace="0em" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.1.cmml">⋅</mo><mo id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.3.2.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m3.1b"><apply id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2"><times id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.1"></times><ci id="S4.SS0.SSS0.Px2.p1.4.m3.1.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.2.2">ℛ</ci><ci id="S4.SS0.SSS0.Px2.p1.4.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m3.1c">\mathcal{R(\cdot)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m3.1d">caligraphic_R ( ⋅ )</annotation></semantics></math> represents a function to narrow down the search range of subgraphs, considering the efficiency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Graph-Enhanced Generation (G-Generation)</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.2">The graph-enhanced generation phase involves synthesizing meaningful outputs or responses based on the retrieved graph data. This could encompass answering user queries, generating reports, etc. In this stage, a generator takes the query, retrieved graph elements, and an optional prompt as input to generate a response, which can be denoted as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E6">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E6X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle a^{*}" class="ltx_Math" display="inline" id="S4.E6X.2.1.1.m1.1"><semantics id="S4.E6X.2.1.1.m1.1a"><msup id="S4.E6X.2.1.1.m1.1.1" xref="S4.E6X.2.1.1.m1.1.1.cmml"><mi id="S4.E6X.2.1.1.m1.1.1.2" xref="S4.E6X.2.1.1.m1.1.1.2.cmml">a</mi><mo id="S4.E6X.2.1.1.m1.1.1.3" xref="S4.E6X.2.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.E6X.2.1.1.m1.1b"><apply id="S4.E6X.2.1.1.m1.1.1.cmml" xref="S4.E6X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.E6X.2.1.1.m1.1.1.1.cmml" xref="S4.E6X.2.1.1.m1.1.1">superscript</csymbol><ci id="S4.E6X.2.1.1.m1.1.1.2.cmml" xref="S4.E6X.2.1.1.m1.1.1.2">𝑎</ci><times id="S4.E6X.2.1.1.m1.1.1.3.cmml" xref="S4.E6X.2.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6X.2.1.1.m1.1c">\displaystyle a^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.E6X.2.1.1.m1.1d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\textbf{G-Generator}(q,G^{*})" class="ltx_Math" display="inline" id="S4.E6X.3.2.2.m1.2"><semantics id="S4.E6X.3.2.2.m1.2a"><mrow id="S4.E6X.3.2.2.m1.2.2" xref="S4.E6X.3.2.2.m1.2.2.cmml"><mi id="S4.E6X.3.2.2.m1.2.2.3" xref="S4.E6X.3.2.2.m1.2.2.3.cmml"></mi><mo id="S4.E6X.3.2.2.m1.2.2.2" xref="S4.E6X.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S4.E6X.3.2.2.m1.2.2.1" xref="S4.E6X.3.2.2.m1.2.2.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E6X.3.2.2.m1.2.2.1.3" xref="S4.E6X.3.2.2.m1.2.2.1.3a.cmml">G-Generator</mtext><mo id="S4.E6X.3.2.2.m1.2.2.1.2" xref="S4.E6X.3.2.2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S4.E6X.3.2.2.m1.2.2.1.1.1" xref="S4.E6X.3.2.2.m1.2.2.1.1.2.cmml"><mo id="S4.E6X.3.2.2.m1.2.2.1.1.1.2" stretchy="false" xref="S4.E6X.3.2.2.m1.2.2.1.1.2.cmml">(</mo><mi id="S4.E6X.3.2.2.m1.1.1" xref="S4.E6X.3.2.2.m1.1.1.cmml">q</mi><mo id="S4.E6X.3.2.2.m1.2.2.1.1.1.3" xref="S4.E6X.3.2.2.m1.2.2.1.1.2.cmml">,</mo><msup id="S4.E6X.3.2.2.m1.2.2.1.1.1.1" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1.cmml"><mi id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.2" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1.2.cmml">G</mi><mo id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.3" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1.3.cmml">∗</mo></msup><mo id="S4.E6X.3.2.2.m1.2.2.1.1.1.4" stretchy="false" xref="S4.E6X.3.2.2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6X.3.2.2.m1.2b"><apply id="S4.E6X.3.2.2.m1.2.2.cmml" xref="S4.E6X.3.2.2.m1.2.2"><eq id="S4.E6X.3.2.2.m1.2.2.2.cmml" xref="S4.E6X.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S4.E6X.3.2.2.m1.2.2.3.cmml" xref="S4.E6X.3.2.2.m1.2.2.3">absent</csymbol><apply id="S4.E6X.3.2.2.m1.2.2.1.cmml" xref="S4.E6X.3.2.2.m1.2.2.1"><times id="S4.E6X.3.2.2.m1.2.2.1.2.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.2"></times><ci id="S4.E6X.3.2.2.m1.2.2.1.3a.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.3"><mtext class="ltx_mathvariant_bold" id="S4.E6X.3.2.2.m1.2.2.1.3.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.3">G-Generator</mtext></ci><interval closure="open" id="S4.E6X.3.2.2.m1.2.2.1.1.2.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.1.1"><ci id="S4.E6X.3.2.2.m1.1.1.cmml" xref="S4.E6X.3.2.2.m1.1.1">𝑞</ci><apply id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1">superscript</csymbol><ci id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.2.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1.2">𝐺</ci><times id="S4.E6X.3.2.2.m1.2.2.1.1.1.1.3.cmml" xref="S4.E6X.3.2.2.m1.2.2.1.1.1.1.3"></times></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6X.3.2.2.m1.2c">\displaystyle=\textbf{G-Generator}(q,G^{*})</annotation><annotation encoding="application/x-llamapun" id="S4.E6X.3.2.2.m1.2d">= G-Generator ( italic_q , italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E6Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\,max}_{a\in A}p_{\phi}(a|q,G^{*})" class="ltx_Math" display="inline" id="S4.E6Xa.2.1.1.m1.2"><semantics id="S4.E6Xa.2.1.1.m1.2a"><mrow id="S4.E6Xa.2.1.1.m1.2.2" xref="S4.E6Xa.2.1.1.m1.2.2.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.3" xref="S4.E6Xa.2.1.1.m1.2.2.3.cmml"></mi><mo id="S4.E6Xa.2.1.1.m1.2.2.2" xref="S4.E6Xa.2.1.1.m1.2.2.2.cmml">=</mo><mrow id="S4.E6Xa.2.1.1.m1.2.2.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.cmml"><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.cmml"><munder id="S4.E6Xa.2.1.1.m1.2.2.1.3.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.cmml"><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.2.cmml">arg</mi><mo id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.1" lspace="0.170em" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.1.cmml">⁢</mo><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.2.cmml">a</mi><mo id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.1.cmml">∈</mo><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.3.cmml">A</mi></mrow></munder><mo id="S4.E6Xa.2.1.1.m1.2.2.1.3a" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.cmml">⁡</mo><msub id="S4.E6Xa.2.1.1.m1.2.2.1.3.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2.2.cmml">p</mi><mi id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2.3.cmml">ϕ</mi></msub></mrow><mo id="S4.E6Xa.2.1.1.m1.2.2.1.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.1.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.cmml"><mo id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.2" stretchy="false" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.3.cmml">a</mi><mo fence="false" id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.2.cmml">|</mo><mrow id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S4.E6Xa.2.1.1.m1.1.1" xref="S4.E6Xa.2.1.1.m1.1.1.cmml">q</mi><mo id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.2.cmml">,</mo><msup id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">∗</mo></msup></mrow></mrow><mo id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.3" stretchy="false" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6Xa.2.1.1.m1.2b"><apply id="S4.E6Xa.2.1.1.m1.2.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2"><eq id="S4.E6Xa.2.1.1.m1.2.2.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.2"></eq><csymbol cd="latexml" id="S4.E6Xa.2.1.1.m1.2.2.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.3">absent</csymbol><apply id="S4.E6Xa.2.1.1.m1.2.2.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1"><times id="S4.E6Xa.2.1.1.m1.2.2.1.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.2"></times><apply id="S4.E6Xa.2.1.1.m1.2.2.1.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3"><apply id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1"><csymbol cd="ambiguous" id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1">subscript</csymbol><apply id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2"><times id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.1"></times><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.2">arg</ci><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.2.3">max</ci></apply><apply id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3"><in id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.1"></in><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.2">𝑎</ci><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.1.3.3">𝐴</ci></apply></apply><apply id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2"><csymbol cd="ambiguous" id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2">subscript</csymbol><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2.2">𝑝</ci><ci id="S4.E6Xa.2.1.1.m1.2.2.1.3.2.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.3.2.3">italic-ϕ</ci></apply></apply><apply id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.2">conditional</csymbol><ci id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.3">𝑎</ci><list id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1"><ci id="S4.E6Xa.2.1.1.m1.1.1.cmml" xref="S4.E6Xa.2.1.1.m1.1.1">𝑞</ci><apply id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.2">𝐺</ci><times id="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E6Xa.2.1.1.m1.2.2.1.1.1.1.1.1.1.3"></times></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6Xa.2.1.1.m1.2c">\displaystyle=\operatorname*{arg\,max}_{a\in A}p_{\phi}(a|q,G^{*})</annotation><annotation encoding="application/x-llamapun" id="S4.E6Xa.2.1.1.m1.2d">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_a ∈ italic_A end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_a | italic_q , italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E6Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\,max}_{a\in A}p_{\phi}(a|\mathcal{F}(q,G^{*}))," class="ltx_Math" display="inline" id="S4.E6Xb.2.1.1.m1.2"><semantics id="S4.E6Xb.2.1.1.m1.2a"><mrow id="S4.E6Xb.2.1.1.m1.2.2.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.cmml"><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.3.cmml"></mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.cmml"><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.cmml"><munder id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.cmml"><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.2.cmml">arg</mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.1" lspace="0.170em" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.1.cmml">⁢</mo><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.2.cmml">a</mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.1.cmml">∈</mo><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.3.cmml">A</mi></mrow></munder><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3a" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.cmml">⁡</mo><msub id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.2.cmml">p</mi><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.3.cmml">ϕ</mi></msub></mrow><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.3.cmml">a</mi><mo fence="false" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">ℱ</mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E6Xb.2.1.1.m1.1.1" xref="S4.E6Xb.2.1.1.m1.1.1.cmml">q</mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">∗</mo></msup><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E6Xb.2.1.1.m1.2.2.1.2" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6Xb.2.1.1.m1.2b"><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1"><eq id="S4.E6Xb.2.1.1.m1.2.2.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.2"></eq><csymbol cd="latexml" id="S4.E6Xb.2.1.1.m1.2.2.1.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.3">absent</csymbol><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1"><times id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.2"></times><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3"><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1">subscript</csymbol><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2"><times id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.1"></times><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.2">arg</ci><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.2.3">max</ci></apply><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3"><in id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.1"></in><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.2">𝑎</ci><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.1.3.3">𝐴</ci></apply></apply><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2">subscript</csymbol><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.2">𝑝</ci><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.3.2.3">italic-ϕ</ci></apply></apply><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.2">conditional</csymbol><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.3">𝑎</ci><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1"><times id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.3">ℱ</ci><interval closure="open" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1"><ci id="S4.E6Xb.2.1.1.m1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.1.1">𝑞</ci><apply id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">𝐺</ci><times id="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E6Xb.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3"></times></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6Xb.2.1.1.m1.2c">\displaystyle=\operatorname*{arg\,max}_{a\in A}p_{\phi}(a|\mathcal{F}(q,G^{*})),</annotation><annotation encoding="application/x-llamapun" id="S4.E6Xb.2.1.1.m1.2d">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_a ∈ italic_A end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_a | caligraphic_F ( italic_q , italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">where <math alttext="\mathcal{F}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.m1.2"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.2a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.2.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.2.cmml">ℱ</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.1.cmml">⁢</mo><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml"><mo id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">(</mo><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" lspace="0em" rspace="0em" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">⋅</mo><mo id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.2" rspace="0em" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">,</mo><mo id="S4.SS0.SSS0.Px3.p1.1.m1.2.2" lspace="0em" rspace="0em" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.2.cmml">⋅</mo><mo id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.2b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3"><times id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.1"></times><ci id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.2">ℱ</ci><interval closure="open" id="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.3.3.2"><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1">⋅</ci><ci id="S4.SS0.SSS0.Px3.p1.1.m1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.2c">\mathcal{F}(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.m1.2d">caligraphic_F ( ⋅ , ⋅ )</annotation></semantics></math> is a function that converts graph data into a form the generator can process.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Graph-Based Indexing</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The construction and indexing of graph databases form the foundation of GraphRAG, where the quality of the graph database directly impacts GraphRAG’s performance. In this section, we categorize and summarize the selection or construction of graph data and various indexing methods that have been employed.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Graph Data</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Various types of graph data are utilized in GraphRAG for retrieval and generation. Here, we categorize these data into two categories based on their sources, including Open Knowledge Graphs and Self-Constructed Graph Data.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Open Knowledge Graphs</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Open knowledge graphs refer to graph data sourced from publicly available repositories or databases <cite class="ltx_cite ltx_citemacro_citep">(Vrandečić and Krötzsch, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib164" title="">2014</a>; Bollacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib11" title="">2008a</a>; Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib5" title="">2007</a>; Suchanek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib151" title="">2007</a>)</cite>. Using these knowledge graphs could dramatically reduce the time and resources required to develop and maintain. In this survey, we further classify them into two categories according to their scopes, i.e., General Knowledge Graphs and Domain Knowledge Graphs.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) General Knowledge Graphs</h5>
<div class="ltx_para" id="S5.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.Px1.p1.1">General knowledge graphs primarily store general, structured knowledge, and typically rely on collective input and updates from a global community, ensuring a comprehensive and continually refreshed repository of information.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.Px1.p2.1">Encyclopedic knowledge graphs are a typical type of general knowledge graph, which contains large-scale real-world knowledge collected from human experts and encyclopedias. For example, Wikidata<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.wikidata.org/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Vrandečić and Krötzsch, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib164" title="">2014</a>)</cite> is a free and open knowledge base that stores structured data of its Wikimedia sister projects like Wikipedia, Wikivoyage, Wiktionary, and others. Freebase<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://www.freebase.be/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Bollacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib11" title="">2008a</a>)</cite> is an extensive, collaboratively edited knowledge base that compiles data from various sources, including individual contributions and structured data from databases like Wikipedia. DBpedia<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.dbpedia.org/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib5" title="">2007</a>)</cite> represents information about millions of entities, including people, places, and things, by leveraging the infoboxes and categories present in Wikipedia articles.
YAGO<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://yago-knowledge.org/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Suchanek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib151" title="">2007</a>)</cite> collects knowledge from Wikipedia, WordNet, and GeoNames.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.Px1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.Px1.p3.1">Commonsense knowledge graphs are another type of general knowledge graph. They include abstract commonsense knowledge, such as semantic associations between concepts and causal relationships between events. Typical Commonsense Knowledge Graphs include: ConceptNet<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://conceptnet.io/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Liu and Singh, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib101" title="">2004</a>)</cite> is a semantic network built from nodes representing words or phrases connected by edges denoting semantic relationships. ATOMIC <cite class="ltx_cite ltx_citemacro_citep">(Sap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib143" title="">2019a</a>; Hwang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib65" title="">2021</a>)</cite> models the causal relationships between events.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Domain Knowledge Graphs</h5>
<div class="ltx_para" id="S5.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS1.Px2.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S1" title="1. Introduction ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, domain-specific knowledge graphs are crucial for enhancing LLMs in addressing domain-specific questions. These KGs offer specialized knowledge in particular fields, aiding models in gaining deeper insights and a more comprehensive understanding of complex professional relationships.
In the biomedical field, CMeKG<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://cmekg.pcl.ac.cn/</span></span></span> encompasses a wide range of data, including diseases, symptoms, treatments, medications, and relationships between medical concepts. CPubMed-KG<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://cpubmed.openi.org.cn/graph/wiki</span></span></span> is a medical knowledge database in Chinese, building on the extensive repository of biomedical literature in PubMed.
In the movie domain, Wiki-Movies <cite class="ltx_cite ltx_citemacro_citep">(Miller et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib122" title="">2016</a>)</cite> extracts structured information from Wikipedia articles related to films, compiling data about movies, actors, directors, genres, and other relevant details into a structured format. Additionally, 

<cite class="ltx_cite ltx_citemacro_citet">Jin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>)</cite>
 construct a dataset named GR-Bench, which includes five domain knowledge graphs spanning academic, E-commerce, literature, healthcare, and legal fields.
Furthermore, 

<cite class="ltx_cite ltx_citemacro_citet">He et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>)</cite>
 convert triplet-format and JSON files from ExplaGraphs and SceneGraphs into a standard graph format and selects questions requiring 2-hop reasoning from WebQSP to create the universal graph-format dataset GraphQA for evaluating GraphRAG systems.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Self-Constructed Graph Data</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Self-Constructed Graph Data facilitates the customization and integration of proprietary or domain-specific knowledge into the retrieval process. For downstream tasks that do not inherently involve graph data, researchers often propose constructing a graph from multiple sources (e.g., documents, tables, and other databases) and leveraging GraphRAG to enhance task performance. Generally, these self-constructed graphs are closely tied to the specific design of the method, distinguishing them from the open-domain graph data previously mentioned.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">To model the structural relationships between the documents, 

<cite class="ltx_cite ltx_citemacro_citet">Munikoti et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib125" title="">2023</a>)</cite>
 propose to construct a heterogeneous document graph capturing multiple document-level relations, including co-citation, co-topic, co-venue, etc. 



<cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib97" title="">2024b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib173" title="">2024b</a>)</cite>
 establish relationship between passages according to shared keywords. To capture the relations between entities in documents, 



<cite class="ltx_cite ltx_citemacro_citet">Delile et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib27" title="">2024</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Edge et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Gutiérrez et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite>
 utilize the named entity recognition tools to extract entities from documents and language models to further extract relations between entities, where the retrieved entities and relations then form a knowledge graph. There are also some mapping methods for downstream tasks that need to be designed based on the characteristics of the task itself. For example, to solve the patent phrase similarity inference task, 

<cite class="ltx_cite ltx_citemacro_citet">Peng and Yang (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib134" title="">2024</a>)</cite>
 convert the patent database into a patent-phrase graph. Connections between patent nodes and phrase nodes are established if the phrases appear in the patents, while connections between patent nodes are based on citation relations. Targeting customer service technical support scenarios, 

<cite class="ltx_cite ltx_citemacro_citet">Xu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib184" title="">2024</a>)</cite>
 propose to model historical issues into a KG, which transforms the issues into tree representations to maintain the intra-issue relations, and utilize semantic similarities and a threshold to preserve inter-issue relations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Indexing</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Graph-Based Indexing plays a crucial role in enhancing the efficiency and speed of query operations on graph databases, directly influencing subsequent retrieval methods and granularity. Common graph-based indexing methods include graph indexing, text indexing, and vector indexing.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S5.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The overview of graph-based indexing.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Graph Indexing</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Graph indexing represents the most commonly used approach, preserving the entire structure of the graph. This method ensures that for any given node, all its edges and neighboring nodes are easily accessible. During subsequent retrieval stages, classic graph search algorithms such as BFS and Shortest Path Algorithms can be employed to facilitate retrieval tasks <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>; Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>; Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Text Indexing</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Text indexing involves converting graph data into textual descriptions to optimize retrieval processes. These descriptions are stored in a text corpus, where various text-based retrieval techniques, such as sparse retrieval and dense retrieval, can be applied. Some approaches transform knowledge graphs into human-readable text using predefined rules or templates. For instance, 



<cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Huang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib96" title="">2024a</a>)</cite>
 use predefined templates to convert each triple in knowledge graphs into natural language, while 

<cite class="ltx_cite ltx_citemacro_citet">Yu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib194" title="">2023</a>)</cite>
 merge triplets with the same head entity into passages. Additionally, some methods convert subgraph-level information into textual descriptions. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Edge et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>
 perform community detection on the graph and generate summaries for each community using LLMs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3. </span>Vector Indexing</h4>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">Vector indexing transforms graph data into vector representations to enhance retrieval efficiency, facilitating rapid retrieval and effective query processing. For example, entity linking can be seamlessly applied through query embeddings, and efficient vector search algorithms such as Locality Sensitive Hashing (LSH) <cite class="ltx_cite ltx_citemacro_citep">(Jafari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib67" title="">2021</a>)</cite> can be utilized. G-Retriever <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>)</cite> employs language models to encode textual information associated with each node and edge within the graph, while GRAG <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>)</cite> uses language models to convert <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS3.p1.1.m1.1"><semantics id="S5.SS2.SSS3.p1.1.m1.1a"><mi id="S5.SS2.SSS3.p1.1.m1.1.1" xref="S5.SS2.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p1.1.m1.1b"><ci id="S5.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS3.p1.1.m1.1d">italic_k</annotation></semantics></math>-hop ego networks into graph embeddings, thereby better preserving structural information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4. </span>Hybrid Indexing</h4>
<div class="ltx_para" id="S5.SS2.SSS4.p1">
<p class="ltx_p" id="S5.SS2.SSS4.p1.1">Each of the above three indexing methods offers distinct advantages: graph indexing facilitates easy access to structural information, text indexing simplifies retrieval of textual content, and vector indexing enables quick and efficient searches. Therefore, in practical applications, a hybrid approach combining these indexing methods is often preferred over relying solely on one. For instance, HybridRAG <cite class="ltx_cite ltx_citemacro_citep">(Sarmah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib145" title="">2024</a>)</cite> retrieves both vector and graph data simultaneously to enhance the content retrieved. While EWEK-QA <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite> uses both text and knowledge graphs.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S5.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The general architectures of graph-based retrieval.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Graph-Guided Retrieval</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In GraphRAG, the retrieval process is crucial for ensuring the quality and relevance of generated outputs by extracting pertinent and high-quality graph data from external graph databases. However, retrieving graph data presents two significant challenges: (1) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">Explosive Candidate Subgraphs</em>: As the graph size increases, the number of candidate subgraphs grows exponentially, requiring heuristic search algorithms to efficiently explore and retrieve relevant subgraphs. (2) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.2">Insufficient Similarity Measurement</em>: Accurately measuring similarity between textual queries and graph data necessitates the development of algorithms capable of understanding both textual and structural information.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Considerable efforts have previously been dedicated to optimizing the retrieval process to address the above challenges. This survey focuses on examining various aspects of the retrieval process within GraphRAG, including the selection of the retriever, retrieval paradigm, retrieval granularity, and effective enhancement techniques. The general architectures of Graph-Guided Retrieval are depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S5.F4" title="Figure 4 ‣ 5.2.4. Hybrid Indexing ‣ 5.2. Indexing ‣ 5. Graph-Based Indexing ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Retriever</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">In GraphRAG, various retrievers possess unique strengths for addressing different aspects of retrieval tasks. We categorize retrievers into three types based on their underlying models: Non-parametric Retriever, LM-based Retriever, and GNN-based Retriever. It is important to note that models used in pre-processing steps, such as query encoding and entity linking, are not considered here, as these models vary across different methods and are not the primary focus of this paper.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Non-parametric Retriever</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Non-parametric retrievers, based on heuristic rules or traditional graph search algorithms, do not rely on deep-learning models, thereby achieving high retrieval efficiency. For instance, 



<cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 retrieve <math alttext="k" class="ltx_Math" display="inline" id="S6.SS1.SSS1.p1.1.m1.1"><semantics id="S6.SS1.SSS1.p1.1.m1.1a"><mi id="S6.SS1.SSS1.p1.1.m1.1.1" xref="S6.SS1.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p1.1.m1.1b"><ci id="S6.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS1.p1.1.m1.1d">italic_k</annotation></semantics></math>-hop paths containing the topic entities of each question-choice pair. G-Retriever <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>)</cite> enhances the conventional Prize-Collecting Steiner Tree (PCST) algorithm by incorporating edge prices and optimizing relevant subgraph extraction. 



<cite class="ltx_cite ltx_citemacro_citet">Delile et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib27" title="">2024</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Mavromatis and Karypis (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite>
 first extract entities mentioned in the query and then retrieve the shortest path related to these entities. These methods often involve an entity linking pre-processing step to identify nodes in the graph before retrieval.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>LM-based Retriever</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">LMs serve as effective retrievers in GraphRAG due to their strong natural language understanding capabilities. These models excel in processing and interpreting diverse natural language queries, making them versatile for a wide range of retrieval tasks within graph-based frameworks. We primarily categorized LMs into two types: discriminative and generative language models. Subgraph Retriever <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite> trains RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib108" title="">2019</a>)</cite> as the retriever, which expands from the topic entity and retrieves the relevant paths in a sequential decision process. KG-GPT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite> adopts LLMs to generate the set of top-<math alttext="K" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p1.1.m1.1"><semantics id="S6.SS1.SSS2.p1.1.m1.1a"><mi id="S6.SS1.SSS2.p1.1.m1.1.1" xref="S6.SS1.SSS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p1.1.m1.1b"><ci id="S6.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS2.p1.1.m1.1d">italic_K</annotation></semantics></math> relevant relations of the specific entity. 

<cite class="ltx_cite ltx_citemacro_citet">Wold et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib177" title="">2023</a>)</cite>
 utilize fine-tuned GPT-2 to generate reasoning paths. StructGPT <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>)</cite> utilizes LLMs to automatically invoke several pre-defined functions, by which relevant information can be retrieved and combined to assist further reasoning.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>GNN-based Retriever</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">GNNs are adept at understanding and leveraging complex graph structures. GNN-based retrievers typically encode graph data and subsequently score different retrieval granularities based on their similarity to the query. For example, GNN-RAG <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> first encodes the graph, assigns a score to each entity, and retrieves entities relevant to the query based on a threshold. EtD <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib100" title="">2024e</a>)</cite> iterates multiple times to retrieve relevant paths. During each iteration, it first uses LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib161" title="">2023</a>)</cite> to select edges connecting the current node, then employs GNNs to obtain embeddings of the new layer of nodes for the next round of LLM selection.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4. </span>Discussion</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">During the retrieval process, non-parametric retrievers exhibit good retrieval efficiency, but they may suffer from inaccurate retrieval due to a lack of training on downstream tasks. Meanwhile, although LM-based retrievers and GNN-based retrievers offer higher retrieval accuracy, they require significant computational overhead. Considering this complementarity, many methods propose hybrid retrieval approaches to improve both retrieval efficiency and accuracy. Many approaches adopt a multi-stage retrieval strategy, employing different models at each stage. For example, RoG <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite> first utilizes LLMs to generate planning paths and then extracts paths satisfying the planning paths from knowledge graphs. GenTKGQA <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib45" title="">2024a</a>)</cite> infers crucial relations and constraints from the query using LLMs and extracts triplets according to these constraints.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Retrieval Paradigm</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Within GraphRAG, different retrieval paradigms, including once retrieval, iterative retrieval, and multi-stage retrieval, play crucial roles in improving the relevance and depth of the retrieved information. Once retrieval aims to gather all pertinent information in a single operation. Iterative retrieval conducts further searches based on previously retrieved information, progressively narrowing down to the most relevant results. Here we further divide iterative retrieval into adaptive retrieval and non-adaptive retrieval, with the only difference lying in whether the stopping of the retrieval is determined by the model. Another retrieval paradigm is multi-stage retrieval, where retrieval is divided into multiple stages. Different types of retrievers may be employed at each stage for more precise and diversified search results. Below, we will provide a detailed introduction to these types of retrieval paradigms.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Once Retrieval</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.2">Once retrieval aims to retrieve all the relevant information in a single query. One category of approaches <cite class="ltx_cite ltx_citemacro_citep">(Gutiérrez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>)</cite> utilize embedding similarities to retrieve the most relevant pieces of information. Another category of methods design pre-defined rules or patterns to directly extract specific structured information such as triplets, paths or subgraphs from graph databases. For example, G-Retriever <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>)</cite> utilizes an extended PCST algorithm to retrieve the most relevant subgraph. KagNet <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib98" title="">2019</a>)</cite> extracts paths between all pairs of topic entities with lengths not exceeding <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.1.m1.1"><semantics id="S6.SS2.SSS1.p1.1.m1.1a"><mi id="S6.SS2.SSS1.p1.1.m1.1.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.1.m1.1b"><ci id="S6.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.1.m1.1d">italic_k</annotation></semantics></math>. 



<cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 extract the subgraph that contains all topic entities along with their <math alttext="2" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.2.m2.1"><semantics id="S6.SS2.SSS1.p1.2.m2.1a"><mn id="S6.SS2.SSS1.p1.2.m2.1.1" xref="S6.SS2.SSS1.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.2.m2.1b"><cn id="S6.SS2.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S6.SS2.SSS1.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.2.m2.1d">2</annotation></semantics></math>-hop neighbors.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">Furthermore, in this subsection, we also include some multiple retrieval methods that involve decoupled and independent retrievals, allowing them to be computed in parallel and executed only once. For example, 



<cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Cheng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib21" title="">2024</a>)</cite>
 first instruct LLMs to generate multiple reasoning paths and then use a BFS retriever to sequentially search for subgraphs in the knowledge graphs that match each path. KG-GPT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite> decomposes the original query into several sub-queries, retrieving relevant information for each sub-query in a single retrieval process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Iterative Retrieval</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">In iterative retrieval, multiple retrieval steps are employed, with subsequent searches depending on the results of prior retrievals. These methods aim to deepen the understanding or completeness of the retrieved information over successive iterations. In this survey, we further classify iterative retrieval into two categories: (1) non-adaptive and (2) adaptive retrieval. We provide a detailed summary of these two categories of methods below.</p>
</div>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Non-Adaptive Retrieval</h5>
<div class="ltx_para" id="S6.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px1.p1.1">Non-adaptive methods typically follow a fixed sequence of retrieval, and the termination of retrieval is determined by setting a maximum time or a threshold. For example, PullNet <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite> retrieves problem-relevant subgraphs through <math alttext="T" class="ltx_Math" display="inline" id="S6.SS2.SSS2.Px1.p1.1.m1.1"><semantics id="S6.SS2.SSS2.Px1.p1.1.m1.1a"><mi id="S6.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S6.SS2.SSS2.Px1.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.Px1.p1.1.m1.1b"><ci id="S6.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS2.Px1.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.Px1.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.Px1.p1.1.m1.1d">italic_T</annotation></semantics></math> iterations. In each iteration, the paper designs a retrieval rule to select a subset of retrieved entities, and then expands these entities by searching relevant edges in the knowledge graph. In each iteration, KGP <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib173" title="">2024b</a>)</cite> first selects seed nodes based on the similarity between the context and the nodes in the graph. It then uses LLMs to summarize and update the context of the neighboring nodes of the seed nodes, which is utilized in the subsequent iteration.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Adaptive Retrieval</h5>
<div class="ltx_para" id="S6.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px2.p1.1">One distinctive characteristic of adaptive retrieval is to let models autonomously determine the optimal moments to finish the retrieval activities. For instance, <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite> leverage an LM for hop prediction, which serves as an indicator to end the retrieval. There is also a group of researchers who utilize model-generated special tokens or texts as termination signals for the retrieval process. For example, ToG <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite> prompts the LLM agent to explore the multiple possible reasoning paths until the LLM determines the question can be answered based on the current reasoning path. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite> trains a RoBERTa to expand a path from each topic entity. In the process, a virtual relation named as “[END]” is introduced to terminate the retrieval process.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.Px2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.Px2.p2.1">Another common approach involves treating the large model as an agent, enabling it to directly generate answers to questions to signal the end of iteration. For instance, <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib171" title="">2023d</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite> propose LLM-based agents to reason on graphs. These agents could autonomously determine the information for retrieval, invoke the pre-defined retrieval tools, and cease the retrieval process based on the retrieved information.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3. </span>Multi-Stage Retrieval</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.4">Multi-stage retrieval divides the retrieval process linearly into multiple stages, with additional steps such as retrieval enhancement, and even generation processes occurring between these stages. In multi-stage retrieval, different stages may employ various types of retrievers, which enables the system to incorporate various retrieval techniques tailored to different aspects of the query. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite>
 first utilize a non-parametric retriever to extract <math alttext="n" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p1.1.m1.1"><semantics id="S6.SS2.SSS3.p1.1.m1.1a"><mi id="S6.SS2.SSS3.p1.1.m1.1.1" xref="S6.SS2.SSS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p1.1.m1.1b"><ci id="S6.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p1.1.m1.1d">italic_n</annotation></semantics></math>-hop paths of entities in the query’s reasoning chain, then after a pruning stage, it further retrieves the one-hop neighbors of the entities in the pruned subgraph. OpenCSR <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib54" title="">2023</a>)</cite> divides the retrieval process into two stages. In the first stage, it retrieves all <math alttext="1" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p1.2.m2.1"><semantics id="S6.SS2.SSS3.p1.2.m2.1a"><mn id="S6.SS2.SSS3.p1.2.m2.1.1" xref="S6.SS2.SSS3.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p1.2.m2.1b"><cn id="S6.SS2.SSS3.p1.2.m2.1.1.cmml" type="integer" xref="S6.SS2.SSS3.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p1.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p1.2.m2.1d">1</annotation></semantics></math>-hop neighbors of the topic entity. In the second stage, it compares the similarity between these neighbor nodes and other nodes, selecting the top-<math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p1.3.m3.1"><semantics id="S6.SS2.SSS3.p1.3.m3.1a"><mi id="S6.SS2.SSS3.p1.3.m3.1.1" xref="S6.SS2.SSS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p1.3.m3.1b"><ci id="S6.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S6.SS2.SSS3.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p1.3.m3.1d">italic_k</annotation></semantics></math> nodes with the highest similarity for retrieval. GNN-RAG <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> first employs GNNs to retrieve the top-<math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p1.4.m4.1"><semantics id="S6.SS2.SSS3.p1.4.m4.1a"><mi id="S6.SS2.SSS3.p1.4.m4.1.1" xref="S6.SS2.SSS3.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p1.4.m4.1b"><ci id="S6.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S6.SS2.SSS3.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p1.4.m4.1d">italic_k</annotation></semantics></math> nodes most likely to be the answer. Subsequently, it retrieves all shortest paths between query entities and answer entities pairwise.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.4. </span>Discussion</h4>
<div class="ltx_para" id="S6.SS2.SSS4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.p1.1">In GraphRAG, once retrieval typically exhibits lower complexity and shorter response times, making it suitable for scenarios requiring real-time responsiveness. In contrast, iterative retrieval often involves higher time complexity, especially when employing LLMs as retrievers, potentially leading to longer processing times. However, this approach can yield higher retrieval accuracy by iteratively refining retrieved information and generating responses. Therefore, the choice of retrieval paradigm should balance accuracy and time complexity based on specific use cases and requirements.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Retrieval Granularity</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">According to different task scenarios and indexing types, researchers design distinct retrieval granularities (i.e., the form of related knowledge retrieved from graph data), which can be divided into nodes, triplets, paths, and subgraphs. Each retrieval granularity has its own advantages, making it suitable for different practical scenarios. We will introduce the details of these granularities in the following sections.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Nodes</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">Nodes allow for precise retrieval focused on individual elements within the graph, which is ideal for targeted queries and specific information extraction. In general, for knowledge graphs, nodes refer to entities. For other types of text attribute graphs, nodes may include textual information that describes the node’s attributes. By retrieving nodes within the graph, GraphRAG systems could provide detailed insights into their attributes, relationships, and contextual information. For example, 



<cite class="ltx_cite ltx_citemacro_citet">Munikoti et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib125" title="">2023</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib97" title="">2024b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib173" title="">2024b</a>)</cite>
 construct document graphs and retrieves relevant passage nodes. 



<cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib100" title="">2024e</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Sun et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Gutiérrez et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>)</cite>
 retrieve entities from constructed knowledge graphs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Triplets</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">Generally, triplets consist of entities and their relationships in the form of subject-predicate-object tuples, providing a structured representation of relational data within a graph. The structured format of triplets allows for clear and organized data retrieval, making it advantageous in scenarios where understanding relationships and contextual relevance between entities is critical. 

<cite class="ltx_cite ltx_citemacro_citet">Yang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib186" title="">2024a</a>)</cite>
 retrieve triplets containing topic entities as relevant information. 



<cite class="ltx_cite ltx_citemacro_citet">Huang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib96" title="">2024a</a>)</cite>
 first convert each triplet of graph data into textual sentences using predefined templates and subsequently adopt a text retriever to extract relevant triplets. However, directly retrieving triplets from graph data may still lack contextual breadth and depth, thus being unable to capture indirect relationships or reasoning chains. To address this challenge, 

<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>)</cite>
 propose to generate the logical chains based on the original question, and retrieve the relevant triplets of each logical chain.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.3. </span>Paths</h4>
<div class="ltx_para" id="S6.SS3.SSS3.p1">
<p class="ltx_p" id="S6.SS3.SSS3.p1.1">The retrieval of path-granularity data can be seen as capturing sequences of relationships between entities, enhancing contextual understanding and reasoning capabilities. In GraphRAG, retrieving paths offers distinct advantages due to their ability to capture complex relationships and contextual dependencies within a graph.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS3.p2">
<p class="ltx_p" id="S6.SS3.SSS3.p2.1">However, path retrieval can be challenging due to the exponential growth in possible paths as graph size increases, which escalates computational complexity. To address this, some methods retrieve relevant paths based on pre-defined rules. For example, 



<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Lo and Lim (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib109" title="">2023</a>)</cite>
 first select entity pairs in the query and then traverse to find all the paths between them within <math alttext="n" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p2.1.m1.1"><semantics id="S6.SS3.SSS3.p2.1.m1.1a"><mi id="S6.SS3.SSS3.p2.1.m1.1.1" xref="S6.SS3.SSS3.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.1.m1.1b"><ci id="S6.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS3.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p2.1.m1.1d">italic_n</annotation></semantics></math>-hop. HyKGE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>)</cite> first defines three types of paths: path, co-ancestor chain, and co-occurrence chain, and then utilizes corresponding rules to retrieve each of these three types of paths. In addition, some methods utilize models to perform path searching on graphs. ToG <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite> proposes to prompt the LLM agent to perform the beam search on KGs and find multiple possible reasoning paths that help answer the question. 



<cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Guo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite>
 first utilizes the model to generate faithful reasoning plans and then retrieves relevant paths based on these plans. GNN-RAG <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> first identifies the entities in the question. Subsequently, all paths between entities that satisfy a certain length relationship are extracted.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.4. </span>Subgraphs</h4>
<div class="ltx_para" id="S6.SS3.SSS4.p1">
<p class="ltx_p" id="S6.SS3.SSS4.p1.1">Retrieving subgraphs offers significant advantages due to its ability to capture comprehensive relational contexts within a graph. This granularity enables GraphRAG to extract and analyze complex patterns, sequences, and dependencies embedded within larger structures, facilitating deeper insights and a more nuanced understanding of semantic connections.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS4.p2">
<p class="ltx_p" id="S6.SS3.SSS4.p2.1">To ensure both information completeness and retrieval efficiency, some methods propose an initial rule-based approach to retrieve candidate subgraphs, which are subsequently refined or processed further. 

<cite class="ltx_cite ltx_citemacro_citet">Peng and Yang (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib134" title="">2024</a>)</cite>
 retrieve the ego graph of the patent phrase from the self-constructed patent-phrase graph. 



<cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Feng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib41" title="">2020</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 first select the topic entities and their two-hop neighbors as the node set, and then choose the edges with head and tail entities both in the node set to form the subgraph. Besides, there are also some embedding-based subgraph retrieval methods. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Hu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>)</cite>
 first encode all the <math alttext="k" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p2.1.m1.1"><semantics id="S6.SS3.SSS4.p2.1.m1.1a"><mi id="S6.SS3.SSS4.p2.1.m1.1.1" xref="S6.SS3.SSS4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p2.1.m1.1b"><ci id="S6.SS3.SSS4.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS4.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p2.1.m1.1d">italic_k</annotation></semantics></math>-hop ego networks from the graph database, then retrieve subgraphs related to the query based on the similarities between embeddings. 



<cite class="ltx_cite ltx_citemacro_citet">Wen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib176" title="">2024</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite>
 extract two types of graphs, including Path evidence subgraphs and Neighbor evidence subgraphs, based on pre-defined rules. OpenCSR <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib54" title="">2023</a>)</cite> starts from a few initial seed nodes and gradually expands to new nodes, eventually forming a subgraph.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS4.p3">
<p class="ltx_p" id="S6.SS3.SSS4.p3.1">In addition to the aforementioned direct subgraph retrieval methods, some works propose first retrieving relevant paths and then constructing related subgraphs from them. For instance, 

<cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>
 train a RoBERTa model to identify multiple reasoning paths through a sequential decision process, subsequently merging identical entities from different paths to induce a final subgraph.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.5. </span>Hybrid Granularties</h4>
<div class="ltx_para" id="S6.SS3.SSS5.p1">
<p class="ltx_p" id="S6.SS3.SSS5.p1.1">Considering the advantages and disadvantages of various retrieval granularities mentioned above, some researchers propose using hybrid granularities, that is, retrieving relevant information of multiple granularities from graph data. This type of granularity enhances the system’s ability to capture both detailed relationships and broader contextual understanding, thus reducing noise while improving the relevance of the retrieved data. Various previous works propose to utilize LLM agents to retrieve complex hybrid information. 



<cite class="ltx_cite ltx_citemacro_citet">Jin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib171" title="">2023d</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Sun et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>
 propose to adopt LLM-based agents for adaptively selecting nodes, triplets, paths, and subgraphs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.6. </span>Discussion</h4>
<div class="ltx_para" id="S6.SS3.SSS6.p1">
<p class="ltx_p" id="S6.SS3.SSS6.p1.1">(1) In real applications, there are no clear boundaries between these retrieval granularities, as subgraphs can be composed of multiple paths, and paths can be formed by several triplets. (2) Various granularities such as nodes, triplets, paths, and subgraphs offer distinct advantages in the GraphRAG process. Balancing between retrieval content and efficiency is crucial when selecting the granularity, depending on the specific context of the task. For straightforward queries or when efficiency is paramount, finer granularities such as entities or triplets may be preferred to optimize retrieval speed and relevance. In contrast, complex scenarios often benefit from a hybrid approach that combines multiple granularities. This approach ensures a more comprehensive understanding of the graph structure and relationships, enhancing the depth and accuracy of the generated responses. Thus, GraphRAG’s flexibility in granularity selection allows it to adapt effectively to diverse information retrieval needs across various domains.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Retrieval Enhancement</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">To ensure high retrieval quality, researchers propose techniques to enhance both user queries and the knowledge retrieved. In this paper, we categorize query enhancement into query expansion and query decomposition, and knowledge enhancement into merging and pruning. These strategies collectively optimize the retrieval process. Although other techniques such as query rewriting <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib115" title="">2023</a>; Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib133" title="">2024</a>; Mao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib118" title="">2024b</a>; Qiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib138" title="">2024</a>)</cite> are commonly used in RAG, they are less frequently applied in GraphRAG. We do not delve into these methods, despite their potential adaptation for GraphRAG.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.1. </span>Query Enhancement</h4>
<div class="ltx_para" id="S6.SS4.SSS1.p1">
<p class="ltx_p" id="S6.SS4.SSS1.p1.1">Strategies applied to queries typically involve pre-processing techniques that enrich the information for better retrieval. This may include query expansion and query decomposition.</p>
</div>
<section class="ltx_paragraph" id="S6.SS4.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Query Expansion</h5>
<div class="ltx_para" id="S6.SS4.SSS1.Px1.p1">
<p class="ltx_p" id="S6.SS4.SSS1.Px1.p1.1">Due to the generally short length of queries and their limited information content, query expansion aims to improve search results by supplementing or refining the original query with additional relevant terms or concepts. 

<cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>
 generate relation paths grounded by KGs with LLMs to enhance the retrieval query. 

<cite class="ltx_cite ltx_citemacro_citet">Cheng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib21" title="">2024</a>)</cite>
 adopt SPARQL to get all the aliases of the query entities from Wikidata to augment the retrieval queries, which capture lexical variations of the same entity. 

<cite class="ltx_cite ltx_citemacro_citet">Huang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite>
 propose a consensus-view knowledge retrieval method to improve retrieval accuracy, which first discover semantically relevant queries, and then re-weight the original query terms to enhance the retrieval performance. HyKGE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>)</cite> utilizes a large model to generate the hypothesis output of the question, concatenating the hypothesis output with the query as input to the retriever. Golden-Retriever <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib3" title="">2024</a>)</cite> first recognizes the jargon in the query and then retrieves explanations of the jargon as a supplement to the query.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS4.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Query Decomposition</h5>
<div class="ltx_para" id="S6.SS4.SSS1.Px2.p1">
<p class="ltx_p" id="S6.SS4.SSS1.Px2.p1.1">Query decomposition techniques break down or decompose the original user query into smaller, more specific sub-queries. Each sub-query typically focuses on a particular aspect or component of the original query, which successfully alleviates the complexity and ambiguity of language queries. For instance, <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>; Choudhary and Reddy, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib23" title="">2024</a>)</cite> breaks down the primary question into sub-sentences, each representing a distinct relation, and sequentially retrieves the pertinent triplets for each sub-sentence.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.2. </span>Knowledge Enhancement</h4>
<div class="ltx_para" id="S6.SS4.SSS2.p1">
<p class="ltx_p" id="S6.SS4.SSS2.p1.1">After retrieving initial results, knowledge enhancement strategies are employed to refine and improve the retriever’s results. This phase often involves knowledge merging and knowledge pruning processes to present the most pertinent information prominently. These techniques aim to ensure that the final set of retrieved results is not only comprehensive but also highly relevant to the user’s information needs.</p>
</div>
<section class="ltx_paragraph" id="S6.SS4.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Knowledge Merging</h5>
<div class="ltx_para" id="S6.SS4.SSS2.Px1.p1">
<p class="ltx_p" id="S6.SS4.SSS2.Px1.p1.1">Knowledge merging retrieved information enables compression and aggregation of information, which assists in obtaining a more comprehensive view by consolidating relevant details from multiple sources. This approach not only enhances the completeness and coherence of the information but also mitigates issues related to input length constraints in models. KnowledgeNavigator <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite> merges nodes and condenses the retrieved
sub-graph through triple aggregation to enhance the
reasoning efficiency. In Subgraph Retrieval <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>, after retrieving top-<math alttext="k" class="ltx_Math" display="inline" id="S6.SS4.SSS2.Px1.p1.1.m1.1"><semantics id="S6.SS4.SSS2.Px1.p1.1.m1.1a"><mi id="S6.SS4.SSS2.Px1.p1.1.m1.1.1" xref="S6.SS4.SSS2.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.Px1.p1.1.m1.1b"><ci id="S6.SS4.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S6.SS4.SSS2.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.SSS2.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> paths from each topic entity to form a single subgraph, researchers propose to merge the same entities from different subgraphs to form the final subgraph. 



<cite class="ltx_cite ltx_citemacro_citet">Wen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib176" title="">2024</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite>
 merge retrieved subgraphs based on relations, combining head entities and tail entities that satisfy the same relation into two distinct entity sets, ultimately forming a relation paths.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS4.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Knowledge Pruning</h5>
<div class="ltx_para" id="S6.SS4.SSS2.Px2.p1">
<p class="ltx_p" id="S6.SS4.SSS2.Px2.p1.1">Knowledge pruning involves filtering out less relevant or redundant retrieved information to refine the results. Previous approaches for pruning encompass two main categories: (re)-ranking-based approaches and LLM-based approaches. (Re)-ranking methods involve the reordering or prioritization of retrieved information using tailored metrics or criteria.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS2.Px2.p2">
<p class="ltx_p" id="S6.SS4.SSS2.Px2.p2.1">One line of methods introduces stronger models for reranking. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>
 concatenate each retrieved triplet with the question-choice pair, and adopt a pre-trained cross-encoder <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib141" title="">2019</a>)</cite> to re-rank the retrieved triplets. 

<cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>)</cite>
 utilize the FlagEmbedding to encode the text to re-rank top-k documents returned by embedding model “bge_reranker_large”. 

<cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib102" title="">2024b</a>)</cite>
 train a PLM to</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS2.Px2.p3">
<p class="ltx_p" id="S6.SS4.SSS2.Px2.p3.1">Another category utilizes the similarity between queries and retrieved information for ranking. For instance, 

<cite class="ltx_cite ltx_citemacro_citet">Cheng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib21" title="">2024</a>)</cite>
 re-rank the candidate subgraphs based on the similarity for both relation and fine-grained concept between subgraphs and the query. 

<cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 first cluster the 2-hop neighbors and then delete the cluster with the lowest similarity score with the input query. 

<cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>
 prune the retrieved subgraph according to the relevance score between the question context and the KG entity nodes calculated by a pre-trained language model. 



<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Gutiérrez et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib111" title="">2023</a>)</cite>
 adopt Personalized PageRank algorithm to rank the retrieved candidate information for further filtering. 

<cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib102" title="">2024b</a>)</cite>
 trains a PLM to score the similarity between the retrieved information and the query, and rerank the retrieved paths based on the similarity score. G-G-E <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib44" title="">2022</a>)</cite> first divides the retrieved subgraph into several smaller subgraphs, then compares the similarity between each smaller subgraph and the query. Subgraphs with low similarity are removed, and the remaining smaller subgraphs are merged into a larger subgraph.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS2.Px2.p4">
<p class="ltx_p" id="S6.SS4.SSS2.Px2.p4.1">Additionally, a third category of methods proposes new metrics for reranking. For example,  

<cite class="ltx_cite ltx_citemacro_citet">Munikoti et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib125" title="">2023</a>)</cite>
 propose a metric that measures both the impact and recency of the retrieved text chunks. KagNet <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib98" title="">2019</a>)</cite> decomposes the retrieved paths into triplets and reranks the paths based on the confidence score measured by the knowledge graph embedding (KGE) techniques. LLM-based methods excel in capturing complex linguistic patterns and semantic nuances, which enhances their ability to rank search results or generate responses more accurately. To avoid introducing noisy information,  



<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Kim et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite>
 propose to prune the irrelevant graph data by calling LLMs to check.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S6.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The overview of graph-enhanced generation.</figcaption>
</figure>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Graph-Enhanced Generation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The generation stage is another crucial step in GraphRAG, aimed at integrating the retrieved graph data with the query to enhance response quality. In this stage, suitable generation models must be selected based on the downstream tasks. The retrieved graph data is then transformed into formats compatible with the generators. The generator takes both the query and the transformed graph data as inputs to produce the final response. Beyond these fundamental processes, generative enhancement techniques can further improve the output by intensifying the interaction between the query and the graph data and enriching the content generation itself. The organization of this section and the overview of graph-enhanced generation are depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S6.F5" title="Figure 5 ‣ (2) Knowledge Pruning ‣ 6.4.2. Knowledge Enhancement ‣ 6.4. Retrieval Enhancement ‣ 6. Graph-Guided Retrieval ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Generators</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">The selection of generators often depends on the type of downstream task at hand. For discriminative tasks (e.g., multi-choice question answering) or generative tasks that can be formulated as discriminative tasks (e.g., KBQA), one can utilize GNNs or discriminative language models to learn representations of the data. These representations can then be mapped to the logits associated with different answer options to provide responses. Alternatively, generative language models can be employed to directly generate answers. For generative tasks, however, the use of GNNs and discriminative language models alone is insufficient. These tasks require the generation of text, which necessitates the deployment of decoders.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1. </span>GNNs</h4>
<div class="ltx_para" id="S7.SS1.SSS1.p1">
<p class="ltx_p" id="S7.SS1.SSS1.p1.1">Due to the powerful representational capabilities of GNNs for graph data, they are particularly effective for discriminative tasks. GNNs can directly encode graph data, capturing complex relationships and node features inherent in the graph structure. This encoding is then processed through a Multi-Layer Perceptron (MLP) to generate predictive outcomes. These approaches primarily utilize classical GNN models (e.g., GCN <cite class="ltx_cite ltx_citemacro_citep">(Kipf and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib84" title="">2017</a>)</cite>, GAT <cite class="ltx_cite ltx_citemacro_citep">(Veličković et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib163" title="">2018</a>)</cite>, GraphSAGE <cite class="ltx_cite ltx_citemacro_citep">(Hamilton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib53" title="">2017</a>)</cite>, and Graph Transformers <cite class="ltx_cite ltx_citemacro_citep">(Shehzad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib148" title="">2024</a>)</cite>), either in their original form or modified to better align with downstream tasks. For example, HamQA <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib31" title="">2023b</a>)</cite> designs a hyperbolic GNN to learn the representations of retrieved graph data, which learns from the mutual hierarchical information between query and graphs. 

<cite class="ltx_cite ltx_citemacro_citet">Sun et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib153" title="">2018</a>)</cite>
 compute PageRank scores for neighboring nodes and aggregates them weighted by these scores, during message-passing. This approach enhances the central node’s ability to assimilate information from its most relevant neighboring nodes. 

<cite class="ltx_cite ltx_citemacro_citet">Mavromatis and Karypis (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib119" title="">2022</a>)</cite>
 decode the query into several vectors (instructions), and enhances instruction decoding and execution for effective reasoning by emulating breadth-first search (BFS) with GNNs to improve instruction execution and using adaptive reasoning to update the instructions with KG-aware information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2. </span>LMs</h4>
<div class="ltx_para" id="S7.SS1.SSS2.p1">
<p class="ltx_p" id="S7.SS1.SSS2.p1.1">LMs possess strong capabilities in text understanding, which also allows them to function as generators. In the context of integrating LMs with graph data, it is necessary to first convert the retrieved graph data into specific graph formats. This conversion process ensures that the structured information is effectively understood and utilized by the LMs. These formats, which will be elaborated on in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.SS2" title="7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">7.2</span></a>, are crucial for preserving the relational and hierarchical structure of the graph data, thereby enhancing the model’s ability to interpret complex data types. Once the graph data is formatted, it is then combined with a query and fed into an LM.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p2">
<p class="ltx_p" id="S7.SS1.SSS2.p2.1">For encoder-only models, such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib29" title="">2019</a>)</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib108" title="">2019</a>)</cite>, their primary use is in discriminative tasks. Similar to GNNs, these models first encode the input text and then utilize MLPs to map it to the answer space <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>. On the other hand, encoder-decoder and decoder-only models, such as T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib139" title="">2020</a>)</cite>, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib128" title="">2024</a>)</cite>, and LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib32" title="">2024</a>)</cite>, are adept at both discriminative and generative tasks. These models excel in text understanding, generation, and reasoning, allowing them to process textual inputs directly and generate textual responses <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3. </span>Hybrid Models</h4>
<div class="ltx_para" id="S7.SS1.SSS3.p1">
<p class="ltx_p" id="S7.SS1.SSS3.p1.1">Considering the strengths of GNNs at representing the structure of graph data, and the robust understanding of text demonstrated by LMs, many studies are exploring the integration of these two technologies to generate coherent responses. This paper categorizes the hybrid generative approaches into two distinct types: cascaded paradigm and parallel paradigm.</p>
</div>
<section class="ltx_paragraph" id="S7.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Cascaded Paradigm</h5>
<div class="ltx_para" id="S7.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S7.SS1.SSS3.Px1.p1.1">In the cascaded approaches, the process involves a sequential interaction where the output from one model serves as the input for the next. Specifically, the GNN processes the graph data first, encapsulating its structural and relational information into a form that the LM can understand. Subsequently, this transformed data is fed into the LM, which then generates the final text-based response. These methods leverage the strengths of each model in a step-wise fashion, ensuring detailed attention to both structural and textual data.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.Px1.p2">
<p class="ltx_p" id="S7.SS1.SSS3.Px1.p2.1">In these methods, prompt tuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib107" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib106" title="">2022</a>; Li and Liang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib92" title="">2021</a>; Lester et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib89" title="">2021</a>)</cite> is a typical approach, where GNNs are commonly employed to encode the retrieved graph data. The encoded graph data is subsequently pre-pended as a prefix to the input text embeddings of an LM. The GNN is then optimized through downstream tasks to produce enhanced encodings of the graph data <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib198" title="">2024b</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib45" title="">2024a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Parallel Paradigm</h5>
<div class="ltx_para" id="S7.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S7.SS1.SSS3.Px2.p1.1">On the other hand, the parallel approach operates by concurrently utilizing the capabilities of both the GNN and the LLM. In this setup, both models receive the initial inputs simultaneously and work in tandem to process different facets of the same data. The outputs are then merged, often through another model or a set of rules, to produce a unified response that integrates insights from both the graphical structure and the textual content.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.Px2.p2">
<p class="ltx_p" id="S7.SS1.SSS3.Px2.p2.1">In the parallel paradigm, a typical approach involves separately encoding inputs using both GNNs and LMs, followed by integrating these two representations, or directly integrating their output responses. For instance, 

<cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib69" title="">2022</a>)</cite>
 aggregate predictions from GNNs and LMs by weighted summation to obtain the final answer. 



<cite class="ltx_cite ltx_citemacro_citet">Lin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib98" title="">2019</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Pahuja et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib130" title="">2023</a>)</cite>
 integrate the graph representations derived from GNNs and the text representations generated by LMs using attention mechanisms. 



<cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Munikoti et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib125" title="">2023</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 directly concatenate graph representations with text representations.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.Px2.p3">
<p class="ltx_p" id="S7.SS1.SSS3.Px2.p3.1">Another approach involves designing dedicated modules that integrate GNNs with LMs, enabling the resulting representations to encapsulate both structural and textual information. For instance, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib200" title="">2022a</a>)</cite> introduce a module called the GreaseLM Layer, which incorporates both GNN and LM layers. At each layer, this module integrates textual and graph representations using a two-layer MLP before passing them to the next layer. Similarly, ENGINE <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib205" title="">2024</a>)</cite> proposes G-Ladders, which combine LMs and GNNs through a side structure, enhancing node representations for downstream tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS3.Px3">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>
<div class="ltx_para" id="S7.SS1.SSS3.Px3.p1">
<p class="ltx_p" id="S7.SS1.SSS3.Px3.p1.1">Hybrid models that harness both the representation capabilities of GNNs for graph data and LMs for text data hold promising applications. However, effectively integrating information from these two modalities remains a significant challenge.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Graph Formats</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">When using GNNs as generators, the graph data can be directly encoded. However, when utilizing LMs as generators, the non-Euclidean nature of graph data poses a challenge, as it cannot be directly combined with textual data for input into the LMs. To address this, graph translators are employed to convert the graph data into a format compatible with LMs. This conversion enhances the generative capabilities of LMs by enabling them to effectively process and utilize structured graph information. In this survey, we summarize two distinct graph formats: graph languages and graph embeddings. We illustrate this process with an example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S7.F6" title="Figure 6 ‣ 7.2. Graph Formats ‣ 7. Graph-Enhanced Generation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a>, with detailed introductions provided below.</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S7.F6.g1" src="x6.png" width="567"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Illustration of the graph languages. Given the retrieved subgraph on the left part, we show how to transform it into adjacency/edge table, natural language, node sequence, code-like forms and syntax trees to adapt the input form requirements of different generators.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S7.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1. </span>Graph Languages</h4>
<div class="ltx_para" id="S7.SS2.SSS1.p1">
<p class="ltx_p" id="S7.SS2.SSS1.p1.1">A graph description language is a formalized system of notation that is specifically crafted to characterize and represent graph data. It prescribes a uniform syntax and semantic framework that describes the components and interconnections within a graph. Through these languages, users can consistently generate, manipulate, and interpret graph data in a comprehensible format to machines. They enable the definition of graph architectures, the specification of attributes for nodes and edges, and the implementation of operations and queries on graph structures. Next, we will introduce five types of graph languages separately: Adjacency / Edge Table, Natural Language, Codes, Syntax Tree, and Node Sequence.</p>
</div>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Adjacency / Edge Table</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px1.p1.1">The adjacency table and the edge table are widely used methods for describing graph structures <cite class="ltx_cite ltx_citemacro_citep">(Fatemi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib39" title="">2023</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib50" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib166" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>. The adjacency table enumerates the immediate neighbors of each vertex, offering a compact way to represent connections in sparse graphs. For example, KG-GPT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite> linearizes the triples in the retrieved subgraph, which are then concatenated and fed into the LLMs. Conversely, the edge table details all the edges within the graph, providing a straightforward representation that is particularly useful for processing and analyzing graphs in a linear format. Both two methods are brief, easy to understand, and intuitive.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Natural Language</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px2.p1.2">Given that user queries are typically presented in natural language, and considering the outstanding natural language comprehension capabilities of LMs, it becomes a compelling approach to describe the retrieved graph data using natural language. By translating graph data into descriptive, easily comprehensible language, LMs can bridge the gap between raw data representation and user-friendly information, facilitating more effective interactions with data-driven applications. For example, some researchers <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite> propose defining a natural language template for each type of edge in advance and subsequently filling in the endpoints of each edge into the corresponding template based on its type. 

<cite class="ltx_cite ltx_citemacro_citet">Ye et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib191" title="">2024</a>)</cite>
 employ natural language to describe the information of <math alttext="1" class="ltx_Math" display="inline" id="S7.SS2.SSS1.Px2.p1.1.m1.1"><semantics id="S7.SS2.SSS1.Px2.p1.1.m1.1a"><mn id="S7.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S7.SS2.SSS1.Px2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.Px2.p1.1.m1.1b"><cn id="S7.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S7.SS2.SSS1.Px2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.Px2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.SSS1.Px2.p1.1.m1.1d">1</annotation></semantics></math>-hop and <math alttext="2" class="ltx_Math" display="inline" id="S7.SS2.SSS1.Px2.p1.2.m2.1"><semantics id="S7.SS2.SSS1.Px2.p1.2.m2.1a"><mn id="S7.SS2.SSS1.Px2.p1.2.m2.1.1" xref="S7.SS2.SSS1.Px2.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.Px2.p1.2.m2.1b"><cn id="S7.SS2.SSS1.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S7.SS2.SSS1.Px2.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.Px2.p1.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.SSS1.Px2.p1.2.m2.1d">2</annotation></semantics></math>-hop neighboring nodes of the central node. 

<cite class="ltx_cite ltx_citemacro_citet">Edge et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>
 utilize LLMs to generate report-like summaries for each detected graph community. 



<cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Guo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite>
 adopt LMs to rewrite the edge table of retrieved subgraphs, generating a natural language description. 

<cite class="ltx_cite ltx_citemacro_citet">Fatemi et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib39" title="">2023</a>)</cite>
 explore different representations of nodes (e.g., Integer encoding, alphabet letters, names, etc.) and edges (e.g., parenthesis, arrows, incident, etc.). 



<cite class="ltx_cite ltx_citemacro_citet">Jin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib171" title="">2023d</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Sun et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>
 integrate information from different granularities within the graph into prompts through natural language in the form of dialogue.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">(3) Code-Like Forms</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px3.p1.2">Considering that natural language descriptions and other <math alttext="1" class="ltx_Math" display="inline" id="S7.SS2.SSS1.Px3.p1.1.m1.1"><semantics id="S7.SS2.SSS1.Px3.p1.1.m1.1a"><mn id="S7.SS2.SSS1.Px3.p1.1.m1.1.1" xref="S7.SS2.SSS1.Px3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.Px3.p1.1.m1.1b"><cn id="S7.SS2.SSS1.Px3.p1.1.m1.1.1.cmml" type="integer" xref="S7.SS2.SSS1.Px3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.Px3.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.SSS1.Px3.p1.1.m1.1d">1</annotation></semantics></math>-D sequences are inherently inadequate for directly representing the <math alttext="2" class="ltx_Math" display="inline" id="S7.SS2.SSS1.Px3.p1.2.m2.1"><semantics id="S7.SS2.SSS1.Px3.p1.2.m2.1a"><mn id="S7.SS2.SSS1.Px3.p1.2.m2.1.1" xref="S7.SS2.SSS1.Px3.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.Px3.p1.2.m2.1b"><cn id="S7.SS2.SSS1.Px3.p1.2.m2.1.1.cmml" type="integer" xref="S7.SS2.SSS1.Px3.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.Px3.p1.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.SSS1.Px3.p1.2.m2.1d">2</annotation></semantics></math>-D structure of graph data, and given the robust code comprehension capabilities of LMs, many researchers <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib50" title="">2023</a>)</cite> explore using code-like formats to represent graph structures. For example,  

<cite class="ltx_cite ltx_citemacro_citet">Guo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib50" title="">2023</a>)</cite>
 examine the use of Graph Modeling Language (GML) <cite class="ltx_cite ltx_citemacro_citep">(Himsolt, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib57" title="">1996</a>)</cite> and Graph Markup Language (GraphML) <cite class="ltx_cite ltx_citemacro_citep">(Rong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib142" title="">2020</a>)</cite> for representing graphs.
These standardized languages are specifically designed for graph data, providing comprehensive descriptions that encompass nodes, edges, and their interrelationships.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">(4) Syntax Tree</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px4.p1.1">Compared to direct flattening of graphs, some research <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib202" title="">2023</a>)</cite> propose transforming graphs into structures akin to syntax trees. Syntax trees possess a hierarchical structure and, being topological graphs, also maintain a topological order. This method retains more structural information, enhancing the understanding and analysis of the graph’s intrinsic properties. Such a transformation not only preserves the relational dynamics between different graph elements but also facilitates more sophisticated algorithms for graph analysis and processing. GRAPHTEXT <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib202" title="">2023</a>)</cite> proposes transforming the ego network of a central node into a graph-syntax tree format. This format not only encapsulates structural information but also integrates the features of the nodes. By traversing this syntax tree, it is possible to obtain a node sequence that maintains both topological order and hierarchical structure.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px5">
<h5 class="ltx_title ltx_title_paragraph">(5) Node Sequence</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px5.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px5.p1.1">Some studies <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib19" title="">2024</a>; Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> propose representing graphs through sequences of nodes, which are often generated using predefined rules. Compared to natural language descriptions, these node sequences are more concise and incorporate prior knowledge, specifically the structural information emphasized by the rules. 



<cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Sun et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>
 transform the retrieved paths into node sequences and input them into an LLM to enhance the task performance. LLaGA <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib19" title="">2024</a>)</cite> proposes two templates that can transform graphs into node sequences. The first template, known as the Neighborhood Detail Template, offers a detailed examination of the central node along with its immediate surroundings. The second, termed the Hop-Field Overview Template, provides a summarized perspective of a node’s neighborhood, which can be expanded to encompass broader areas. GNN-RAG <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite> inputs the retrieved reasoning paths into LMs in the form of node sequences as prompts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS1.Px6">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>
<div class="ltx_para" id="S7.SS2.SSS1.Px6.p1">
<p class="ltx_p" id="S7.SS2.SSS1.Px6.p1.1">Good graph languages should be complete, concise, and comprehensible. Completeness entails capturing all essential information within the graph structure, ensuring no critical details are omitted. Conciseness refers to the necessity of keeping textual descriptions brief to avoid the “lost in the middle” phenomenon <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib105" title="">2024a</a>)</cite> or exceeding the length limitations of LMs. Lengthy inputs can hinder LMs’ processing capabilities, potentially causing loss of context or truncated data interpretation. Comprehensibility ensures that the language used is easily understood by LLMs, facilitating accurate representation of the graph’s structure. Due to the characteristics of different graph languages, their choice can significantly impact the performance of downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(Fatemi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib39" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2. </span>Graph Embeddings</h4>
<div class="ltx_para" id="S7.SS2.SSS2.p1">
<p class="ltx_p" id="S7.SS2.SSS2.p1.1">The above graph language methods transform graph data into text sequences, which may result in overly lengthy contexts, incurring high computational costs and potentially exceeding the processing limits of LLMs. Additionally, LLMs currently struggle to fully comprehend graph structures even with graph languages <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib50" title="">2023</a>)</cite>. Thus, using GNNs to represent graphs as embeddings presents a promising alternative. The core challenge lies in integrating graph embeddings with textual representations into a unified semantic space. Current research focuses on utilizing prompt tuning methodologies, as discussed earlier. There are also some methods that adopt FiD (Fusion-in-Decoder) <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib66" title="">2021</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib195" title="">2022</a>)</cite>, which first convert the graph data into text, then encode it using an LM-based encoder and input it into the decoders <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib38" title="">2024a</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib194" title="">2023</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib30" title="">2023a</a>)</cite>. Notably, feeding graph representations into LMs is feasible primarily with open-source LMs, not closed-source models like GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib128" title="">2024</a>)</cite>. While graph embedding methods avoid handling long text inputs, they face other challenges, such as difficulty in preserving precise information like specific entity names and poor generalization.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Generation Enhancement</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">In the generation phase, besides converting the retrieved graph data into formats acceptable by the generator and inputting it together with the query to generate the final response, many researchers explore various methods of generation enhancement techniques to improve the quality of output responses. These methods can be classified into three categories based on their application stages: pre-generation enhancement, mid-generation enhancement, and post-generation enhancement.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1. </span>Pre-Generation Enhancement</h4>
<div class="ltx_para" id="S7.SS3.SSS1.p1">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1">Pre-generation enhancement techniques focus on improving the quality of input data or representations before feeding them into the generator. In fact, there is no clear boundary between Pre-Generation Enhancement and Retrieval. In this survey, we categorize the retrieval stage as the process of retrieving knowledge from the original graph, and merging and pruning retrieved knowledge. Subsequent operations are considered Pre-Generation Enhancements.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p2">
<p class="ltx_p" id="S7.SS3.SSS1.p2.1">Commonly used pre-generation enhancement approaches primarily involve semantically enriching the retrieved graph data to achieve tighter integration between the graph data and textual query. 

<cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>
 employ LLMs to rewrite retrieved graph data, enhancing the naturalness and semantic richness of the transformed natural language output. This method not only ensures that graph data is converted into more fluent and natural language but also enriches its semantic content. Conversely, DALK <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite> utilizes the retrieved graph data to rewrite the query. 

<cite class="ltx_cite ltx_citemacro_citet">Cheng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib21" title="">2024</a>)</cite>
 first leverage LLMs to generate a reasoning plan and answer queries according to the plan. 



<cite class="ltx_cite ltx_citemacro_citet">Taunk et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>
 aim to enhance GNNs by enabling them to learn graph representations relevant to queries. They achieve this by extracting all nouns from the QA pairs (or the QA pairs themselves) and inserting them as nodes into the retrieved subgraph. 

<cite class="ltx_cite ltx_citemacro_citet">Mavromatis and Karypis (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib119" title="">2022</a>)</cite>
 propose a method where, prior to generation, the representation of the query is decomposed into multiple vectors termed “instructions”, each representing different features of the query. These instructions are used as conditions during message passing when applying GNNs to learn from retrieved subgraphs. In addition, there are methods that incorporate additional information beyond graph data. For example, PullNet <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite> incorporates documents relevant to entities and MVP-Tuning <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite> retrieves other related questions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2. </span>Mid-Generation Enhancement</h4>
<div class="ltx_para" id="S7.SS3.SSS2.p1">
<p class="ltx_p" id="S7.SS3.SSS2.p1.1">Mid-generation enhancement involves techniques applied during the generation process. These methods typically adjust the generation strategies based on intermediate results or contextual cues. TIARA <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib149" title="">2022</a>)</cite> introduces constrained decoding to control the output space and reduce generation errors. When generating logical forms, if the constrained decoder detects that it is currently generating a pattern item, it restricts the next generated token to options that exist in tries containing KB classes and relations. Compared with the Beam Search, this approach ensures that pattern items generated are guaranteed to exist in the knowledge graph, thereby reducing generation errors. There are other methods adjusting the prompts of LLMs to achieve multi-step reasoning. For example, MindMap <cite class="ltx_cite ltx_citemacro_citep">(Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib176" title="">2024</a>)</cite> not only produces answers but also generates the reasoning process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3. </span>Post-Generation Enhancement</h4>
<div class="ltx_para" id="S7.SS3.SSS3.p1">
<p class="ltx_p" id="S7.SS3.SSS3.p1.1">Post-generation enhancement occurs after the initial response is generated. Post-generation enhancement methods primarily involve integrating multiple generated responses to obtain the final response. Some methods focus on integrating outputs from the same generator under different conditions or inputs. For example, 

<cite class="ltx_cite ltx_citemacro_citet">Edge et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>
 generate a summary for each graph community, followed by generating responses to queries based on the summary, and then scoring these responses using an LLM. Ultimately, the responses are sorted in descending order according to their scores and sequentially incorporated into the prompt until the token limit is reached. Subsequently, the LLM generates the final response. 



<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Kim et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite>
 first decompose the query into several sub-questions, then generate answers for each sub-question, and finally merge the answers of all sub-questions to obtain the final answer. Alternatively, other methods combine or select responses generated by different models. 



<cite class="ltx_cite ltx_citemacro_citet">Lin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib98" title="">2019</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Jiang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib69" title="">2022</a>)</cite>
 combine the outputs generated by both GNNs and LLMs to reach a synergistic effect. UniOQA <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib96" title="">2024a</a>)</cite> explores two methods for generating answers: one involves generating queries in Cypher Query Language (CQL) to execute and obtain results, while the other method directly generates answers based on retrieved triplets. The final answer is determined through a dynamic selection mechanism. In EmbedKGQA <cite class="ltx_cite ltx_citemacro_citep">(Saxena et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib146" title="">2020</a>)</cite>, besides the learned scoring function, researchers additionally design a rule-based score based on the graph structures. These two scores are combined to find the answer entity. 

<cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>
 combine answers based on retrieved graph data with responses generated according to the LLM’s own knowledge. In addition to integrating multiple responses, KALMV <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib8" title="">2023c</a>)</cite> trains a verifier to judge whether the generated answer is correct, and if it is not, to further determine whether the error is due to generation or retrieval.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Training</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this section, we summarize the individual training of retrievers, generators, and their joint training. We categorize previous works into Training-Free and Training-Based approaches based on whether explicit training is required. Training-Free methods are commonly employed when using closed-source LLMs such as GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib128" title="">2024</a>)</cite> as retrievers or generators. These methods primarily rely on carefully crafted prompts to control the retrieval and generation capabilities of LLMs. Despite LLMs’ strong abilities in text comprehension and reasoning, a challenge of Training-Free methods lies in the potential sub-optimality of results due to the lack of specific optimization for downstream tasks. Conversely, Training-Based methods involve training or fine-tuning models using supervised signals. These approaches enhance the model performance by adapting them to specific task objectives, thereby potentially improving the quality and relevance of retrieved or generated content. Joint training of retrievers and generators aims to enhance their synergy, thereby boosting performance on downstream tasks. This collaborative approach leverages the complementary strengths of both components to achieve more robust and effective results in information retrieval and content generation applications.</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>Training Strategies of Retriever</h3>
<section class="ltx_subsubsection" id="S8.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.1. </span>Training-Free</h4>
<div class="ltx_para" id="S8.SS1.SSS1.p1">
<p class="ltx_p" id="S8.SS1.SSS1.p1.1">There are two primary types of Training-Free Retrievers currently in use. The first type consists of non-parametric retrievers. These retrievers rely on pre-defined rules or traditional graph search algorithms rather than specific models <cite class="ltx_cite ltx_citemacro_citep">(Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>; Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>. The second type utilizes pre-trained LMs as retrievers. Specifically, one group of works utilizes pre-trained embedding models to encode the queries and perform retrieval directly based on the similarity between the query and graph elements <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>. Another group of works adopts generative language models for training-free retrieval. Candidate graph elements such as entities, triples, paths, or subgraphs are included as part of the prompt input to the LLMs. The LLMs then leverage semantic associations to select appropriate graph elements based on the provided prompt <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>; Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite>. These methods harness the powerful semantic understanding capabilities of LMs to retrieve relevant graph elements without the need for explicit training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.2. </span>Training-Based</h4>
<div class="ltx_para" id="S8.SS1.SSS2.p1">
<p class="ltx_p" id="S8.SS1.SSS2.p1.1">When the retrieval granularity is nodes or triplets, many methods train retrievers to maximize the similarity between the retrieval ground truth and the query. For instance, MemNNs <cite class="ltx_cite ltx_citemacro_citep">(Bordes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib13" title="">2015a</a>)</cite> leverages metric learning to closely align the ground truth with the query in semantic space while differentiating unrelated facts from the query. On the contrary, when the retrieval granularity is paths, training retrievers often adopts an autoregressive approach, where the previous relationship path is concatenated to the end of the query. The model then predicts the next relation based on the concatenated input <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p2">
<p class="ltx_p" id="S8.SS1.SSS2.p2.1">However, the lack of ground truth for retrieval content in the majority of datasets poses a significant challenge. To address this issue, many methods attempt to construct reasoning paths based on distant supervision to guide retriever training. For example, 



<cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>

, <cite class="ltx_cite ltx_citemacro_citet">Feng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib40" title="">2023</a>)</cite>
 and <cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>
 extract all paths (or shortest paths) between entities in the queries and entities in the answers, using them as training data for the retriever. In addition, 

<cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>
 also employ a relationship extraction dataset for distant supervision in unsupervised settings. There is another category of methods that utilize implicit intermediate supervision signals to train Retrievers. For instance, NSM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib55" title="">2021</a>)</cite> employs a bidirectional search strategy, where two retrievers start searching from the head entity and tail entity, respectively. The supervised objective is to ensure that the paths searched by the two retrievers converge as closely as possible. KnowGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib199" title="">2024a</a>)</cite> and MINERVA <cite class="ltx_cite ltx_citemacro_citep">(Das et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib24" title="">2018</a>)</cite> treat the selection of adjacent nodes to build paths or subgraphs as a Markov process. They design the reward function around the inclusion of the answer in the retrieved information and adopt reinforcement learning methods e.g. policy gradient to optimize the retriever.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p3">
<p class="ltx_p" id="S8.SS1.SSS2.p3.1">Some methods argue that distant supervision signals or implicit intermediate supervision signals may contain considerable noise, making it challenging to train effective retrievers. Therefore, they consider employing self-supervised methods for pre-training retrievers. SKP <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib30" title="">2023a</a>)</cite> pre-trains the DPR (Dense Passage Retrieval) model <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib79" title="">2020</a>)</cite>. Initially, it conducts random sampling on subgraphs and transforms the sampled subgraphs into passages. Subsequently, it randomly masks passages, trains the model using a Masked Language Model (MLM), and employs contrastive learning by treating the masked passages and original passages as positive pairs for comparison.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Training of Generator</h3>
<section class="ltx_subsubsection" id="S8.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.1. </span>Training-Free</h4>
<div class="ltx_para" id="S8.SS2.SSS1.p1">
<p class="ltx_p" id="S8.SS2.SSS1.p1.1">Training-Free Generators primarily cater to closed-source LLMs or scenarios where avoiding high training costs is essential. In these methods, the retrieved graph data is fed into the LLM alongside the query. The LLMs then generate responses based on the task description provided in the prompt, heavily relying on their inherent ability to understand both the query and the graph data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.2. </span>Training-Based</h4>
<div class="ltx_para" id="S8.SS2.SSS2.p1">
<p class="ltx_p" id="S8.SS2.SSS2.p1.1">Training the generator can directly receive supervised signals from downstream tasks. For generative LLMs, fine-tuning can be achieved using supervised fine-tuning (SFT), where task descriptions, queries, and graph data are inputted, and the output is compared against the ground truth for the downstream task <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib59" title="">2024</a>)</cite>. On the other hand, for GNNs or discriminative models functioning as generators, specialized loss functions tailored to the downstream tasks are employed to train the models effectively <cite class="ltx_cite ltx_citemacro_citep">(Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>; Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib69" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib200" title="">2022a</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>Joint Training</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p" id="S8.SS3.p1.1">Jointly training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework.</p>
</div>
<div class="ltx_para" id="S8.SS3.p2">
<p class="ltx_p" id="S8.SS3.p2.1">Other methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite> adopts an alternating training paradigm, where the retriever’s parameters are fixed to use the graph data for training the generator. Subsequently, the generator’s parameters are fixed, and feedback from the generator is used to guide the retriever’s training. This iterative process helps both components refine their performance in a coordinated manner.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Applications and Evaluation</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">In this section, we will summarize the downstream tasks, application domains, benchmarks and metrics, and industrial applications related to GraphRAG. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.T1" title="Table 1 ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a> collects existing GraphRAG techniques, categorizing them by downstream tasks, benchmarks, methods, and evaluation metrics. This table serves as a comprehensive overview, highlighting the various aspects and applications of GraphRAG technologies across different domains.</p>
</div>
<figure class="ltx_table" id="S9.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>The tasks, benchmarks, methods, and metrics of GraphRAG.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S9.T1.5" style="width:433.6pt;height:409.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-334.5pt,316.2pt) scale(0.393269405551619,0.393269405551619) ;">
<table class="ltx_tabular ltx_align_middle" id="S9.T1.5.5">
<tr class="ltx_tr" id="S9.T1.5.5.6">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S9.T1.5.5.6.1">Tasks</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T1.5.5.6.2">Benchmarks</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T1.5.5.6.3">Methods</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T1.5.5.6.4">Metrics</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.7.1" rowspan="19"><span class="ltx_text" id="S9.T1.5.5.7.1.1">QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.7.2" rowspan="13"><span class="ltx_text" id="S9.T1.5.5.7.2.1">KBQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.7.3">WebQSP <cite class="ltx_cite ltx_citemacro_citep">(Yih et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib193" title="">2016</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T1.5.5.7.4">
<span class="ltx_inline-block ltx_align_top" id="S9.T1.5.5.7.4.1">
<span class="ltx_p" id="S9.T1.5.5.7.4.1.1" style="width:305.9pt;"><cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib168" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib112" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib6" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib149" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib100" title="">2024e</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib194" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib30" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib44" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib111" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib7" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Saxena et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib146" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib55" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib22" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib153" title="">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib72" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib8" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T1.5.5.7.5" rowspan="19"><span class="ltx_text" id="S9.T1.5.5.7.5.1"><span class="ltx_text" id="S9.T1.5.5.7.5.1.1"></span> <span class="ltx_text" id="S9.T1.5.5.7.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S9.T1.5.5.7.5.1.2.1">
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.1.1">Accuracy,</span></span>
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.2.1">EM,</span></span>
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.3.1">Recall,</span></span>
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.4.1">F1,</span></span>
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.5.1">BERTScore,</span></span>
<span class="ltx_tr" id="S9.T1.5.5.7.5.1.2.1.6">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S9.T1.5.5.7.5.1.2.1.6.1">GPT-4 Average Ranking</span></span>
</span></span> <span class="ltx_text" id="S9.T1.5.5.7.5.1.3"></span></span></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.8">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.8.1">WebQ <cite class="ltx_cite ltx_citemacro_citep">(Berant et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib9" title="">2013</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.8.2">
<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib61" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib119" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib72" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Bordes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib13" title="">2015a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.9">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.9.1">CWQ <cite class="ltx_cite ltx_citemacro_citep">(Talmor and Berant, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib157" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.9.2">
<span class="ltx_inline-block ltx_align_top" id="S9.T1.5.5.9.2.1">
<span class="ltx_p" id="S9.T1.5.5.9.2.1.1" style="width:305.9pt;"><cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib61" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib197" title="">2022b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib168" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib112" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib119" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Lan and Jiang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib88" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib100" title="">2024e</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib120" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib194" title="">2023</a>)</cite>,
<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib44" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib111" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib55" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib22" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.10">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.10.1">GrailQA <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib48" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.10.2">
<cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib70" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib149" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.11">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.11.1">QALD10-en <cite class="ltx_cite ltx_citemacro_citep">(Perevalov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib135" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.11.2">
<cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.12">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.12.1">SimpleQuestions <cite class="ltx_cite ltx_citemacro_citep">(Bordes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib14" title="">2015b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.12.2">
<cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib6" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Bordes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib13" title="">2015a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.13">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.13.1">CMCQA<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>. https://github.com/WENGSYX/CMCQA</span></span></span>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.13.2"><cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib172" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.14">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.14.1">MetaQA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib201" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.14.2">
<span class="ltx_inline-block ltx_align_top" id="S9.T1.5.5.14.2.1">
<span class="ltx_p" id="S9.T1.5.5.14.2.1.1" style="width:305.9pt;"><cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib113" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib183" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib51" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib165" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mavromatis and Karypis, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib119" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib152" title="">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Saxena et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib146" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib68" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib100" title="">2024e</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib55" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib22" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib102" title="">2024b</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.15">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.15.1">Natural Question <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib85" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.15.2">
<cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib61" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib8" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.16">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.16.1">TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib78" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.16.2">
<cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib61" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib71" title="">2023b</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.17">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.17.1">HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib188" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.17.2">
<cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib61" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Gutiérrez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib52" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib8" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dehghan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib25" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.18">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.18.1">Mintaka <cite class="ltx_cite ltx_citemacro_citep">(Sen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib147" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.18.2">
<cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib6" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib7" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib8" title="">2023c</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.19">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.19.1">FreebaseQA <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib73" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.19.2">
<cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib194" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib111" title="">2023</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.20">
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.20.1" rowspan="6"><span class="ltx_text" id="S9.T1.5.5.20.1.1">CSQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.20.2">CSQA <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib158" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T1.5.5.20.3">
<cite class="ltx_cite ltx_citemacro_citep">(Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib98" title="">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib40" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib31" title="">2023b</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.21">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.21.1">OBQA <cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib121" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.21.2">
<cite class="ltx_cite ltx_citemacro_citep">(Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib190" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib91" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib40" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib54" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib31" title="">2023b</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.22">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.22.1">MedQA <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib77" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.22.2">
<cite class="ltx_cite ltx_citemacro_citep">(Taunk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib159" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib40" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.23">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.23.1">SocialIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib144" title="">2019b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.23.2"><cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.24">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.24.1">PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib10" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.24.2"><cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.25">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.25.1">RiddleSenseQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib99" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.25.2"><cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib64" title="">2023a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.1.1.1.2" rowspan="4"><span class="ltx_text" id="S9.T1.1.1.1.2.1">IE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.1.1.1.3" rowspan="2"><span class="ltx_text" id="S9.T1.1.1.1.3.1">Entity Linking</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.1.1.1.4">ZESHEL <cite class="ltx_cite ltx_citemacro_citep">(Logeswaran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib110" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T1.1.1.1.5"><cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib181" title="">2023a</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S9.T1.1.1.1.1.1">Recall@<math alttext="K" class="ltx_Math" display="inline" id="S9.T1.1.1.1.1.1.m1.1"><semantics id="S9.T1.1.1.1.1.1.m1.1a"><mi id="S9.T1.1.1.1.1.1.m1.1.1" xref="S9.T1.1.1.1.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S9.T1.1.1.1.1.1.m1.1b"><ci id="S9.T1.1.1.1.1.1.m1.1.1.cmml" xref="S9.T1.1.1.1.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T1.1.1.1.1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S9.T1.1.1.1.1.1.m1.1d">italic_K</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.26">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.26.1">CoNLL <cite class="ltx_cite ltx_citemacro_citep">(Hoffart et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib58" title="">2011</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.26.2"><cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib181" title="">2023a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.27">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.27.1" rowspan="2"><span class="ltx_text" id="S9.T1.5.5.27.1.1">Relation Extraction</span></td>
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.27.2">T-Rex <cite class="ltx_cite ltx_citemacro_citep">(ElSahar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib34" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.27.3">
<cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.27.4" rowspan="2"><span class="ltx_text" id="S9.T1.5.5.27.4.1">Hits@1</span></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.28">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.28.1">ZsRE <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib136" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.28.2">
<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.29">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S9.T1.5.5.29.1" rowspan="8"><span class="ltx_text" id="S9.T1.5.5.29.1.1">Others</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.29.2" rowspan="2"><span class="ltx_text" id="S9.T1.5.5.29.2.1">Fact Verification</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.29.3">Creak <cite class="ltx_cite ltx_citemacro_citep">(Onoe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib127" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T1.5.5.29.4">
<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">2024a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib114" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.5.5.29.5" rowspan="2"><span class="ltx_text" id="S9.T1.5.5.29.5.1">Accuracy, F1</span></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.30">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.30.1">FACTKG <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib83" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.30.2">
<cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib81" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Lan and Jiang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib88" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib102" title="">2024b</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="S9.T1.2.2.2.2" rowspan="4"><span class="ltx_text" id="S9.T1.2.2.2.2.1">Link Prediction</span></td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.2.2.3">FB15K-237 <cite class="ltx_cite ltx_citemacro_citep">(Toutanova et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib160" title="">2015</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.2.2.2.4">
<cite class="ltx_cite ltx_citemacro_citep">(Choudhary and Reddy, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib23" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Pahuja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib130" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.2.2.1" rowspan="4"><span class="ltx_text" id="S9.T1.2.2.2.1.1">MRR, Hits@<math alttext="K" class="ltx_Math" display="inline" id="S9.T1.2.2.2.1.1.m1.1"><semantics id="S9.T1.2.2.2.1.1.m1.1a"><mi id="S9.T1.2.2.2.1.1.m1.1.1" xref="S9.T1.2.2.2.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S9.T1.2.2.2.1.1.m1.1b"><ci id="S9.T1.2.2.2.1.1.m1.1.1.cmml" xref="S9.T1.2.2.2.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T1.2.2.2.1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S9.T1.2.2.2.1.1.m1.1d">italic_K</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.31">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.31.1">FB15k <cite class="ltx_cite ltx_citemacro_citep">(Bollacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib12" title="">2008b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.31.2"><cite class="ltx_cite ltx_citemacro_citep">(Choudhary and Reddy, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib23" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.32">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.32.1">WN18RR <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib28" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.32.2"><cite class="ltx_cite ltx_citemacro_citep">(Pahuja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib130" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.33">
<td class="ltx_td ltx_align_center" id="S9.T1.5.5.33.1">NELL995 <cite class="ltx_cite ltx_citemacro_citep">(Carlson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib16" title="">2010</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.5.5.33.2"><cite class="ltx_cite ltx_citemacro_citep">(Choudhary and Reddy, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib23" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S9.T1.3.3.3">
<td class="ltx_td ltx_align_center" id="S9.T1.3.3.3.2">Dialogue Systems</td>
<td class="ltx_td ltx_align_center" id="S9.T1.3.3.3.3">OpenDialKG <cite class="ltx_cite ltx_citemacro_citep">(Moon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib123" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="S9.T1.3.3.3.4"><cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib6" title="">2023b</a>)</cite></td>
<td class="ltx_td ltx_align_center" id="S9.T1.3.3.3.1">MRR, Hits@<math alttext="K" class="ltx_Math" display="inline" id="S9.T1.3.3.3.1.m1.1"><semantics id="S9.T1.3.3.3.1.m1.1a"><mi id="S9.T1.3.3.3.1.m1.1.1" xref="S9.T1.3.3.3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S9.T1.3.3.3.1.m1.1b"><ci id="S9.T1.3.3.3.1.m1.1.1.cmml" xref="S9.T1.3.3.3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T1.3.3.3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S9.T1.3.3.3.1.m1.1d">italic_K</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S9.T1.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T1.5.5.5.3">Recommendation</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T1.5.5.5.4">Yelp<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://www.yelp.com/dataset/</span></span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S9.T1.5.5.5.5"><cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib169" title="">2022</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T1.5.5.5.2">NDCG@<math alttext="K" class="ltx_Math" display="inline" id="S9.T1.4.4.4.1.m1.1"><semantics id="S9.T1.4.4.4.1.m1.1a"><mi id="S9.T1.4.4.4.1.m1.1.1" xref="S9.T1.4.4.4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S9.T1.4.4.4.1.m1.1b"><ci id="S9.T1.4.4.4.1.m1.1.1.cmml" xref="S9.T1.4.4.4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T1.4.4.4.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S9.T1.4.4.4.1.m1.1d">italic_K</annotation></semantics></math>, Recall@<math alttext="K" class="ltx_Math" display="inline" id="S9.T1.5.5.5.2.m2.1"><semantics id="S9.T1.5.5.5.2.m2.1a"><mi id="S9.T1.5.5.5.2.m2.1.1" xref="S9.T1.5.5.5.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S9.T1.5.5.5.2.m2.1b"><ci id="S9.T1.5.5.5.2.m2.1.1.cmml" xref="S9.T1.5.5.5.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.T1.5.5.5.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S9.T1.5.5.5.2.m2.1d">italic_K</annotation></semantics></math>
</td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1. </span>Downstream Tasks</h3>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">GraphRAG is applied in various downstream tasks (especially NLP tasks), including Question Answering, Information Extraction, and others.</p>
</div>
<section class="ltx_subsubsection" id="S9.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.1. </span>Question Answering</h4>
<div class="ltx_para" id="S9.SS1.SSS1.p1">
<p class="ltx_p" id="S9.SS1.SSS1.p1.1">The QA tasks specifically include Knowledge Base Question Answering (KBQA) and CommonSense Question Answering (CSQA).</p>
</div>
<section class="ltx_paragraph" id="S9.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) KBQA</h5>
<div class="ltx_para" id="S9.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S9.SS1.SSS1.Px1.p1.1">KBQA serves as a cornerstone downstream task for GraphRAG. In KBQA, questions typically pertain to specific knowledge graphs, and answers often involve entities, relationships, or operations between sets of entities within the knowledge graph. The task tests the systems’ ability to retrieve and reason over structured knowledge bases, which is crucial in facilitating complex query responses.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) CSQA</h5>
<div class="ltx_para" id="S9.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S9.SS1.SSS1.Px2.p1.1">Distinguished from KBQA, CSQA primarily takes the form of multiple-choice questions. Commonsense reasoning typically presents a commonsense question along with several answer options, each potentially representing either the name of an entity or a statement. The objective is for machines to utilize external commonsense knowledge graphs, such as ConceptNet, to find relevant knowledge pertaining to the question and options, and to engage in appropriate reasoning and derive the correct answer.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S9.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.2. </span>Information Retrieval</h4>
<div class="ltx_para" id="S9.SS1.SSS2.p1">
<p class="ltx_p" id="S9.SS1.SSS2.p1.1">Information Retrieval tasks consist of two categories: Entity Linking (EL) and Relation Extraction (RE).</p>
</div>
<section class="ltx_paragraph" id="S9.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Entity Linking</h5>
<div class="ltx_para" id="S9.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S9.SS1.SSS2.Px1.p1.1">Entity Linking (EL) is a critical task in the field of natural language processing that involves identifying entities mentioned in text segments and linking them to their corresponding entities in a knowledge graph. By leveraging a system such as Graph RAG, it is possible to retrieve relevant information from the knowledge graph, which facilitates the accurate inference of the specific entities that match the mentions in the text <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib181" title="">2023a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Relation Extraction</h5>
<div class="ltx_para" id="S9.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S9.SS1.SSS2.Px2.p1.1">Relation Extraction (RE) aims at identifying and classifying semantic relationships between entities within a text. GraphRAG can significantly enhance this task by using graph-based structures to encode and exploit the interdependencies among entities, thus facilitating more accurate and contextually nuanced extraction of relational data from diverse text sources <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S9.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.3. </span>Others</h4>
<div class="ltx_para" id="S9.SS1.SSS3.p1">
<p class="ltx_p" id="S9.SS1.SSS3.p1.1">In addition to the aforementioned downstream tasks, GraphRAG can be applied to various other tasks in the realm of natural language processing such as fact verification, link prediction, dialogue systems, and recommendation.</p>
</div>
<section class="ltx_paragraph" id="S9.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Fact Verification</h5>
<div class="ltx_para" id="S9.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S9.SS1.SSS3.Px1.p1.1">The fact verification task typically involves assessing the truthfulness of a factual statement using knowledge graphs. Models are tasked with determining the validity of a given factual assertion by leveraging structured knowledge repositories. GraphRAG techniques can be utilized to extract evidential connections between entities to enhance the system’s efficiency and accuracy <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib95" title="">2024f</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib155" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib156" title="">a</a>; Qi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib137" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Link Prediction</h5>
<div class="ltx_para" id="S9.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S9.SS1.SSS3.Px2.p1.1">Link prediction involves predicting missing relationships or potential connections between entities in a graph. GraphRAG is applied to this task <cite class="ltx_cite ltx_citemacro_citep">(Choudhary and Reddy, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib23" title="">2024</a>; Pahuja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib130" title="">2023</a>)</cite> by leveraging its ability to retrieve and analyze structured information from graphs, enhancing prediction accuracy by uncovering latent relationships and patterns within the graph data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS3.Px3">
<h5 class="ltx_title ltx_title_paragraph">(3) Dialogue Systems</h5>
<div class="ltx_para" id="S9.SS1.SSS3.Px3.p1">
<p class="ltx_p" id="S9.SS1.SSS3.Px3.p1.1">Dialogue Systems is designed to converse with humans using natural language, handling various tasks such as answering questions, providing information, or facilitating user interactions. By structuring conversation histories and contextual relationships in a graph-based framework, GraphRAG systems <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib6" title="">2023b</a>)</cite> can improve the model’s ability to generate coherent and contextually relevant responses.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS3.Px4">
<h5 class="ltx_title ltx_title_paragraph">(4) Recommendation</h5>
<div class="ltx_para" id="S9.SS1.SSS3.Px4.p1">
<p class="ltx_p" id="S9.SS1.SSS3.Px4.p1.1">In the context of E-commerce platforms, the purchase relationships between users and products naturally form a network graph. The primary objective of recommendation within these platforms is to predict the future purchasing intentions of users, effectively forecasting the potential connections within this graph <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib169" title="">2022</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2. </span>Application Domains</h3>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">GraphRAG is widely applied in E-commerce and biomedical, academic, literature, legal, and other application scenarios for its outstanding ability to integrate structured knowledge graphs with natural language processing, which will be introduced below.</p>
</div>
<section class="ltx_subsubsection" id="S9.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.1. </span>E-Commerce</h4>
<div class="ltx_para" id="S9.SS2.SSS1.p1">
<p class="ltx_p" id="S9.SS2.SSS1.p1.1">The primary goal in the E-commerce area involves improving customer shopping experiences and increasing sales through personalized recommendations and intelligent customer services. In this area, historical interactions between users and products can naturally form a graph, which implicitly encapsulates users’ behavioral patterns and preference information. However, due to the increasing number of E-commerce platforms and the growing volume of user interaction data, using GraphRAG technology to extract key subgraphs is crucial. 

<cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib169" title="">2022</a>)</cite>
 ensemble multiple retrievers under different types or with different parameters to extract relevant subgraphs, which are then encoded for temporal user action prediction. To improve the model performance of customer service question answering systems, 

<cite class="ltx_cite ltx_citemacro_citet">Xu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib184" title="">2024</a>)</cite>
 construct a past-issue graph with intra-issue and inter-issue relations. For each given query, subgraphs of similar past issues are retrieved to enhance the system’s response quality.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.2. </span>Biomedical</h4>
<div class="ltx_para" id="S9.SS2.SSS2.p1">
<p class="ltx_p" id="S9.SS2.SSS2.p1.1">Recently, GraphRAG techniques are increasingly applied in biomedical question answering systems, achieving advanced medical decision-making performance. In this area, each disease is associated with specific symptoms, and every medication contains certain active ingredients that target and treat particular diseases. Some researchers <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>; Delile et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib27" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib178" title="">2024d</a>)</cite> construct KGs for specific task scenarios, while others <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib186" title="">2024a</a>; Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib176" title="">2024</a>)</cite> utilize open-source knowledge graphs such as CMeKG and CPubMed-KG as retrieval sources. Existing methods generally begin with non-parametric retrievers for initial search, followed by designing methods to filter retrieved content through re-ranking <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib74" title="">2024a</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib186" title="">2024a</a>; Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib176" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>; Delile et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib27" title="">2024</a>)</cite>. Additionally, some approaches propose rewriting model inputs using retrieved information to enhance generation effectiveness <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib90" title="">2024e</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.3. </span>Academic</h4>
<div class="ltx_para" id="S9.SS2.SSS3.p1">
<p class="ltx_p" id="S9.SS2.SSS3.p1.1">In the academic research domain, each paper is authored by one or more researchers and is associated with a field of study. Authors are affiliated with institutions, and there exist relationships among authors, such as collaboration or shared institutional affiliations. These elements can be structured into a graph format. Utilizing GraphRAG on this graph can facilitate academic exploration, including predicting potential collaborators for an author, identifying trends within a specific field, etc.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.4. </span>Literature</h4>
<div class="ltx_para" id="S9.SS2.SSS4.p1">
<p class="ltx_p" id="S9.SS2.SSS4.p1.1">Similar to academic research, a knowledge graph can be constructed in the realm of literature, with nodes representing books, authors, publishers, and series, and edges labeled “written-by”, “published-in”, and “book-series”. GraphRAG can be utilized to enhance realistic applications like smart libraries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.5. </span>Legal</h4>
<div class="ltx_para" id="S9.SS2.SSS5.p1">
<p class="ltx_p" id="S9.SS2.SSS5.p1.1">In legal contexts, extensive citation connections exist between cases and judicial opinions, as judges frequently reference previous opinions when making new decisions. This naturally creates a structured graph where nodes represent opinions, opinion clusters, dockets, and courts, and edges encompass relationships such as “opinion-citation”, “opinion-cluster”, “cluster-docket”, and “docket-court”. The application of GraphRAG in legal scenarios could aid lawyers and legal researchers in various tasks such as case analysis and legal consultation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.6. </span>Others</h4>
<div class="ltx_para" id="S9.SS2.SSS6.p1">
<p class="ltx_p" id="S9.SS2.SSS6.p1.1">In addition to the above applications, GraphRAG is also applied to other real-world scenarios such as intelligence report generation <cite class="ltx_cite ltx_citemacro_citep">(Ranade and Joshi, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib140" title="">2023</a>)</cite>, patent phrase similarity detection <cite class="ltx_cite ltx_citemacro_citep">(Peng and Yang, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib134" title="">2024</a>)</cite> and software understanding <cite class="ltx_cite ltx_citemacro_citep">(Alhanahnah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib2" title="">2024</a>)</cite>. 

<cite class="ltx_cite ltx_citemacro_citet">Ranade and Joshi (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib140" title="">2023</a>)</cite>
 first construct an Event Plot Graph (EPG) and retrieve the critical aspects of the events to aid the generation of intelligence reports. 

<cite class="ltx_cite ltx_citemacro_citet">Peng and Yang (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib134" title="">2024</a>)</cite>
 create a patent-phrase graph and retrieve the ego network of the given patent phrase to assist the judgment of phrase similarity.  

<cite class="ltx_cite ltx_citemacro_citet">Alhanahnah et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib2" title="">2024</a>)</cite>
 propose a Chatbot to understand properties about dependencies in a given software package, which first automatically constructs the dependency graph and then the user can ask questions about the dependencies in the dependency graph.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S9.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3. </span>Benchmarks and Metrics</h3>
<section class="ltx_subsubsection" id="S9.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.3.1. </span>Benchmarks</h4>
<div class="ltx_para" id="S9.SS3.SSS1.p1">
<p class="ltx_p" id="S9.SS3.SSS1.p1.1">The benchmarks used to evaluate the performance of the GraphRAG system can be divided into two categories. The first category is the corresponding datasets of downstream tasks. We summarize the benchmarks and papers tested with them according to the classification in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.SS1" title="9.1. Downstream Tasks ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">9.1</span></a>, details of which are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#S9.T1" title="Table 1 ‣ 9. Applications and Evaluation ‣ Graph Retrieval-Augmented Generation: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>. The second category consists of benchmarks specifically designed for the GraphRAG systems. These benchmarks usually cover multiple task domains to provide a comprehensive test result. For example, STARK <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib180" title="">2024c</a>)</cite> benchmarks LLM Retrieval on semi-structured knowledge bases covering three domains, including product search, academic paper search, and queries in precision medicine to access the capacity of current GraphRAG systems. 

<cite class="ltx_cite ltx_citemacro_citet">He et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib56" title="">2024</a>)</cite>
 propose a flexible question-answering benchmark targeting real-world textual graphs, named GraphQA, which is applicable to multiple applications including scene graph understanding, commonsense reasoning, and knowledge graph reasoning. Graph Reasoning Benchmark (GRBENCH) <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib76" title="">2024b</a>)</cite> is constructed to facilitate the research of augmenting LLMs with graphs, which contains 1,740 questions that can be answered with the knowledge from 10 domain graphs. CRAG <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib187" title="">2024b</a>)</cite> provides a structured query dataset, with additional mock APIs to
access information from underlying mock KGs to achieve fair comparison.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.3.2. </span>Metrics</h4>
<div class="ltx_para" id="S9.SS3.SSS2.p1">
<p class="ltx_p" id="S9.SS3.SSS2.p1.1">The evaluation metrics for GraphRAG can be broadly categorized into two main types: downstream task evaluation (generation quality) and retrieval quality.</p>
</div>
<section class="ltx_paragraph" id="S9.SS3.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">(1) Downstream Task Evaluation (Generation Quality)</h5>
<div class="ltx_para" id="S9.SS3.SSS2.Px1.p1">
<p class="ltx_p" id="S9.SS3.SSS2.Px1.p1.1">In the majority of research studies, downstream task evaluation metrics serve as the primary method for assessing GraphRAG’s performance. For example, in KBQA, Exact Match (EM) and F1 score are commonly used to measure the accuracy of answering entities. In addition, many researchers utilize BERT4Score and GPT4Score to mitigate instances where LLMs generate entities that are synonymous with the ground truth but not exact matches. In CSQA, Accuracy is the most commonly used evaluation metric. For generative tasks such as QA systems, metrics like BLEU, ROUGE-L, METEOR, and others are commonly employed to assess the quality of the text generated by the model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS3.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">(2) Retrieval Quality Evaluation</h5>
<div class="ltx_para" id="S9.SS3.SSS2.Px2.p1">
<p class="ltx_p" id="S9.SS3.SSS2.Px2.p1.1">While evaluating GraphRAG based on downstream task performance is feasible, directly measuring the accuracy of retrieved content poses challenges. Therefore, many studies employ specific metrics to gauge the precision of retrieved content. For instance, when ground truth entities are available, retrieval systems face a balance between the quantity of retrieved information and the coverage of answers. Hence, some studies utilize the ratio between answer coverage and the size of the retrieval subgraph to evaluate the performance of the retrieval system. In addition, several studies have explored metrics such as query relevance, diversity, and faithfulness score to respectively assess the similarity between retrieved content and queries, the diversity of retrieved content, and the faithfulness of the information retrieved.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S9.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4. </span>GraphRAG in Industry</h3>
<div class="ltx_para" id="S9.SS4.p1">
<p class="ltx_p" id="S9.SS4.p1.1">In this section, we mainly focus on industrial GraphRAG systems. These systems are characterized by their reliance on industrial graph database systems or their focus on large-scale graph data, details of which are as follows.</p>
</div>
<div class="ltx_para" id="S9.SS4.p2">
<p class="ltx_p" id="S9.SS4.p2.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S9.SS4.p2.1.m1.1"><semantics id="S9.SS4.p2.1.m1.1a"><mo id="S9.SS4.p2.1.m1.1.1" xref="S9.SS4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S9.SS4.p2.1.m1.1b"><ci id="S9.SS4.p2.1.m1.1.1.cmml" xref="S9.SS4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S9.SS4.p2.1.m1.1d">∙</annotation></semantics></math> GraphRAG (by Microsoft)<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://github.com/microsoft/graphrag</span></span></span>: The system uses LLMs to construct entity-based knowledge graphs and pre-generate community summaries of related entity groups, which enables the capture of both local and global relationships within a document collection, thereby enhancing Query-Focused Summarization (QFS) task <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>. The project can also utilize open-source RAG toolkits for rapid implementation, such as LlamaIndex<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://docs.llamaindex.ai/en/stable/ examples/index structs/knowledge graph/KnowledgeGraphDemo.html</span></span></span>, LangChain<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://python.langchain.com/docs/use_cases/graph</span></span></span>, etc.</p>
</div>
<div class="ltx_para" id="S9.SS4.p3">
<p class="ltx_p" id="S9.SS4.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S9.SS4.p3.1.m1.1"><semantics id="S9.SS4.p3.1.m1.1a"><mo id="S9.SS4.p3.1.m1.1.1" xref="S9.SS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S9.SS4.p3.1.m1.1b"><ci id="S9.SS4.p3.1.m1.1.1.cmml" xref="S9.SS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S9.SS4.p3.1.m1.1d">∙</annotation></semantics></math> GraphRAG (by NebulaGraph)<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://www.nebula-graph.io/posts/graph-RAG</span></span></span>: The project is the first industrial GraphRAG system, which is developed by NebulaGraph Corporation. The project integrates LLMs into the NebulaGraph database, which aims to deliver more intelligent and precise search results.</p>
</div>
<div class="ltx_para" id="S9.SS4.p4">
<p class="ltx_p" id="S9.SS4.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S9.SS4.p4.1.m1.1"><semantics id="S9.SS4.p4.1.m1.1a"><mo id="S9.SS4.p4.1.m1.1.1" xref="S9.SS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S9.SS4.p4.1.m1.1b"><ci id="S9.SS4.p4.1.m1.1.1.cmml" xref="S9.SS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S9.SS4.p4.1.m1.1d">∙</annotation></semantics></math> GraphRAG (by Antgroup)<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>https://github.com/eosphoros-ai/DB-GPT</span></span></span>: The framework is developed on the foundation of several AI engineering frameworks such as DB-GPT, knowledge graph engine OpenSPG, and graph database TuGraph. Specifically, the system begins by extracting triples from documents using LLMs, which are then stored in the graph database. During the retrieval phase, it identifies keywords from the query, locates corresponding nodes in the graph database, and traverses the subgraph using BFS or DFS. In the generation phase, the retrieved subgraph data is formatted into text and submitted along with the context and query for processing by LLMs.</p>
</div>
<div class="ltx_para" id="S9.SS4.p5">
<p class="ltx_p" id="S9.SS4.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S9.SS4.p5.1.m1.1"><semantics id="S9.SS4.p5.1.m1.1a"><mo id="S9.SS4.p5.1.m1.1.1" xref="S9.SS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S9.SS4.p5.1.m1.1b"><ci id="S9.SS4.p5.1.m1.1.1.cmml" xref="S9.SS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S9.SS4.p5.1.m1.1d">∙</annotation></semantics></math> NallM (by Neo4j)<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>https://github.com/neo4j/NaLLM</span></span></span>: The NaLLM (Neo4j and Large Language Models) framework integrates Neo4j graph database technology with LLMs. It aims to explore and demonstrate the synergy between Neo4j and LLMs, focusing on three primary use cases: Natural Language Interface to a Knowledge Graph, Creating a Knowledge Graph from Unstructured Data, and Generate Reports Using Both Static Data and LLM Data.</p>
</div>
<div class="ltx_para" id="S9.SS4.p6">
<p class="ltx_p" id="S9.SS4.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S9.SS4.p6.1.m1.1"><semantics id="S9.SS4.p6.1.m1.1a"><mo id="S9.SS4.p6.1.m1.1.1" xref="S9.SS4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S9.SS4.p6.1.m1.1b"><ci id="S9.SS4.p6.1.m1.1.1.cmml" xref="S9.SS4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S9.SS4.p6.1.m1.1d">∙</annotation></semantics></math> LLM Graph Builder (by Neo4j)<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>https://github.com/neo4j-labs/llm-graph-builder</span></span></span>: It is a project developed by Neo4j for automatically constructing knowledge graphs, suitable for the GraphRAG’s Graph Database Construction and Indexing phase. The project primarily utilizes LLMs to extract nodes, relationships, and their properties from unstructured data, and utilizes the LangChain framework to create structured knowledge graphs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10. </span>Future Prospects</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">While GraphRAG technology has made substantial strides, it continues to face enduring challenges that demand comprehensive exploration. This section will delve into the prevalent obstacles and outline prospective avenues for future research in the field of GraphRAG.</p>
</div>
<section class="ltx_subsection" id="S10.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1. </span>Dynamic and Adaptive Graphs</h3>
<div class="ltx_para" id="S10.SS1.p1">
<p class="ltx_p" id="S10.SS1.p1.1">Most GraphRAG methods <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>; Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib42" title="">2020</a>; Lan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib87" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib86" title="">2021</a>; Yani and Krisnadhi, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib189" title="">2021</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib112" title="">2024a</a>)</cite> are built upon static databases; however, as time progresses, new entities and relationships inevitably emerge <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib182" title="">2024a</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib21" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib45" title="">2024a</a>)</cite>. Rapidly updating these changes is both promising and challenging. Incorporating updated information is crucial for achieving better results and addressing emerging trends that require current data. Developing efficient methods for dynamic updates and real-time integration of new data will significantly enhance the effectiveness and relevance of GraphRAG systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2. </span>Multi-Modality Information Integration</h3>
<div class="ltx_para" id="S10.SS2.p1">
<p class="ltx_p" id="S10.SS2.p1.1">Most knowledge graphs primarily encompass textual information, thereby lacking the inclusion of other modalities such as images, audio, and videos, which hold the potential to significantly enhance the overall quality and richness of the database <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib175" title="">2019</a>)</cite>. The incorporation of these diverse modalities could provide a more comprehensive and nuanced understanding of the stored knowledge. However, the integration of such multi-modal data presents considerable challenges. As the volume of information increases, the graph’s complexity and size grow exponentially, rendering it increasingly difficult to manage and maintain. This escalation in scale necessitates the development of advanced methodologies and sophisticated tools to efficiently handle and seamlessly integrate the diverse data types into the existing graph structure, ensuring both the accuracy and accessibility of the enriched knowledge graph.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3. </span>Scalable and Efficient Retrieval Mechanisms</h3>
<div class="ltx_para" id="S10.SS3.p1">
<p class="ltx_p" id="S10.SS3.p1.1">Knowledge graphs in the industrial setting may encompass millions or even billions of entities, representing a vast and intricate scale. However, most contemporary methods are tailored for small-scale knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib33" title="">2024</a>)</cite>, which may only comprise thousands of entities. Efficiently and effectively retrieving pertinent entities within large-scale knowledge graphs remains a practical and significant challenge. Developing advanced retrieval algorithms and scalable infrastructure is essential to address this issue, ensuring that the system can manage the extensive data volume while maintaining high performance and accuracy in entity retrieval.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.4. </span>Combination with Graph Foundation Model</h3>
<div class="ltx_para" id="S10.SS4.p1">
<p class="ltx_p" id="S10.SS4.p1.1">Recently, graph foundation models <cite class="ltx_cite ltx_citemacro_citep">(Galkin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib43" title="">2023</a>; Mao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib116" title="">2024a</a>)</cite>, which can effectively address a wide range of graph tasks, have achieved significant success. Deploying these models to enhance the current GraphRAG pipeline is an essential problem. The input data for graph foundation models is inherently graph-structured, enabling them to handle such data more efficiently than LLM models. Integrating these advanced models into the GraphRAG framework could greatly improve the system’s ability to process and utilize graph-structured information, thereby enhancing overall performance and capability.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.5. </span>Lossless Compression of Retrieved Context</h3>
<div class="ltx_para" id="S10.SS5.p1">
<p class="ltx_p" id="S10.SS5.p1.1">In GraphRAG, the retrieved information is organized into a graph structure containing entities and their interrelations. This information is then transformed into a sequence that can be understood by LLMs, resulting in a very long context. There are two issues with inputting such long contexts: LLMs cannot handle very long sequences, and extensive computation during inference can be a hindrance for individuals. To address these problems, lossless compression of long contexts is crucial. This approach removes redundant information and compresses lengthy sentences into shorter, yet meaningful ones. It helps LLMs capture the essential parts of the context and accelerates inference. However, designing a lossless compression technique is challenging. Current works <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib42" title="">2020</a>; Lan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib87" title="">2023</a>)</cite> make a trade-off between compression and preserving information. Developing an effective lossless compression technique is crucial but challenging for GraphRAG.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.6. </span>Standard Benchmarks</h3>
<div class="ltx_para" id="S10.SS6.p1">
<p class="ltx_p" id="S10.SS6.p1.1">GraphRAG is a relatively new field that lacks unified and standard benchmarks for evaluating different methods. Establishing a standard benchmark is crucial for this area as it can provide a consistent framework for comparison, facilitate objective assessments of various approaches, and drive progress by identifying strengths and weaknesses. This benchmark should encompass diverse and representative datasets, well-defined evaluation metrics, and comprehensive test scenarios to ensure robust and meaningful evaluations of GraphRAG methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.7. </span>Broader Applications</h3>
<div class="ltx_para" id="S10.SS7.p1">
<p class="ltx_p" id="S10.SS7.p1.1">Current GraphRAG applications primarily focus on common tasks such as customer service systems <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib184" title="">2024</a>)</cite>, recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Deldjoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib26" title="">2024</a>)</cite>, and KBQA <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib42" title="">2020</a>)</cite>. Extending GraphRAG to broader applications such as healthcare <cite class="ltx_cite ltx_citemacro_citep">(Kashyap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib80" title="">2024</a>)</cite>, financial services <cite class="ltx_cite ltx_citemacro_citep">(Arslan and Cruz, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib4" title="">2024</a>)</cite>, legal and compliance <cite class="ltx_cite ltx_citemacro_citep">(Kim and Min, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib82" title="">2024</a>)</cite>, smart cities and IoT <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08921v2#bib.bib150" title="">2020</a>)</cite>, and more, involves incorporating more complex techniques.
For instance, in healthcare, GraphRAG can support medical diagnosis, patient record analysis, and personalized treatment plans by integrating medical literature, patient histories, and real-time health data. In financial services, GraphRAG can be utilized for fraud detection, risk assessment, and personalized financial advice by analyzing transactional data, market trends, and customer profiles. Legal and compliance applications can benefit from GraphRAG by enabling comprehensive legal research, contract analysis, and regulatory compliance monitoring through the integration of legal documents, case law, and regulatory updates.
Expanding GraphRAG to these diverse and complex domains will enhance its utility and impact, providing more sophisticated and targeted solutions across various fields.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11. </span>Conclusion</h2>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">In summary, this survey offers a comprehensive retrospective of GraphRAG technology, systematically categorizing and organizing its fundamental techniques, training methodologies, and application scenarios. GraphRAG significantly enhances the relevance, accuracy, and comprehensiveness of information retrieval by leveraging pivotal relational knowledge derived from graph datasets, thereby addressing critical limitations associated with traditional Retrieval-Augmented Generation approaches. Furthermore, as GraphRAG represents a relatively nascent field of study, we delineate the benchmarks, analyze prevailing challenges, and illuminate prospective future research directions within this domain.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work is supported by Ant Group through Ant Research Intern Program.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alhanahnah et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mohannad Alhanahnah, Yazan Boshmaf, and Benoit Baudry. 2024.

</span>
<span class="ltx_bibblock">DepsRAG: Towards Managing Software Dependencies using Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.20455 [cs.SE]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.20455" title="">https://arxiv.org/abs/2405.20455</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">An et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhiyu An, Xianzhong Ding, Yen-Chun Fu, Cheng-Chung Chu, Yan Li, and Wan Du. 2024.

</span>
<span class="ltx_bibblock">Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2408.00798 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2408.00798" title="">https://arxiv.org/abs/2408.00798</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arslan and Cruz (2024)</span>
<span class="ltx_bibblock">
Muhammad Arslan and Christophe Cruz. 2024.

</span>
<span class="ltx_bibblock">Business-RAG: Information Extraction for Business Insights.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ICSBT 2024</em> (2024), 88.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007.

</span>
<span class="ltx_bibblock">DBpedia: A Nucleus for a Web of Open Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007</em> <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(Lecture Notes in Computer Science, Vol. 4825)</em>. 722–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jinheon Baek, Alham Fikri Aji, Jens Lehmann, and Sung Ju Hwang. 2023b.

</span>
<span class="ltx_bibblock">Direct Fact Retrieval from Knowledge Graphs without Entity Linking. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>. 10038–10055.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023a.

</span>
<span class="ltx_bibblock">Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2306.04136 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.04136" title="">https://arxiv.org/abs/2306.04136</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang. 2023c.

</span>
<span class="ltx_bibblock">Knowledge-Augmented Language Model Verification. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>. 1720–1736.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berant et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.

</span>
<span class="ltx_bibblock">Semantic Parsing on Freebase from Question-Answer Pairs. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL</em>. 1533–1544.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>. 7432–7439.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bollacker et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2008a)</span>
<span class="ltx_bibblock">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008a.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring human knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</em>. 1247–1250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bollacker et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2008b)</span>
<span class="ltx_bibblock">
Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. 2008b.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring human knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008</em>. 1247–1250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bordes et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2015a)</span>
<span class="ltx_bibblock">
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015a.

</span>
<span class="ltx_bibblock">Large-scale Simple Question Answering with Memory Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1506.02075 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1506.02075" title="">https://arxiv.org/abs/1506.02075</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bordes et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2015b)</span>
<span class="ltx_bibblock">
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015b.

</span>
<span class="ltx_bibblock">Large-scale Simple Question Answering with Memory Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1506.02075 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1506.02075" title="">https://arxiv.org/abs/1506.02075</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib15.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlson et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010.

</span>
<span class="ltx_bibblock">Toward an Architecture for Never-Ending Language Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010</em>. 1306–1313.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakraborty (2024)</span>
<span class="ltx_bibblock">
Abir Chakraborty. 2024.

</span>
<span class="ltx_bibblock">Multi-hop Question Answering over Knowledge Graphs using Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.19234 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.19234" title="">https://arxiv.org/abs/2404.19234</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2024)</span>
<span class="ltx_bibblock">
Huajun Chen. 2024.

</span>
<span class="ltx_bibblock">Large Knowledge Model: Perspectives and Challenges.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.02706 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.02706" title="">https://arxiv.org/abs/2312.02706</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024.

</span>
<span class="ltx_bibblock">LLaGA: Large Language and Graph Assistant.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.08170 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.08170" title="">https://arxiv.org/abs/2402.08170</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, Jian-Guang Lou, and Feng Jiang. 2021.

</span>
<span class="ltx_bibblock">ReTraCk: A flexible and efficient framework for knowledge base question answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: system demonstrations</em>. 325–336.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, and Di Wang. 2024.

</span>
<span class="ltx_bibblock">Multi-hop Question Answering under Temporal Knowledge Editing.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.00492 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.00492" title="">https://arxiv.org/abs/2404.00492</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hyeong Kyu Choi, Seunghun Lee, Jaewon Chu, and Hyunwoo J. Kim. 2023.

</span>
<span class="ltx_bibblock">NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhary and Reddy (2024)</span>
<span class="ltx_bibblock">
Nurendra Choudhary and Chandan K. Reddy. 2024.

</span>
<span class="ltx_bibblock">Complex Logical Reasoning over Knowledge Graphs using Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.01157 [cs.LO]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.01157" title="">https://arxiv.org/abs/2305.01157</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2018.

</span>
<span class="ltx_bibblock">Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghan et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, and Mehdi Rezagholizadeh. 2024.

</span>
<span class="ltx_bibblock">EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024</em>. 14169–14187.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldjoo et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024.

</span>
<span class="ltx_bibblock">A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.00579 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.00579" title="">https://arxiv.org/abs/2404.00579</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delile et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Julien Delile, Srayanta Mukherjee, Anton Van Pamel, and Leonid Zhukov. 2024.

</span>
<span class="ltx_bibblock">Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.12352 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.12352" title="">https://arxiv.org/abs/2402.12352</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.

</span>
<span class="ltx_bibblock">Convolutional 2D Knowledge Graph Embeddings. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018</em>. 1811–1818.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>. 4171–4186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. 2023a.

</span>
<span class="ltx_bibblock">Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</em>. 3854–3859.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. 2023b.

</span>
<span class="ltx_bibblock">Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023</em>. ACM, 2519–2527.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, and et al. 2024.

</span>
<span class="ltx_bibblock">The Llama 3 Herd of Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.21783 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.21783" title="">https://arxiv.org/abs/2407.21783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edge et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024.

</span>
<span class="ltx_bibblock">From Local to Global: A Graph RAG Approach to Query-Focused Summarization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.16130 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.16130" title="">https://arxiv.org/abs/2404.16130</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ElSahar et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018.

</span>
<span class="ltx_bibblock">T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024a.

</span>
<span class="ltx_bibblock">A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.06211 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.06211" title="">https://arxiv.org/abs/2405.06211</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, and Qing Li. 2024b.

</span>
<span class="ltx_bibblock">Graph Machine Learning in the Era of Large Language Models (LLMs).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.14928 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.14928" title="">https://arxiv.org/abs/2404.14928</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haishuo Fang, Xiaodan Zhu, and Iryna Gurevych. 2024b.

</span>
<span class="ltx_bibblock">DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.07080 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.07080" title="">https://arxiv.org/abs/2406.07080</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald. 2024a.

</span>
<span class="ltx_bibblock">REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024</em>. 2094–2112.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fatemi et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023.

</span>
<span class="ltx_bibblock">Talk like a Graph: Encoding Graphs for Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.04560 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.04560" title="">https://arxiv.org/abs/2310.04560</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chao Feng, Xinyu Zhang, and Zichu Fei. 2023.

</span>
<span class="ltx_bibblock">Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.03118 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.03118" title="">https://arxiv.org/abs/2309.03118</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020.

</span>
<span class="ltx_bibblock">Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>. 1295–1309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020.

</span>
<span class="ltx_bibblock">A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.13069 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.13069" title="">https://arxiv.org/abs/2007.13069</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galkin et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. 2023.

</span>
<span class="ltx_bibblock">Towards Foundation Models for Knowledge Graph Reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, and Bo Long. 2022.

</span>
<span class="ltx_bibblock">Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022 - Volume 1: Long Papers, Online Only, November 20-23, 2022</em>. 82–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, and Dongsheng Li. 2024a.

</span>
<span class="ltx_bibblock">Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.16568 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.16568" title="">https://arxiv.org/abs/2402.16568</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024b.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.10997 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.10997" title="">https://arxiv.org/abs/2312.10997</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghimire et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aashish Ghimire, James Prather, and John Edwards. 2024.

</span>
<span class="ltx_bibblock">Generative AI in Education: A Study of Educators’ Awareness, Sentiments, and Influencing Factors.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.15586 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.15586" title="">https://arxiv.org/abs/2403.15586</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021.

</span>
<span class="ltx_bibblock">Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">WWW ’21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021</em>. 3477–3488.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu and Su (2022)</span>
<span class="ltx_bibblock">
Yu Gu and Yu Su. 2022.

</span>
<span class="ltx_bibblock">ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>. 1718–1731.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. 2023.

</span>
<span class="ltx_bibblock">GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.15066 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.15066" title="">https://arxiv.org/abs/2305.15066</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, and Yingyou Wen. 2024.

</span>
<span class="ltx_bibblock">KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.15880 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.15880" title="">https://arxiv.org/abs/2312.15880</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutiérrez et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024.

</span>
<span class="ltx_bibblock">HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.14831 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.14831" title="">https://arxiv.org/abs/2405.14831</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017.

</span>
<span class="ltx_bibblock">Inductive Representation Learning on Large Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>. 1024–1034.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhen Han, Yue Feng, and Mingming Sun. 2023.

</span>
<span class="ltx_bibblock">A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.10395 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.10395" title="">https://arxiv.org/abs/2303.10395</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021.

</span>
<span class="ltx_bibblock">Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">WSDM ’21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021</em>. 553–561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024.

</span>
<span class="ltx_bibblock">G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.07630 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.07630" title="">https://arxiv.org/abs/2402.07630</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Himsolt (1996)</span>
<span class="ltx_bibblock">
Michael Himsolt. 1996.

</span>
<span class="ltx_bibblock">GML: Graph Modelling Language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">University of Passau</em> (1996).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffart et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011.

</span>
<span class="ltx_bibblock">Robust Disambiguation of Named Entities in Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL</em>. 782–792.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024.

</span>
<span class="ltx_bibblock">GRAG: Graph Retrieval-Augmented Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.16506 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.16506" title="">https://arxiv.org/abs/2405.16506</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and Lu (2024)</span>
<span class="ltx_bibblock">
Yucheng Hu and Yuxing Lu. 2024.

</span>
<span class="ltx_bibblock">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.19543 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.19543" title="">https://arxiv.org/abs/2404.19543</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun. 2022.

</span>
<span class="ltx_bibblock">Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>. 9562–9581.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023b.

</span>
<span class="ltx_bibblock">A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.05232 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.05232" title="">https://arxiv.org/abs/2311.05232</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Huang (2024)</span>
<span class="ltx_bibblock">
Yizheng Huang and Jimmy Huang. 2024.

</span>
<span class="ltx_bibblock">A Survey on Retrieval-Augmented Text Generation for Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.10981 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.10981" title="">https://arxiv.org/abs/2404.10981</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, and Liwei Wang. 2023a.

</span>
<span class="ltx_bibblock">MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>. 13417–13432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">(Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>. 6384–6392.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</em>. 874–880.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jafari et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker Mushfiqul Islam, and Chidambaram Crushev. 2021.

</span>
<span class="ltx_bibblock">A Survey on Locality Sensitive Hashing Algorithms and their Applications.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2102.08942 [cs.DB]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2102.08942" title="">https://arxiv.org/abs/2102.08942</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023a.

</span>
<span class="ltx_bibblock">StructGPT: A General Framework for Large Language Model to Reason over Structured Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>. 9237–9251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022.

</span>
<span class="ltx_bibblock">$Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>. 1730–1741.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2024b.

</span>
<span class="ltx_bibblock">KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.11163 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.11163" title="">https://arxiv.org/abs/2402.11163</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock">UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023c.

</span>
<span class="ltx_bibblock">UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019.

</span>
<span class="ltx_bibblock">FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>. 318–323.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang. 2024a.

</span>
<span class="ltx_bibblock">HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.15883 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.15883" title="">https://arxiv.org/abs/2312.15883</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2024a.

</span>
<span class="ltx_bibblock">Large Language Models on Graphs: A Comprehensive Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.02783 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.02783" title="">https://arxiv.org/abs/2312.02783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. 2024b.

</span>
<span class="ltx_bibblock">Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.07103 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.07103" title="">https://arxiv.org/abs/2404.07103</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020.

</span>
<span class="ltx_bibblock">What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2009.13081 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2009.13081" title="">https://arxiv.org/abs/2009.13081</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</em>. 1601–1611.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>. 6769–6781.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kashyap et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sohum Kashyap et al<span class="ltx_text" id="bib.bib80.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Knowledge Graph Assisted Large Language Models.

</span>
<span class="ltx_bibblock">(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. 2023a.

</span>
<span class="ltx_bibblock">KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>. 9410–9421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Min (2024)</span>
<span class="ltx_bibblock">
Jaewoong Kim and Moohong Min. 2024.

</span>
<span class="ltx_bibblock">From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.01717 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.01717" title="">https://arxiv.org/abs/2402.01717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. 2023b.

</span>
<span class="ltx_bibblock">FactKG: Fact Verification via Reasoning on Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>. 16190–16206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipf and Welling (2017)</span>
<span class="ltx_bibblock">
Thomas N. Kipf and Max Welling. 2017.

</span>
<span class="ltx_bibblock">Semi-Supervised Classification with Graph Convolutional Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: a Benchmark for Question Answering Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Trans. Assoc. Comput. Linguistics</em> 7 (2019), 452–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021.

</span>
<span class="ltx_bibblock">A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021</em>. 4483–4491.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Complex Knowledge Base Question Answering: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">IEEE Trans. Knowl. Data Eng.</em> 35, 11 (2023), 11196–11215.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan and Jiang (2020)</span>
<span class="ltx_bibblock">
Yunshi Lan and Jing Jiang. 2020.

</span>
<span class="ltx_bibblock">Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>. 969–974.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock">The Power of Scale for Parameter-Efficient Prompt Tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>. 3045–3059.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, and Tianlong Chen. 2024e.

</span>
<span class="ltx_bibblock">DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.04819 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.04819" title="">https://arxiv.org/abs/2405.04819</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. 2023.

</span>
<span class="ltx_bibblock">Graph Reasoning for Question Answering with Triplet Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>. 3366–3375.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-Tuning: Optimizing Continuous Prompts for Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>. 4582–4597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2024c.

</span>
<span class="ltx_bibblock">A Survey of Graph Meets Large Language Model: Progress and Future Directions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.12399 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.12399" title="">https://arxiv.org/abs/2311.12399</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2024d.

</span>
<span class="ltx_bibblock">Large Language Models in Finance: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.10723 [q-fin.GN]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.10723" title="">https://arxiv.org/abs/2311.10723</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2024f)</span>
<span class="ltx_bibblock">
Yihao Li, Ru Zhang, and Jianyi Liu. 2024f.

</span>
<span class="ltx_bibblock">An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.04978 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.04978" title="">https://arxiv.org/abs/2402.04978</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, and Junzhao Du. 2024a.

</span>
<span class="ltx_bibblock">UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.02110 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.02110" title="">https://arxiv.org/abs/2406.02110</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. 2024b.

</span>
<span class="ltx_bibblock">Graph Neural Network Enhanced Retrieval for Question Answering of LLMs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.06572 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.06572" title="">https://arxiv.org/abs/2406.06572</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019.

</span>
<span class="ltx_bibblock">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>. 2829–2839.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021.

</span>
<span class="ltx_bibblock">RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em> <em class="ltx_emph ltx_font_italic" id="bib.bib99.4.2">(Findings of ACL, Vol. ACL/IJCNLP 2021)</em>. 1504–1515.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2024e.

</span>
<span class="ltx_bibblock">Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.01145 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.01145" title="">https://arxiv.org/abs/2406.01145</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Singh (2004)</span>
<span class="ltx_bibblock">
H Liu and P Singh. 2004.

</span>
<span class="ltx_bibblock">ConceptNet—a practical commonsense reasoning tool-kit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">BT technology journal</em> 22, 4 (2004), 211–226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, and Jundong Li. 2024b.

</span>
<span class="ltx_bibblock">Knowledge Graph-Enhanced Large Language Models via Path Selection. In <em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024</em>. 6311–6321.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. 2024d.

</span>
<span class="ltx_bibblock">Towards Graph Foundation Models: A Survey and Beyond.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.11829 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.11829" title="">https://arxiv.org/abs/2310.11829</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, and Kui Ren. 2024c.

</span>
<span class="ltx_bibblock">A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.03712 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.03712" title="">https://arxiv.org/abs/2406.03712</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a.

</span>
<span class="ltx_bibblock">Lost in the Middle: How Language Models Use Long Contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Trans. Assoc. Comput. Linguistics</em> 12 (2024), 157–173.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>. 61–68.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock">GPT Understands, Too.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2103.10385 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.10385" title="">https://arxiv.org/abs/2103.10385</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining Approach.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1907.11692 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1907.11692" title="">https://arxiv.org/abs/1907.11692</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo and Lim (2023)</span>
<span class="ltx_bibblock">
Pei-Chi Lo and Ee-Peng Lim. 2023.

</span>
<span class="ltx_bibblock">Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">ACM Trans. Inf. Syst.</em> 41, 1 (2023), 1:1–1:38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Logeswaran et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019.

</span>
<span class="ltx_bibblock">Zero-Shot Entity Linking by Reading Entity Descriptions. In <em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>. 3449–3460.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dan Luo, Jiawei Sheng, Hongbo Xu, Lihong Wang, and Bin Wang. 2023.

</span>
<span class="ltx_bibblock">Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">International Joint Conference on Neural Networks, IJCNN 2023, Gold Coast, Australia, June 18-23, 2023</em>. 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh Tuan. 2024a.

</span>
<span class="ltx_bibblock">ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.08975 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.08975" title="">https://arxiv.org/abs/2310.08975</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024b.

</span>
<span class="ltx_bibblock">Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.01061 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.01061" title="">https://arxiv.org/abs/2310.01061</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo. 2024.

</span>
<span class="ltx_bibblock">Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.10805 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.10805" title="">https://arxiv.org/abs/2407.10805</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting for Retrieval-Augmented Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.14283 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.14283" title="">https://arxiv.org/abs/2305.14283</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, and Jiliang Tang. 2024a.

</span>
<span class="ltx_bibblock">Position: Graph Foundation Models Are Already Here. In <em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Forty-first International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. 2024c.

</span>
<span class="ltx_bibblock">Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.05952 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.05952" title="">https://arxiv.org/abs/2402.05952</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024b.

</span>
<span class="ltx_bibblock">RaFe: Ranking Feedback Improves Query Rewriting for RAG.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.14431 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.14431" title="">https://arxiv.org/abs/2405.14431</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mavromatis and Karypis (2022)</span>
<span class="ltx_bibblock">
Costas Mavromatis and George Karypis. 2022.

</span>
<span class="ltx_bibblock">ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>. 2447–2458.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mavromatis and Karypis (2024)</span>
<span class="ltx_bibblock">
Costas Mavromatis and George Karypis. 2024.

</span>
<span class="ltx_bibblock">GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.20139 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.20139" title="">https://arxiv.org/abs/2405.20139</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.

</span>
<span class="ltx_bibblock">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>. 2381–2391.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016.

</span>
<span class="ltx_bibblock">Key-Value Memory Networks for Directly Reading Documents. In <em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016</em>. 1400–1409.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019.

</span>
<span class="ltx_bibblock">OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>. 845–854.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. 2020.

</span>
<span class="ltx_bibblock">TUDataset: A collection of benchmark datasets for learning with graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munikoti et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana. 2023.

</span>
<span class="ltx_bibblock">ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.12289 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.12289" title="">https://arxiv.org/abs/2311.12289</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024.

</span>
<span class="ltx_bibblock">A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.11903 [q-fin.GN]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.11903" title="">https://arxiv.org/abs/2406.11903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Onoe et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021.

</span>
<span class="ltx_bibblock">CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al<span class="ltx_text" id="bib.bib129.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.4.1">Advances in neural information processing systems</em> 35 (2022), 27730–27744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pahuja et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Vardaan Pahuja, Boshi Wang, Hugo Latapie, Jayanth Srinivasa, and Yu Su. 2023.

</span>
<span class="ltx_bibblock">A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</em>. 1992–2002.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. 2023.

</span>
<span class="ltx_bibblock">Large Language Models and Knowledge Graphs: Opportunities and Challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">TGDK</em> 1, 1 (2023), 2:1–2:38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024.

</span>
<span class="ltx_bibblock">Unifying Large Language Models and Knowledge Graphs: A Roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">IEEE Trans. Knowl. Data Eng.</em> 36, 7 (2024), 3580–3599.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock">Large Language Model based Long-tail Query Rewriting in Taobao Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">Companion Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024</em>. 20–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng and Yang (2024)</span>
<span class="ltx_bibblock">
Zhuoyi Peng and Yi Yang. 2024.

</span>
<span class="ltx_bibblock">Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.16265 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.16265" title="">https://arxiv.org/abs/2403.16265</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perevalov et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck, and Andreas Both. 2022.

</span>
<span class="ltx_bibblock">QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. In <em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">16th IEEE International Conference on Semantic Computing, ICSC 2022, Laguna Hills, CA, USA, January 26-28, 2022</em>. 229–234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock">KILT: a Benchmark for Knowledge Intensive Language Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>. 2523–2544.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023.

</span>
<span class="ltx_bibblock">FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.10173 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.10173" title="">https://arxiv.org/abs/2308.10173</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, and Shikun Zhang. 2024.

</span>
<span class="ltx_bibblock">Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.08116 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.08116" title="">https://arxiv.org/abs/2406.08116</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">J. Mach. Learn. Res.</em> 21 (2020), 140:1–140:67.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranade and Joshi (2023)</span>
<span class="ltx_bibblock">
Priyanka Ranade and Anupam Joshi. 2023.

</span>
<span class="ltx_bibblock">FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction. In <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Proceedings of the International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023, Kusadasi, Turkey, November 6-9, 2023</em>. 603–610.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>. 3980–3990.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rong et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020.

</span>
<span class="ltx_bibblock">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019a.

</span>
<span class="ltx_bibblock">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019</em>. 3027–3035.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019b.

</span>
<span class="ltx_bibblock">SocialIQA: Commonsense Reasoning about Social Interactions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1904.09728 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1904.09728" title="">https://arxiv.org/abs/1904.09728</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarmah et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. 2024.

</span>
<span class="ltx_bibblock">HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2408.04948 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2408.04948" title="">https://arxiv.org/abs/2408.04948</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxena et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020.

</span>
<span class="ltx_bibblock">Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings. In <em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>. 4498–4507.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sen et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022.

</span>
<span class="ltx_bibblock">Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022</em>. 1604–1619.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shehzad et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ahsan Shehzad, Feng Xia, Shagufta Abid, Ciyuan Peng, Shuo Yu, Dongyu Zhang, and Karin Verspoor. 2024.

</span>
<span class="ltx_bibblock">Graph Transformers: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.09777 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.09777" title="">https://arxiv.org/abs/2407.09777</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F. Karlsson, Tingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022.

</span>
<span class="ltx_bibblock">TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2210.12925 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.12925" title="">https://arxiv.org/abs/2210.12925</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Saurabh Srivastava, Milind D Jain, Harshita Jain, Kritik Jaroli, VJ Mayank Patel, and L Khan. 2020.

</span>
<span class="ltx_bibblock">IOT monitoring bin for smart cities. In <em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">3rd Smart Cities Symposium (SCS 2020)</em>, Vol. 2020. IET, 533–536.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suchanek et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007.

</span>
<span class="ltx_bibblock">Yago: a core of semantic knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">Proceedings of the 16th international conference on World Wide Web</em>. 697–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Haitian Sun, Tania Bedrax-Weiss, and William W. Cohen. 2019.

</span>
<span class="ltx_bibblock">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib152.3.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>. 2380–2390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. 2018.

</span>
<span class="ltx_bibblock">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>. 4231–4242.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock">History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>. 3521–3533.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2024b.

</span>
<span class="ltx_bibblock">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.07697 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.07697" title="">https://arxiv.org/abs/2307.07697</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. 2024a.

</span>
<span class="ltx_bibblock">ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.07677 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.07677" title="">https://arxiv.org/abs/2404.07677</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor and Berant (2018)</span>
<span class="ltx_bibblock">
Alon Talmor and Jonathan Berant. 2018.

</span>
<span class="ltx_bibblock">The Web as a Knowledge-Base for Answering Complex Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)</em>. 641–651.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.

</span>
<span class="ltx_bibblock">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>. 4149–4158.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taunk et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dhaval Taunk, Lakshya Khanna, Siri Venkata Pavan Kumar Kandru, Vasudeva Varma, Charu Sharma, and Makarand Tapaswi. 2023.

</span>
<span class="ltx_bibblock">GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023</em>. 1138–1144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toutanova et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015.

</span>
<span class="ltx_bibblock">Representing Text for Joint Embedding of Text and Knowledge Bases. In <em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015</em>. 1499–1509.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, and et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.09288 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is All you Need. In <em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>. 5998–6008.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veličković et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018.

</span>
<span class="ltx_bibblock">Graph Attention Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1710.10903 [stat.ML]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1710.10903" title="">https://arxiv.org/abs/1710.10903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vrandečić and Krötzsch (2014)</span>
<span class="ltx_bibblock">
Denny Vrandečić and Markus Krötzsch. 2014.

</span>
<span class="ltx_bibblock">Wikidata: a free collaborative knowledgebase.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">Commun. ACM</em> 57, 10 (2014), 78–85.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2023c.

</span>
<span class="ltx_bibblock">keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.00426 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.00426" title="">https://arxiv.org/abs/2401.00426</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. 2023b.

</span>
<span class="ltx_bibblock">Can Language Models Solve Graph Problems in Natural Language?. In <em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, and Runhe Huang. 2024c.

</span>
<span class="ltx_bibblock">A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.10303 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.10303" title="">https://arxiv.org/abs/2406.10303</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, and Zhang Xiong. 2023a.

</span>
<span class="ltx_bibblock">Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.13259 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.13259" title="">https://arxiv.org/abs/2308.13259</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib169.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruijie Wang, Zheng Li, Danqing Zhang, Qingyu Yin, Tong Zhao, Bing Yin, and Tarek F. Abdelzaher. 2022.

</span>
<span class="ltx_bibblock">RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib169.3.1">WWW ’22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022</em>. 462–472.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024d.

</span>
<span class="ltx_bibblock">Large Language Models for Education: A Survey and Outlook.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.18105 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.18105" title="">https://arxiv.org/abs/2403.18105</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023d.

</span>
<span class="ltx_bibblock">KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.11761 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.11761" title="">https://arxiv.org/abs/2308.11761</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yuqi Wang, Boran Jiang, Yi Luo, Dawei He, Peng Cheng, and Liangcai Gao. 2024a.

</span>
<span class="ltx_bibblock">Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.10384 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.10384" title="">https://arxiv.org/abs/2404.10384</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib173.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa F. Siu, Ruiyi Zhang, and Tyler Derr. 2024b.

</span>
<span class="ltx_bibblock">Knowledge Graph Prompting for Multi-Document Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib173.3.1">Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada</em>. 19206–19214.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, and Siliang Tang. 2024e.

</span>
<span class="ltx_bibblock">Bridging Local Details and Global Context in Text-Attributed Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.12608 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.12608" title="">https://arxiv.org/abs/2406.12608</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019.

</span>
<span class="ltx_bibblock">MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In <em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">Proceedings of the 27th ACM international conference on multimedia</em>. 1437–1445.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yilin Wen, Zifeng Wang, and Jimeng Sun. 2024.

</span>
<span class="ltx_bibblock">MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.09729 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.09729" title="">https://arxiv.org/abs/2308.09729</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wold et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sondre Wold, Lilja Øvrelid, and Erik Velldal. 2023.

</span>
<span class="ltx_bibblock">Text-To-KG Alignment: Comparing Current Methods on Classification Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2306.02871 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.02871" title="">https://arxiv.org/abs/2306.02871</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024d.

</span>
<span class="ltx_bibblock">Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2408.04187 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2408.04187" title="">https://arxiv.org/abs/2408.04187</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue. 2024b.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Natural Language Processing: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.13193 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.13193" title="">https://arxiv.org/abs/2407.13193</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024c.

</span>
<span class="ltx_bibblock">STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.13207 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.13207" title="">https://arxiv.org/abs/2404.13207</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Taiqiang Wu, Xingyu Bai, Weigang Guo, Weijie Liu, Siheng Li, and Yujiu Yang. 2023a.

</span>
<span class="ltx_bibblock">Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib181.3.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023</em>. 1021–1029.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yuxia Wu, Yuan Fang, and Lizi Liao. 2024a.

</span>
<span class="ltx_bibblock">Retrieval Augmented Generation for Dynamic Graph Modeling.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2408.14523 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2408.14523" title="">https://arxiv.org/abs/2408.14523</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib183.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, and Wei Song. 2023b.

</span>
<span class="ltx_bibblock">Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.11206 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.11206" title="">https://arxiv.org/abs/2309.11206</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib184.3.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024</em>. 2905–2909.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib185.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang, and et al. 2024c.

</span>
<span class="ltx_bibblock">Qwen2 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.10671 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.10671" title="">https://arxiv.org/abs/2407.10671</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, and Irene Li. 2024a.

</span>
<span class="ltx_bibblock">KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.05881 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05881" title="">https://arxiv.org/abs/2403.05881</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024b.

</span>
<span class="ltx_bibblock">CRAG – Comprehensive RAG Benchmark.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.04744 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.04744" title="">https://arxiv.org/abs/2406.04744</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib188.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib188.3.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>. 2369–2380.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yani and Krisnadhi (2021)</span>
<span class="ltx_bibblock">
Mohammad Yani and Adila Alfa Krisnadhi. 2021.

</span>
<span class="ltx_bibblock">Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">Inf.</em> 12, 7 (2021), 271.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021.

</span>
<span class="ltx_bibblock">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>. 535–546.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib191.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2024.

</span>
<span class="ltx_bibblock">Language is All a Graph Needs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.07134 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.07134" title="">https://arxiv.org/abs/2308.07134</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib192.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021.

</span>
<span class="ltx_bibblock">Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib192.3.1">arXiv preprint arXiv:2109.08678</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yih et al<span class="ltx_text" id="bib.bib193.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016.

</span>
<span class="ltx_bibblock">The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib193.3.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib194.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, and Bing Xiang. 2023.

</span>
<span class="ltx_bibblock">DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. In <em class="ltx_emph ltx_font_italic" id="bib.bib194.3.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib195.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2022.

</span>
<span class="ltx_bibblock">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib195.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>. 4961–4974.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib196.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024.

</span>
<span class="ltx_bibblock">Evaluation of Retrieval-Augmented Generation: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.07437 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.07437" title="">https://arxiv.org/abs/2405.07437</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib197.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. 2022b.

</span>
<span class="ltx_bibblock">Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib197.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>. 5773–5784.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib198.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. 2024b.

</span>
<span class="ltx_bibblock">GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib198.3.1">Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024</em>. 1003–1014.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib199.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. 2024a.

</span>
<span class="ltx_bibblock">KnowGPT: Knowledge Graph based Prompting for Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.06185 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.06185" title="">https://arxiv.org/abs/2312.06185</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib200.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. 2022a.

</span>
<span class="ltx_bibblock">GreaseLM: Graph REASoning Enhanced Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib200.3.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib201.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. 2018.

</span>
<span class="ltx_bibblock">Variational Reasoning for Question Answering With Knowledge Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib201.3.1">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018</em>. 6069–6076.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib202.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. 2023.

</span>
<span class="ltx_bibblock">GraphText: Graph Reasoning in Text Space.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.01089 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.01089" title="">https://arxiv.org/abs/2310.01089</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib203.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for AI-Generated Content: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.19473 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.19473" title="">https://arxiv.org/abs/2402.19473</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib204.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and Philip S. Yu. 2024.

</span>
<span class="ltx_bibblock">Large Language Models for Medicine: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.13055 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.13055" title="">https://arxiv.org/abs/2405.13055</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib205.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024.

</span>
<span class="ltx_bibblock">Efficient Tuning and Inference for Large Language Models on Textual Graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.15569 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.15569" title="">https://arxiv.org/abs/2401.15569</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 15:35:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
