<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu</title>
<!--Generated on Thu Aug  1 09:10:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Pain Recognition,  Behaviour Recognition,  Human Centered Computing,  Statistics,  Explainable AI
" lang="en" name="keywords"/>
<base href="/html/2404.00320v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S1" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S2" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS1" title="In III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Dataset Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS2" title="In III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Dataset Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3" title="In III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Experiment Design</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3.SSS1" title="In III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Decision-Level Fusion with Singular Modality (Benchmark Model)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3.SSS2" title="In III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Bifurcated Modality Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3.SSS3" title="In III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>3 </span>Quadrifurcated Modality Approach (Incorporating Human Factors)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3.SSS4" title="In III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>4 </span>Average Weighted Voting as a Comparative Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS3.SSS5" title="In III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>5 </span>Comparative Analysis &amp; Model Performance Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results and Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.SS1" title="In IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Cross Validation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S5" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S6" title="In Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#Sx1.SS0.SSS0.Px1" title="In Ethical Impact Statement ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title">Explainability and Transparency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#Sx1.SS0.SSS0.Px2" title="In Ethical Impact Statement ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title">Liability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#Sx1.SS0.SSS0.Px3" title="In Ethical Impact Statement ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title">Fairness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#Sx1.SS0.SSS0.Px4" title="In Ethical Impact Statement ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title">Efficacy and Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#Sx1.SS0.SSS0.Px5" title="In Ethical Impact Statement ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_title">Privacy and Security</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion*
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><span class="ltx_text ltx_font_bold" id="id1.id1.1">This research is supported by Holistic AI and University College London.</span> Corresponding Author: Zekun Wu</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingrui Gu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">Electrical Engineering and Computer Science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.2.id2">University of California, Berkeley
<br class="ltx_break"/></span>Berkeley, CA 
<br class="ltx_break"/>xingrui_gu@berkeley.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhixuan Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">Department of Computer Science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.2.id2">University College London
<br class="ltx_break"/></span>London, UK 
<br class="ltx_break"/>ggzhwzx@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Irisa Jin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.1.id1">Halıcıoğlu Data Science Institute</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.2.id2">University of California, San Diego
<br class="ltx_break"/></span>San Diego, United States 
<br class="ltx_break"/>irisajin23@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zekun Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.1.id1">Holistic AI
<br class="ltx_break"/>University College London
<br class="ltx_break"/>London, UK
<br class="ltx_break"/></span>zekun.wu@holisticai.com
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">This research presents a novel multimodal data fusion methodology for pain behavior recognition, integrating statistical correlation analysis with human-centered insights. Our approach introduces two key innovations: 1) integrating data-driven statistical relevance weights into the fusion strategy to effectively utilize complementary information from heterogeneous modalities, and 2) incorporating human-centric movement characteristics into multimodal representation learning for detailed modeling of pain behaviors. Validated across various deep learning architectures, our method demonstrates superior performance and broad applicability. We propose a customizable framework that aligns each modality with a suitable classifier based on statistical significance, advancing personalized and effective multimodal fusion. Furthermore, our methodology provides explainable analysis of multimodal data, contributing to interpretable and explainable AI in healthcare. By highlighting the importance of data diversity and modality-specific representations, we enhance traditional fusion techniques and set new standards for recognizing complex pain behaviors. Our findings have significant implications for promoting patient-centered healthcare interventions and supporting explainable clinical decision-making.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Pain Recognition, Behaviour Recognition, Human Centered Computing, Statistics, Explainable AI

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Affective Computing, an interdisciplinary field within Human-Computer Interaction, holds immense potential for enhancing human-computer interfaces and health surveillance by recognizing and responding to human emotions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib1" title="">1</a>]</cite>. Its application in understanding human pain behavior is particularly promising, as it acknowledges pain as a complex emotional state rather than merely a physical condition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib2" title="">2</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib3" title="">3</a>]</cite>. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">1</span></a>, pain is closely intertwined with anxiety, which can lead to protective behaviors such as guarding<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib4" title="">4</a>]</cite>. However, accurate pain recognition remains challenging due to the multi-dimensionality and subjectivity of pain experiences, which stem from the intricate interplay of physiological, psychological, and social factors<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib5" title="">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib6" title="">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="83" id="S1.F1.g1" src="extracted/5763724/Image/relationship.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Relationship between pain, emotion and protective behaviour<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib4" title="">4</a>]</cite></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Multimodal data fusion has emerged as a powerful approach in human-centered computing to address these challenges by integrating information from diverse sources such as physiological signals, behavioral cues, and self-reports<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib8" title="">8</a>]</cite>. Nevertheless, existing multimodal pain recognition methods often struggle with data heterogeneity, alignment issues, and limited interpretability<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib9" title="">9</a>]</cite>. Moreover, conventional machine learning and deep learning techniques often fail to capture the complexities of pain experiences and lack the adaptability to personalize pain recognition<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To overcome these limitations, this research introduces a novel approach to multimodal pain recognition that synergistically integrates statistical methods with a human-centric perspective. Our key innovations lie in: 1) employing statistical inference and hypothesis testing to explore the relationships between different modalities and the target variable (pain states), identifying the most informative features for pain recognition; and 2) incorporating human-centered insights into the representation and modeling of pain experiences to ensure the interpretability and ethical alignment of our models. By dynamically adjusting the contributions of different modalities through adaptive weighting, our data-driven, personalized approach enhances the precision, efficiency, and adaptability of pain recognition. Ultimately, this research contributes to the advancement of pain recognition, affective computing, AI-assisted healthcare, and the development of empathetic and socially responsible AI systems, paving the way for more effective and ethically grounded pain management strategies.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Previous studies have suggested that traditional machine learning and deep learning approaches can serve as alternatives to manual pain scoring. For instance, using a conventional neural network based on Google’s InceptionV3 model, researchers predicted binary labels of “pain” and “no pain” from mouse facial images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib11" title="">11</a>]</cite>. Additionally, Surface Electromyography (SEMG) recordings have proven valuable in assessing chronic low back pain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib12" title="">12</a>]</cite>. Building on this foundational research, a new perspective in pain recognition emphasizes the integration of multimodal data to leverage the strengths of various sources, thereby enhancing the accuracy and precision of predicting pain-related behaviors. However, it is crucial first to establish and evaluate pain as a human emotion. Previous research has identified overlapping brain regions within the Central Nervous System (CNS) that are involved in both pain and other emotions, including the amygdala, thalamus, and Anterior Cingulate Cortex (ACC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Building upon this foundation of research, there emerges a novel perspective within affective computing and human centered computing, wherein the integration of multimodal data can use the strengths of various sources of data to enhance the accuracy and precision of predicting pain-related behavior. The recognition of emotion and pain requires a nuanced understanding of complex constructs, synthesizing from a diverse array of modal features, such as facial expressions, vocal tones, postures, physiological signals, sensory experiences, emotional states, and cognitive evaluations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib6" title="">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">This complexity necessitates an integrated, multimodal analytical approach that goes beyond singular data sources. Consequently, state-of-the-art deep learning algorithms, including CNN-LSTMs, have been leveraged to significantly improve the accuracy and speed of pain recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib14" title="">14</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib15" title="">15</a>]</cite>. However, the challenge of effectively integrating and processing this heterogeneous data persists, and overcoming this challenge is essential for advancing multimodal data fusion and maximizing the efficacy of analytical methods for pain recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Reflecting on this necessity, historical research reinforces the value of integrating multiple data types, such as body movements and muscle activity, to enhance pain recognition accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib16" title="">16</a>]</cite>. These findings highlight the crucial interconnectivity of different modalities, underpinning their essential role in pain recognition and supporting the advancement of multimodal methodologies in this study.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Subsequent explorations have further elucidated this concept, with studies illustrating the efficacy of combining muscle and motion signals for the accurate identification of protective behaviors—a key aspect of pain recognition. Notably, central (model-level) fusion approaches have been shown to outperform both feature and decision-level fusion methods in the context of Protective Behavior Detection (PBD), showcasing their superior capability in harnessing the potential of multimodal data<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib17" title="">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib18" title="">18</a>]</cite>. This advancement signals a critical insight into the differential impact of various fusion layers on the process of pain recognition, highlighting the imperative of selecting and implementing the most effective fusion strategies to achieve a more integrative and holistic understanding of pain behaviors through the synthesis of multimodal information.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Innovative model architectures have been developed to enhance the accuracy and applicability of pain recognition studies. The P-STEMR framework is a notable example, utilizing human activity recognition datasets to classify pain levels, showcasing the utility of supervised learning in situations with limited labeled data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib19" title="">19</a>]</cite>. Additionally, the introduction of an advanced hierarchical HAR-PBD architecture represents significant progress in real-time pain recognition. This architecture combines Human Activity Recognition (HAR) with Protective Behavior Detection (PBD) using graph convolution and Long Short-Term Memory (LSTM) networks. Moreover, this model addresses the common challenge of data classification imbalance by implementing the Class-Balanced Focal Classification Cross-Entropy (CFCC) loss function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib20" title="">20</a>]</cite>, thereby enhancing the reliability and effectiveness of pain monitoring systems.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">The challenges in multimodal data fusion involve integrating heterogeneous data from diverse sources for accurate pain state recognition, where increased dimensionality can reduce model interpretability. While previous research has employed machine learning and deep learning for automated pain recognition, these methods often struggle with complex, multi-source pain behaviors. Our novel approach emphasizes a multimodal data fusion strategy grounded in statistical relevance, complemented by a human-centric perspective to enhance model effectiveness. This methodology not only accounts for data diversity but also transforms the relationships between different modalities and outcomes into weighted contributions for model decision-making, resulting in more precise and comprehensive detection of pain behaviors.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this study, our objective is to investigate and substantiate the efficacy and relevance of statistical approaches in amalgamating multimodal data, with a particular focus on domains centered around human computation, such as the analysis of pain. The endeavor is to amalgamate a variety of statistical instruments to refine the integration process of multimodal data, thereby augmenting the precision and efficiency in the analysis of pain behaviors. We postulate that employing this strategy will facilitate a more nuanced comprehension of the intricacies and multifaceted nature of pain, consequently enabling the provision of solutions for pain management that are both more targeted and individualized.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The central focus of this research is on developing methodologies for the effective fusion and processing of multimodal data from diverse sources and types. This involves utilizing statistical techniques to integrate modal features, conducting hypothesis tests on target variables, and performing correlation analyses. Additionally, the study aims to evaluate the significance of different modalities and determine dynamic weight distribution based on statistical insights. It also seeks to optimize the combination of features to improve the accuracy and effectiveness of models in predicting pain.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Dataset Introduction</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this research, we utilized the EmoPain dataset, a key resource for exploring the relationship between body movements and pain intensity levels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib21" title="">21</a>]</cite>. The dataset is divided into training and validation sets, with data from 10 chronic pain sufferers and 6 healthy controls in the training set, and 4 chronic pain individuals and 3 healthy controls in the validation set. As detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.SS1" title="III-A Dataset Introduction ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.F2" title="Figure 2 ‣ III-A Dataset Introduction ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">2</span></a>, the dataset includes X, Y, and Z coordinates of body joints, categorized in columns 1-22, 23-44, and 45-66, respectively. The core of our analysis focuses on vector 73, which measures protective behavior, distinguishing non-protective actions (coded as 0) from protective behaviors (coded as 1). This complex interplay between protective behaviors and pain, mediated by emotional states, positions our study at the intersection of behavioral analysis, pain recognition, and emotional computation.</p>
</div>
<figure class="ltx_table" id="S3.SS1.tab1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.SS1.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.SS1.tab1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.SS1.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.tab1.1.1.1.1.1">Columns</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="S3.SS1.tab1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.1.1.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.1.1.2.1.1" style="width:213.4pt;"><span class="ltx_text ltx_font_bold" id="S3.SS1.tab1.1.1.1.2.1.1.1">Description</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS1.tab1.1.2.2.1">1-22</th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.SS1.tab1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.2.2.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.2.2.2.1.1" style="width:213.4pt;">X coordinates of 22 body joints.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.tab1.1.3.3.1">23-44</th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.SS1.tab1.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.3.3.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.3.3.2.1.1" style="width:213.4pt;">Y coordinates of 22 body joints.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.tab1.1.4.4.1">45-66</th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.SS1.tab1.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.4.4.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.4.4.2.1.1" style="width:213.4pt;">Z coordinates of 22 body joints.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.tab1.1.5.5.1">67-70</th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.SS1.tab1.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.5.5.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.5.5.2.1.1" style="width:213.4pt;">Surface electromyography data from the lumbar and upper trapezius muscles.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.tab1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.SS1.tab1.1.6.6.1">73</th>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S3.SS1.tab1.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS1.tab1.1.6.6.2.1">
<span class="ltx_p" id="S3.SS1.tab1.1.6.6.2.1.1" style="width:213.4pt;">Protective behaviour label (0 for not protective, 1 for protective).</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="201" id="S3.F2.g1" src="extracted/5763724/Image/body_angle.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The arrangement of the 22 body joints<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib22" title="">22</a>]</cite></figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Statistical Methods</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1" style="width:28.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1">Method</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1.1.1">Usage</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.1.3.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1.1.1">Data Distribution</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1.1" style="width:28.5pt;">ANOVA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.2.1">
<span class="ltx_p" id="S3.T1.1.2.1.2.1.1" style="width:99.6pt;">Independent groups</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.3.1">
<span class="ltx_p" id="S3.T1.1.2.1.3.1.1" style="width:99.6pt;">Normal, Equal variances</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.1.1">
<span class="ltx_p" id="S3.T1.1.3.2.1.1.1" style="width:28.5pt;">Pearson</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1" style="width:99.6pt;">Linear relation</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.3.1">
<span class="ltx_p" id="S3.T1.1.3.2.3.1.1" style="width:99.6pt;">Normal</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.1.1">
<span class="ltx_p" id="S3.T1.1.4.3.1.1.1" style="width:28.5pt;">Spearman</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1" style="width:99.6pt;">Monotonic relation</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.3.1">
<span class="ltx_p" id="S3.T1.1.4.3.3.1.1" style="width:99.6pt;">Any, including non-normal</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.1.1">
<span class="ltx_p" id="S3.T1.1.5.4.1.1.1" style="width:28.5pt;">Kendall</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.2.1">
<span class="ltx_p" id="S3.T1.1.5.4.2.1.1" style="width:99.6pt;">Ordered pairs, small
sample size or with tied ranks</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.3.1">
<span class="ltx_p" id="S3.T1.1.5.4.3.1.1" style="width:99.6pt;">Any, including non-normal</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="119" id="S3.F3.g1" src="extracted/5763724/Image/normal1.png" width="240"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="119" id="S3.F3.g2" src="extracted/5763724/Image/normal2.png" width="240"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Left: Train Data Distribution. Right: Valid Data Distribution.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Dataset Analysis</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The histograms and Q-Q plots in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.F3" title="Figure 3 ‣ III-A Dataset Introduction ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">3</span></a> for the training and validation data provide crucial insights into the normality of the dataset’s distribution. From the histograms, both datasets display a pronounced peak, but it doesn’t align with the theoretical normal distribution curve, suggesting a discrepancy from normality. This is further evidenced by the asymmetry and apparent deviations from the central peak.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The Q-Q plots reinforce these findings. Ideally, data points should closely follow the red line if they were normally distributed. However, in both training and validation datasets, significant deviations occur, particularly in the tails of the distribution. These deviations manifest as pronounced curves away from the expected line, indicating heavier tails than those of a normal distribution and suggesting a skew in the data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Given the distribution characteristics of our dataset, as outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.T1" title="TABLE I ‣ III-A Dataset Introduction ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">I</span></a>, we identify the limitations and suitability of various statistical methods for our analysis. The assumption of normality renders <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">ANOVA</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.2">Pearson correlation</span> ineffective for our purposes, while <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.3">Kendall’s rank correlation</span>, despite its robustness, is impractical due to computational demands and is more suited for smaller datasets. Consequently, we opt for <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.4">Spearman’s rank correlation</span> as the most fitting choice, owing to its efficiency with large datasets and applicability to non-linear relationships, ensuring a more accurate and relevant analysis of our multimodal data. This decision strategically aligns with our analytical needs, highlighting Spearman’s correlation as the optimal tool for exploring the complex relationships within our heterogeneous dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Experiment Design</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The experimental design rigorously explores the influence of diverse data fusion strategies on the effectiveness of protective behavior recognition models. It delves into how decision-level fusion, enhanced by statistical analysis of data heterogeneity and human-centered modalities, affects model performance in the spectrum of protective behavior identification.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.4.1.1">III-C</span>1 </span>Decision-Level Fusion with Singular Modality (Benchmark Model)</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">The foundational experiment initiates with all 70 features amalgamated as a singular modality, subjected to a single Classification model. This setup, while ostensibly a feature-level fusion, is underscored as a decision-level fusion where the entire feature set is implicitly accorded a 100% weightage. This model establishes the baseline for performance metrics against which the outcomes of subsequent fusion techniques are compared (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.F4" title="Figure 4 ‣ III-C1 Decision-Level Fusion with Singular Modality (Benchmark Model) ‣ III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="106" id="S3.F4.g1" src="extracted/5763724/Image/One_Model.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Singular Modality (Benchmark Model)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.4.1.1">III-C</span>2 </span>Bifurcated Modality Approach</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Progressing to acknowledge the heterogeneity inherent in the data, the experiment bifurcates the features into two distinct modalities: the XYZ coordinates and sEMG signals. Each modality is then independently processed through an identical Classification model framework. Post-training, a weighted voting mechanism—rooted in decision-level fusion—is employed to amalgamate the predictive insights from each modality. The weights are meticulously derived from Spearman rank correlation coefficients, mirroring the relative significance of each modality’s contribution to pain level prediction. This phase aims to unravel whether segmenting features based on their heterogeneity and integrating them through a weighted decision-making process can elevate the model’s performance beyond the benchmark (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.F5" title="Figure 5 ‣ III-C2 Bifurcated Modality Approach ‣ III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="106" id="S3.F5.g1" src="extracted/5763724/Image/Two_Model.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Bifurcated Modality Approach</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS3.4.1.1">III-C</span>3 </span>Quadrifurcated Modality Approach (Incorporating Human Factors)</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Further dissecting the data through a lens of human factors, this segment of the experiment categorizes the XYZ coordinates into three additional modalities, each representing a distinct body segment (upper limbs, lower limbs, and trunk), alongside the sEMG modality, culminating in a four-modality framework. Each modality, processed through the same Classification model, undergoes an analogous weighted voting system post-training, with weights again anchored in Spearman rank correlation coefficients(See Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S3.F6" title="Figure 6 ‣ III-C3 Quadrifurcated Modality Approach (Incorporating Human Factors) ‣ III-C Experiment Design ‣ III Methodology ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="85" id="S3.F6.g1" src="extracted/5763724/Image/Four_Model.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Quadrifurcated Modality Approach (Incorporating Human Factors)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS4.4.1.1">III-C</span>4 </span>Average Weighted Voting as a Comparative Benchmark</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">To enhance the comparison within our study, we have implemented an average weighted voting mechanism across the four modalities, which presumes each modality contributes equally, without considering their statistical correlation to the protective behaviour states being analysed. This baseline method is essential for our analysis as it starkly contrasts with the statistically weighted voting approach, thereby underscoring the advantages of utilizing statistical correlations for modality weighting. This comparative analysis not only highlights the efficacy of statistical correlations but also emphasizes the significance of adopting a human-centered perspective in modality segmentation, showcasing its potential to yield more nuanced and accurate emotion recognition results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS5.4.1.1">III-C</span>5 </span>Comparative Analysis &amp; Model Performance Evaluation</h4>
<div class="ltx_para" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">By comparing the performance of the singular modality (benchmark model) with both the bifurcated and quadrifurcated modality approaches, as well as the average weighted voting benchmark, the experiment aspires to illuminate whether a granular, human-centered feature segmentation supplemented by statistically weighted decision-level fusion markedly optimizes model accuracy in pain recognition. The transition from a singular, homogenously weighted modality to a nuanced, statistically or evenly weighted integration of multiple modalities endeavors to elucidate the symbiotic relationship between data-driven and human-centered segmentation, and their collective prowess in enhancing pain recognition models. This comprehensive exploration aims to crystallize the efficacy of each strategic approach and underscore their cumulative contribution towards refining model performance in the nuanced domain of pain recognition.s</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results and Evaluation</span>
</h2>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S4.F7.g1" src="extracted/5763724/Image/Modal_Architecture.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Three experimental model architectures</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Model Performance Metrics</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="5" id="S4.T2.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Sample Distribution:</span> Y=1 N=171, Y=0 N=2698</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">Model</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.2.1">
<span class="ltx_p" id="S4.T2.1.2.1.2.1.1" style="width:25.6pt;">Acc.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.3.1">
<span class="ltx_p" id="S4.T2.1.2.1.3.1.1" style="width:25.6pt;">Prec.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.4.1">
<span class="ltx_p" id="S4.T2.1.2.1.4.1.1" style="width:25.6pt;">Rec.</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.5.1">
<span class="ltx_p" id="S4.T2.1.2.1.5.1.1" style="width:25.6pt;">F1 score</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.2.1">LSTM (1 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.1.3.2.2.1.1" style="width:25.6pt;">0.93</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.3.1">
<span class="ltx_p" id="S4.T2.1.3.2.3.1.1" style="width:25.6pt;">0.52</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.4.1">
<span class="ltx_p" id="S4.T2.1.3.2.4.1.1" style="width:25.6pt;">0.50</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.5.1">
<span class="ltx_p" id="S4.T2.1.3.2.5.1.1" style="width:25.6pt;">0.49</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1">LSTM+Stat (2 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.2.1">
<span class="ltx_p" id="S4.T2.1.4.3.2.1.1" style="width:25.6pt;">0.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.3.1">
<span class="ltx_p" id="S4.T2.1.4.3.3.1.1" style="width:25.6pt;">0.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.4.1">
<span class="ltx_p" id="S4.T2.1.4.3.4.1.1" style="width:25.6pt;">0.81</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.5.1">
<span class="ltx_p" id="S4.T2.1.4.3.5.1.1" style="width:25.6pt;">0.65</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1">LSTM+Stat (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.2.1">
<span class="ltx_p" id="S4.T2.1.5.4.2.1.1" style="width:25.6pt;">0.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.3.1">
<span class="ltx_p" id="S4.T2.1.5.4.3.1.1" style="width:25.6pt;">0.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.4.1">
<span class="ltx_p" id="S4.T2.1.5.4.4.1.1" style="width:25.6pt;">0.78</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.5.1">
<span class="ltx_p" id="S4.T2.1.5.4.5.1.1" style="width:25.6pt;">0.65</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.5.1">LSTM+Avg (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.2.1">
<span class="ltx_p" id="S4.T2.1.6.5.2.1.1" style="width:25.6pt;">0.94</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.3.1">
<span class="ltx_p" id="S4.T2.1.6.5.3.1.1" style="width:25.6pt;">0.63</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.4.1">
<span class="ltx_p" id="S4.T2.1.6.5.4.1.1" style="width:25.6pt;">0.52</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.5.1">
<span class="ltx_p" id="S4.T2.1.6.5.5.1.1" style="width:25.6pt;">0.52</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.7.6.1">CNN (1 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.2.1">
<span class="ltx_p" id="S4.T2.1.7.6.2.1.1" style="width:25.6pt;">0.87</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.3.1">
<span class="ltx_p" id="S4.T2.1.7.6.3.1.1" style="width:25.6pt;">0.58</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.4.1">
<span class="ltx_p" id="S4.T2.1.7.6.4.1.1" style="width:25.6pt;">0.64</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.7.6.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.5.1">
<span class="ltx_p" id="S4.T2.1.7.6.5.1.1" style="width:25.6pt;">0.60</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.7.1">CNN+Stat (2 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.2.1">
<span class="ltx_p" id="S4.T2.1.8.7.2.1.1" style="width:25.6pt;">0.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.3.1">
<span class="ltx_p" id="S4.T2.1.8.7.3.1.1" style="width:25.6pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.4.1">
<span class="ltx_p" id="S4.T2.1.8.7.4.1.1" style="width:25.6pt;">0.74</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.5.1">
<span class="ltx_p" id="S4.T2.1.8.7.5.1.1" style="width:25.6pt;">0.68</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.9.8.1">CNN+Stat (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.2.1">
<span class="ltx_p" id="S4.T2.1.9.8.2.1.1" style="width:25.6pt;">0.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.3.1">
<span class="ltx_p" id="S4.T2.1.9.8.3.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.3.1.1.1">0.66</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.9.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.4.1">
<span class="ltx_p" id="S4.T2.1.9.8.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.4.1.1.1">0.75</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.9.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.5.1">
<span class="ltx_p" id="S4.T2.1.9.8.5.1.1" style="width:25.6pt;">0.69</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.9.1">CNN+Avg (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.9.2.1">
<span class="ltx_p" id="S4.T2.1.10.9.2.1.1" style="width:25.6pt;">0.87</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.10.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.9.3.1">
<span class="ltx_p" id="S4.T2.1.10.9.3.1.1" style="width:25.6pt;">0.58</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.10.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.9.4.1">
<span class="ltx_p" id="S4.T2.1.10.9.4.1.1" style="width:25.6pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.10.9.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.10.9.5.1">
<span class="ltx_p" id="S4.T2.1.10.9.5.1.1" style="width:25.6pt;">0.60</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.11.10.1">CNN-Attention (1 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.10.2.1">
<span class="ltx_p" id="S4.T2.1.11.10.2.1.1" style="width:25.6pt;">0.79</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.11.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.10.3.1">
<span class="ltx_p" id="S4.T2.1.11.10.3.1.1" style="width:25.6pt;">0.55</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.11.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.10.4.1">
<span class="ltx_p" id="S4.T2.1.11.10.4.1.1" style="width:25.6pt;">0.64</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.11.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.11.10.5.1">
<span class="ltx_p" id="S4.T2.1.11.10.5.1.1" style="width:25.6pt;">0.54</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.12.11.1">CNN-Attention+Stat (2 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.11.2.1">
<span class="ltx_p" id="S4.T2.1.12.11.2.1.1" style="width:25.6pt;">0.87</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.12.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.11.3.1">
<span class="ltx_p" id="S4.T2.1.12.11.3.1.1" style="width:25.6pt;">0.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.12.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.11.4.1">
<span class="ltx_p" id="S4.T2.1.12.11.4.1.1" style="width:25.6pt;">0.83</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.12.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.12.11.5.1">
<span class="ltx_p" id="S4.T2.1.12.11.5.1.1" style="width:25.6pt;">0.67</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.13.12.1">CNN-Attention+Stat (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.12.2.1">
<span class="ltx_p" id="S4.T2.1.13.12.2.1.1" style="width:25.6pt;">0.89</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.13.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.12.3.1">
<span class="ltx_p" id="S4.T2.1.13.12.3.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.3.1.1.1">0.66</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.13.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.12.4.1">
<span class="ltx_p" id="S4.T2.1.13.12.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.4.1.1.1">0.85</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.13.12.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.13.12.5.1">
<span class="ltx_p" id="S4.T2.1.13.12.5.1.1" style="width:25.6pt;">0.70</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.14.13.1">CNN-Attention+Avg (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.13.2.1">
<span class="ltx_p" id="S4.T2.1.14.13.2.1.1" style="width:25.6pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.14.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.13.3.1">
<span class="ltx_p" id="S4.T2.1.14.13.3.1.1" style="width:25.6pt;">0.59</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.14.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.13.4.1">
<span class="ltx_p" id="S4.T2.1.14.13.4.1.1" style="width:25.6pt;">0.72</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.14.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.14.13.5.1">
<span class="ltx_p" id="S4.T2.1.14.13.5.1.1" style="width:25.6pt;">0.62</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.15.14.1">Multi-head SA(1 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.14.2.1">
<span class="ltx_p" id="S4.T2.1.15.14.2.1.1" style="width:25.6pt;">0.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.15.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.14.3.1">
<span class="ltx_p" id="S4.T2.1.15.14.3.1.1" style="width:25.6pt;">0.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.15.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.14.4.1">
<span class="ltx_p" id="S4.T2.1.15.14.4.1.1" style="width:25.6pt;">0.67</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.15.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.15.14.5.1">
<span class="ltx_p" id="S4.T2.1.15.14.5.1.1" style="width:25.6pt;">0.61</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.16.15.1">Multi-head SA+Stat (2 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.15.2.1">
<span class="ltx_p" id="S4.T2.1.16.15.2.1.1" style="width:25.6pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.16.15.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.15.3.1">
<span class="ltx_p" id="S4.T2.1.16.15.3.1.1" style="width:25.6pt;">0.68</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.16.15.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.15.4.1">
<span class="ltx_p" id="S4.T2.1.16.15.4.1.1" style="width:25.6pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.16.15.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.16.15.5.1">
<span class="ltx_p" id="S4.T2.1.16.15.5.1.1" style="width:25.6pt;">0.66</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.17.16.1">Multi-head SA+Stat (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.16.2.1">
<span class="ltx_p" id="S4.T2.1.17.16.2.1.1" style="width:25.6pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.17.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.16.3.1">
<span class="ltx_p" id="S4.T2.1.17.16.3.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.16.3.1.1.1">0.72</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.17.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.16.4.1">
<span class="ltx_p" id="S4.T2.1.17.16.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.16.4.1.1.1">0.71</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.1.17.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.17.16.5.1">
<span class="ltx_p" id="S4.T2.1.17.16.5.1.1" style="width:25.6pt;">0.72</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.18.17.1">Multi-head SA+Avg (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.1.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.18.17.2.1">
<span class="ltx_p" id="S4.T2.1.18.17.2.1.1" style="width:25.6pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.1.18.17.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.18.17.3.1">
<span class="ltx_p" id="S4.T2.1.18.17.3.1.1" style="width:25.6pt;">0.69</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.1.18.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.18.17.4.1">
<span class="ltx_p" id="S4.T2.1.18.17.4.1.1" style="width:25.6pt;">0.69</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.1.18.17.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.18.17.5.1">
<span class="ltx_p" id="S4.T2.1.18.17.5.1.1" style="width:25.6pt;">0.69</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In our study, as delineated in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.T2" title="TABLE II ‣ IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">II</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.F7" title="Figure 7 ‣ IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">7</span></a>, we elucidate the efficacy of integrated strategies, ranging from foundational neural networks to an advanced BodyAttention network and CNN with Multi-head Self-Attention mechanism, on pivotal performance metrics. This investigation clearly delineates the impact of varied data fusion strategies on precision, accuracy, recall, and F1 scores during protective behavior detection. Through the integration of statistical weighting in LSTM and CNN base models, expansion to BodyAttention Network (BANet) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib23" title="">23</a>]</cite>, and the adoption of Multi-head Self-Attention, we aim to evaluate the enhancement in model performance facilitated by these methodologies across diverse neural network architectures. Our research particularly explores how the amalgamation of statistical correlations with a human-centered approach can ameliorate model outcomes, especially within the challenges posed by imbalanced datasets.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Our findings emphasize the pivotal role of modality segmentation and decision-level fusion, informed by statistical insights, in augmenting the accuracy of protective behavior detection. They also highlight the broad applicability of statistically driven weighting strategies within various complex neural frameworks. Through an in-depth analysis of performance shifts across different configurations, we reveal the critical role of integrating sophisticated data processing techniques, human-centered perspectives, and statistical correlations. This holistic strategy markedly enhances the precision and effectiveness of protective behavior recognition in complex physiological datasets, representing a significant advance toward nuanced patient-centered healthcare solutions. Given the imbalanced nature of our dataset, we prioritize precision, recall, and F1 score over accuracy to more accurately reflect model performance across labels. We employed four distinct models— LSTM, CNN, CNN-Attention, and CNN with Multi-head Self-Attention—to diversify our evaluation and mitigate single-model reliance risks. It is important to note that a comparative performance analysis among these models is not within this paper’s scope. This methodology underscores our dedication to a thorough evaluation, aiming to deepen the understanding of multi-modal data fusion’s impact on model efficacy amidst dataset imbalances.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The data presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.T2" title="TABLE II ‣ IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">II</span></a> provide a detailed analysis of performance differences across models using various modality integrations. A key observation is that models based on a single modality can achieve high accuracy, up to 0.93, but often at the expense of other metrics like precision, recall, and F1-score, which hover around 0.55. This indicates a potential trade-off between maximizing accuracy and ensuring balanced performance across all metrics. Examining models with dual modalities reveals different impacts on performance metrics. For example, while the LSTM model experiences a slight drop in accuracy with dual-modality integration, other architectures show improvements. This variation highlights the limitations of relying solely on a single modality, which, despite high accuracy, may not fully capture a broader range of evaluative measures. Conversely, incorporating an additional modality significantly enhances precision, recall, and F1-scores, with increases ranging from 12% to 62%. This strongly supports the notion that integrating multiple modalities enhances model robustness. Specifically, segregating input features into distinct spatial and sEMG modalities, rather than combining all 70 features, proves to be a strategic advantage. This approach improves the model’s ability to identify and evaluate protective behaviors, resulting in better precision and sensitivity.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">The analysis delineates that while models based on a singular modality bypass the complexities of decision-layer weighting, rendering the choice between statistical and average weighting irrelevant, the introduction of a statistically driven weighting strategy in dual-modality configurations significantly enhances model performance by leveraging data diversity and modality-specific importance. This strategic use of statistical weighting in models integrating two modalities distinctly outperforms single-modality models, affirming the superiority of a multimodal fusion approach. This research underscores the efficacy of partitioning input features into distinct modalities coupled with the judicious use of statistical weighting at the decision layer, thereby substantially improving the precision and reliability of protective behavior detection. It paves the way for sophisticated multimodal integration in human-centered computing, setting a benchmark for handling complex, varied datasets with enhanced accuracy.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Adopting a human-centered approach to segment the dataset into four modalities demonstrates potential enhancements in model performance. Particularly, models employing CNN-Attention mechanisms exhibit slight but positive differences, suggesting an improved capability in capturing pain behavior features. This indicates that modality segmentation, guided by human-centered principles, can amplify model effectiveness. Further analysis reveals that models employing statistical weighting generally outperform those using mean weighting, except in the case of LSTM with four modalities employing average weighting—a scenario that mirrors the trade-offs observed in single-modality LSTM models.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">This investigation emphasizes the advantage of multimodal strategies, where strategic feature grouping and statistical decision-making markedly elevate model efficacy. Our findings particularly highlight the utility of attention mechanisms, like CNN-Attention and CNN with Multi-head Self-Attention, in a four-modality framework, reinforcing the benefits of human-centered modality segmentation. Overall, the transition to a four-modality model, grounded in human-centered design and statistical weighting, is validated as superior to single-modality approaches, bolstering the case for sophisticated multimodal fusion in enhancing protective behavior recognition.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">The examination distinctly emphasizes the superiority of statistical weighting over average weighting in multimodal configurations, enhancing model performance. However, an exception was observed in the LSTM model utilizing average weighting, suggesting a nuanced balance between accuracy and other metrics. Despite this, the evidence strongly supports the advantages of the multimodal approach over singular modality frameworks. This finding reinforces the idea that strategic segmentation of modalities, when combined with statistical weighting, markedly advances protective behavior detection models.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">This study highlights the critical importance of a human-centered modality segmentation strategy and the precise application of statistical methods in decision-making processes to optimize model outcomes. The integration of diverse modalities, guided by careful grouping and statistical weighting, not only elevates model performance but also marks a significant advance towards crafting more nuanced, interpretative models within protective behavior recognition. Consequently, this methodological approach not only pushes the boundaries of multimodal data fusion but also underlines the necessity of integrating human-centric perspectives and statistical insights into the broader narrative of human-centered computing.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Cross Validation</span>
</h3>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Leave-One-Out Cross-Validation (LOOCV)</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.1.2.1.1" style="width:19.9pt;">Acc.</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.3.1">
<span class="ltx_p" id="S4.T3.1.1.1.3.1.1" style="width:19.9pt;">Rec.</span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.4.1">
<span class="ltx_p" id="S4.T3.1.1.1.4.1.1" style="width:19.9pt;">F1 score</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1">CNN-Attention (1 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.2.1">
<span class="ltx_p" id="S4.T3.1.2.1.2.1.1" style="width:19.9pt;">0.800</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.3.1">
<span class="ltx_p" id="S4.T3.1.2.1.3.1.1" style="width:19.9pt;">0.631</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.4.1">
<span class="ltx_p" id="S4.T3.1.2.1.4.1.1" style="width:19.9pt;">0.547</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1">CNN-Attention+Stat (2 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.2.1">
<span class="ltx_p" id="S4.T3.1.3.2.2.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.2.1.1.1">0.907</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.3.1">
<span class="ltx_p" id="S4.T3.1.3.2.3.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.3.1.1.1">0.647</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.4.1">
<span class="ltx_p" id="S4.T3.1.3.2.4.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.4.1.1.1">0.631</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1">CNN-Attention+Stat (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.3.2.1">
<span class="ltx_p" id="S4.T3.1.4.3.2.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.2.1.1.1">0.908</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.3.3.1">
<span class="ltx_p" id="S4.T3.1.4.3.3.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.3.1.1.1">0.642</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.3.4.1">
<span class="ltx_p" id="S4.T3.1.4.3.4.1.1" style="width:19.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.4.1.1.1">0.628</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.5.4.1">CNN-Attention+Avg (4 mod.)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.4.2.1">
<span class="ltx_p" id="S4.T3.1.5.4.2.1.1" style="width:19.9pt;">0.886</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.4.3.1">
<span class="ltx_p" id="S4.T3.1.5.4.3.1.1" style="width:19.9pt;">0.635</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.4.4.1">
<span class="ltx_p" id="S4.T3.1.5.4.4.1.1" style="width:19.9pt;">0.603</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">From the experimental metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.T2" title="TABLE II ‣ IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">II</span></a>, the CNN-Attention model demonstrates robust performance on the Emopain dataset. To substantiate the efficacy of our approach, we adopt a Leave-One-Out Cross-Validation (LOOCV) strategy, meticulously testing each dataset instance as an individual test case, with the remaining data serving for training. This technique guarantees a thorough model evaluation across diverse configurations, notably different modalities delineated by human-centered principles. By scrutinizing the CNN-Attention architecture and integrating statistical weighting for feature selection, we methodically investigate how modality fusion and statistical weighting influence model effectiveness.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The Leave-One-Out Cross-Validation (LOOCV) metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#S4.T3" title="TABLE III ‣ IV-A Cross Validation ‣ IV Results and Evaluation ‣ Advancing Pain Recognition through Statistical Correlation-Driven Multimodal Fusion* This research is supported by Holistic AI and University College London. Corresponding Author: Zekun Wu"><span class="ltx_text ltx_ref_tag">III</span></a> demonstrate that both dual-modality and four-modality configurations surpass the single-modality training approach, affirming the benefits of data heterogeneity segmentation and human-centered modality segmentation. Notably, the CNN-Attention model, when expanded to include two modalities with statistical weighting, shows a significant improvement in accuracy from 0.800 to 0.907 and an increase in F1-score from 0.547 to 0.631. This enhancement highlights the clear advantage of integrating multiple data sources over a single modality framework, indicating that the inclusion of diverse data types enriches the model’s performance in predicting outcomes.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Furthermore, transitioning from a two-modality to a four-modality configuration with statistical weighting (CNN-Attention+Stat (4 mod.)) marginally enhances the model’s accuracy to 0.908, maintaining superior recall (Rec.) and F1-score metrics compared to the single-modality model. This slight improvement evidences the nuanced benefits of adopting a human-centered approach to modality segmentation, where data is divided into four distinct modalities based on its relevance and interaction with human behavioral patterns.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Moreover, a comparison between the four-modality configurations—statistical weighting versus average weighting (CNN-Attention+Avg (4 mod.))—reveals a distinct advantage for the former. The model employing statistical weighting (CNN-Attention+Stat (4 mod.)) achieves higher accuracy, recall, and F1-score than the model utilizing average weighting, with respective metrics of <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">0.908</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.2">0.642</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.3">0.628</span> against 0.886, 0.635, and 0.603. This differential highlights the effectiveness of our statistical relevance weighting strategy, proving it to be a more effective method for integrating diverse modalities than merely averaging their contributions.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">In summation, the Leave-One-Out Cross-Validation (LOOCV) findings robustly advocate for the deployment of multimodal fusion frameworks, amalgamating statistical correlations and human-centered methodologies. The analysis distinctly highlights the pivotal role of statistical weighting in augmenting model efficacy. It evidences that modality segmentation, underpinned by human-centered considerations, not only contributes positively but that the strategic application of statistical weighting across such segmented modalities markedly optimizes model performance. This approach signifies a notable progression in protective behavior detection, establishing the profound impact of integrating statistical insights with human-centered design principles on enhancing computational models within this domain.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This research introduces an innovative approach to affective computing and multimodal data fusion by integrating statistical methods with human-centered computation. Our findings indicate that statistical algorithms can more effectively extract data representations across various modalities, which enhances our understanding and modeling of human behavior. Traditional machine learning methods often struggle with the complexity of human behavior and the interplay between different factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib24" title="">24</a>]</cite>. However, our approach successfully identifies key features linked to pain behavior by combining statistical correlations with human expert knowledge. This not only enhances model performance but also improves interpretability, providing valuable insights for the development of future interactive intelligent systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The study underscores the importance of data-driven statistical methods in pain recognition representation extraction, particularly in situations where complete reliance on patient self-reporting is not feasible. While self-reporting is a crucial means of pain recognition, it is limited by subjectivity, communication barriers, psychological factors, sociocultural influences, and feedback time delays<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib26" title="">26</a>]</cite>. Feature extraction based on statistical correlations provides an objective, continuous, and non-invasive method for pain representation extraction, complementing the limitations of self-reporting. This data-driven feature extraction can assist healthcare professionals in more accurately assessing patients’ pain conditions, especially when patients are unable to self-report<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Furthermore, given the substantial variability in human samples, personalisation has long been a potential barrier to digital healthcare. However, by utilising modality feature extraction driven by statistical correlations, our system can capture unique pain expression patterns for each individual, contributing to more precise and effective treatment strategies. This approach not only has the potential to improve patients’ quality of life but may also reduce the risk of drug abuse, particularly in the management of chronic pain<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Finally, the statistical correlation-driven multimodal fusion framework and its representation learning approach that we have demonstrated possess extensive application potential, with the possibility of further extension to other domains involving complex human-centred computing. This research provides a viable framework for integrating statistical methods, machine learning, and human expertise to address complex challenges of human-computer interaction. This interdisciplinary approach not only enhances technical performance but also augments its credibility and acceptability in practical applications. As technology continues to advance, we anticipate seeing more innovative applications based on this framework, ultimately realising more intelligent and humanised interactive systems. This integrated approach opens up new possibilities for future human-machine collaboration, with the potential to play a crucial role in various complex human-centred computing tasks.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This research venture has systematically unveiled the efficacy of integrating statistical methods and human-centered perspectives within the ambit of multimodal pain behaviours recognition, employing an array of deep learning architectures including convolutional neural networks (CNN), long short-term memory networks (LSTM), CNN-Attention networks and CNN with Multi-head Self Attention. The cornerstone of our exploration was to enhance the precision and utility of complex pain recognition endeavors through the lens of statistical relevance and human-centered modality segmentation.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The incorporation of statistically correlated vote weights and a human-centered approach to data segmentation stands as this study’s central innovation. This methodology significantly enhances model performance and paves the way for a deeper understanding and interpretation of multimodal data. It signals a shift towards choosing optimal classifiers for each modality, refining our voting strategy for the ultimate decision-making process. Considering the diversity and weak correlations among modalities, selecting a classifier tailored to the characteristics of each modality is crucial. This sophisticated strategy highlights the importance of a customized approach to modality-specific model selection, thereby boosting the efficacy of multimodal fusion.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Furthermore, the research integrates statistical significance with a human-centered perspective, paving the way for explainable AI in pain recognition. This approach delves into the distinct impact of each data modality on model outcomes, promoting a model of explainability that not only clarifies the workings of complex models but also enhances their adaptability and accuracy. This shift highlights our methodology’s broad applicability, not only in pain management but across diverse domains of human-centered computing, spotlighting its potential to revolutionize how we interact with and leverage AI technologies.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In summary, this research not only pioneers the integration of statistical correlations with human-centered methods for multimodal data fusion in pain recognition but also advances the field by offering novel insights into modality fusion strategies. It enhances the discussion on improving model performance and interpretability within human-centered computing. The comprehensive framework established for multimodal fusion application spotlights the potential of statistical insights for model explainability, fostering trust in AI systems. Consequently, this work paves the way for transformative applications in AI systems closely aligned with human needs and behaviors, promising significant advancements across various human-focused domains.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Ethical Impact Statement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research intersects affective computing, multimodal data analysis, and pain recognition, aiming to enhance the precision and personalization of pain management through statistical methods. We conscientiously address the potential societal, environmental, and ethical impacts, focusing on aspects such as explainability, transparency, liability, fairness, efficacy, robustness, privacy, security, and sustainability.</p>
</div>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Explainability and Transparency</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px1.p1.1">Our study is committed to ensuring that advancements in pain recognition are both explainable and transparent. We leverage statistical models and human-centered perspectives to allow users and stakeholders to understand the rationale behind our predictions. This fosters trust and facilitates deeper understanding and acceptance of our technology in clinical settings.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Liability</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px2.p1.1">Acknowledging the profound implications of our work in enhancing pain recognition, we recognize our responsibility to ensure the reliability and accuracy of our findings. Misinterpretations or inaccuracies could significantly impact patient care and treatment outcomes. Therefore, we adhere to stringent validation protocols, underpinned by ethical considerations, to mitigate potential risks and liabilities associated with our research outputs.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Fairness</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px3.p1.1">Our research actively addresses the issue of fairness by incorporating the EmoPain dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib21" title="">21</a>]</cite> that includes chronic pain participants and healthy individuals. This diversity ensures our models do not inadvertently perpetuate biases against certain demographics. By integrating a human-centered approach, we actively seek to understand and incorporate diverse pain expressions and experiences, thereby mitigating the risk of biases that could otherwise compromise the fairness of our models.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Efficacy and Robustness</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px4.p1.1">The intersection of statistical methods and human-centered design in our work aims to enhance the efficacy and robustness of pain recognition technologies. Our approach ensures that our models are not only accurate but also resilient to the complexities and variabilities inherent in human pain experiences, thereby supporting reliable pain management practices.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Privacy and Security</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px5.p1.1">The sensitivity of health-related data necessitates stringent privacy and security measures. Our research select the EmoPain dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.00320v2#bib.bib21" title="">21</a>]</cite> that adheres to the highest standards of data protection, ensuring all participant data is anonymized and securely stored. Access controls, encryption, and ethical data handling practices protect against unauthorized access and data breaches.</p>
</div>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px5.p2.1">In conclusion, our work not only signifies a step forward in the technical domain but also embodies a comprehensive ethical approach. Through our commitment to ethical considerations, we aspire to contribute meaningfully to the field of affective computing, ensuring that our innovations in pain recognition are responsible, equitable, and beneficial for all stakeholders involved.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We’d like to thank Prof. Nadia Berthouze and Dr. Chuang Yu for helpful suggestion. This research is fully supporteds by Holistic AI and partially supported by University College London Interaction Center, University College London, UK.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. W. Picard, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Affective computing</em>.   MIT press, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. M. S. A. Abdullah, S. Y. A. Ameen, M. A. Sadeeq, and S. Zeebaree, “Multimodal emotion recognition using deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Journal of Applied Science and Technology Trends</em>, vol. 2, no. 02, pp. 52–58, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. B. Wade, D. D. Price, R. M. Hamer, S. M. Schwartz, and R. P. Hart, “An emotional component analysis of chronic pain,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Pain</em>, vol. 40, no. 3, pp. 303–310, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Olugbade, N. Bianchi-Berthouze, and A. C. d. C. Williams, “The relationship between guarding, pain, and emotion,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Pain reports</em>, vol. 4, no. 4, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Wang, W. Song, W. Tao, A. Liotta, D. Yang, X. Li, S. Gao, Y. Sun, W. Ge, W. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “A systematic review on affective computing: Emotion models, databases, and recent advances,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Information Fusion</em>, vol. 83, pp. 19–52, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. F. Barrett, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">How emotions are made: The secret life of the brain</em>.   Pan Macmillan, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Khera and V. Rangasamy, “Cognition and pain: a review,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Frontiers in psychology</em>, vol. 12, p. 1819, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">Information fusion</em>, vol. 58, pp. 82–115, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. S. Salekin, G. Zamzmi, D. Goldgof, P. R. Mouton, K. J. Anand, T. Ashmeade, S. Prescott, Y. Huang, and Y. Sun, “Attentional generative multimodal network for neonatal postoperative pain estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>.   Springer, 2022, pp. 749–759.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Gao, P. Li, Z. Chen, and J. Zhang, “A survey on deep learning for multimodal data fusion,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Neural Computation</em>, vol. 32, no. 5, pp. 829–864, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. H. Tuttle, M. J. Molinaro, J. F. Jethwa, S. G. Sotocinal, J. C. Prieto, M. A. Styner, J. S. Mogil, and M. J. Zylka, “A deep neural network to assess spontaneous pain from mouse facial expressions,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Molecular pain</em>, vol. 14, p. 1744806918763658, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Ambroz, A. Scott, A. Ambroz, and E. O. Talbott, “Chronic low back pain assessment using surface electromyography,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of occupational and environmental medicine</em>, vol. 42, no. 6, pp. 660–669, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
G. Gilam, J. J. Gross, T. D. Wager, F. J. Keefe, and S. C. Mackey, “What is the relationship between pain and emotion? bridging constructs and communities,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Neuron</em>, vol. 107, no. 1, pp. 17–21, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. A. Haque, R. B. Bautista, F. Noroozi, K. Kulkarni, C. B. Laursen, R. Irani, M. Bellantonio, S. Escalera, G. Anbarjafari, K. Nasrollahi <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Deep multimodal pain recognition: a database and comparison of spatio-temporal visual modalities,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</em>.   IEEE, 2018, pp. 250–257.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P. Werner, A. Al-Hamadi, R. Niese, S. Walter, S. Gruss, and H. C. Traue, “Automatic pain recognition from video and biomedical signals,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2014 22nd international conference on pattern recognition</em>.   IEEE, 2014, pp. 4582–4587.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T. A. Olugbade, M. H. Aung, N. Bianchi-Berthouze, N. Marquardt, and A. C. Williams, “Bi-modal detection of painful reaching for chronic pain rehabilitation systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 16th international conference on multimodal interaction</em>, 2014, pp. 455–458.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Wang, T. A. Olugbade, A. Mathur, A. C. D. C. Williams, N. D. Lane, and N. Bianchi-Berthouze, “Chronic pain protective behavior detection with deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ACM Transactions on Computing for Healthcare</em>, vol. 2, no. 3, pp. 1–24, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G. Cen, C. Wang, T. A. Olugbade, A. C. d. C. Williams, and N. Bianchi-Berthouze, “Exploring multimodal fusion for continuous protective behavior detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze, “Movement representation learning for pain level classification,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE Transactions on Affective Computing</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Wang, Y. Gao, A. Mathur, A. C. De C. Williams, N. D. Lane, and N. Bianchi-Berthouze, “Leveraging activity recognition to enable protective behavior detection in continuous data,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, vol. 5, no. 2, pp. 1–27, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. S. Aung, S. Kaltwang, B. Romera-Paredes, B. Martinez, A. Singh, M. Cella, M. Valstar, H. Meng, A. Kemp, M. Shafizadeh <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal emopain dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">IEEE transactions on affective computing</em>, vol. 7, no. 4, pp. 435–451, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Wang, Y. Gao, A. Mathur, A. C. De C. Williams, N. D. Lane, and N. Bianchi-Berthouze, “Leveraging activity recognition to enable protective behavior detection in continuous data,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, vol. 5, no. 2, jun 2021. [Online]. Available: https://doi.org/10.1145/3463508

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C. Wang, M. Peng, T. A. Olugbade, N. D. Lane, A. C. D. C. Williams, and N. Bianchi-Berthouze, “Learning temporal and bodily attention in protective movement behavior detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</em>.   IEEE, 2019, pp. 324–330.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">nature</em>, vol. 521, no. 7553, pp. 436–444, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Holzinger, “From machine learning to explainable ai,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2018 world symposium on digital intelligence for systems and machines (DISA)</em>.   IEEE, 2018, pp. 55–66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. A. Hanson, H. C. Powell Jr, A. T. Barth, K. Ringgenberg, B. H. Calhoun, J. H. Aylor, and J. Lach, “Body area sensor networks: Challenges and opportunities,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Computer</em>, vol. 42, no. 1, pp. 58–65, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T. B. Moeslund, A. Hilton, and V. Krüger, “A survey of advances in vision-based human motion capture and analysis,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Computer vision and image understanding</em>, vol. 104, no. 2-3, pp. 90–126, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R. H. Dworkin, D. C. Turk, J. T. Farrar, J. A. Haythornthwaite, M. P. Jensen, N. P. Katz, R. D. Kerns, G. Stucki, R. R. Allen, N. Bellamy <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Core outcome measures for chronic pain clinical trials: Immpact recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">pain</em>, vol. 113, no. 1, pp. 9–19, 2005.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug  1 09:10:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
