<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models</title>
<!--Generated on Thu Jul 13 18:32:33 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="css/ar5iv.min.css" rel="stylesheet" type="text/css"/>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" rel="stylesheet"/>
<script crossorigin="anonymous" integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz" src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/addons.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/feedbackOverlay.js"></script>
<link href="https://services.dev.arxiv.org/html/styles.css" rel="stylesheet" type="text/css"/></head>
<body>
<div class="ltx_page_main" id="main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\interspeechcameraready</span><span class="ltx_ERROR undefined" id="id2">\name</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.16">Holly Wilson<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.1.m1.1"><semantics id="p1.1.m1.1a"><msup id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml"><mi id="p1.1.m1.1.1a" xref="p1.1.m1.1.1.cmml"></mi><mn id="p1.1.m1.1.1.1" xref="p1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><apply id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1"><cn id="p1.1.m1.1.1.1.cmml" type="integer" xref="p1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Scott Wellington<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.2.m2.1"><semantics id="p1.2.m2.1a"><msup id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml"><mi id="p1.2.m2.1.1a" xref="p1.2.m2.1.1.cmml"></mi><mn id="p1.2.m2.1.1.1" xref="p1.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><apply id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1"><cn id="p1.2.m2.1.1.1.cmml" type="integer" xref="p1.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Foteini Simistira Liwicki<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.3.m3.1"><semantics id="p1.3.m3.1a"><msup id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml"><mi id="p1.3.m3.1.1a" xref="p1.3.m3.1.1.cmml"></mi><mn id="p1.3.m3.1.1.1" xref="p1.3.m3.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><apply id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1"><cn id="p1.3.m3.1.1.1.cmml" type="integer" xref="p1.3.m3.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.3.m3.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Vibha Gupta<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><msup id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml"><mi id="p1.4.m4.1.1a" xref="p1.4.m4.1.1.cmml"></mi><mn id="p1.4.m4.1.1.1" xref="p1.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><apply id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1"><cn id="p1.4.m4.1.1.1.cmml" type="integer" xref="p1.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Rajkumar Saini<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.5.m5.1"><semantics id="p1.5.m5.1a"><msup id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml"><mi id="p1.5.m5.1.1a" xref="p1.5.m5.1.1.cmml"></mi><mn id="p1.5.m5.1.1.1" xref="p1.5.m5.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><apply id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1"><cn id="p1.5.m5.1.1.1.cmml" type="integer" xref="p1.5.m5.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.5.m5.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Kanjar De<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.6.m6.1"><semantics id="p1.6.m6.1a"><msup id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml"><mi id="p1.6.m6.1.1a" xref="p1.6.m6.1.1.cmml"></mi><mn id="p1.6.m6.1.1.1" xref="p1.6.m6.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><apply id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1"><cn id="p1.6.m6.1.1.1.cmml" type="integer" xref="p1.6.m6.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.6.m6.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Nosheen Abid<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.7.m7.1"><semantics id="p1.7.m7.1a"><msup id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml"><mi id="p1.7.m7.1.1a" xref="p1.7.m7.1.1.cmml"></mi><mn id="p1.7.m7.1.1.1" xref="p1.7.m7.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><apply id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1"><cn id="p1.7.m7.1.1.1.cmml" type="integer" xref="p1.7.m7.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.7.m7.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Sumit Rakesh<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.8.m8.1"><semantics id="p1.8.m8.1a"><msup id="p1.8.m8.1.1" xref="p1.8.m8.1.1.cmml"><mi id="p1.8.m8.1.1a" xref="p1.8.m8.1.1.cmml"></mi><mn id="p1.8.m8.1.1.1" xref="p1.8.m8.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.8.m8.1b"><apply id="p1.8.m8.1.1.cmml" xref="p1.8.m8.1.1"><cn id="p1.8.m8.1.1.1.cmml" type="integer" xref="p1.8.m8.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.8.m8.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.8.m8.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Johan Eriksson<math alttext="{}^{3}" class="ltx_Math" display="inline" id="p1.9.m9.1"><semantics id="p1.9.m9.1a"><msup id="p1.9.m9.1.1" xref="p1.9.m9.1.1.cmml"><mi id="p1.9.m9.1.1a" xref="p1.9.m9.1.1.cmml"></mi><mn id="p1.9.m9.1.1.1" xref="p1.9.m9.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="p1.9.m9.1b"><apply id="p1.9.m9.1.1.cmml" xref="p1.9.m9.1.1"><cn id="p1.9.m9.1.1.1.cmml" type="integer" xref="p1.9.m9.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.9.m9.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="p1.9.m9.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Oliver Watts<math alttext="{}^{4}" class="ltx_Math" display="inline" id="p1.10.m10.1"><semantics id="p1.10.m10.1a"><msup id="p1.10.m10.1.1" xref="p1.10.m10.1.1.cmml"><mi id="p1.10.m10.1.1a" xref="p1.10.m10.1.1.cmml"></mi><mn id="p1.10.m10.1.1.1" xref="p1.10.m10.1.1.1.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="p1.10.m10.1b"><apply id="p1.10.m10.1.1.cmml" xref="p1.10.m10.1.1"><cn id="p1.10.m10.1.1.1.cmml" type="integer" xref="p1.10.m10.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.10.m10.1c">{}^{4}</annotation><annotation encoding="application/x-llamapun" id="p1.10.m10.1d">start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Xi Chen<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.11.m11.1"><semantics id="p1.11.m11.1a"><msup id="p1.11.m11.1.1" xref="p1.11.m11.1.1.cmml"><mi id="p1.11.m11.1.1a" xref="p1.11.m11.1.1.cmml"></mi><mn id="p1.11.m11.1.1.1" xref="p1.11.m11.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.11.m11.1b"><apply id="p1.11.m11.1.1.cmml" xref="p1.11.m11.1.1"><cn id="p1.11.m11.1.1.1.cmml" type="integer" xref="p1.11.m11.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.11.m11.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.11.m11.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Mohammad Golbabaee<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.12.m12.1"><semantics id="p1.12.m12.1a"><msup id="p1.12.m12.1.1" xref="p1.12.m12.1.1.cmml"><mi id="p1.12.m12.1.1a" xref="p1.12.m12.1.1.cmml"></mi><mn id="p1.12.m12.1.1.1" xref="p1.12.m12.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.12.m12.1b"><apply id="p1.12.m12.1.1.cmml" xref="p1.12.m12.1.1"><cn id="p1.12.m12.1.1.1.cmml" type="integer" xref="p1.12.m12.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.12.m12.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.12.m12.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Michael J. Proulx<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.13.m13.1"><semantics id="p1.13.m13.1a"><msup id="p1.13.m13.1.1" xref="p1.13.m13.1.1.cmml"><mi id="p1.13.m13.1.1a" xref="p1.13.m13.1.1.cmml"></mi><mn id="p1.13.m13.1.1.1" xref="p1.13.m13.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.13.m13.1b"><apply id="p1.13.m13.1.1.cmml" xref="p1.13.m13.1.1"><cn id="p1.13.m13.1.1.1.cmml" type="integer" xref="p1.13.m13.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.13.m13.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.13.m13.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Marcus Liwicki<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p1.14.m14.1"><semantics id="p1.14.m14.1a"><msup id="p1.14.m14.1.1" xref="p1.14.m14.1.1.cmml"><mi id="p1.14.m14.1.1a" xref="p1.14.m14.1.1.cmml"></mi><mn id="p1.14.m14.1.1.1" xref="p1.14.m14.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p1.14.m14.1b"><apply id="p1.14.m14.1.1.cmml" xref="p1.14.m14.1.1"><cn id="p1.14.m14.1.1.1.cmml" type="integer" xref="p1.14.m14.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.14.m14.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p1.14.m14.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Eamonn O'Neill<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.15.m15.1"><semantics id="p1.15.m15.1a"><msup id="p1.15.m15.1.1" xref="p1.15.m15.1.1.cmml"><mi id="p1.15.m15.1.1a" xref="p1.15.m15.1.1.cmml"></mi><mn id="p1.15.m15.1.1.1" xref="p1.15.m15.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.15.m15.1b"><apply id="p1.15.m15.1.1.cmml" xref="p1.15.m15.1.1"><cn id="p1.15.m15.1.1.1.cmml" type="integer" xref="p1.15.m15.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.15.m15.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.15.m15.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Benjamin Metcalfe<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p1.16.m16.1"><semantics id="p1.16.m16.1a"><msup id="p1.16.m16.1.1" xref="p1.16.m16.1.1.cmml"><mi id="p1.16.m16.1.1a" xref="p1.16.m16.1.1.cmml"></mi><mn id="p1.16.m16.1.1.1" xref="p1.16.m16.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p1.16.m16.1b"><apply id="p1.16.m16.1.1.cmml" xref="p1.16.m16.1.1"><cn id="p1.16.m16.1.1.1.cmml" type="integer" xref="p1.16.m16.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p1.16.m16.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p1.16.m16.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Decoding inner speech from the brain signal via hybridisation of fMRI and EEG data is explored to investigate the performance benefits over unimodal models. Two different bimodal fusion approaches are examined: concatenation of probability vectors output from unimodal fMRI and EEG machine learning models, and data fusion with feature engineering. Same-task inner speech data are recorded from four participants, and different processing strategies are compared and contrasted to previously-employed hybridisation methods. Data across participants are discovered to encode different underlying structures, which results in varying decoding performances between subject-dependent fusion models. Decoding performance is demonstrated as improved when pursuing bimodal fMRI-EEG fusion strategies, if the data show underlying structure.</p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1">Index Terms</span>: brain signal decoding; inner speech; EEG; fMRI; bimodal models; data fusion</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Since the inception and uptake of functional magnetic resonance imaging (fMRI) in the 90s and early 00s, much neurolinguistic research has been undertaken to discover the neural correlates of speech production and perception <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>. A concurrent improvement to existing electroencephalography (EEG) technology has improved access and lowered cost, and neurolinguistic research has gradually shifted in its favour. However, for decoding inner and imagined speech, many EEG studies note the processing advantages that are gained from the signals offered by fMRI data (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>, <a class="ltx_ref" href="#bib.bib3" title="">3</a>, <a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>). As such, a nascent interest in combining the two modalities has emerged: for speech and language processing, bimodal fusion methods have more recently been applied to paradigms for auditory decoding from oddball paradigms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>, speech-gesture integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>, <a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, and speech and language mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Both recording modalities have their own benefits: EEG has fine-grained millisecond temporal accuracy of up to 16,000 Hz, but limited spatial resolution. In contrast, fMRI has high  1mm spatial precision, but weak temporal resolution. Combining these two modalities can compensate for their individual limitations, and has been shown to improve decoding performance over unimodal decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Whilst fMRI has been used extensively to investigate the neural correlates of speech (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib13" title="">13</a>, <a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>), it is only recently gaining traction for brain-computer interface (BCI)-motivated inner speech decoding. Using three subjects, Tang <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite> demonstrate how fMRI may successfully be used to reconstruct perceived continuous natural language based on its semantic representation; the decoder was used to subsequently identify, via pair-wise classification, which of five story transcripts the participant was imagining. Other support for fMRI-based language decoding comes from Correia <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">et al.</span>'s study in which spoken words were shown to be decodable across languages based on semantic representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">EEG is also an increasingly-popular BCI tool for inner speech decoding; recently, van den Berg <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite> report a 35% model accuracy with a 4-class inner speech decoding paradigm, while Kiroy <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite> report model accuracies from 33% to 40% with a 7-class inner speech task—notably both of these studies employed paradigms restricted to `spatial direction’ words. Decoding performances have also been shown to remain stable with commercial-grade dry electrode EEG devices which record the brain signal with lower fidelity and signal-to-noise ratio (SNR): Clayton <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite> report a 69% accuracy for a binary consonant-vowel classification task for an imagined speech paradigm, though model performances remained statistically significant for select participants who reported performing inner speech <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">post hoc</span>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Fusion can be simultaneous (i.e. EEG and fMRI record the brain signal concurrently), or non-simultaneous, in which same-task data is gathered independently for the two modalities at different times. Simultaneous fMRI-EEG recording provides different views of the same data, enabling temporal alignment of the two datasets. However, it also introduces artifacts, such as magnetic field distortions from the EEG electrodes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite>, meaning data quality can be reduced and extra denoising required. In fMRI-EEG fusion, increasing the SNR is crucial for exploring optimal data representation and fusion strategies. In this context, non-simultaneous recording is a valuable alternative: not only is the ideal BCI robust across subjects, but also across the same task, recorded at different time-points.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Fusion of non-simultaneous EEG and fMRI has been shown to boost classification performances over unimodal classification between 8 inner speech words <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>. Results were significantly improved, with an average of 30.29% ±2.71% accuracy for 8-word classification, compared to  18% for fMRI, and  22% for EEG. Data can be fused at different stages: early, intermediate or late. In Simistira Liwicki <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, late fusion is evoked via an ensemble classification paradigm: probability vector predictions from EEG and fMRI subj-models are concatenated, and then input to a random forest (RF) classifier.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Whilst late fusion is promising for hybrid fMRI-EEG systems for inner speech, there are likely interdependencies between the two data types that can be exploited for improved decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>. Early fusion and intermediate fusion enable such cross-modal relationships to be found. Further, finding the optimal data representation of each view is another criterion for building successful hybrid systems. High dimensionality and low SNR can limit a classification model from discriminating between classes. In the research presented here, we explore and compare model performances between early and late fusion. With limited training data per class, we also use data augmentation to explore the data-driven performance of our models.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="323" id="S1.F1.g1" src="x1.png" width="436"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration of the iSpeech protocol: in our same-task protocol, participants performed the same inner speech paradigm within independent EEG and fMRI recordings.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>iSpeech protocol</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The dataset employed by this study (iSpeech), is a publicly-available dataset shared by Simistira Liwicki <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>. The words selected included four words each from two categories, numbers: <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">four</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">three</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.4">ten</span> <span class="ltx_text ltx_font_italic" id="S2.p1.1.5">six</span>; and social: <span class="ltx_text ltx_font_italic" id="S2.p1.1.6">daughter</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.7">father</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.8">wife</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.9">child</span> based on the research by Huth <span class="ltx_text ltx_font_italic" id="S2.p1.1.10">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The EEG experimental paradigm (see Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">1</span></a>) involved one session with 40 trials for each of the eight words. A trial consisted of a 1-second fixation point, followed by 2 seconds of inner speech cued by the word stimuli presented on a screen, and a rest of 1 second. fMRI data were gathered in two sessions, with 40 trials for each word across the two sessions. In a single trial, the word was again cued on the screen, but 4 seconds for the inner speech task, followed by a 10-second rest. This longer rest time is important for the lag in the fMRI blood-oxygen-level-dependent (BOLD) signal to return to baseline.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.6">A BioSemi Active2 system with 64 electrodes was used to gather EEG data, with a sampling rate of 512 Hz, and 16-bit resolution. Impedances were kept between -20 k<math alttext="\Omega" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" mathvariant="normal" xref="S2.p3.1.m1.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">roman_Ω</annotation></semantics></math> and 20 k<math alttext="\Omega" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" mathvariant="normal" xref="S2.p3.2.m2.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">roman_Ω</annotation></semantics></math>. For fMRI, a Siemens Magnetom Prisma MRI system was used. For functional images, TR = 2.16 s, voxel size = 2 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><mo id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><times id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">×</annotation></semantics></math> 2 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p3.4.m4.1"><semantics id="S2.p3.4.m4.1a"><mo id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><times id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.1d">×</annotation></semantics></math> 2 mm, and for anatomical, a sagittal T1-weighted, TR = 662.0 and voxel size 3 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p3.5.m5.1"><semantics id="S2.p3.5.m5.1a"><mo id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><times id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m5.1d">×</annotation></semantics></math> 3 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p3.6.m6.1"><semantics id="S2.p3.6.m6.1a"><mo id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><times id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p3.6.m6.1d">×</annotation></semantics></math> 2 mm. Technical validation of this data, alongside further acquisition details can be found in the data descriptor paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="392" id="S3.F2.g1" src="x2.png" width="446"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the processing pipelines for 'late fusion' and 'early fusion' bimodal models. In the late fusion model, probability vectors from each submodel are concatenated and fed into an SVM classifier. In the early fusion model, data features rather than model outputs are concatenated. </figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="566" id="S3.F3.g1" src="x3.png" width="463"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>EEG and fMRI data from <span class="ltx_text ltx_font_italic" id="S3.F3.4.1">subj-03</span>, <span class="ltx_text ltx_font_italic" id="S3.F3.5.2">subj-01</span> and <span class="ltx_text ltx_font_italic" id="S3.F3.6.3">subj-05</span> projected onto a two-dimensional manifold space using isometric mapping and spectral embedding, to visualise the underlying structure in relation to our eight classes.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">EEG and fMRI preprocessing</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">EEG data were referenced to two external electrodes placed on the mastoid processes, before applying a 1 Hz high-pass filter to remove DC drift. Line noise was removed with notch filters at 50 Hz and all harmonics up to the Nyquist frequency. ICA was applied to the data, and the highest-ranked component best describing electro-oculogram (EOG) artifacts was removed from all data. To prevent double-filtering the data with subsequent bandpass operations, the ICA weights calculated from this sanitised data are used to decompose a copy of the EEG data at the point of re-referencing, followed by the remaining cleaning operations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">fMRI data were motion-corrected after calculating spatial displacement maps, slice-time corrected and then co-registered to the T1-weighted structural scan. The data were then normalised to a canonical template, specifically Montreal Neurological Institute (MNI) space. To acquire beta images for classification, we estimated a single-trial based general linear model (GLM), convolved with a canonical haemodynamic response function. Movement parameters were used as additional regressors. Smoothing using a 8mm Gaussian kernel was applied, and background information removed using masking.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Classification Approaches</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Classification is performed for each subject separately. All cross validation is 4-fold resulting in 480 train exemplars and 160 test for each fold, with the exception of the augmented version, with 8720 training and 2906 test respectively. Augmentation involved shuffling the EEG and fMRI data within the training set, to make new pairings between the EEG and fMRI labels. See Figure <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Methodology ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">2</span></a> for a visual depiction of the methodologies. Taking fMRI beta images flattens the time dimension, resulting in a single volumetric image per exemplar, which is flattened using masking into 320 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">×</annotation></semantics></math> 216713 matrices. All code—including the fixed seeds with which experiments were run—will be made available on the lead author's GitHub repository for replicability.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Unimodal Classification</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">For <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">fMRI unimoda</span>l classification, the top 1 percentile of discriminative features were selected using ANOVA. The data was standardised and then input to the Support Vector Machine (SVM) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>, for which a grid search strategy was used to select the hyperparameters of C = 0.1 (the penalty value which sets the model's tolerance for misclassification) using a linear kernel. <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.2">EEG unimodal</span> classification was equivalent except, with the top 2 percentile of features and no scaling applied as the classifier used was a random forest classifier <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite> with 100 estimators (where each estimator is a decision tree classifier fit to subj-samples of the data using bootstrapping).
Deep learning models were created for unimodal decoding. fMRI 3D-CNN was composed of five 3D convolutional layers, with a final fully connected layer. The convolutional layers use dropout (0.3) and maxpooling followed by ReLU activation. ADAM optimizer was selected with cross entropy loss function and a learning rate of 0.0005. EEGNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> was used for unimodal EEG.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Bimodal Classification</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In the current study, we employed both an early and a late fusion approach to combine the two modalities. For <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">late fusion</span>, we use the unimodal classification but probability vectors for each subj-model are outputted rather than prediction vectors. The probability vectors are concatenated and fed into a linear SVM. In the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.2">early fusion</span> approach, the EEG and fMRI data features are concatenated, and subsequently one percentile of features are selected on the joint data representation. Grid search was used to establish the best-performing SVM model used a linear kernel, with parameter C = 0.1. In the early fusion augmentation version, 15 new EEG and fMRI pairs are created for each label instance independently for the train and test sets, prior to the concatenation step.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span class="ltx_text ltx_font_italic" id="S3.SS3.1.1">Non-linear Dimensionality Reduction</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Non-linear dimensionality reduction was applied to each subject and modality to investigate the underlying structure relating to the eight classes. Strategies to project the data into a two dimensional embedding space, included isometric mapping, spectral embedding and local linear embedding.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="398" id="S4.F4.g1" src="x4.png" width="463"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Classification accuracy for 8-class inner speech decoding using different data views and fusion approaches. Chance accuracy is 12.5%. Baseline results refer to those obtained by Simistira Liwicki <span class="ltx_text ltx_font_italic" id="S4.F4.2.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Manifold Structure</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Figure <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 Methodology ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">3</span></a> visualises the underlying structure of the neural data after projection into a manifold space using isometric mapping and spectral embedding. For the fMRI data, the clearest structure is for <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">subj-03</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">subj-05</span>. Almost no structure is identifiable for <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">subj-01</span> for either fMRI or EEG data. For EEG, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">subj-05</span> is the only subject to have visible structure.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">Unimodal Classification</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our improved data representation of the fMRI data increased performance above the 21.88% and 16.56% classification baseline for <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">subj-03</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">subj-05</span> with decoding accuracies of 46.24% and 27.18% when using an SVM classifier with a linear kernel. Decoding on <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">subj-01</span>'s fMRI data did not perform above chance level. For EEG, using an RF classifier, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">subj-01</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.5">subj-03</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.6">subj-05</span> were decodable just above chance (16-17%). See Table <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3 Bimodal Fusion ‣ 4 Results ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">1</span></a>.
After 500 epochs of training and validation, fMRI 3D-CNN achieved above chance on the test set, though worse than the SVM model, except with <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.7">subj-01</span>. EEG-NET did not classify above baseline for any of the subjects.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Bimodal Fusion</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">subj-03</span>, late fusion with improved data representation led to an improved classification accuracy on the fusion baseline, from 30.11% to 44.96% respectively. No performance enhancement was found for the remaining subjects (see Table <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3 Bimodal Fusion ‣ 4 Results ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">1</span></a>). Adding augmentation to early fusion improved results marginally for <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">subj-3</span>, but decreased results for <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.3">subj-1</span>. We suggest that the low data quality (see Figure <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 Methodology ‣ Performance of data-driven inner speech decoding with same-task EEG-fMRI data fusion and bimodal models"><span class="ltx_text ltx_ref_tag">3</span></a>) of <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.4">subj-01</span> resulted in augmentation efforts lowering the SNR by increasing the presence of noise.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Unimodal and bimodal fusion model accuracies (%).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Subject</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">sub-01</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">sub-02</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">sub-03</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">sub-05</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.6.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">EEG b.line</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.2">21.88</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.3">23.44</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.4">20.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.5">23.44</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.6">22.19</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.3.2.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.2">2.47</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">2.47</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.4">2.04</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.5">1.56</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.6">2.14</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">EEG RF</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.2">15.31</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.3">12.55</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.4">16.56</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.5">14.68</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.6">14.78</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.5.4.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.2">6.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.3">3.40</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.4">3.50</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.5">1.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.6">3.63</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">fMRI b.line</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.2">15.63</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.3">15.94</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.4">21.88</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.5">16.56</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.6">17.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.7.6.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.6.2">4.07</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.6.3">1.82</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.6.4">4.31</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.6.5">1.25</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.6.6">2.86</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">fMRI SVM</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.2">9.69</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.3">22.50</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.4">46.24</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.5">27.18</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.6">26.40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.8.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.9.8.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.2">4.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.3">7.50</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.4">7.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.5">2.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.6">5.13</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.10.9.1">Fusion b.line</th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.10.9.2">28.98</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.10.9.3">31.84</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.10.9.4">30.11</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.10.9.5">30.25</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.10.9.6">30.29</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.11.10.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.11.10.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.11.10.2">1.49</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.11.10.3">3.94</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.11.10.4">1.24</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.11.10.5">1.24</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.11.10.6">2.71</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.12.11.1">Early fusion</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.2">13.12</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.3">22.49</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.4">41.25</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.5">28.75</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.6">26.40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.13.12.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.13.12.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.13.12.2">2.80</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.13.12.3">6.60</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.13.12.4">5.60</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.13.12.5">3.10</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.13.12.6">4.53</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.14.13.1">Early f. (aug)</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.14.13.2">10.29</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.14.13.3">22.99</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.14.13.4">44.96</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.14.13.5">29.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.14.13.6">26.81</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.15.14.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.15.14.1.1">std.</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.15.14.2">4.60</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.15.14.3">6.00</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.15.14.4">6.80</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.15.14.5">1.80</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.15.14.6">4.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.16.15.1">Late fusion</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.16.15.2">10.93</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.16.15.3">20.63</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.16.15.4">47.50</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.16.15.5">26.56</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.16.15.6">26.41</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.17.16.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.17.16.1.1">std.</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.17.16.2">4.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.17.16.3">5.70</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.17.16.4">9.10</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.17.16.5">1.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.17.16.6">4.95</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Decoding inner speech from brain signals offers an exciting but challenging pursuit. fMRI-EEG fusion is proposed as a beneficial strategy for boosting decoding performance due to the distinct views that the modalities offer on inner speech processing.
We built on our previous work fusing EEG and fMRI data for inner speech decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite> to explore how choices made for enhanced data representation strategy, data augmentation and data fusion, impact classification success.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The improvements made to performance were inconsistent across subjects. Namely, <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">subj-05</span> and <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">subj-03</span> benefited from the new fMRI data representation and <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">subj-05</span> from fusion strategies. Additionally, unimodal EEG decoding achieved barely above chance level. On a surface level these results are discouraging, and certainly from just the decoding performance, no robust recommendation can be made to use one fusion strategy over another or indeed to use fusion at all. To shed light on what might be driving the variance in improvement between subjects, we investigated the underlying data structure of EEG and fMRI for each subject. To do so, we applied non-linear dimensionality reduction via isometric mapping and spectral embedding, which enables class clusters and separation to be visualised. The findings from each subject's visualisation align with, and help explain, their respective decoding results. Structure is visible for all subjects' fMRI data, except <span class="ltx_text ltx_font_italic" id="S5.p2.1.4">subj-01</span>. This is consistent with unimodal fMRI decoding performance: <span class="ltx_text ltx_font_italic" id="S5.p2.1.5">subj-01</span> scored below chance, whilst <span class="ltx_text ltx_font_italic" id="S5.p2.1.6">subj-02</span>, <span class="ltx_text ltx_font_italic" id="S5.p2.1.7">03</span>, and <span class="ltx_text ltx_font_italic" id="S5.p2.1.8">05</span> ranged from 23–46% accuracy. Conversely, coherent visible structure for EEG data was present in just <span class="ltx_text ltx_font_italic" id="S5.p2.1.9">subj-01</span> and <span class="ltx_text ltx_font_italic" id="S5.p2.1.10">05</span>. Remarkably, <span class="ltx_text ltx_font_italic" id="S5.p2.1.11">subj-05</span> was the only subject for which structure is visible in both the EEG and fMRI data, and for which fusion notably boosted performance beyond unimodal fMRI. In comparison, <span class="ltx_text ltx_font_italic" id="S5.p2.1.12">subj-01</span> shows no meaningful structure for the fMRI data and very little for the EEG data.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Using improved processing techniques, the present study achieved an average of 26% decoding accuracy on fMRI data, surpassing the 18% baseline result in Liwicki <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>. This indicates the promise of using feature selection and preprocessing techniques that reduce the data dimensionality, such as applying a mask to remove background information surrounding the brain and taking only a top percentile of discrimiative features. Lower dimensionality enables complex decoding challenges to be solved with simpler models, as demonstrated with the SVM model used in this work. Further, smoothing the fMRI data can reduce uncorrelated random noise in voxels and increase SNR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>. fMRI exhibits potential in hybrid BCIs, particularly in capturing semantic-based representations, as evidenced by the manifold visualisations. Though likely, there is further information such as phonological representation that can be extracted and exploited in fMRI.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Using deep learning to decode inner and imagined speech has revealed mixed results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>, <a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>. In this preliminary study, both the number of participants and trials for each word were insufficient for more powerful methods such as deep learning and intermediate fusion approaches to have any benefit, as demonstrated by the poor decoding performance of both our EEG and fMRI deep learning models. Interestingly, van den Berg <span class="ltx_text ltx_font_italic" id="S5.p4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite> report low deep learning decoding performances for all but one subject, for which they obtained 34.5% accuracy on an 8-class problem. This highlights the variance between subjects' decodability. We recommend assessing the underlying structure of each subject before classification. Gathering large datasets for such tasks is a challenge, but will be paramount for conclusive evaluations of complementary data in hybrid systems.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In this study we fused intra-subject data. The idea of combining different but related views of data can be extended. Fusing same-task inter-subject data boosts the dataset size and facilitates generalisable BCIs. Anatomical and functional alignment are well-demonstrated approaches to transform inter-subject fMRI data into a common space, whilst maintaining the underlying data structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>. For EEG, inter-subject fusion is less explored (though see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite> for inter-subject non-linear normalisation). However, if a common underlying structure can be discovered between fMRI and EEG that is consistent across subjects, then a hybrid bimodal and inter-subject fusion could potentially be applied. Again, this necessitates the creation of large datasets.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">For an inner speech decoding task, we assessed the performance benefits of hybrid fMRI-EEG over unimodal models. Subject-specific decoding performance aligned with visualisations of the underlying data structure. This indicates that data with a more apparent structure may benefit from fusion techniques. Interestingly, fMRI showed higher unimodal decoding performance than EEG for two subjects, supporting fMRI's efficacy for inner speech decoding. Deep learning models performed poorly on this dataset, highlighting a need for larger datasets at both a subject and exemplar level. Such datasets will present exciting opportunities to exploit inter-subject and multimodality fusion for inner speech decoding.
</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This research was funded by: the Grants for Excellent Research Projects of SRT.ai 2022; the United Kingdom Research Institute (UKRI; grant EP/S023437/1); and the Engineering and Physical Sciences Research Council (EPSRC; grant EP/S515279/1).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
L. Dong, C. Luo, X. Liu, S. Jiang, F. Li, H. Feng, J. Li, D. Gong, and D. Yao,
``Neuroscience information toolbox: An open source toolbox for eeg–fmri
multimodal fusion analysis,'' <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Frontiers in Neuroinformatics</em>, vol. 12,
p. 56, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. R. Sereshkeh, R. Trott, A. Bricout, and T. Chau, ``Eeg classification of
covert speech using regularized neural networks,'' <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE/ACM
Transactions on Audio, Speech, and Language Processing</em>, vol. 25, no. 12, pp.
2292–2300, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N. Janssen, M. v. d. Meij, P. J. López-Pérez, and H. A. Barber,
``Exploring the temporal dynamics of speech production with eeg and group
ica,'' <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Scientific reports</em>, vol. 10, no. 1, p. 3667, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Gillis, J. Vanthornhout, J. Z. Simon, T. Francart, and C. Brodbeck, ``Neural
markers of speech comprehension: measuring eeg tracking of linguistic speech
representations, controlling the speech acoustics,'' <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of
Neuroscience</em>, vol. 41, no. 50, pp. 10 316–10 329, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
N. M. Correa, Y.-O. Li, T. Adali, and V. D. Calhoun, ``Fusion of fmri, smri,
and eeg data using canonical correlation analysis,'' in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2009 IEEE
International Conference on Acoustics, Speech and Signal Processing</em>.   IEEE, 2009, pp. 385–388.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. A. Akhonda, Y. Levin-Schwartz, S. Bhinge, V. D. Calhoun, and T. Adali,
``Consecutive independence and correlation transform for multimodal fusion:
Application to eeg and fmri data,'' in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">2018 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2018, pp. 2311–2315.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. He, H. Gebhardt, M. Steines, G. Sammer, T. Kircher, A. Nagels, and
B. Straube, ``The eeg and fmri signatures of neural integration: An
investigation of meaningful gestures and corresponding speech,''
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Neuropsychologia</em>, vol. 72, pp. 27–42, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. He, M. Steines, J. Sommer, H. Gebhardt, A. Nagels, G. Sammer, T. Kircher,
and B. Straube, ``Spatial–temporal dynamics of gesture–speech integration:
a simultaneous eeg-fmri study,'' <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Brain Structure and Function</em>, vol.
223, pp. 3073–3089, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. Morillon, K. Lehongre, R. S. Frackowiak, A. Ducorps, A. Kleinschmidt,
D. Poeppel, and A.-L. Giraud, ``Neurophysiological origin of human brain
asymmetry for speech and language,'' <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the National
Academy of Sciences</em>, vol. 107, no. 43, pp. 18 688–18 693, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Puschmann, S. Steinkamp, I. Gillich, B. Mirkovic, S. Debener, and C. M.
Thiel, ``The right temporoparietal junction supports speech tracking during
selective listening: Evidence from concurrent eeg-fmri,'' <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Journal of
Neuroscience</em>, vol. 37, no. 47, pp. 11 505–11 516, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Perronnet, A. Lécuyer, M. Mano, E. Bannier, F. Lotte, M. Clerc, and
C. Barillot, ``Unimodal versus bimodal eeg-fmri neurofeedback of a motor
imagery task,'' <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Frontiers in Human Neuroscience</em>, vol. 11, p. 193,
2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. Belin, S. Fecteau, and C. Bedard, ``Thinking the voice: neural correlates of
voice perception,'' <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Trends in cognitive sciences</em>, vol. 8, no. 3, pp.
129–135, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. H. Fu, G. N. Vythelingum, M. J. Brammer, S. C. Williams, E. Amaro Jr, C. M.
Andrew, L. Yágüez, N. E. Van Haren, K. Matsumoto, and P. K. McGuire,
``An fmri study of verbal self-monitoring: neural correlates of auditory
verbal feedback,'' <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Cerebral Cortex</em>, vol. 16, no. 7, pp. 969–977,
2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I. K. Christoffels, E. Formisano, and N. O. Schiller, ``Neural correlates of
verbal feedback processing: an fmri study employing overt speech,''
<em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Human brain mapping</em>, vol. 28, no. 9, pp. 868–879, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Tang, A. LeBel, S. Jain, and A. G. Huth, ``Semantic reconstruction of
continuous language from non-invasive brain recordings,'' <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">bioRxiv</em>, pp.
2022–09, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Correia, E. Formisano, G. Valente, L. Hausfeld, B. Jansma, and M. Bonte,
``Brain-based translation: fmri decoding of spoken words in bilinguals
reveals language-independent semantic representations in anterior temporal
lobe,'' <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Journal of Neuroscience</em>, vol. 34, no. 1, pp. 332–338, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. van den Berg, S. van Donkelaar, and M. Alimardani, ``Inner speech
classification using eeg signals: A deep learning approach,'' in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2021
IEEE 2nd International Conference on Human-Machine Systems (ICHMS)</em>.   IEEE, 2021, pp. 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V. N. Kiroy, O. Bakhtin, E. Krivko, D. M. Lazurenko, E. Aslanyan,
D. Shaposhnikov, and I. V. Shcherban, ``Spoken and inner speech-related eeg
connectivity in different spatial direction,'' <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Biomedical Signal
Processing and Control</em>, vol. 71, p. 103224, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Clayton, S. Wellington, C. Valentini-Botinhao, and O. Watts, ``Decoding
imagined, heard, and spoken speech: Classification and regression of eeg
using a 14-channel dry-contact mobile headset.'' in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">INTERSPEECH</em>, 2020,
pp. 4886–4890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Uludağ and A. Roebroeck, ``General overview on the merits of
multimodal neuroimaging data fusion,'' <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Neuroimage</em>, vol. 102, pp.
3–10, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F. S. Liwicki, V. Gupta, R. Saini, K. De, N. Abid, S. Rakesh, S. Wellington,
H. Wilson, M. Liwicki, and J. Eriksson, ``Bimodal pilot study on inner
speech decoding reveals the potential of combining EEG and fMRI,'' 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. R. Stahlschmidt, B. Ulfenborg, and J. Synnergren, ``Multimodal deep learning
for biomedical data fusion: a review,'' <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Briefings in Bioinformatics</em>,
vol. 23, no. 2, p. bbab569, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
F. S. Liwicki, V. Gupta, R. Saini, K. De, N. Abid, S. Rakesh, S. Wellington,
H. Wilson, M. Liwicki, and J. Eriksson, ``Bimodal
electroencephalography-functional magnetic resonance imaging dataset for
inner-speech recognition,'' <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">bioRxiv</em>, pp. 2022–05, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. G. Huth <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, ``Natural speech reveals the semantic maps that tile
human cerebral cortex,'' <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Nature</em>, vol. 532, no. 7600, pp. 453–458,
2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Cortes and V. Vapnik, ``Support-vector networks,'' <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Machine learning</em>,
vol. 20, pp. 273–297, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. K. Ho, ``Random decision forests,'' in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of 3rd
international conference on document analysis and recognition</em>, vol. 1.   IEEE, 1995, pp. 278–282.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung, and
B. J. Lance, ``Eegnet: a compact convolutional neural network for eeg-based
brain–computer interfaces,'' <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Journal of neural engineering</em>, vol. 15,
no. 5, p. 056013, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. A. Lindquist, ``The statistical analysis of fmri data,'' <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Statistical
Science</em>, vol. 23, no. 4, pp. 439–464, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. Cooney, R. Folli, and D. Coyle, ``Mel frequency cepstral coefficients
enhance imagined speech decoding accuracy from eeg,'' in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2018 29th
Irish Signals and Systems Conference (ISSC)</em>.   IEEE, 2018, pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. V. Haxby, J. S. Guntupalli, A. C. Connolly, Y. O. Halchenko, B. R. Conroy,
M. I. Gobbini, M. Hanke, and P. J. Ramadge, ``A common, high-dimensional
model of the representational space in human ventral temporal cortex,''
<em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Neuron</em>, vol. 72, no. 2, pp. 404–416, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Arevalillo-Herráez, M. Cobos, S. Roger, and M. García-Pineda,
``Combining inter-subject modeling with a subject-based data transformation
to improve affect recognition from eeg signals,'' <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Sensors</em>, vol. 19,
no. 13, p. 2999, 2019.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="{Author name(s) withheld}" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="{Submitted to INTERSPEECH}" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 13 18:32:33 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
