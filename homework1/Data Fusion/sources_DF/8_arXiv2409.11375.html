<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification</title>
<!--Generated on Tue Sep 17 17:22:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="AI,  Self-supervised,  Supervised,  Transformer,  Deep Learning,  SwinV2,  MAE,  Autoencoder,  OCT,  classification,  Multi-modal,  Multi-class,  Transfer Learning" lang="en" name="keywords"/>
<base href="/html/2409.11375v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S1" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S2" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS1" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS2" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Self-Supervised Pre-training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS2.SSS1" title="In 3.2. Self-Supervised Pre-training ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Swin-based MAE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS2.SSS2" title="In 3.2. Self-Supervised Pre-training ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>SwinV2-based MAE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS3" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Supervised Fine-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS4" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Baseline Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS5" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Loss Function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS6" title="In 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS6.SSS1" title="In 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>DS1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS6.SSS2" title="In 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>DS2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.SS6.SSS3" title="In 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span>DS3</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS1" title="In 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span> Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS1.SSS1" title="In 4.1. Implementation Details ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Self-Supervised Pre-training Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS1.SSS2" title="In 4.1. Implementation Details ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Supervised Fine-Tuning Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS1.SSS3" title="In 4.1. Implementation Details ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Baseline Model: ResNet-50 Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2" title="In 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2.SSS1" title="In 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2.SSS2" title="In 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>AUC-ROC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2.SSS3" title="In 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>AUC-PR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2.SSS4" title="In 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>F1-Score</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS2.SSS5" title="In 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.5 </span>Penalty-Based Performance Index</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS3" title="In 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Self-Supervised Pre-training Result</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4" title="In 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Supervised Fine-tuning Result</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4.SSS1" title="In 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Performance Comparison with Different Encoder Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4.SSS2" title="In 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Performance Evaluation without Data Fusion during Pre-training Phase</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4.SSS3" title="In 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Performance Evaluation on the Effect of Self-supervised Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4.SSS4" title="In 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>Performance Comparison in Limited Data Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.SS4.SSS5" title="In 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="color:#000000;">4.4.5</span> </span><span class="ltx_text" style="color:#000000;">Performance Evaluation on the Effect of Correcting Data Imbalance</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S5" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S6" title="In Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fatema-E- Jannat
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fjannat@charlotte.edu">fjannat@charlotte.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of North Carolina at Charlotte</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">9201 University City Blvd</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Charlotte</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">North Carolina</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6">28223</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sina Gholami
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of North Carolina at Charlotte</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">9201 University City Blvd</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Charlotte</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">North Carolina</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">28223</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sgholami@charlotte.edu">sgholami@charlotte.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer I. Lim, MD
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">University of Illinois at Chicago</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Chicago</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">IL</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Theodore Leng, MD
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Stanford University School of Medicine</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Stanford</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minhaj Nur Alam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:minhaj.alam@charlotte.edu">minhaj.alam@charlotte.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">University of North Carolina at Charlotte</span><span class="ltx_text ltx_affiliation_streetaddress" id="id22.2.id2">9201 University City Blvd</span><span class="ltx_text ltx_affiliation_city" id="id23.3.id3">Charlotte</span><span class="ltx_text ltx_affiliation_state" id="id24.4.id4">North Carolina</span><span class="ltx_text ltx_affiliation_country" id="id25.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id26.6.id6">28223</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hamed Tabkhi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:htabkhiv@charlotte.edu">htabkhiv@charlotte.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id27.1.id1">University of North Carolina at Charlotte</span><span class="ltx_text ltx_affiliation_streetaddress" id="id28.2.id2">9201 University City Blvd</span><span class="ltx_text ltx_affiliation_city" id="id29.3.id3">Charlotte</span><span class="ltx_text ltx_affiliation_state" id="id30.4.id4">North Carolina</span><span class="ltx_text ltx_affiliation_country" id="id31.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id32.6.id6">28223</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id33.id1">In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns. Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training. The capacity to generalize effectively on smaller datasets remains a persistent challenge. The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions. To address this issue, we’ve combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model’s ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images. We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method. Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.</p>
</div>
<div class="ltx_keywords">AI, Self-supervised, Supervised, Transformer, Deep Learning, SwinV2, MAE, Autoencoder, OCT, classification, Multi-modal, Multi-class, Transfer Learning
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Transfer learning</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent days, the artificial intelligence domain has witnessed a revolutionary breakthrough. However, in the medical field, a significant gap persists due to the scarcity of data. Machine learning models require extensive datasets for effective training, yet the medical domain faces constraints in this regard, primarily due to privacy concerns surrounding patient data. This scarcity poses a substantial challenge, hindering the progress and application of scalable medical AI solutions in healthcare.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="531" id="S1.F1.1.1.g1" src="x1.png" width="664"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>A graphical depiction of our methodology, consolidating fundamental concepts and procedural steps.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To bridge this gap, our research addresses two key challenges. First, we focus on developing a robust machine learning classifier based on Large Language Models (LLM) to detect eye diseases from optical coherence tomography (OCT) images for AI-based eye care management. Second, we address the challenge of creating a machine learning model capable of learning from varied unlabeled data, making it useful in real-world situations with new and unseen data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Age-related macular degeneration (AMD), along with other sight-threatening conditions such as diabetic macular edema (DME), choroidal neovascularization (CNV), and diabetic retinopathy (DR), ranks among the leading causes of irreversible blindness and vision impairment (VI) globally. VI affects nearly 2.2 billion people worldwide, with almost 1 billion cases potentially preventable through early diagnosis and intervention <cite class="ltx_cite ltx_citemacro_citep">(World Health Organization, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib49" title="">2023</a>)</cite>. Therefore, it is critical to identify those who are at risk of developing the disease or seeing it progress, especially from the early stages to the more advanced stages, as prompt intervention can stop the disease’s progression or slow it down, ultimately preventing irreversible VI. Individuals at high risk for VI would greatly benefit from more frequent ophthalmic examinations, continuous monitoring, and prompt treatment <cite class="ltx_cite ltx_citemacro_citep">(Scott and Bressler, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib36" title="">2013</a>)</cite>. Leveraging AI-based tools for early detection and continuous monitoring could significantly enhance our ability to identify at-risk individuals and intervene promptly, potentially saving countless individuals from needlessly suffering vision loss and impairment <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib53" title="">2009</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Automating diagnosis in ophthalmology has shown great promise with machine learning (ML) and deep learning (DL) techniques <cite class="ltx_cite ltx_citemacro_citep">(Friberg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib13" title="">2011</a>; Alam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib3" title="">2020b</a>; Schmidt-Erfurth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib35" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib47" title="">2016</a>)</cite>. However, the limited diversity of training datasets frequently impedes their effectiveness in the application of real-world clinical settings. When implementing these models in clinical settings, it is important to make a variety of datasets sourced from multiple institutions accessible to optimize their usefulness in clinical workflows. These datasets use different OCT image-capturing devices, cover a variety of demographics, and follow different protocols. By exposing our models to a range of datasets, we can increase their scalability, versatility, and adaptability, which will ultimately improve their performance and usefulness in real-world clinical scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This work improves retinal imaging detection and diagnosis, especially in automated ophthalmic diagnosis, by utilizing recent advances in large pre-trained transformer networks. The evolution of transformer models from Natural Language Processing (NLP) to the computer vision domain underscores their capability and scalability. The transformer model, first introduced in 2017 by Vaswani et al <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib45" title="">2017</a>)</cite> for the NLP task leverages the self-attention mechanism to learn the contextual relationship among words within a sentence. This innovative approach revolutionized the NLP by enabling advancements in critical tasks like text summarization, translation, and sentence completion. Transformer model BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib10" title="">2018</a>)</cite>, introduced in 2018, and trained on an extensive corpus of text data, stands as one of the most popular models capable of performing diverse tasks in NLP.
This breakthrough performance of transformer models in NLP has raised great interest in the computer vision community. Since this architecture can learn the long-range dependencies within the data, it allows us to grasp the spatial and temporal relationships within images. By processing images as a sequence of patches, it can also capture the global context which is very important in tasks such as image classification, object detection, and pose estimation. With the introduction of the first image-based transformer architecture, Vision Transformer (ViT)<cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib11" title="">2020</a>)</cite>, in 2020 by Alexey Dosovitskiy et al., it achieved state-of-the-art performance on image classification tasks, showcasing the potential applications of this transformer architecture in the field of computer vision such as object detection, image segmentation, object tracking, etc. Later several transformer models were developed such as DeiT <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib43" title="">2021</a>)</cite>, DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib8" title="">2020</a>)</cite>, Swin Transformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib30" title="">2021</a>)</cite>, SwinV2 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib29" title="">2022</a>)</cite>, VisionLLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib46" title="">2023</a>)</cite> for the computer vision domain.
However, employing a transformer architecture requires a significant amount of computational resources, posing a challenge for communities with limited computational infrastructure. Moreover, a large dataset is crucial for effective training, creating problems in cases where such extensive datasets are unavailable. Despite these challenges, researchers are actively working on solutions to address these issues through continuous advancements in hardware, the development of efficient algorithms, the implementation of data augmentation techniques, and the integration of synthetic data.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Inspired by masked autoencoders <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib15" title="">2022</a>)</cite>, we have developed a large-scale self-supervised model with random masking, utilizing a variation of transformer models the SwinV2 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib29" title="">2022</a>)</cite> backbone which is specifically for the classification of retinal diseases from optical coherence tomography (OCT) images. Our SwinV2-based classifier leverages the transformer architecture, a foundational component also employed in Large Language Models. Its attention mechanism enables the model to dynamically determine the relative importance of various input data regions, allowing it to capture complex patterns and relationships in the data. Similar to how LLMs process and comprehend textual data by concentrating on context and semantic relationships, SwinV2 performs well in visual tasks by paying attention to relevant image regions, which improves the accuracy of feature extraction and classification. The self-attention mechanism is applied to image patches by the SwinV2 model. SwinV2 is able to model global and local features more effectively by treating images as sequences of patches, in a manner similar to how text is processed as sequences of words or tokens. This leads to enhanced performance in image classification tasks. The integration of SwinV2 in our OCT image classification task demonstrates the versatility of transformer architectures beyond natural language processing. Notably, our focus extends to the multi-class classification task, encompassing the discrimination of normal cases from those presenting with AMD, choroidal neovascularization (CNV), diabetic macular edema (DME), and diabetic retinopathy (DR). Expanding on our earlier study, ”OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection” <cite class="ltx_cite ltx_citemacro_citep">(Jannat et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib18" title="">2024</a>)</cite> which concentrated solely on binary classification, this work extends the scope to a multi-class problem.
Through the utilization of self-supervised learning (SSL), our model aims to alleviate the necessity for extensive manual annotations by experts, thereby reducing workload and facilitating broader clinical AI applications within retinal imaging data. Importantly, our model exhibits the capability to learn versatile and generalizable features from unlabeled retinal OCT datasets, a critical aspect for developing AI systems requiring fewer labeled examples to adapt to diverse diagnostic tasks.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our study stands in contrast to previous research <cite class="ltx_cite ltx_citemacro_citep">(Leandro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib25" title="">2023</a>; Tsuji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib44" title="">2020</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib27" title="">2017</a>; Awais et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib5" title="">2017</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib32" title="">2018</a>)</cite> which have primarily focused on the analysis of singular datasets in isolation. In this methodology the training centers on individual datasets, where models are trained on specific segments and then evaluated on the remainder. However, this type of framework presents challenges, particularly in practical clinical settings with limited dataset sizes. Deep learning models require larger datasets for effective training, and smaller datasets often result in poorer accuracy. Moreover, deploying a model trained on one clinical dataset to another setting is problematic due to variations in device settings and environmental factors. To handle these challenges, we investigate a more intricate method by examining complexities posed by domain adaptation across multiple datasets. We integrate several OCT datasets—DS1, DS2, and DS3—sourced from three distinct studies conducted by Kermany et al. <cite class="ltx_cite ltx_citemacro_citep">(Kermany et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib20" title="">2018</a>)</cite>, Srinivasan et al. <cite class="ltx_cite ltx_citemacro_citep">(Srinivasan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib40" title="">2014</a>)</cite>, and Li et al.<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib28" title="">2020</a>)</cite>, respectively.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text" id="S1.p8.1.1" style="color:#000000;">This approach was motivated by several key considerations. First, integrating multiple datasets will allow the pre-trained model to learn from a more diverse dataset with different clinical settings, capturing a broader spectrum of imaging conditions. This diversity will help to develop a robust model that will generalize well across unseen patient populations and clinical settings. Second, the combined dataset will increase the amount of available training data. A larger dataset enables more effective training, as the model can be exposed to a wider variety of examples, reducing the risk of overfitting. Third, pre-training the model on this comprehensive dataset will allow it to leverage transfer learning effectively. The weights learned during the pre-training stage will serve as a strong foundation for the classifier network during fine-tuning. By starting with a model already familiar with a wide range of visual features, we can achieve better performance with less training data specific to our final task, improving both training efficiency and final accuracy.</span></p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text" id="S1.p9.1.1" style="color:#000000;">This comprehensive fusion of datasets allows us to construct a unified dataset, leveraging insights and data from various modalities to develop a more holistic understanding of domain adaptation complexities.</span></p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1"><span class="ltx_text" id="S1.p10.1.1" style="color:#000000;">As a result of our approach, our model’s ability to generalize to novel, unseen data is enhanced. This proves especially beneficial in situations where access to extensive datasets is limited. Within the self-supervised pre-training phase, the model undergoes the training and validation processes on a combined large dataset. During this stage, we leverage the combined training and validation datasets from DS1, DS2, and DS3 to ensure comprehensive learning. Following pre-training, the model proceeds to the fine-tuning phase, where it is individually fine-tuned on each dataset. This fine-tuning process allows the model to adapt its learned representations to the specific characteristics of each dataset. Subsequently, the model is cross-evaluated across all test sets to assess its robustness and effectiveness across different datasets.</span></p>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">Our proposed method follows a comprehensive process that encompasses several key stages. Initially, the data fusion process occurs during the self-supervised pre-training phase, where information from multiple datasets is integrated to enhance model understanding and performance. Subsequently, fine-tuning is conducted on individual datasets to tailor the model to specific domain characteristics and optimize its performance further. Following fine-tuning, evaluation takes place on respective test sets, providing on-domain assessment measurements. Additionally, the model’s generalization capability is evaluated on other test sets for off-domain evaluation to measure its performance across diverse datasets and scenarios, thereby assessing its robustness and generalization capability. This process as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">1</span></a>, illustrates the systematic approach employed in our methodology.</p>
</div>
<div class="ltx_para" id="S1.p12">
<p class="ltx_p" id="S1.p12.1">Comparative analysis against the baseline model, ResNet-50, offers valuable insights into the performance of our proposed framework, Multi-OCT-SelfNet. Utilizing ResNet-50 as a benchmark, we trained it on individual datasets and conducted cross-evaluation across all test sets to establish a reference point for comparison. Through an ablation study, we consistently observed the superiority of our Multi-OCT-SelfNet framework. AUC-ROC (Area Under the Receiver Operating Characteristic curve) and AUC-PR (Area Under the Precision-Recall curve) values are specifically used to measure performance. When comparing our suggested framework to the baseline ResNet-50 model, these metrics provide strong indications of its efficacy and dependability.</p>
</div>
<div class="ltx_para" id="S1.p13">
<p class="ltx_p" id="S1.p13.1">The primary contributions of this paper are outlined as follows:</p>
</div>
<div class="ltx_para" id="S1.p14">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">This work presents an approach of combining multiple datasets and utilizing different modalities and showcases its efficacy in significantly improving classification performance on unseen datasets, while also demonstrating robust domain generalization capabilities.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text" id="S1.I1.i2.p1.1.1" style="color:#000000;">This paper introduces a two-phase methodology: firstly, employing a SwinV2-based masked autoencoder during pre-training, followed by a fine-tuning stage classifier for the classification of retinal diseases, specifically designed for Optical Coherence Tomography (OCT) use cases and multi-class classification tasks.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Extensive evaluation and ablation studies conducted in this paper illustrate the robustness and generalization capabilities of the proposed approach. Remarkably, even across different test sets this method exhibits improved performance without additional fine-tuning. Such findings have promising implications for the integration into real-world clinical settings.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In recent years, computer vision has emerged as an important tool in medical imaging, facilitating advanced diagnostics, treatment planning, and disease monitoring. The integration of deep learning (DL) techniques has particularly revolutionized this field, by automating tasks such as image classification, segmentation, and disease diagnosis, thereby revolutionizing medical image analysis. Concurrently, transformer networks, originally designed for natural language processing, have shown promising potential, broadening the scope of applications within medical imaging. This section explores the latest developments and research efforts in leveraging DL and transformer networks for medical image analysis, with an emphasis on the contributions and advances made by them.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The field of computer vision has undergone a significant transformation due to the evolution of deep learning (DL), which began with the introduction of AlexNet <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib23" title="">2012</a>)</cite> in 2012. Serving as one of the first deep convolutional neural network (CNN) models, AlexNet’s success in the ImageNet was a major turning point in computer vision methodologies, transitioning from traditional methods to automated DL techniques. Subsequently, numerous backbone models like VGG <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib38" title="">2014</a>)</cite>, ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib16" title="">2015</a>)</cite>, and Inception <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib41" title="">2015</a>)</cite> have developed, further advancing image analysis capabilities. These advancements extended beyond traditional computer vision, impacting medical domains by demonstrating effectiveness in tasks such as medical image classification <cite class="ltx_cite ltx_citemacro_citep">(Shazia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib37" title="">2021</a>; Krishnapriya and Karuna, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib22" title="">2023</a>; Srinivas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib39" title="">2022</a>; Bressem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib7" title="">2020</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib52" title="">2021</a>; Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib9" title="">2021</a>; Le et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib24" title="">2020</a>; Alam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib2" title="">2020a</a>)</cite>. Notably, DL techniques have found significant application in the classification of optical coherence tomography (OCT) images, particularly in diagnosing conditions like age-related macular degeneration (AMD), choroidal neovascularization (CNV), and diabetic macular edema (DME). Studies such as those referenced by <cite class="ltx_cite ltx_citemacro_citep">(Tsuji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib44" title="">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib26" title="">[n. d.]</a>)</cite> have underscored the remarkable accuracy and effectiveness of DL in distinguishing abnormal OCT images from normal ones, indicating the potential for automated screening and the development of computer-aided diagnostic tools. However, DL algorithms, particularly CNNs, need substantial amounts of training data, which can be challenging to obtain in medical imaging where data scarcity is common. Consequently, techniques such as transfer learning and domain adaptation have become essential for leveraging knowledge from source tasks to enhance performance in target tasks, addressing the data scarcity issue.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In recent advancements within medical image analysis, transformer models have emerged as a promising avenue for enhancing diagnostic accuracy and efficiency. Originally developed for natural language processing tasks, transformers have been adapted to handle the complex spatial relationships present in medical images.
In 2017 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib45" title="">2017</a>)</cite>, the transformer model was first introduced for the NLP task which leverages the self-attention mechanism to learn the contextual relationship among words within a sentence. Drawing inspiration from this concept, the vision transformer (ViT) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib11" title="">2020</a>)</cite> utilizes multi-head attention mechanisms to understand the contextual relationships among image pixels. This design excels in learning long-range dependencies in data, enhancing its ability to interpret spatial and temporal aspects of images. By treating images as sequences of patches, ViT effectively comprehends the overall context, a crucial factor in image classification, object detection, and pose estimation. The introduction of ViT marks a significant milestone, setting new benchmarks in image classification and showcasing its immense potential in various computer vision applications including medical image analysis. The application of transformer models in medical imaging, particularly in ophthalmology, has been extensively studied, as evidenced by works such as Ayana et al. <cite class="ltx_cite ltx_citemacro_citep">(Ayana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib6" title="">2023</a>)</cite>, Okolo et al. <cite class="ltx_cite ltx_citemacro_citep">(Okolo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib33" title="">2022</a>)</cite>, Alshammari et al. <cite class="ltx_cite ltx_citemacro_citep">(Alshammari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib4" title="">2022</a>)</cite>, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib48" title="">2022</a>)</cite>, and Kihara et al. <cite class="ltx_cite ltx_citemacro_citep">(Kihara et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib21" title="">2022</a>)</cite>. Notably, Wu et al. <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib50" title="">2021</a>)</cite> proposed a transformer-based approach tailored for fundus image analysis, wherein images are segmented into patches for sequential classification. This method has demonstrated remarkable performance in contrast to traditional convolutional neural networks (CNNs) across various metrics such as accuracy, specificity, precision, sensitivity, and quadratic weighted kappa score. The success of this method underscores the effectiveness of the applicability of attention mechanisms in diagnosing diabetic retinopathy. However, the adoption of a Vision Transformer (ViT) architecture poses challenges due to its heavy computational requirements, as highlighted by Islam et al. <cite class="ltx_cite ltx_citemacro_citep">(Islam, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib17" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The development of a large language model, named BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib10" title="">2018</a>)</cite>, has revolutionized natural language processing tasks, demonstrating the power of self-supervised learning techniques. Inspired by their success in language understanding, researchers have begun exploring the application of these models in the computer vision domain. By leveraging the pre-trained representations learned from vast amounts of data, these models provide a novel way to address problems in medical image analysis, like limited labeled data scenarios. The subsequent introduction of Masked Image Modeling (MIM) marked a significant advancement in the field of self-supervised learning (SSL). MIM techniques, such as the masked autoencoder (MAE) introduced by He et al. <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib15" title="">2022</a>)</cite>, focus on reconstructing masked portions of input data, allowing models to learn robust feature representations by capturing the underlying structure of visual data. Building upon this foundation, SimMIM <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib51" title="">2022</a>)</cite> introduced a straightforward but successful method that directly predicts the pixel values of masked patches in images. These advancements highlight the potential of MIM-based approaches to address challenges in medical imaging, including data scarcity and the need for robust feature extraction, ultimately advancing diagnostic capabilities.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Several studies, including those by Fang et al. <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib12" title="">2022</a>)</cite>, Qiu et al. <cite class="ltx_cite ltx_citemacro_citep">(Qiu and Sun, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib34" title="">2019</a>)</cite>, and Jing et al. <cite class="ltx_cite ltx_citemacro_citep">(Jing and Tian, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib19" title="">2020</a>)</cite>, have underscored the growing importance of self-supervised learning (SSL) in the domain of ophthalmology-focused deep learning research. Tang et al. <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib42" title="">2022</a>)</cite> demonstrated the effectiveness of SSL in conjunction with the Swin UNETR architecture for analyzing 3D medical images, achieving state-of-the-art performance. Their findings highlight the potential of SSL to address challenges such as the scarcity of labeled data and the necessity for patient-specific diagnostic tools.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">These works advance our understanding of how transformer and SSL can effectively extract meaningful information from large quantities of unlabeled data, ultimately advancing the capabilities of applicability of scalable medical AI solutions.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this work, we have taken a holistic approach by combining datasets sourced from three distinct origins. This process adds a wider range of information to the overall representation learning because each dataset contributes distinct modalities. Combining a wide range of data sources improves performance and the model’s capacity to generalize to new data by giving it a deeper understanding of the data representation. We trained an SSL MAE network with SwinV2 as its backbone architecture by utilizing this combined dataset. This pre-trained weight makes a robust foundation for classifying retinal diseases effectively in the downstream tasks.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center" id="S3.F2.1"><span class="ltx_text" id="S3.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S3.F2.1.1.g1" src="x2.png" width="789"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of the Framework: (a) In the initial pre-training phase, the framework utilizes masked image autoencoder as a self-supervised task to learn representations from unlabeled images. In this process, a random subset of image patches is masked and fed into the auto-encoder to reconstruct it. (b) In this phase, the pre-trained encoder from the first phase is employed along with a linear classifier for the classification task. The learned weights from the pre-training phase are transferred to the fine-tuning phase.</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">There are four essential stages in our proposed framework.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">(1) Data Fusion: Our study used three OCT image datasets, combining their training and validation sets for Self-Supervised Pre-training, followed by individual fine-tuning on each dataset to evaluate classification performance and generalization.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">(2) Self-Supervised Pre-training: In this initial stage of training, a self-supervised pre-training is conducted on a combined collection of unlabeled Optical Coherence Tomography (OCT) images, employing a transformer-based Masked Autoencoder(MAE) approach to extract detailed visual representations. Through this self-supervised learning process, the model gains an understanding of the structure and features within the multimodal OCT images. Subsequently, this learned weight is transferred to a supervised classifier model, leveraging the learned representations to improve the classification task.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">(3) Supervised Fine-tuning: Following the self-supervised pre-training phase, a supervised fine-tuning is conducted. This fine-tuning process aims to refine the model’s classification capabilities by leveraging the weights transferred from the pre-trained model. By exposing the model to labeled data and adjusting its parameters based on the specific task requirements, the fine-tuning stage further optimizes the model’s performance, enhancing its ability to accurately classify retinal diseases from OCT images.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S3.I1.ix4.p1">
<p class="ltx_p" id="S3.I1.ix4.p1.1">(4) Baseline Training: In our evaluation study, ResNet50 was used as the baseline model against which we compared the performance of our proposed model. By employing ResNet50 as a benchmark, we were able to assess the efficacy of our proposed approaches in improving our task.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Data Fusion</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For our study, we utilized three distinct datasets comprising Optical Coherence Tomography (OCT) images depicting various retinal diseases. Each dataset was partitioned into training, validation, and test sets. During the Self-Supervised Pre-training phase, we combined the training and validation sets from all three datasets into a unified training and validation set. This combination of data modalities aims to enhance the diversity and richness of the training data, facilitating a broader representation learning of the model. By training on this combined dataset, the model acquired a more comprehensive representation of OCT images, which ultimately contributed to improving the generalization of unseen data. Subsequently, in the Supervised Fine-tuning stage, the classifier underwent fine-tuning on each training set and was evaluated on all test sets to assess classification performance and generalization capabilities across different datasets. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F3" title="Figure 3 ‣ 3.1. Data Fusion ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">3</span></a> the overall data combination process is shown.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<p class="ltx_p ltx_align_center" id="S3.F3.1"><span class="ltx_text" id="S3.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="246" id="S3.F3.1.1.g1" src="extracted/5860915/figures/data_split.png" width="419"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Illustration of the data combination process for the Self-Supervised Pre-training phase. Training and validation sets from three distinct Optical Coherence Tomography (OCT) datasets are merged to form a unified training and validation set, enhancing diversity and richness in the model’s representation learning.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Self-Supervised Pre-training</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Self-supervised learning (SSL) is a method where models learn from unlabeled data by understanding its structure. In this study, we used a technique called Masked Autoencoder (MAE), which masks parts of input data randomly and trains the model to recreate the original by learning the representation of the input data. The MAE consists of two parts: an encoder and a decoder. We resized images to (224×224) and fed them through the encoder, which randomly masks 70% of the input image. For the encoder component, we explored the performance of two distinct networks—Swin and SwinV2—as backbone architectures, facilitating a comprehensive investigation into their effectiveness.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Swin-based MAE</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The encoder used in the Swin transformer-based Masked Autoencoder (MAE) is built with a Swin transformer backbone and has an embedding size of 96. The architecture of the Swin transformer has different numbers of layers at each stage (2, 2, 18, 2), which corresponds to the distribution of layers at each stage. The model employs shifted window attention mechanisms in each step to concentrate on local information within 4x4 patches, gradually constructing a global understanding through the connecting of shifted windows. With each step, the number of attention heads—which is set to (6, 12, 24, 48)—doubles, allowing the model to attend to progressively finer details and identify hierarchical features in the input image. In the meantime, a more expressive representation is made possible during the decoding process by the decoder, which is constructed with an embedding size of 768.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">The decoder network has a similar number of attention heads and layers as the encoder, along with Swin transformer layers that are set up to restore the spatial dimensions of the encoded features and a patch-expanding mechanism. This layer-wise design ensures a gradual reconstruction of the original image dimensions, facilitating effective decoding of the encoded representation acquired by the encoder.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>SwinV2-based MAE</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">For this task, we utilize a SwinV2-based Masked Autoencoder (MAE), capitalizing on the superior performance of the SwinV2 network. While retaining the Swin-based decoder, we used the SwinV2 for the encoder component to address challenges related to training stability, high-resolution processing, and data efficiency. The enhanced capabilities of SwinV2 align with our requirements, creating a balance between detail-oriented feature extraction and computational efficiency. Leveraging an embedding dimension of 96, depths configured as (2, 2, 6, 2), and attention heads ranging from (3, 6, 12, 24), this customized approach allowed the SwinV2-based MAEs to excel in capturing intricate details essential for our task.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">For another experiment, we enhanced the Swin-V2 encoder to increase its dimensionality and depth, resulting in a more complex network, which we denote as SwinV2-large. In this architecture, we adjusted the embedding dimension to 196, while configuring depths as (4, 4, 4, 4), and attention heads as(6, 12, 24, 48).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Supervised Fine-Tuning</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We added a classification head in place of the decoder in the classifier network. The classification head employed a linear layer to process the encoder’s features and produce class logits, which were then used for classification.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The linear layer consisted of three consecutive dense layers, each incorporating Rectified Linear Unit (ReLU) activation functions. The input image was gradually transformed into highly encoded features by these layers. The initial layer had an input size equal to the dimension of the positional embedding from the encoder, with an output size of 512. The subsequent layer refined these features, mapping them to a 256-dimensional space, followed by a final layer compressing them into a 128-dimensional feature vector.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The purpose of this hierarchical transformation was to prime the model for successful classification tasks by highlighting and reducing the amount of intrinsic discriminative features in the input. During training, the linear layer learns weights and then class logits are transformed into class probabilities using the softmax function, allowing the model to predict classes accurately.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">This methodology included fine-tuning one dataset, followed by assessing the model’s classification performance on the corresponding test set. Additionally, two separate test sets from distinct datasets were utilized to assess the model’s generalization and robustness. This iterative cross-data evaluation process was replicated across all three datasets, providing a thorough examination of the model’s adaptability to varying data sources.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">The development of Multi-OCT-SelfNet, which combines fusion data with supervised fine-tuning and self-supervised pre-training techniques, offers a strong framework for the classification of retinal diseases in Optical Coherence Tomography (OCT) images. This framework attempts to enhance the model’s ability to generalize to new, unseen data by leveraging learned representations by combining the Masked Autoencoder (MAE) architecture with a subsequent classifier model. This holistic approach underscores the importance of utilizing fusion data with both self-supervised and supervised techniques to attain comprehensive and effective disease classification in OCT imaging.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">The overall framework is given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F2" title="Figure 2 ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">2</span></a> where it is shown the two-phase training approach, Firstly, it utilizes a masked image autoencoder for self-supervised learning from unlabeled images. Then, in the second phase, the pre-trained encoder is combined with a linear classifier for classification tasks, transferring the learned weights from the initial phase. This strategy optimizes the model’s efficiency and effectiveness in handling classification tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Baseline Model</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">ResNet50 stands as a versatile solution for handling intricate tasks like classifying age-related macular degeneration (AMD) and diabetic retinopathy using Optical Coherence Tomography (OCT) images, showcasing its efficacy in medical imaging analysis. To benchmark our proposed method’s performance, we employed ResNet50 as our baseline model. The ResNet50 architecture comprises a 7×7 kernel convolution and a max pooling layer, succeeded by a series of convolutional layers with varying sizes and numbers of kernels. With 50 convolutional layers, the network is then followed by average pooling and fully connected layers, with the number of nodes matching the classes for multi-class classification, employing softmax activation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Loss Function</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">For the pre-training stage, we have used a loss function, which only takes into account the pixels where the mask is active, and uses the mean squared error (MSE) between the predicted image and the original image. In Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.E1" title="In 3.5. Loss Function ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">1</span></a>, the loss function is provided.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{1}{{m_{ratio}}}\times\frac{1}{N}\sum_{i=1}^{N}(\bar{x_{i}}-{x_{i}})^{2}%
\times{m_{i}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mfrac id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mn id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">1</mn><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">m</mi><mrow id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.3.2" xref="S3.E1.m1.1.1.3.2.3.3.2.cmml">r</mi><mo id="S3.E1.m1.1.1.3.2.3.3.1" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3" xref="S3.E1.m1.1.1.3.2.3.3.3.cmml">a</mi><mo id="S3.E1.m1.1.1.3.2.3.3.1a" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.3.3.4" xref="S3.E1.m1.1.1.3.2.3.3.4.cmml">t</mi><mo id="S3.E1.m1.1.1.3.2.3.3.1b" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.3.3.5" xref="S3.E1.m1.1.1.3.2.3.3.5.cmml">i</mi><mo id="S3.E1.m1.1.1.3.2.3.3.1c" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.3.3.6" xref="S3.E1.m1.1.1.3.2.3.3.6.cmml">o</mi></mrow></msub></mfrac><mo id="S3.E1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.3.1.cmml">×</mo><mfrac id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">1</mn><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">N</mi></mfrac></mrow><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><munderover id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml">¯</mo></mover><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E1.m1.1.1.1.1.2" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.2.cmml">×</mo><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">m</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><times id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></times><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><divide id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2"></divide><cn id="S3.E1.m1.1.1.3.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.2">1</cn><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝑚</ci><apply id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3"><times id="S3.E1.m1.1.1.3.2.3.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.2.3.3.4.cmml" xref="S3.E1.m1.1.1.3.2.3.3.4">𝑡</ci><ci id="S3.E1.m1.1.1.3.2.3.3.5.cmml" xref="S3.E1.m1.1.1.3.2.3.3.5">𝑖</ci><ci id="S3.E1.m1.1.1.3.2.3.3.6.cmml" xref="S3.E1.m1.1.1.3.2.3.3.6">𝑜</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><divide id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3"></divide><cn id="S3.E1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.2">1</cn><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑁</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.2.2.3"><eq id="S3.E1.m1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E1.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.1">¯</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\frac{1}{{m_{ratio}}}\times\frac{1}{N}\sum_{i=1}^{N}(\bar{x_{i}}-{x_{i}})^{2}%
\times{m_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">divide start_ARG 1 end_ARG start_ARG italic_m start_POSTSUBSCRIPT italic_r italic_a italic_t italic_i italic_o end_POSTSUBSCRIPT end_ARG × divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ( over¯ start_ARG italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT × italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.4">Here <math alttext="\bar{x_{i}}" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mover accent="true" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><msub id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2.2" xref="S3.SS5.p2.1.m1.1.1.2.2.cmml">x</mi><mi id="S3.SS5.p2.1.m1.1.1.2.3" xref="S3.SS5.p2.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS5.p2.1.m1.1.1.1" xref="S3.SS5.p2.1.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><ci id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1">¯</ci><apply id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.2.1.cmml" xref="S3.SS5.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2.2">𝑥</ci><ci id="S3.SS5.p2.1.m1.1.1.2.3.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\bar{x_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">over¯ start_ARG italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> is the predicted image, <math alttext="{x_{i}}" class="ltx_Math" display="inline" id="S3.SS5.p2.2.m2.1"><semantics id="S3.SS5.p2.2.m2.1a"><msub id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">{x_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the original input image, <math alttext="{m_{i}}" class="ltx_Math" display="inline" id="S3.SS5.p2.3.m3.1"><semantics id="S3.SS5.p2.3.m3.1a"><msub id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">m</mi><mi id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">𝑚</ci><ci id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">{m_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the mask, <math alttext="{m_{ratio}}" class="ltx_Math" display="inline" id="S3.SS5.p2.4.m4.1"><semantics id="S3.SS5.p2.4.m4.1a"><msub id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mi id="S3.SS5.p2.4.m4.1.1.2" xref="S3.SS5.p2.4.m4.1.1.2.cmml">m</mi><mrow id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml"><mi id="S3.SS5.p2.4.m4.1.1.3.2" xref="S3.SS5.p2.4.m4.1.1.3.2.cmml">r</mi><mo id="S3.SS5.p2.4.m4.1.1.3.1" xref="S3.SS5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p2.4.m4.1.1.3.3" xref="S3.SS5.p2.4.m4.1.1.3.3.cmml">a</mi><mo id="S3.SS5.p2.4.m4.1.1.3.1a" xref="S3.SS5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p2.4.m4.1.1.3.4" xref="S3.SS5.p2.4.m4.1.1.3.4.cmml">t</mi><mo id="S3.SS5.p2.4.m4.1.1.3.1b" xref="S3.SS5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p2.4.m4.1.1.3.5" xref="S3.SS5.p2.4.m4.1.1.3.5.cmml">i</mi><mo id="S3.SS5.p2.4.m4.1.1.3.1c" xref="S3.SS5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p2.4.m4.1.1.3.6" xref="S3.SS5.p2.4.m4.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.2">𝑚</ci><apply id="S3.SS5.p2.4.m4.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3"><times id="S3.SS5.p2.4.m4.1.1.3.1.cmml" xref="S3.SS5.p2.4.m4.1.1.3.1"></times><ci id="S3.SS5.p2.4.m4.1.1.3.2.cmml" xref="S3.SS5.p2.4.m4.1.1.3.2">𝑟</ci><ci id="S3.SS5.p2.4.m4.1.1.3.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3.3">𝑎</ci><ci id="S3.SS5.p2.4.m4.1.1.3.4.cmml" xref="S3.SS5.p2.4.m4.1.1.3.4">𝑡</ci><ci id="S3.SS5.p2.4.m4.1.1.3.5.cmml" xref="S3.SS5.p2.4.m4.1.1.3.5">𝑖</ci><ci id="S3.SS5.p2.4.m4.1.1.3.6.cmml" xref="S3.SS5.p2.4.m4.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">{m_{ratio}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.4.m4.1d">italic_m start_POSTSUBSCRIPT italic_r italic_a italic_t italic_i italic_o end_POSTSUBSCRIPT</annotation></semantics></math> is the mask ratio and N is the number of total sample.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">The MSE is multiplied by the mask to calculate the loss on the pixels where the mask is active. The mask ratio indicates the percentage of the image that is masked. Since the mask is being used to focus only on specific areas of the image, the loss is calculated by dividing the mean square error (MSE) by the mask ratio. This allows us to properly normalize the loss to the proportion of the image that is masked and scale the loss accordingly.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Datasets</h3>
<figure class="ltx_figure" id="S3.F4">
<p class="ltx_p ltx_align_center" id="S3.F4.1"><span class="ltx_text" id="S3.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="515" id="S3.F4.1.1.g1" src="extracted/5860915/figures/data_distribution_multi.png" width="479"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Distribution of Retinal Disease Samples Across Three Datasets: Grouped-bar diagrams show sample counts in training, validation, and test sets for each retinal disease category in datasets DS1, DS2, and DS3. The Donut charts display the overall percentage distribution per dataset.
</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1. </span>DS1</h4>
<div class="ltx_para" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">This dataset comprises a total of 109,559 Optical Coherence Tomography (OCT) retinal images acquired using Spectralis OCT from Heidelberg Engineering, Germany. These images are categorized into four classes: Normal, Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen. Upon identifying identical images, as documented previously <cite class="ltx_cite ltx_citemacro_citep">(Gholami et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib14" title="">2023</a>)</cite>, we followed their methodology to cleanse the dataset, resulting in 101,565 images. Subsequently, we reclassified Drusen images as Age-related Macular Degeneration (AMD). DS1 was then partitioned into training, testing, and validation sets using an 80%, 10%, and 10% ratio, respectively. The distribution of samples across each category within each set is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F4" title="Figure 4 ‣ 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2. </span>DS2</h4>
<div class="ltx_para" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">The DS2 dataset is acquired using Heidelberg Engineering Spectralis SD-OCT protocols approved by the Institutional Review Board (IRB) <cite class="ltx_cite ltx_citemacro_citep">(Srinivasan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib40" title="">2014</a>)</cite>. It consists of retinal images sourced from 45 subjects, distributed as follows: 15 normal subjects, 15 patients diagnosed with dry age-related macular degeneration (AMD), and 15 patients with diabetic macular edema (DME). To organize the dataset for training, validation, and testing purposes, we split the subjects accordingly: the first 10 subjects from each category were allocated to the training set, subjects 11 to 12 were assigned to the validation set, and subjects 13 to 15 were designated to the test set. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F4" title="Figure 4 ‣ 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a> shows the distribution of samples within each set, across each category.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.3. </span>DS3</h4>
<div class="ltx_para" id="S3.SS6.SSS3.p1">
<p class="ltx_p" id="S3.SS6.SSS3.p1.1">This dataset comprising OCT images from 500 subjects, was obtained using two different fields of view: 3-mm and 6-mm. Each 3-mm file contains 304 scans per patient, while a 6-mm file contains 400 scans. Our analysis focused on the slice images of the fovea (image numbers 160-240 from the 6-mm scans), capturing the most prominent retinal features, while peripheral retinal sections were deemed of limited significance for classification. This dataset comprises categories such as NORMAL, AMD, CNV, DR, OTHERS, RVO, and CSC. Given the relatively low number of samples in the RVO and CSC categories, we excluded them. The ”OTHERS” category comprises diseases, including retinal detachment (RD), retinal hemorrhage (RH), and retinitis pigmentosa (RP), among others, which were also discarded due to their lack of particularity. All OCT images were captured using a spectral-domain OCT system with a center wavelength of 840 nm (RTVue-XR, Optovue, CA) [ ]. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F4" title="Figure 4 ‣ 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a> provides an overview of the sample distribution across each category within each set.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS3.p2">
<p class="ltx_p" id="S3.SS6.SSS3.p2.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F4" title="Figure 4 ‣ 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a> the distribution of retinal disease samples across training, validation, and test sets for each category in three datasets (DS1, DS2, and DS3) is shown. Grouped-bar diagrams depict the counts of samples in each category for training, validation, and test sets. Additionally, the donut charts illustrate the overall percentage distribution of samples for each dataset, providing a comprehensive view of the distribution of retinal disease samples in the study. Class presence and sample counts differ significantly; not all classes are present in all datasets in the same way. For example, although ’NORMAL’ is present in all datasets, its frequency varies greatly: DS1 has a high count of 50,310 samples, DS3 has 4,941 samples, and DS2 has a significantly lower count of 1,407 samples. Similarly, the ’AMD’ class exhibits varying degrees of representation, with 8,195 samples in DS1, 723 samples in DS2, and 2,349 samples in DS3. Such disparities extend further, with ’DME’ being present in DS1 and DS2 but absent in DS3, while ’DR’ finds exclusivity in DS3. These variations in class presence and sample counts highlight the complexities present in dataset composition and the implications for model development and evaluation.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS3.p3">
<p class="ltx_p" id="S3.SS6.SSS3.p3.1">The model was fully trained with all classes found in datasets DS1, DS2, and DS3 during the self-supervised pre-training phase, which enabled it to fully understand the complexities of representation learning. Because of this inclusive training strategy, the network was able to capture a wide range of features and patterns that were present in the various classes. However, in the supervised fine-tuning stage, our focus narrowed to the classification tasks specifically targeting ’NORMAL’ categories from the ’AMD’, ’DME’, ’CNV’, and ’DR’ classes.
This two-stage training process, which includes thorough pre-training and specialized fine-tuning, strategically directs the model’s learning of its representation and maximizes its performance for the desired classification goal. This methodology helps the model to be optimally tuned to achieve the intended classification objectives by shifting from a general comprehension of all classes to a targeted refinement of the desired classification tasks.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span> Implementation Details</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Self-Supervised Pre-training Implementation Details</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.4">For this stage of self-supervised pre-training, all experiments were conducted using the computational power of the NVIDIA Tesla V100 graphical processing unit (GPU).
Specific hyperparameters were selected to optimize model performance. The learning rate was set to <math alttext="1.5\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">1.5</mn><mo id="S4.SS1.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS1.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.SSS1.p1.1.m1.1.1.3.3a" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><times id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">1.5</cn><apply id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS1.SSS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3"><minus id="S4.SS1.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3"></minus><cn id="S4.SS1.SSS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">1.5\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">1.5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, and the Adam optimizer was employed with a weight decay of <math alttext="0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.2.m2.1"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mn id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><cn id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" type="float" xref="S4.SS1.SSS1.p1.2.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.2.m2.1d">0.05</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib31" title="">2019</a>)</cite>. Additionally, the optimizer utilized <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.3.m3.1"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><msub id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p1.3.m3.1.1.2" xref="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS1.p1.3.m3.1.1.3" xref="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.1b"><apply id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.2">𝛽</ci><cn id="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.3.m3.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.4.m4.1"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><msub id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS1.p1.4.m4.1.1.3" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><apply id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.2">𝛽</ci><cn id="S4.SS1.SSS1.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.4.m4.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> values of 0.9 and 0.95, respectively. Input data consisted of batches comprising 32 normalized images. Training proceeded for a total of 100 epochs. To ensure the most robust configuration, the model with the lowest validation loss was saved for subsequent fine-tuning iterations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Supervised Fine-Tuning Implementation Details</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.4">During the fine-tuning stage, similar to the training phase, experiments were conducted using the NVIDIA Tesla V100 GPU, and the hyperparameters were selected as follows, the learning rate was adjusted to <math alttext="3\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mrow id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml">3</mn><mo id="S4.SS1.SSS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.SSS2.p1.1.m1.1.1.3.3a" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><times id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">3</cn><apply id="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3"><minus id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3"></minus><cn id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">3\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, and the Adam optimizer was employed with a weight decay of <math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">1</mn><mo id="S4.SS1.SSS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.SSS2.p1.2.m2.1.1.3.3a" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><times id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></times><cn id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">1</cn><apply id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">superscript</csymbol><cn id="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3"><minus id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3"></minus><cn id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">1\times 10^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">1 × 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>. The optimizer utilized <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><msub id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">𝛽</ci><cn id="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.3.m3.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.4.m4.1"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><msub id="S4.SS1.SSS2.p1.4.m4.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS2.p1.4.m4.1.1.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.1b"><apply id="S4.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.2">𝛽</ci><cn id="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.4.m4.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> values of 0.9 and 0.99, respectively. As the loss function, the categorical cross-entropy loss is employed. The training process involved the application of multiple data augmentation techniques, such as random resized crop, random horizontal flip, color jitter, random grayscale, and ImageNet normalization, to improve the robustness of the model. Training extended over 100 epochs, with early stopping criteria implemented using patience of 10 epochs. The model with the highest validation accuracy was saved for subsequent testing.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">During training and fine-tuning, all images were resized to 224 x 224. Python 3.10.9, Pytorch 1.12.1, and CUDA 11.2 were used to implement the codes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Baseline Model: ResNet-50 Implementation Details</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.4">During the experiments with the baseline model, an NVIDIA GeForce RTX 3060 Ti GPU was used. The learning rate was set to <math alttext="3\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.1.m1.1"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mrow id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS3.p1.1.m1.1.1.2" xref="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml">3</mn><mo id="S4.SS1.SSS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS3.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS3.p1.1.m1.1.1.3" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS3.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS3.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.SSS3.p1.1.m1.1.1.3.3a" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS3.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><apply id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1"><times id="S4.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.1.1.2">3</cn><apply id="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS1.SSS3.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3"><minus id="S4.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3"></minus><cn id="S4.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">3\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.1.m1.1d">3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, accompanied by a weight decay of <math alttext="10^{-6}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.2.m2.1"><semantics id="S4.SS1.SSS3.p1.2.m2.1a"><msup id="S4.SS1.SSS3.p1.2.m2.1.1" xref="S4.SS1.SSS3.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS3.p1.2.m2.1.1.2" xref="S4.SS1.SSS3.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS3.p1.2.m2.1.1.3" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.cmml"><mo id="S4.SS1.SSS3.p1.2.m2.1.1.3a" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS3.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.2.m2.1b"><apply id="S4.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1">superscript</csymbol><cn id="S4.SS1.SSS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.2.m2.1.1.2">10</cn><apply id="S4.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3"><minus id="S4.SS1.SSS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3"></minus><cn id="S4.SS1.SSS3.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.2.m2.1c">10^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.2.m2.1d">10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>, a batch size of 24, and the utilization of the Adam optimizer with decoupled weight decay <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#bib.bib31" title="">2019</a>)</cite>. Momentum and adaptive learning rate scaling were set by <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.3.m3.1"><semantics id="S4.SS1.SSS3.p1.3.m3.1a"><msub id="S4.SS1.SSS3.p1.3.m3.1.1" xref="S4.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS3.p1.3.m3.1.1.2" xref="S4.SS1.SSS3.p1.3.m3.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS3.p1.3.m3.1.1.3" xref="S4.SS1.SSS3.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.3.m3.1b"><apply id="S4.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1.2">𝛽</ci><cn id="S4.SS1.SSS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.3.m3.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.3.m3.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.4.m4.1"><semantics id="S4.SS1.SSS3.p1.4.m4.1a"><msub id="S4.SS1.SSS3.p1.4.m4.1.1" xref="S4.SS1.SSS3.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS3.p1.4.m4.1.1.2" xref="S4.SS1.SSS3.p1.4.m4.1.1.2.cmml">β</mi><mn id="S4.SS1.SSS3.p1.4.m4.1.1.3" xref="S4.SS1.SSS3.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.4.m4.1b"><apply id="S4.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1.2">𝛽</ci><cn id="S4.SS1.SSS3.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS1.SSS3.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.4.m4.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.4.m4.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> values of 0.9 and 0.999, respectively. Training proceeded for a maximum of 100 epochs, with early stopping criteria based on validation loss, incorporating a patience of 10 epochs. To enhance the model’s robustness and generalization capabilities, various data augmentation techniques were employed, including random rotation, horizontal flip, color jittering, Gaussian blurring, and elastic transform.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="color:#000000;">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S3.F4" title="Figure 4 ‣ 3.6. Datasets ‣ 3. Methodology ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a>, all datasets exhibit a substantial class imbalance, with a predominance of ’NORMAL’ cases. Given this, relying solely on accuracy can be misleading. To mitigate this issue, we employ AUC-ROC as the primary evaluation measure, as it effectively provides a more robust evaluation by considering the trade-off between true positive and false positive rates across different thresholds. Additionally, we incorporate accuracy, AUC-PR, and F1-score to offer a comprehensive evaluation.</span></p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Accuracy</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The number of accurate predictions made by the model is known as accuracy, and it is determined by dividing the total number of predictions by the number of correct predictions. The accuracy formula is provided by Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.E2" title="In 4.2.1. Accuracy ‣ 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Accuracy=\frac{TP+TN}{TP+TN+FP+FN}" class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">A</mi><mo id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1b" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.5" xref="S4.E2.m1.1.1.2.5.cmml">u</mi><mo id="S4.E2.m1.1.1.2.1c" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.6" xref="S4.E2.m1.1.1.2.6.cmml">r</mi><mo id="S4.E2.m1.1.1.2.1d" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.7" xref="S4.E2.m1.1.1.2.7.cmml">a</mi><mo id="S4.E2.m1.1.1.2.1e" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.8" xref="S4.E2.m1.1.1.2.8.cmml">c</mi><mo id="S4.E2.m1.1.1.2.1f" xref="S4.E2.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.2.9" xref="S4.E2.m1.1.1.2.9.cmml">y</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mrow id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.3.2.2.2.cmml">T</mi><mo id="S4.E2.m1.1.1.3.2.2.1" xref="S4.E2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.3.2.2.3.cmml">P</mi></mrow><mo id="S4.E2.m1.1.1.3.2.1" xref="S4.E2.m1.1.1.3.2.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.3.2.3" xref="S4.E2.m1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.3.2.3.2.cmml">T</mi><mo id="S4.E2.m1.1.1.3.2.3.1" xref="S4.E2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.3.2.3.3.cmml">N</mi></mrow></mrow><mrow id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml"><mrow id="S4.E2.m1.1.1.3.3.2" xref="S4.E2.m1.1.1.3.3.2.cmml"><mi id="S4.E2.m1.1.1.3.3.2.2" xref="S4.E2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S4.E2.m1.1.1.3.3.2.1" xref="S4.E2.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.3.2.3" xref="S4.E2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S4.E2.m1.1.1.3.3.1" xref="S4.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.3.3.3" xref="S4.E2.m1.1.1.3.3.3.cmml"><mi id="S4.E2.m1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.2.cmml">T</mi><mo id="S4.E2.m1.1.1.3.3.3.1" xref="S4.E2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.cmml">N</mi></mrow><mo id="S4.E2.m1.1.1.3.3.1a" xref="S4.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.3.3.4" xref="S4.E2.m1.1.1.3.3.4.cmml"><mi id="S4.E2.m1.1.1.3.3.4.2" xref="S4.E2.m1.1.1.3.3.4.2.cmml">F</mi><mo id="S4.E2.m1.1.1.3.3.4.1" xref="S4.E2.m1.1.1.3.3.4.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.3.4.3" xref="S4.E2.m1.1.1.3.3.4.3.cmml">P</mi></mrow><mo id="S4.E2.m1.1.1.3.3.1b" xref="S4.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.3.3.5" xref="S4.E2.m1.1.1.3.3.5.cmml"><mi id="S4.E2.m1.1.1.3.3.5.2" xref="S4.E2.m1.1.1.3.3.5.2.cmml">F</mi><mo id="S4.E2.m1.1.1.3.3.5.1" xref="S4.E2.m1.1.1.3.3.5.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.3.3.5.3" xref="S4.E2.m1.1.1.3.3.5.3.cmml">N</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝐴</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">𝑐</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">𝑐</ci><ci id="S4.E2.m1.1.1.2.5.cmml" xref="S4.E2.m1.1.1.2.5">𝑢</ci><ci id="S4.E2.m1.1.1.2.6.cmml" xref="S4.E2.m1.1.1.2.6">𝑟</ci><ci id="S4.E2.m1.1.1.2.7.cmml" xref="S4.E2.m1.1.1.2.7">𝑎</ci><ci id="S4.E2.m1.1.1.2.8.cmml" xref="S4.E2.m1.1.1.2.8">𝑐</ci><ci id="S4.E2.m1.1.1.2.9.cmml" xref="S4.E2.m1.1.1.2.9">𝑦</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><divide id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3"></divide><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><plus id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2.1"></plus><apply id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2"><times id="S4.E2.m1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2.2">𝑇</ci><ci id="S4.E2.m1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3">𝑃</ci></apply><apply id="S4.E2.m1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.3.2.3"><times id="S4.E2.m1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.2">𝑇</ci><ci id="S4.E2.m1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3">𝑁</ci></apply></apply><apply id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3"><plus id="S4.E2.m1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.1"></plus><apply id="S4.E2.m1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2"><times id="S4.E2.m1.1.1.3.3.2.1.cmml" xref="S4.E2.m1.1.1.3.3.2.1"></times><ci id="S4.E2.m1.1.1.3.3.2.2.cmml" xref="S4.E2.m1.1.1.3.3.2.2">𝑇</ci><ci id="S4.E2.m1.1.1.3.3.2.3.cmml" xref="S4.E2.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S4.E2.m1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3"><times id="S4.E2.m1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3.1"></times><ci id="S4.E2.m1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.2">𝑇</ci><ci id="S4.E2.m1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3">𝑁</ci></apply><apply id="S4.E2.m1.1.1.3.3.4.cmml" xref="S4.E2.m1.1.1.3.3.4"><times id="S4.E2.m1.1.1.3.3.4.1.cmml" xref="S4.E2.m1.1.1.3.3.4.1"></times><ci id="S4.E2.m1.1.1.3.3.4.2.cmml" xref="S4.E2.m1.1.1.3.3.4.2">𝐹</ci><ci id="S4.E2.m1.1.1.3.3.4.3.cmml" xref="S4.E2.m1.1.1.3.3.4.3">𝑃</ci></apply><apply id="S4.E2.m1.1.1.3.3.5.cmml" xref="S4.E2.m1.1.1.3.3.5"><times id="S4.E2.m1.1.1.3.3.5.1.cmml" xref="S4.E2.m1.1.1.3.3.5.1"></times><ci id="S4.E2.m1.1.1.3.3.5.2.cmml" xref="S4.E2.m1.1.1.3.3.5.2">𝐹</ci><ci id="S4.E2.m1.1.1.3.3.5.3.cmml" xref="S4.E2.m1.1.1.3.3.5.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Accuracy=\frac{TP+TN}{TP+TN+FP+FN}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">italic_A italic_c italic_c italic_u italic_r italic_a italic_c italic_y = divide start_ARG italic_T italic_P + italic_T italic_N end_ARG start_ARG italic_T italic_P + italic_T italic_N + italic_F italic_P + italic_F italic_N end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">Here, TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>AUC-ROC</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">One important metric for assessing the performance of the classifier is the Area Under the Receiver Operating Characteristic curve (AUC-ROC). Plotting each class’s true positive rate against its false positive rate across a range of threshold values creates the ROC curve. The area under this curve, or AUC-ROC, gives a thorough overview of the model’s performance in all classes and threshold settings. A higher AUC-ROC signifies better discrimination ability among the different classes, indicating superior classifier performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>AUC-PR</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">The Area Under the Precision-Recall curve (AUC-PR), a performance metric for classifier evaluation, is comparable to AUC-ROC. By plotting precision against recall across different threshold values, the PR curve emphasizes the trade-off between recall and precision. The AUC-PR quantifies the overall performance of the model across various threshold settings by measuring the area under this curve. A higher AUC-PR score indicates better classifier performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>F1-Score</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Another performance evaluation metric that accounts for both recall and precision is the F1-Score. This metric is particularly helpful when dealing with data imbalances. By incorporating both precision and recall, the F1-Score provides a comprehensive evaluation of a classifier’s effectiveness. This metric is especially suitable in situations where accurately capturing both positive and negative instances is critical for decision-making and model evaluation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.1">Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.E3" title="In 4.2.4. F1-Score ‣ 4.2. Evaluation Metrics ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">3</span></a> is used to calculate the F1-Score.</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F1Score=\frac{2*Precision*Recall}{Precision+Recall}" class="ltx_Math" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mrow id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.2.2" xref="S4.E3.m1.1.1.2.2.cmml">F</mi><mo id="S4.E3.m1.1.1.2.1" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.E3.m1.1.1.2.3" xref="S4.E3.m1.1.1.2.3.cmml">1</mn><mo id="S4.E3.m1.1.1.2.1a" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.2.4" xref="S4.E3.m1.1.1.2.4.cmml">S</mi><mo id="S4.E3.m1.1.1.2.1b" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.2.5" xref="S4.E3.m1.1.1.2.5.cmml">c</mi><mo id="S4.E3.m1.1.1.2.1c" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.2.6" xref="S4.E3.m1.1.1.2.6.cmml">o</mi><mo id="S4.E3.m1.1.1.2.1d" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.2.7" xref="S4.E3.m1.1.1.2.7.cmml">r</mi><mo id="S4.E3.m1.1.1.2.1e" xref="S4.E3.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.2.8" xref="S4.E3.m1.1.1.2.8.cmml">e</mi></mrow><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mrow id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml"><mrow id="S4.E3.m1.1.1.3.2.2" xref="S4.E3.m1.1.1.3.2.2.cmml"><mrow id="S4.E3.m1.1.1.3.2.2.2" xref="S4.E3.m1.1.1.3.2.2.2.cmml"><mrow id="S4.E3.m1.1.1.3.2.2.2.2" xref="S4.E3.m1.1.1.3.2.2.2.2.cmml"><mn id="S4.E3.m1.1.1.3.2.2.2.2.2" xref="S4.E3.m1.1.1.3.2.2.2.2.2.cmml">2</mn><mo id="S4.E3.m1.1.1.3.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S4.E3.m1.1.1.3.2.2.2.2.1.cmml">∗</mo><mi id="S4.E3.m1.1.1.3.2.2.2.2.3" xref="S4.E3.m1.1.1.3.2.2.2.2.3.cmml">P</mi></mrow><mo id="S4.E3.m1.1.1.3.2.2.2.1" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.3" xref="S4.E3.m1.1.1.3.2.2.2.3.cmml">r</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1a" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.4" xref="S4.E3.m1.1.1.3.2.2.2.4.cmml">e</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1b" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.5" xref="S4.E3.m1.1.1.3.2.2.2.5.cmml">c</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1c" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.6" xref="S4.E3.m1.1.1.3.2.2.2.6.cmml">i</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1d" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.7" xref="S4.E3.m1.1.1.3.2.2.2.7.cmml">s</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1e" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.8" xref="S4.E3.m1.1.1.3.2.2.2.8.cmml">i</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1f" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.9" xref="S4.E3.m1.1.1.3.2.2.2.9.cmml">o</mi><mo id="S4.E3.m1.1.1.3.2.2.2.1g" xref="S4.E3.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.2.2.10" xref="S4.E3.m1.1.1.3.2.2.2.10.cmml">n</mi></mrow><mo id="S4.E3.m1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S4.E3.m1.1.1.3.2.2.1.cmml">∗</mo><mi id="S4.E3.m1.1.1.3.2.2.3" xref="S4.E3.m1.1.1.3.2.2.3.cmml">R</mi></mrow><mo id="S4.E3.m1.1.1.3.2.1" xref="S4.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.3" xref="S4.E3.m1.1.1.3.2.3.cmml">e</mi><mo id="S4.E3.m1.1.1.3.2.1a" xref="S4.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.4" xref="S4.E3.m1.1.1.3.2.4.cmml">c</mi><mo id="S4.E3.m1.1.1.3.2.1b" xref="S4.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.5" xref="S4.E3.m1.1.1.3.2.5.cmml">a</mi><mo id="S4.E3.m1.1.1.3.2.1c" xref="S4.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.6" xref="S4.E3.m1.1.1.3.2.6.cmml">l</mi><mo id="S4.E3.m1.1.1.3.2.1d" xref="S4.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.2.7" xref="S4.E3.m1.1.1.3.2.7.cmml">l</mi></mrow><mrow id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml"><mrow id="S4.E3.m1.1.1.3.3.2" xref="S4.E3.m1.1.1.3.3.2.cmml"><mi id="S4.E3.m1.1.1.3.3.2.2" xref="S4.E3.m1.1.1.3.3.2.2.cmml">P</mi><mo id="S4.E3.m1.1.1.3.3.2.1" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.3" xref="S4.E3.m1.1.1.3.3.2.3.cmml">r</mi><mo id="S4.E3.m1.1.1.3.3.2.1a" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.4" xref="S4.E3.m1.1.1.3.3.2.4.cmml">e</mi><mo id="S4.E3.m1.1.1.3.3.2.1b" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.5" xref="S4.E3.m1.1.1.3.3.2.5.cmml">c</mi><mo id="S4.E3.m1.1.1.3.3.2.1c" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.6" xref="S4.E3.m1.1.1.3.3.2.6.cmml">i</mi><mo id="S4.E3.m1.1.1.3.3.2.1d" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.7" xref="S4.E3.m1.1.1.3.3.2.7.cmml">s</mi><mo id="S4.E3.m1.1.1.3.3.2.1e" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.8" xref="S4.E3.m1.1.1.3.3.2.8.cmml">i</mi><mo id="S4.E3.m1.1.1.3.3.2.1f" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.9" xref="S4.E3.m1.1.1.3.3.2.9.cmml">o</mi><mo id="S4.E3.m1.1.1.3.3.2.1g" xref="S4.E3.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.2.10" xref="S4.E3.m1.1.1.3.3.2.10.cmml">n</mi></mrow><mo id="S4.E3.m1.1.1.3.3.1" xref="S4.E3.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.3.3.3" xref="S4.E3.m1.1.1.3.3.3.cmml"><mi id="S4.E3.m1.1.1.3.3.3.2" xref="S4.E3.m1.1.1.3.3.3.2.cmml">R</mi><mo id="S4.E3.m1.1.1.3.3.3.1" xref="S4.E3.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.3.3" xref="S4.E3.m1.1.1.3.3.3.3.cmml">e</mi><mo id="S4.E3.m1.1.1.3.3.3.1a" xref="S4.E3.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.3.4" xref="S4.E3.m1.1.1.3.3.3.4.cmml">c</mi><mo id="S4.E3.m1.1.1.3.3.3.1b" xref="S4.E3.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.3.5" xref="S4.E3.m1.1.1.3.3.3.5.cmml">a</mi><mo id="S4.E3.m1.1.1.3.3.3.1c" xref="S4.E3.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.3.6" xref="S4.E3.m1.1.1.3.3.3.6.cmml">l</mi><mo id="S4.E3.m1.1.1.3.3.3.1d" xref="S4.E3.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.3.3.3.7" xref="S4.E3.m1.1.1.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><apply id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"><times id="S4.E3.m1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.2.1"></times><ci id="S4.E3.m1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.2.2">𝐹</ci><cn id="S4.E3.m1.1.1.2.3.cmml" type="integer" xref="S4.E3.m1.1.1.2.3">1</cn><ci id="S4.E3.m1.1.1.2.4.cmml" xref="S4.E3.m1.1.1.2.4">𝑆</ci><ci id="S4.E3.m1.1.1.2.5.cmml" xref="S4.E3.m1.1.1.2.5">𝑐</ci><ci id="S4.E3.m1.1.1.2.6.cmml" xref="S4.E3.m1.1.1.2.6">𝑜</ci><ci id="S4.E3.m1.1.1.2.7.cmml" xref="S4.E3.m1.1.1.2.7">𝑟</ci><ci id="S4.E3.m1.1.1.2.8.cmml" xref="S4.E3.m1.1.1.2.8">𝑒</ci></apply><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><divide id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3"></divide><apply id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2"><times id="S4.E3.m1.1.1.3.2.1.cmml" xref="S4.E3.m1.1.1.3.2.1"></times><apply id="S4.E3.m1.1.1.3.2.2.cmml" xref="S4.E3.m1.1.1.3.2.2"><times id="S4.E3.m1.1.1.3.2.2.1.cmml" xref="S4.E3.m1.1.1.3.2.2.1"></times><apply id="S4.E3.m1.1.1.3.2.2.2.cmml" xref="S4.E3.m1.1.1.3.2.2.2"><times id="S4.E3.m1.1.1.3.2.2.2.1.cmml" xref="S4.E3.m1.1.1.3.2.2.2.1"></times><apply id="S4.E3.m1.1.1.3.2.2.2.2.cmml" xref="S4.E3.m1.1.1.3.2.2.2.2"><times id="S4.E3.m1.1.1.3.2.2.2.2.1.cmml" xref="S4.E3.m1.1.1.3.2.2.2.2.1"></times><cn id="S4.E3.m1.1.1.3.2.2.2.2.2.cmml" type="integer" xref="S4.E3.m1.1.1.3.2.2.2.2.2">2</cn><ci id="S4.E3.m1.1.1.3.2.2.2.2.3.cmml" xref="S4.E3.m1.1.1.3.2.2.2.2.3">𝑃</ci></apply><ci id="S4.E3.m1.1.1.3.2.2.2.3.cmml" xref="S4.E3.m1.1.1.3.2.2.2.3">𝑟</ci><ci id="S4.E3.m1.1.1.3.2.2.2.4.cmml" xref="S4.E3.m1.1.1.3.2.2.2.4">𝑒</ci><ci id="S4.E3.m1.1.1.3.2.2.2.5.cmml" xref="S4.E3.m1.1.1.3.2.2.2.5">𝑐</ci><ci id="S4.E3.m1.1.1.3.2.2.2.6.cmml" xref="S4.E3.m1.1.1.3.2.2.2.6">𝑖</ci><ci id="S4.E3.m1.1.1.3.2.2.2.7.cmml" xref="S4.E3.m1.1.1.3.2.2.2.7">𝑠</ci><ci id="S4.E3.m1.1.1.3.2.2.2.8.cmml" xref="S4.E3.m1.1.1.3.2.2.2.8">𝑖</ci><ci id="S4.E3.m1.1.1.3.2.2.2.9.cmml" xref="S4.E3.m1.1.1.3.2.2.2.9">𝑜</ci><ci id="S4.E3.m1.1.1.3.2.2.2.10.cmml" xref="S4.E3.m1.1.1.3.2.2.2.10">𝑛</ci></apply><ci id="S4.E3.m1.1.1.3.2.2.3.cmml" xref="S4.E3.m1.1.1.3.2.2.3">𝑅</ci></apply><ci id="S4.E3.m1.1.1.3.2.3.cmml" xref="S4.E3.m1.1.1.3.2.3">𝑒</ci><ci id="S4.E3.m1.1.1.3.2.4.cmml" xref="S4.E3.m1.1.1.3.2.4">𝑐</ci><ci id="S4.E3.m1.1.1.3.2.5.cmml" xref="S4.E3.m1.1.1.3.2.5">𝑎</ci><ci id="S4.E3.m1.1.1.3.2.6.cmml" xref="S4.E3.m1.1.1.3.2.6">𝑙</ci><ci id="S4.E3.m1.1.1.3.2.7.cmml" xref="S4.E3.m1.1.1.3.2.7">𝑙</ci></apply><apply id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3"><plus id="S4.E3.m1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.1"></plus><apply id="S4.E3.m1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.2"><times id="S4.E3.m1.1.1.3.3.2.1.cmml" xref="S4.E3.m1.1.1.3.3.2.1"></times><ci id="S4.E3.m1.1.1.3.3.2.2.cmml" xref="S4.E3.m1.1.1.3.3.2.2">𝑃</ci><ci id="S4.E3.m1.1.1.3.3.2.3.cmml" xref="S4.E3.m1.1.1.3.3.2.3">𝑟</ci><ci id="S4.E3.m1.1.1.3.3.2.4.cmml" xref="S4.E3.m1.1.1.3.3.2.4">𝑒</ci><ci id="S4.E3.m1.1.1.3.3.2.5.cmml" xref="S4.E3.m1.1.1.3.3.2.5">𝑐</ci><ci id="S4.E3.m1.1.1.3.3.2.6.cmml" xref="S4.E3.m1.1.1.3.3.2.6">𝑖</ci><ci id="S4.E3.m1.1.1.3.3.2.7.cmml" xref="S4.E3.m1.1.1.3.3.2.7">𝑠</ci><ci id="S4.E3.m1.1.1.3.3.2.8.cmml" xref="S4.E3.m1.1.1.3.3.2.8">𝑖</ci><ci id="S4.E3.m1.1.1.3.3.2.9.cmml" xref="S4.E3.m1.1.1.3.3.2.9">𝑜</ci><ci id="S4.E3.m1.1.1.3.3.2.10.cmml" xref="S4.E3.m1.1.1.3.3.2.10">𝑛</ci></apply><apply id="S4.E3.m1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3"><times id="S4.E3.m1.1.1.3.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.3.1"></times><ci id="S4.E3.m1.1.1.3.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.3.2">𝑅</ci><ci id="S4.E3.m1.1.1.3.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3.3">𝑒</ci><ci id="S4.E3.m1.1.1.3.3.3.4.cmml" xref="S4.E3.m1.1.1.3.3.3.4">𝑐</ci><ci id="S4.E3.m1.1.1.3.3.3.5.cmml" xref="S4.E3.m1.1.1.3.3.3.5">𝑎</ci><ci id="S4.E3.m1.1.1.3.3.3.6.cmml" xref="S4.E3.m1.1.1.3.3.3.6">𝑙</ci><ci id="S4.E3.m1.1.1.3.3.3.7.cmml" xref="S4.E3.m1.1.1.3.3.3.7">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">F1Score=\frac{2*Precision*Recall}{Precision+Recall}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">italic_F 1 italic_S italic_c italic_o italic_r italic_e = divide start_ARG 2 ∗ italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ∗ italic_R italic_e italic_c italic_a italic_l italic_l end_ARG start_ARG italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n + italic_R italic_e italic_c italic_a italic_l italic_l end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5. </span>Penalty-Based Performance Index</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.1">To provide a quantitative assessment of the model’s generalization performance, we created the Penalty-Based Performance Index to evaluate the models’ performance across all test sets. This method calculates a penalty for each score by subtracting it from 1, representing the error rate. The scores considered for this calculation include accuracy, AUC-ROC, F1-score, and AUC-PR, encompassing various aspects of model performance. The average penalty for each model is then computed, indicating the overall error tendency which represents, on average, how much the model’s score deviates from perfect score across all test sets. Finally, these average penalties are transformed into a scale of 1 to 100 to obtain a performance score for each model. A lower score indicates better performance, which means the model’s accuracy is closer to a perfect score with minimal variance across the test sets.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS5.p2">
<p class="ltx_p" id="S4.SS2.SSS5.p2.2">The formula for calculating the Penalty-Based Performance Index for Model 1, Model 2, …, Model <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p2.1.m1.1"><semantics id="S4.SS2.SSS5.p2.1.m1.1a"><mi id="S4.SS2.SSS5.p2.1.m1.1.1" xref="S4.SS2.SSS5.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.1.m1.1b"><ci id="S4.SS2.SSS5.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p2.1.m1.1d">italic_n</annotation></semantics></math> with accuracy scores <math alttext="A_{1},A_{2},\ldots,A_{n}" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p2.2.m2.4"><semantics id="S4.SS2.SSS5.p2.2.m2.4a"><mrow id="S4.SS2.SSS5.p2.2.m2.4.4.3" xref="S4.SS2.SSS5.p2.2.m2.4.4.4.cmml"><msub id="S4.SS2.SSS5.p2.2.m2.2.2.1.1" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1.cmml"><mi id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.2" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1.2.cmml">A</mi><mn id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.3" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS5.p2.2.m2.4.4.3.4" xref="S4.SS2.SSS5.p2.2.m2.4.4.4.cmml">,</mo><msub id="S4.SS2.SSS5.p2.2.m2.3.3.2.2" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2.cmml"><mi id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.2" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2.2.cmml">A</mi><mn id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.3" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS5.p2.2.m2.4.4.3.5" xref="S4.SS2.SSS5.p2.2.m2.4.4.4.cmml">,</mo><mi id="S4.SS2.SSS5.p2.2.m2.1.1" mathvariant="normal" xref="S4.SS2.SSS5.p2.2.m2.1.1.cmml">…</mi><mo id="S4.SS2.SSS5.p2.2.m2.4.4.3.6" xref="S4.SS2.SSS5.p2.2.m2.4.4.4.cmml">,</mo><msub id="S4.SS2.SSS5.p2.2.m2.4.4.3.3" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3.cmml"><mi id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.2" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3.2.cmml">A</mi><mi id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.3" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.2.m2.4b"><list id="S4.SS2.SSS5.p2.2.m2.4.4.4.cmml" xref="S4.SS2.SSS5.p2.2.m2.4.4.3"><apply id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.2.cmml" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1.2">𝐴</ci><cn id="S4.SS2.SSS5.p2.2.m2.2.2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS5.p2.2.m2.2.2.1.1.3">1</cn></apply><apply id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.cmml" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2.2">𝐴</ci><cn id="S4.SS2.SSS5.p2.2.m2.3.3.2.2.3.cmml" type="integer" xref="S4.SS2.SSS5.p2.2.m2.3.3.2.2.3">2</cn></apply><ci id="S4.SS2.SSS5.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1">…</ci><apply id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.cmml" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3">subscript</csymbol><ci id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.2.cmml" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3.2">𝐴</ci><ci id="S4.SS2.SSS5.p2.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.SSS5.p2.2.m2.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.2.m2.4c">A_{1},A_{2},\ldots,A_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p2.2.m2.4d">italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> is as follows:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS5.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="PerformanceIndex_{x}=\frac{\sum_{i=1}^{n}(1-A_{i})}{n}\times 100" class="ltx_Math" display="block" id="S4.E4.m1.1"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.2" xref="S4.E4.m1.1.2.cmml"><mrow id="S4.E4.m1.1.2.2" xref="S4.E4.m1.1.2.2.cmml"><mi id="S4.E4.m1.1.2.2.2" xref="S4.E4.m1.1.2.2.2.cmml">P</mi><mo id="S4.E4.m1.1.2.2.1" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.3" xref="S4.E4.m1.1.2.2.3.cmml">e</mi><mo id="S4.E4.m1.1.2.2.1a" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.4" xref="S4.E4.m1.1.2.2.4.cmml">r</mi><mo id="S4.E4.m1.1.2.2.1b" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.5" xref="S4.E4.m1.1.2.2.5.cmml">f</mi><mo id="S4.E4.m1.1.2.2.1c" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.6" xref="S4.E4.m1.1.2.2.6.cmml">o</mi><mo id="S4.E4.m1.1.2.2.1d" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.7" xref="S4.E4.m1.1.2.2.7.cmml">r</mi><mo id="S4.E4.m1.1.2.2.1e" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.8" xref="S4.E4.m1.1.2.2.8.cmml">m</mi><mo id="S4.E4.m1.1.2.2.1f" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.9" xref="S4.E4.m1.1.2.2.9.cmml">a</mi><mo id="S4.E4.m1.1.2.2.1g" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.10" xref="S4.E4.m1.1.2.2.10.cmml">n</mi><mo id="S4.E4.m1.1.2.2.1h" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.11" xref="S4.E4.m1.1.2.2.11.cmml">c</mi><mo id="S4.E4.m1.1.2.2.1i" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.12" xref="S4.E4.m1.1.2.2.12.cmml">e</mi><mo id="S4.E4.m1.1.2.2.1j" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.13" xref="S4.E4.m1.1.2.2.13.cmml">I</mi><mo id="S4.E4.m1.1.2.2.1k" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.14" xref="S4.E4.m1.1.2.2.14.cmml">n</mi><mo id="S4.E4.m1.1.2.2.1l" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.15" xref="S4.E4.m1.1.2.2.15.cmml">d</mi><mo id="S4.E4.m1.1.2.2.1m" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.2.2.16" xref="S4.E4.m1.1.2.2.16.cmml">e</mi><mo id="S4.E4.m1.1.2.2.1n" xref="S4.E4.m1.1.2.2.1.cmml">⁢</mo><msub id="S4.E4.m1.1.2.2.17" xref="S4.E4.m1.1.2.2.17.cmml"><mi id="S4.E4.m1.1.2.2.17.2" xref="S4.E4.m1.1.2.2.17.2.cmml">x</mi><mi id="S4.E4.m1.1.2.2.17.3" xref="S4.E4.m1.1.2.2.17.3.cmml">x</mi></msub></mrow><mo id="S4.E4.m1.1.2.1" xref="S4.E4.m1.1.2.1.cmml">=</mo><mrow id="S4.E4.m1.1.2.3" xref="S4.E4.m1.1.2.3.cmml"><mfrac id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml"><msubsup id="S4.E4.m1.1.1.1.2" xref="S4.E4.m1.1.1.1.2.cmml"><mo id="S4.E4.m1.1.1.1.2.2.2" xref="S4.E4.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E4.m1.1.1.1.2.2.3" xref="S4.E4.m1.1.1.1.2.2.3.cmml"><mi id="S4.E4.m1.1.1.1.2.2.3.2" xref="S4.E4.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E4.m1.1.1.1.2.2.3.1" xref="S4.E4.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E4.m1.1.1.1.2.2.3.3" xref="S4.E4.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E4.m1.1.1.1.2.3" xref="S4.E4.m1.1.1.1.2.3.cmml">n</mi></msubsup><mrow id="S4.E4.m1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mo id="S4.E4.m1.1.1.1.1.1.2" lspace="0em" stretchy="false" xref="S4.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mn id="S4.E4.m1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E4.m1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E4.m1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.1.1.1.1.1.1.3.2.cmml">A</mi><mi id="S4.E4.m1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S4.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S4.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mi id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml">n</mi></mfrac><mo id="S4.E4.m1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E4.m1.1.2.3.1.cmml">×</mo><mn id="S4.E4.m1.1.2.3.2" xref="S4.E4.m1.1.2.3.2.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.2.cmml" xref="S4.E4.m1.1.2"><eq id="S4.E4.m1.1.2.1.cmml" xref="S4.E4.m1.1.2.1"></eq><apply id="S4.E4.m1.1.2.2.cmml" xref="S4.E4.m1.1.2.2"><times id="S4.E4.m1.1.2.2.1.cmml" xref="S4.E4.m1.1.2.2.1"></times><ci id="S4.E4.m1.1.2.2.2.cmml" xref="S4.E4.m1.1.2.2.2">𝑃</ci><ci id="S4.E4.m1.1.2.2.3.cmml" xref="S4.E4.m1.1.2.2.3">𝑒</ci><ci id="S4.E4.m1.1.2.2.4.cmml" xref="S4.E4.m1.1.2.2.4">𝑟</ci><ci id="S4.E4.m1.1.2.2.5.cmml" xref="S4.E4.m1.1.2.2.5">𝑓</ci><ci id="S4.E4.m1.1.2.2.6.cmml" xref="S4.E4.m1.1.2.2.6">𝑜</ci><ci id="S4.E4.m1.1.2.2.7.cmml" xref="S4.E4.m1.1.2.2.7">𝑟</ci><ci id="S4.E4.m1.1.2.2.8.cmml" xref="S4.E4.m1.1.2.2.8">𝑚</ci><ci id="S4.E4.m1.1.2.2.9.cmml" xref="S4.E4.m1.1.2.2.9">𝑎</ci><ci id="S4.E4.m1.1.2.2.10.cmml" xref="S4.E4.m1.1.2.2.10">𝑛</ci><ci id="S4.E4.m1.1.2.2.11.cmml" xref="S4.E4.m1.1.2.2.11">𝑐</ci><ci id="S4.E4.m1.1.2.2.12.cmml" xref="S4.E4.m1.1.2.2.12">𝑒</ci><ci id="S4.E4.m1.1.2.2.13.cmml" xref="S4.E4.m1.1.2.2.13">𝐼</ci><ci id="S4.E4.m1.1.2.2.14.cmml" xref="S4.E4.m1.1.2.2.14">𝑛</ci><ci id="S4.E4.m1.1.2.2.15.cmml" xref="S4.E4.m1.1.2.2.15">𝑑</ci><ci id="S4.E4.m1.1.2.2.16.cmml" xref="S4.E4.m1.1.2.2.16">𝑒</ci><apply id="S4.E4.m1.1.2.2.17.cmml" xref="S4.E4.m1.1.2.2.17"><csymbol cd="ambiguous" id="S4.E4.m1.1.2.2.17.1.cmml" xref="S4.E4.m1.1.2.2.17">subscript</csymbol><ci id="S4.E4.m1.1.2.2.17.2.cmml" xref="S4.E4.m1.1.2.2.17.2">𝑥</ci><ci id="S4.E4.m1.1.2.2.17.3.cmml" xref="S4.E4.m1.1.2.2.17.3">𝑥</ci></apply></apply><apply id="S4.E4.m1.1.2.3.cmml" xref="S4.E4.m1.1.2.3"><times id="S4.E4.m1.1.2.3.1.cmml" xref="S4.E4.m1.1.2.3.1"></times><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><divide id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1"></divide><apply id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"><apply id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.2">superscript</csymbol><apply id="S4.E4.m1.1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.1.1.1.2">subscript</csymbol><sum id="S4.E4.m1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.1.1.1.2.2.2"></sum><apply id="S4.E4.m1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.1.1.1.2.2.3"><eq id="S4.E4.m1.1.1.1.2.2.3.1.cmml" xref="S4.E4.m1.1.1.1.2.2.3.1"></eq><ci id="S4.E4.m1.1.1.1.2.2.3.2.cmml" xref="S4.E4.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S4.E4.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E4.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E4.m1.1.1.1.2.3.cmml" xref="S4.E4.m1.1.1.1.2.3">𝑛</ci></apply><apply id="S4.E4.m1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1"><minus id="S4.E4.m1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1"></minus><cn id="S4.E4.m1.1.1.1.1.1.1.2.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.1.1.2">1</cn><apply id="S4.E4.m1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.2">𝐴</ci><ci id="S4.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><ci id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3">𝑛</ci></apply><cn id="S4.E4.m1.1.2.3.2.cmml" type="integer" xref="S4.E4.m1.1.2.3.2">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">PerformanceIndex_{x}=\frac{\sum_{i=1}^{n}(1-A_{i})}{n}\times 100</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.1d">italic_P italic_e italic_r italic_f italic_o italic_r italic_m italic_a italic_n italic_c italic_e italic_I italic_n italic_d italic_e italic_x start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( 1 - italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_n end_ARG × 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS5.p4">
<p class="ltx_p" id="S4.SS2.SSS5.p4.1">This evaluation method provides a quantitative measure of model performance, considering the accuracy and error rate, thus offering insights into the overall effectiveness of the models under assessment.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Self-Supervised Pre-training Result</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In our ablation study, we conducted pre-training for 100 epochs on three transformer-based networks. One network utilized the Swin architecture as its backbone, while the other two employed distinct variations of SwinV2 as backbones—referred to as SwinV2 and SwinV2-large, respectively. The objective was to assess the efficacy of these models and their performance in subsequent tasks. In particular, after 100 training epochs, the SwinV2-based MAE demonstrated remarkable proficiency, obtaining a mean squared error (MSE) loss of 0.007 as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F6" title="Figure 6 ‣ 4.3. Self-Supervised Pre-training Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">6</span></a>. Moreover, Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F5" title="Figure 5 ‣ 4.3. Self-Supervised Pre-training Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">5</span></a> provides a detailed visualization of the masked image input and the corresponding image reconstruction from the SwinV2-based MAE at various epochs, offering insights into the model’s learning dynamics throughout the training process. While the reconstructed images may display imperfections, our project prioritized the capture of intricate image structures and patterns over flawless reconstructions. Consequently, in subsequent tasks, we employed these pre-trained weights, capitalizing on their learned representations, rather than initializing the classifiers with random weights. By applying the learned knowledge encoded in the pre-trained weights, this method allowed us to take advantage of the benefits of the transfer learning approach, facilitating enhanced performance and accelerated convergence in downstream tasks.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<p class="ltx_p ltx_align_center" id="S4.F5.1"><span class="ltx_text" id="S4.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="248" id="S4.F5.1.1.g1" src="extracted/5860915/figures/ssl-training-result.png" width="586"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The progression results of the Multi-OCT-SelfNet-SwinV2 model on sample validation images across various epochs illustrate its learning process in reconstructing input images. From left to right, (a) is the corresponding ground truth image, (b) is the masked image input, and (c)-(e) is the reconstructed images in different epochs.
</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6">
<p class="ltx_p ltx_align_center" id="S4.F6.1"><span class="ltx_text" id="S4.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="352" id="S4.F6.1.1.g1" src="x3.png" width="623"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The training and validation MSE Loss curves of Multi-OCT-SelfNet with
SwinV2 backbone, trained with the combined dataset.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Supervised Fine-tuning Result</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Performance Comparison with Different Encoder Network</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">We evaluated different encoder networks against the baseline ResNet-50 model in our extensive experiment. To ensure consistency, we utilized the same encoder from the SSL networks and employed transfer learning to transfer the learned weights. For downstream classification tasks, we also integrated a classifier network. Each supervised network underwent fine-tuning individually for every dataset, followed by evaluations on both the respective test set and the test sets from other datasets. This comprehensive evaluation allowed us to observe the network’s performance on previously unseen test data and to observe its generalization capability. The performance of each model was then compared with that of the baseline model ResNet-50, which underwent training on each dataset and subsequent evaluation on all three test sets. Throughout this experimental process, we applied data augmentation techniques to enhance robustness. Performance metrics including Accuracy, AUC-ROC, AUC-PR, and F1-Score were employed to measure effectiveness. Analysis of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T1" title="Table 1 ‣ 4.4.1. Performance Comparison with Different Encoder Network ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">1</span></a> revealed the consistent performance of the self-supervised fine-tuning approach over the baseline model, with the SwinV2-based classifier demonstrating the most reliable performance across all test sets, particularly in scenarios with smaller datasets such as Dataset-2 and Dataset-3. While Dataset-1, with its larger sample size, yielded similar performance between the proposed framework and the baseline model, it’s noteworthy that our method showcased superior performance on smaller datasets as well. The generalization capabilities during off-domain evaluation, when the model was fine-tuned on one dataset and evaluated on other test sets, significant performance boosts were observed in our proposed method. For instance, while fine-tuning Dataset-1, our Multi-OCT-SelfNet-SwinV2 model achieved AUC-ROC scores of 0.79, 0.97, and 0.90, along with AUC-PR of 0.58, 0.94, and 0.70 on Test Set-1, Test Set-2, and Test Set-3, respectively. In contrast, the baseline model attained AUC-ROC scores of 0.65, 0.98, and 0.59, along with AUC-PR of 0.39, 0.95, and 0.57 on the respective test sets. These results underscore the superior generalization capability of our proposed method, highlighting its potential for robust performance across diverse datasets.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F7" title="Figure 7 ‣ 4.4.1. Performance Comparison with Different Encoder Network ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">7</span></a> presents a bar chart comparing AUC-ROC scores for various classifiers across three datasets and test sets. The grouped bars facilitate a direct performance comparison, revealing that Multi-OCT-SelfNet generally surpasses the baseline ResNet-50. This chart highlights the performance of each classifier across different datasets and underscores Multi-OCT-SelfNet’s superior AUC-ROC scores. AUC-ROC is an ideal metric for evaluating classifiers on imbalanced binary datasets. Therefore, AUC-ROC was chosen for our bar chart to deliver a comprehensive assessment of classifier performance across the datasets and test sets.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1">To further evaluate generalizability and domain adaptability across other datasets, we employed a performance metric known as the Penalty-Based Performance Index, as presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T2" title="Table 2 ‣ 4.4.1. Performance Comparison with Different Encoder Network ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">2</span></a>. This index aggregates scores from three distinct test sets and calculates penalty scores for each specific metric. A lower penalty score indicates superior generalization performance, highlighting the model’s ability to adapt across diverse datasets. Upon observing the results in the table, it becomes evident that the proposed framework consistently exhibits better generalization performance across various metrics. This reaffirms the robustness and adaptability of our model.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Analyzing the Performance with Different Encoder Networks: Comparison of our framework with baseline methods (ResNet-50) on the test sets from three datasets, evaluating multi-class classification performance in terms of accuracy, AUC-ROC, AUC-PR, and F1-score.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T1.3.1.1.3"><span class="ltx_text" id="S4.T1.3.1.1.3.1" style="font-size:70%;">AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T1.3.1.1.4"><span class="ltx_text" id="S4.T1.3.1.1.4.1" style="font-size:70%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T1.3.1.1.5"><span class="ltx_text" id="S4.T1.3.1.1.5.1" style="font-size:70%;">AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T1.3.1.1.6"><span class="ltx_text" id="S4.T1.3.1.1.6.1" style="font-size:70%;">F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T1.3.2.1.1"><span class="ltx_text" id="S4.T1.3.2.1.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.2.1.2"><span class="ltx_text" id="S4.T1.3.2.1.2.1" style="font-size:70%;">Classifier Name</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.3"><span class="ltx_text" id="S4.T1.3.2.1.3.1" style="font-size:70%;">Test-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.4"><span class="ltx_text" id="S4.T1.3.2.1.4.1" style="font-size:70%;">Test-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.5"><span class="ltx_text" id="S4.T1.3.2.1.5.1" style="font-size:70%;">Test-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.6"><span class="ltx_text" id="S4.T1.3.2.1.6.1" style="font-size:70%;">Test-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.7"><span class="ltx_text" id="S4.T1.3.2.1.7.1" style="font-size:70%;">Test-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.8"><span class="ltx_text" id="S4.T1.3.2.1.8.1" style="font-size:70%;">Test-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.9"><span class="ltx_text" id="S4.T1.3.2.1.9.1" style="font-size:70%;">Test-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.10"><span class="ltx_text" id="S4.T1.3.2.1.10.1" style="font-size:70%;">Test-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.11"><span class="ltx_text" id="S4.T1.3.2.1.11.1" style="font-size:70%;">Test-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.12"><span class="ltx_text" id="S4.T1.3.2.1.12.1" style="font-size:70%;">Test-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.13"><span class="ltx_text" id="S4.T1.3.2.1.13.1" style="font-size:70%;">Test-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.14"><span class="ltx_text" id="S4.T1.3.2.1.14.1" style="font-size:70%;">Test-3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.2">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.3.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">0.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">0.95</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.7.1" style="font-size:70%;background-color:#F3F3F3;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.8.1" style="font-size:70%;background-color:#F3F3F3;">0.24</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.9.1" style="font-size:70%;background-color:#F3F3F3;">0.96</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.10.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.11.1" style="font-size:70%;background-color:#F3F3F3;">0.53</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.12.1" style="font-size:70%;background-color:#F3F3F3;">0.95</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.13.1" style="font-size:70%;background-color:#F3F3F3;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.3.2.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.3.2.14.1" style="font-size:70%;background-color:#F3F3F3;">0.25</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.4.3">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.4.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-Swinlarge</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.3.1" style="font-size:70%;background-color:#F3F3F3;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.5.1" style="font-size:70%;background-color:#F3F3F3;">0.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.6.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.7.1" style="font-size:70%;background-color:#F3F3F3;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.8.1" style="font-size:70%;background-color:#F3F3F3;">0.41</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.9.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.10.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.11.1" style="font-size:70%;background-color:#F3F3F3;">0.43</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.12.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.13.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.4.3.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.4.3.14.1" style="font-size:70%;background-color:#F3F3F3;">0.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.4">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.5.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.3.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.5.1" style="font-size:70%;background-color:#F3F3F3;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.6.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.7.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.8.1" style="font-size:70%;background-color:#F3F3F3;">0.46</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.9.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.10.1" style="font-size:70%;background-color:#F3F3F3;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.11.1" style="font-size:70%;background-color:#F3F3F3;">0.42</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.12.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.13.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.5.4.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.5.4.14.1" style="font-size:70%;background-color:#F3F3F3;">0.87</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T1.3.6.5.1"><span class="ltx_text" id="S4.T1.3.6.5.1.1" style="font-size:70%;">DS-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2-large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.3.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.5.1" style="font-size:70%;background-color:#F3F3F3;">0.64</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.6.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.7.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.8.1" style="font-size:70%;background-color:#F3F3F3;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.9.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.10.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.11.1" style="font-size:70%;background-color:#F3F3F3;">0.44</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.12.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.13.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.6.5.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.6.5.14.1" style="font-size:70%;background-color:#F3F3F3;">0.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.6">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.7.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.3"><span class="ltx_text" id="S4.T1.3.7.6.3.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.4"><span class="ltx_text" id="S4.T1.3.7.6.4.1" style="font-size:70%;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.5"><span class="ltx_text" id="S4.T1.3.7.6.5.1" style="font-size:70%;">0.59</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.6"><span class="ltx_text" id="S4.T1.3.7.6.6.1" style="font-size:70%;">0.29</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.7"><span class="ltx_text" id="S4.T1.3.7.6.7.1" style="font-size:70%;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.8"><span class="ltx_text" id="S4.T1.3.7.6.8.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.9"><span class="ltx_text" id="S4.T1.3.7.6.9.1" style="font-size:70%;">0.39</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.10"><span class="ltx_text" id="S4.T1.3.7.6.10.1" style="font-size:70%;">0.95</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.11"><span class="ltx_text" id="S4.T1.3.7.6.11.1" style="font-size:70%;">0.57</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.12"><span class="ltx_text" id="S4.T1.3.7.6.12.1" style="font-size:70%;">0.31</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.13"><span class="ltx_text" id="S4.T1.3.7.6.13.1" style="font-size:70%;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.7.6.14"><span class="ltx_text" id="S4.T1.3.7.6.14.1" style="font-size:70%;">0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.8.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.8.7.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-Swinlarge</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.3"><span class="ltx_text" id="S4.T1.3.8.7.3.1" style="font-size:70%;">0.77</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.4"><span class="ltx_text" id="S4.T1.3.8.7.4.1" style="font-size:70%;">0.96</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.5"><span class="ltx_text" id="S4.T1.3.8.7.5.1" style="font-size:70%;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.6"><span class="ltx_text" id="S4.T1.3.8.7.6.1" style="font-size:70%;">0.62</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.7"><span class="ltx_text" id="S4.T1.3.8.7.7.1" style="font-size:70%;">0.88</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.8"><span class="ltx_text" id="S4.T1.3.8.7.8.1" style="font-size:70%;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.9"><span class="ltx_text" id="S4.T1.3.8.7.9.1" style="font-size:70%;">0.54</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.10"><span class="ltx_text" id="S4.T1.3.8.7.10.1" style="font-size:70%;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.11"><span class="ltx_text" id="S4.T1.3.8.7.11.1" style="font-size:70%;">0.64</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.12"><span class="ltx_text" id="S4.T1.3.8.7.12.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.13"><span class="ltx_text" id="S4.T1.3.8.7.13.1" style="font-size:70%;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.7.14"><span class="ltx_text" id="S4.T1.3.8.7.14.1" style="font-size:70%;">0.59</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.8">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.9.8.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.9.8.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.3"><span class="ltx_text" id="S4.T1.3.9.8.3.1" style="font-size:70%;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.4"><span class="ltx_text" id="S4.T1.3.9.8.4.1" style="font-size:70%;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.5"><span class="ltx_text" id="S4.T1.3.9.8.5.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.6"><span class="ltx_text" id="S4.T1.3.9.8.6.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.7"><span class="ltx_text" id="S4.T1.3.9.8.7.1" style="font-size:70%;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.8"><span class="ltx_text" id="S4.T1.3.9.8.8.1" style="font-size:70%;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.9"><span class="ltx_text" id="S4.T1.3.9.8.9.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.10"><span class="ltx_text" id="S4.T1.3.9.8.10.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.11"><span class="ltx_text" id="S4.T1.3.9.8.11.1" style="font-size:70%;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.12"><span class="ltx_text" id="S4.T1.3.9.8.12.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.13"><span class="ltx_text" id="S4.T1.3.9.8.13.1" style="font-size:70%;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.9.8.14"><span class="ltx_text" id="S4.T1.3.9.8.14.1" style="font-size:70%;">0.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T1.3.10.9.1"><span class="ltx_text" id="S4.T1.3.10.9.1.1" style="font-size:70%;">DS-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.10.9.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2-large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.3"><span class="ltx_text" id="S4.T1.3.10.9.3.1" style="font-size:70%;">0.80</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.4"><span class="ltx_text" id="S4.T1.3.10.9.4.1" style="font-size:70%;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.5"><span class="ltx_text" id="S4.T1.3.10.9.5.1" style="font-size:70%;">0.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.6"><span class="ltx_text" id="S4.T1.3.10.9.6.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.7"><span class="ltx_text" id="S4.T1.3.10.9.7.1" style="font-size:70%;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.8"><span class="ltx_text" id="S4.T1.3.10.9.8.1" style="font-size:70%;">0.28</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.9"><span class="ltx_text" id="S4.T1.3.10.9.9.1" style="font-size:70%;">0.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.10"><span class="ltx_text" id="S4.T1.3.10.9.10.1" style="font-size:70%;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.11"><span class="ltx_text" id="S4.T1.3.10.9.11.1" style="font-size:70%;">0.63</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.12"><span class="ltx_text" id="S4.T1.3.10.9.12.1" style="font-size:70%;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.13"><span class="ltx_text" id="S4.T1.3.10.9.13.1" style="font-size:70%;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.10.9.14"><span class="ltx_text" id="S4.T1.3.10.9.14.1" style="font-size:70%;">0.73</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.11.10">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.11.10.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.3.1" style="font-size:70%;background-color:#F3F3F3;">0.58</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.4.1" style="font-size:70%;background-color:#F3F3F3;">0.60</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.5.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.6.1" style="font-size:70%;background-color:#F3F3F3;">0.55</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.7.1" style="font-size:70%;background-color:#F3F3F3;">0.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.8.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.9.1" style="font-size:70%;background-color:#F3F3F3;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.10.1" style="font-size:70%;background-color:#F3F3F3;">0.82</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.11.1" style="font-size:70%;background-color:#F3F3F3;">0.74</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.12.1" style="font-size:70%;background-color:#F3F3F3;">0.39</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.13.1" style="font-size:70%;background-color:#F3F3F3;">0.53</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.11.10.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.11.10.14.1" style="font-size:70%;background-color:#F3F3F3;">0.82</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.12.11">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.12.11.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-Swinlarge</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.3.1" style="font-size:70%;background-color:#F3F3F3;">0.72</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.4.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.5.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.6.1" style="font-size:70%;background-color:#F3F3F3;">0.51</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.7.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.8.1" style="font-size:70%;background-color:#F3F3F3;">0.88</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.9.1" style="font-size:70%;background-color:#F3F3F3;">0.52</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.10.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.11.1" style="font-size:70%;background-color:#F3F3F3;">0.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.12.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.13.1" style="font-size:70%;background-color:#F3F3F3;">0.82</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.12.11.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.12.11.14.1" style="font-size:70%;background-color:#F3F3F3;">0.85</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.13.12">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T1.3.13.12.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.3.1" style="font-size:70%;background-color:#F3F3F3;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.4.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.5.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.6.1" style="font-size:70%;background-color:#F3F3F3;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.7.1" style="font-size:70%;background-color:#F3F3F3;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.8.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.9.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.10.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.11.1" style="font-size:70%;background-color:#F3F3F3;">0.75</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.12.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.13.1" style="font-size:70%;background-color:#F3F3F3;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.13.12.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.13.12.14.1" style="font-size:70%;background-color:#F3F3F3;">0.83</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.14.13">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.3.14.13.1"><span class="ltx_text" id="S4.T1.3.14.13.1.1" style="font-size:70%;">DS-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2-large</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.3.1" style="font-size:70%;background-color:#F3F3F3;">0.69</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.4.1" style="font-size:70%;background-color:#F3F3F3;">0.88</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.5.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.6.1" style="font-size:70%;background-color:#F3F3F3;">0.52</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.7.1" style="font-size:70%;background-color:#F3F3F3;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.8" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.8.1" style="font-size:70%;background-color:#F3F3F3;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.9.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.10.1" style="font-size:70%;background-color:#F3F3F3;">0.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.11" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.11.1" style="font-size:70%;background-color:#F3F3F3;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.12.1" style="font-size:70%;background-color:#F3F3F3;">0.51</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.13.1" style="font-size:70%;background-color:#F3F3F3;">0.71</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.3.14.13.14" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T1.3.14.13.14.1" style="font-size:70%;background-color:#F3F3F3;">0.85</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F7">
<p class="ltx_p ltx_align_center" id="S4.F7.1"><span class="ltx_text" id="S4.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="213" id="S4.F7.1.1.g1" src="extracted/5860915/figures/table_bar_1.png" width="449"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Comparison of AUC-ROC scores for different classifiers across three datasets and test sets. Each group represents AUC-ROC scores for classifiers within a specific dataset and test set combination, highlighting performance variations.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Analyzing Model’s Generalization Performance on Different Encoder Networks: Comparison of penalty-based performance scores across three test sets for different models and three datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.3.1.1.1"><span class="ltx_text" id="S4.T2.3.1.1.1.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.3.1.1.2"><span class="ltx_text" id="S4.T2.3.1.1.2.1" style="font-size:70%;">Classifier Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.3.1.1.3"><span class="ltx_text" id="S4.T2.3.1.1.3.1" style="font-size:70%;">P-Index of AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.3.1.1.4"><span class="ltx_text" id="S4.T2.3.1.1.4.1" style="font-size:70%;">P-Index of Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.3.1.1.5"><span class="ltx_text" id="S4.T2.3.1.1.5.1" style="font-size:70%;">P-Index of AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.3.1.1.6"><span class="ltx_text" id="S4.T2.3.1.1.6.1" style="font-size:70%;">P-Index of F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.2.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.3.2.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.2.1.2"><span class="ltx_text" id="S4.T2.3.2.1.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.1.3.1" style="font-size:70%;background-color:#F3F3F3;">11.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.2.1.4"><span class="ltx_text" id="S4.T2.3.2.1.4.1" style="font-size:70%;background-color:#F3F3F3;">29.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.1.5.1" style="font-size:70%;background-color:#F3F3F3;">17.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.2.1.6"><span class="ltx_text" id="S4.T2.3.2.1.6.1" style="font-size:70%;background-color:#F3F3F3;">29.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.3.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.3.2.2"><span class="ltx_text" id="S4.T2.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-Swinlarge</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.3.2.3"><span class="ltx_text" id="S4.T2.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">14.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">25.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.3.2.5"><span class="ltx_text" id="S4.T2.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">23.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.3.2.6"><span class="ltx_text" id="S4.T2.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">20.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.3">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.4.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.4.3.2"><span class="ltx_text" id="S4.T2.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.4.3.3"><span class="ltx_text" id="S4.T2.3.4.3.3.1" style="font-size:70%;background-color:#F3F3F3;">16.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.4.3.4"><span class="ltx_text" id="S4.T2.3.4.3.4.1" style="font-size:70%;background-color:#F3F3F3;">26.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.4.3.5"><span class="ltx_text" id="S4.T2.3.4.3.5.1" style="font-size:70%;background-color:#F3F3F3;">23.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.3.6.1" style="font-size:70%;background-color:#F3F3F3;">10.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.4" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T2.3.5.4.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.5.4.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.5.4.2"><span class="ltx_text" id="S4.T2.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2-large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.5.4.3"><span class="ltx_text" id="S4.T2.3.5.4.3.1" style="font-size:70%;background-color:#F3F3F3;">13.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.5.4.4"><span class="ltx_text" id="S4.T2.3.5.4.4.1" style="font-size:70%;background-color:#F3F3F3;">25.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.5.4.5"><span class="ltx_text" id="S4.T2.3.5.4.5.1" style="font-size:70%;background-color:#F3F3F3;">22.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.5.4.6"><span class="ltx_text" id="S4.T2.3.5.4.6.1" style="font-size:70%;background-color:#F3F3F3;">19.99</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.5">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.3.6.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.6.5.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.6.5.3"><span class="ltx_text" id="S4.T2.3.6.5.3.1" style="font-size:70%;">26.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.6.5.4"><span class="ltx_text" id="S4.T2.3.6.5.4.1" style="font-size:70%;">61.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.6.5.5"><span class="ltx_text" id="S4.T2.3.6.5.5.1" style="font-size:70%;">36.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.6.5.6"><span class="ltx_text" id="S4.T2.3.6.5.6.1" style="font-size:70%;">60.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7.6">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.7.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.7.6.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-Swinlarge-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.7.6.3"><span class="ltx_text" id="S4.T2.3.7.6.3.1" style="font-size:70%;">14.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.7.6.4"><span class="ltx_text" id="S4.T2.3.7.6.4.1" style="font-size:70%;">35.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.7.6.5"><span class="ltx_text" id="S4.T2.3.7.6.5.1" style="font-size:70%;">30.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.7.6.6"><span class="ltx_text" id="S4.T2.3.7.6.6.1" style="font-size:70%;">29.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.8.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.8.7.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.8.7.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-SwinV2-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.8.7.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.7.3.1" style="font-size:70%;">11.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.8.7.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.7.4.1" style="font-size:70%;">34.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.8.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.7.5.1" style="font-size:70%;">26.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.8.7.6"><span class="ltx_text" id="S4.T2.3.8.7.6.1" style="font-size:70%;">30.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T2.3.9.8.1"><span class="ltx_text" id="S4.T2.3.9.8.1.1" style="font-size:70%;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.9.8.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.9.8.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-SwinV2-large-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.9.8.3"><span class="ltx_text" id="S4.T2.3.9.8.3.1" style="font-size:70%;">12.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.9.8.4"><span class="ltx_text" id="S4.T2.3.9.8.4.1" style="font-size:70%;">36.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.9.8.5"><span class="ltx_text" id="S4.T2.3.9.8.5.1" style="font-size:70%;">26.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.8.6.1" style="font-size:70%;">21.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.9">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.3.10.9.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.10.9.2"><span class="ltx_text" id="S4.T2.3.10.9.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.10.9.3"><span class="ltx_text" id="S4.T2.3.10.9.3.1" style="font-size:70%;background-color:#F3F3F3;">30.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.10.9.4"><span class="ltx_text" id="S4.T2.3.10.9.4.1" style="font-size:70%;background-color:#F3F3F3;">31.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.10.9.5"><span class="ltx_text" id="S4.T2.3.10.9.5.1" style="font-size:70%;background-color:#F3F3F3;">33.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.10.9.6"><span class="ltx_text" id="S4.T2.3.10.9.6.1" style="font-size:70%;background-color:#F3F3F3;">42.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11.10">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.11.10.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.11.10.2"><span class="ltx_text" id="S4.T2.3.11.10.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-Swinlarge-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.11.10.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.10.3.1" style="font-size:70%;background-color:#F3F3F3;">15.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.11.10.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.10.4.1" style="font-size:70%;background-color:#F3F3F3;">25.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.11.10.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.10.5.1" style="font-size:70%;background-color:#F3F3F3;">24.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.11.10.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.10.6.1" style="font-size:70%;background-color:#F3F3F3;">28.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12.11">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T2.3.12.11.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.12.11.2"><span class="ltx_text" id="S4.T2.3.12.11.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-SwinV2-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.12.11.3"><span class="ltx_text" id="S4.T2.3.12.11.3.1" style="font-size:70%;background-color:#F3F3F3;">16.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.12.11.4"><span class="ltx_text" id="S4.T2.3.12.11.4.1" style="font-size:70%;background-color:#F3F3F3;">28.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.12.11.5"><span class="ltx_text" id="S4.T2.3.12.11.5.1" style="font-size:70%;background-color:#F3F3F3;">27.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.3.12.11.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.12.11.6.1" style="font-size:70%;background-color:#F3F3F3;">28.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.13.12" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T2.3.13.12.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T2.3.13.12.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.2"><span class="ltx_text" id="S4.T2.3.13.12.2.1" style="font-size:70%;background-color:#F3F3F3;">OCT-SelfNet-SwinV2-large-Multi</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.3"><span class="ltx_text" id="S4.T2.3.13.12.3.1" style="font-size:70%;background-color:#F3F3F3;">18.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.4"><span class="ltx_text" id="S4.T2.3.13.12.4.1" style="font-size:70%;background-color:#F3F3F3;">27.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.5"><span class="ltx_text" id="S4.T2.3.13.12.5.1" style="font-size:70%;background-color:#F3F3F3;">28.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.6"><span class="ltx_text" id="S4.T2.3.13.12.6.1" style="font-size:70%;background-color:#F3F3F3;">31.0</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Performance Evaluation without Data Fusion during Pre-training Phase</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">In this experiment, we assessed the performance without the data fusion during the SSL pre-training phase. We opted to pre-train the SSL model with each dataset individually and subsequently evaluated the classifier’s performance solely on the respective test sets. This approach was taken to gain deeper insights into the contribution of data fusion to the classifier’s performance. By restricting the training and evaluation processes to a single dataset, we aimed to investigate the impact of data fusion on the classifier’s performance. This analysis provided valuable insights into the role of data integration of different modalities in enhancing classifier performance.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T3" title="Table 3 ‣ 4.4.2. Performance Evaluation without Data Fusion during Pre-training Phase ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results of the on-domain evaluation for our Multi-OCT-SelfNet framework utilizing the SwinV2 backbone, excluding the data fusion step in the pre-training phase. This analysis aims to quantify the impact of data fusion on performance enhancement. Notably, for Dataset-2, the achieved accuracy, AUC-ROC, AUC-PR, and F1-Score were 0.74, 0.89, 0.80, and 0.74, respectively. Upon incorporating data fusion, substantial improvements were observed, with scores reaching 0.86, 0.97, 0.94, and 0.86, respectively, indicating a significant boost in performance which can be seen from previous Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T1" title="Table 1 ‣ 4.4.1. Performance Comparison with Different Encoder Network ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">1</span></a>. Similarly, for Dataset-3, the scores stood at 0.58, 0.67, 0.35, and 0.47, while data fusion resulted in notable enhancements, yielding scores of 0.86, 0.89, 0.49, and 0.83, respectively, thus demonstrating consistent performance improvements. Conversely, for Dataset-1, where the dataset size was considerably larger and already facilitated robust training, the impact of data fusion on performance enhancement was relatively modest. Although there were slight improvements in accuracy, AUC-ROC, AUC-PR, and F1-Score, from 0.89, 0.97, 0.88, and 0.90 to 0.91, 0.97, 0.89, and 0.90, respectively, these enhancements were not as substantial.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">These findings highlight the significance of data fusion, particularly in scenarios with smaller datasets, where it significantly contributes to performance improvement.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Analyzing the Performance without Data Fusion during Pre-training Phase: Comparison of SwinV2-based classifier performance, where the encoder is pre-trained on individual training sets and the classifier is subsequently fine-tuned on the same training set, followed by evaluation on respective test sets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1">Dataset Name</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2">Classifier Name</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3">AUC-ROC</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.4">Accuracy</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.5">AUC-PR</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.6">F1-Score</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.2.2.1">Dataset-1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.2.2.2">Multi-OCT-SelfNet-SwinV2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.2.2.3">0.97</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.2.2.4">0.89</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.2.2.5">0.88</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.2.2.6">0.90</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.3.3.1">Dataset-2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.3.3.2">Multi-OCT-SelfNet-SwinV2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.3.3.3">0.89</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.3.3.4">0.74</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.3.3.5">0.80</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.3.3.6">0.74</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.4.4.1">Dataset-3</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.4.2">Multi-OCT-SelfNet-SwinV2</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.4.3">0.67</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.4.4">0.58</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.4.5">0.35</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.4.6">0.47</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Performance Evaluation on the Effect of Self-supervised Pre-training</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">In this ablation study, we investigated the impact of self-supervised pre-training on classifier performance in two key aspects: accurate label classification and generalization to diverse test sets. Instead of fine-tuning a pre-trained model as in the previous experiment, we trained the model from scratch. We used random initialization of the model’s parameters while training to evaluate the model’s performance. We found that, even without fine-tuning, the classifier achieved comparable accuracy to the fine-tuned model when evaluated on the same dataset and test sets. However, the model required a higher number of epochs to converge.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS3.p2">
<p class="ltx_p" id="S4.SS4.SSS3.p2.1">Nevertheless, when evaluated on other test sets for off-domain evaluation, the model exhibited poorer performance in terms of generalization, underscoring the crucial role of self-supervised pre-training in enhancing model robustness and adaptability across diverse datasets.
The results presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T4" title="Table 4 ‣ 4.4.3. Performance Evaluation on the Effect of Self-supervised Pre-training ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">4</span></a> indicate that while the on-domain performance for Dataset-1 remains largely consistent, a slight deterioration is observed in the off-domain evaluation. Conversely, for Dataset-2 and Dataset-3, which are smaller datasets, both the on-domain and off-domain evaluation performances exhibit significant deterioration without the self-supervised step.
The grouped bar chart in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F8" title="Figure 8 ‣ 4.4.3. Performance Evaluation on the Effect of Self-supervised Pre-training ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">8</span></a> compares AUC-ROC scores between self-supervised and non-self-supervised methodologies. Across all three datasets and their corresponding test sets, the non-self-supervised approach consistently shows a decline in performance, highlighting the significant impact of the self-supervised methodology.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS3.p3">
<p class="ltx_p" id="S4.SS4.SSS3.p3.1">To further assess the overall generalization capability, we employed the penalty index, as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T5" title="Table 5 ‣ 4.4.3. Performance Evaluation on the Effect of Self-supervised Pre-training ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">5</span></a>. This index offers a quantitative measure of the impact of omitting self-supervised pre-training on generalization performance. Notably, the penalty index values are consistently high across most cases when the model is not equipped with pre-trained weights from the self-supervised stage which indicates lower generalization capability. These findings underscore the critical role of self-supervised pre-training in mitigating generalization degradation, particularly evident in scenarios with smaller datasets.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Analyzing the Impact of Self-Supervised Pre-Training: Comparing Our Framework with SwinV2 Classifier on Test Sets from Three Datasets. Evaluation Includes AUC-ROC, Accuracy, AUC-PR, and F1-Score for Multi-Class Classification Performance.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.3.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T4.3.1.1.3"><span class="ltx_text" id="S4.T4.3.1.1.3.1" style="font-size:70%;">AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T4.3.1.1.4"><span class="ltx_text" id="S4.T4.3.1.1.4.1" style="font-size:70%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T4.3.1.1.5"><span class="ltx_text" id="S4.T4.3.1.1.5.1" style="font-size:70%;">AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T4.3.1.1.6"><span class="ltx_text" id="S4.T4.3.1.1.6.1" style="font-size:70%;">F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T4.3.2.1.1"><span class="ltx_text" id="S4.T4.3.2.1.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.2.1.2"><span class="ltx_text" id="S4.T4.3.2.1.2.1" style="font-size:70%;">Classifier Name</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.3"><span class="ltx_text" id="S4.T4.3.2.1.3.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.4"><span class="ltx_text" id="S4.T4.3.2.1.4.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.5"><span class="ltx_text" id="S4.T4.3.2.1.5.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.6"><span class="ltx_text" id="S4.T4.3.2.1.6.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.7"><span class="ltx_text" id="S4.T4.3.2.1.7.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.8"><span class="ltx_text" id="S4.T4.3.2.1.8.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.9"><span class="ltx_text" id="S4.T4.3.2.1.9.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.10"><span class="ltx_text" id="S4.T4.3.2.1.10.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.11"><span class="ltx_text" id="S4.T4.3.2.1.11.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.12"><span class="ltx_text" id="S4.T4.3.2.1.12.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.13"><span class="ltx_text" id="S4.T4.3.2.1.13.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.2.1.14"><span class="ltx_text" id="S4.T4.3.2.1.14.1" style="font-size:70%;">Test3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.2">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.3.3.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.2"><span class="ltx_text" id="S4.T4.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.3" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.4" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.5"><span class="ltx_text" id="S4.T4.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.6" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.7" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.7.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.8"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.8.1" style="font-size:70%;background-color:#F3F3F3;">0.46</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.9" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.9.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.10" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.10.1" style="font-size:70%;background-color:#F3F3F3;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.11"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.11.1" style="font-size:70%;background-color:#F3F3F3;">0.42</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.12" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.12.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.13" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.13.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.3.2.14"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.14.1" style="font-size:70%;background-color:#F3F3F3;">0.87</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.4.3" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T4.3.4.3.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.2"><span class="ltx_text" id="S4.T4.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.3" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.3.3.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.4.1" style="font-size:70%;background-color:#F3F3F3;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.3.5.1" style="font-size:70%;background-color:#F3F3F3;">0.58</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.6" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.3.6.1" style="font-size:70%;background-color:#F3F3F3;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.7.1" style="font-size:70%;background-color:#F3F3F3;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.8"><span class="ltx_text" id="S4.T4.3.4.3.8.1" style="font-size:70%;background-color:#F3F3F3;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.9.1" style="font-size:70%;background-color:#F3F3F3;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.10.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.11"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.3.11.1" style="font-size:70%;background-color:#F3F3F3;">0.42</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.12.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.4.3.13.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.3.14"><span class="ltx_text" id="S4.T4.3.4.3.14.1" style="font-size:70%;background-color:#F3F3F3;">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.5.4">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.3.5.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.3.1" style="font-size:70%;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.4.1" style="font-size:70%;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.5.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.6.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.7.1" style="font-size:70%;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.8"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.8.1" style="font-size:70%;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.9"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.9.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.10"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.10.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.11"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.11.1" style="font-size:70%;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.12"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.12.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.13"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.13.1" style="font-size:70%;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.5.4.14"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.4.14.1" style="font-size:70%;">0.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T4.3.6.5.1"><span class="ltx_text" id="S4.T4.3.6.5.1.1" style="font-size:70%;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.3"><span class="ltx_text" id="S4.T4.3.6.5.3.1" style="font-size:70%;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.4"><span class="ltx_text" id="S4.T4.3.6.5.4.1" style="font-size:70%;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.5"><span class="ltx_text" id="S4.T4.3.6.5.5.1" style="font-size:70%;">0.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.6"><span class="ltx_text" id="S4.T4.3.6.5.6.1" style="font-size:70%;">0.57</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.7"><span class="ltx_text" id="S4.T4.3.6.5.7.1" style="font-size:70%;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.8"><span class="ltx_text" id="S4.T4.3.6.5.8.1" style="font-size:70%;">0.39</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.9"><span class="ltx_text" id="S4.T4.3.6.5.9.1" style="font-size:70%;">0.47</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.10"><span class="ltx_text" id="S4.T4.3.6.5.10.1" style="font-size:70%;">0.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.11"><span class="ltx_text" id="S4.T4.3.6.5.11.1" style="font-size:70%;">0.37</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.12"><span class="ltx_text" id="S4.T4.3.6.5.12.1" style="font-size:70%;">0.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.13"><span class="ltx_text" id="S4.T4.3.6.5.13.1" style="font-size:70%;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.5.14"><span class="ltx_text" id="S4.T4.3.6.5.14.1" style="font-size:70%;">0.52</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7.6">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.3.7.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.2"><span class="ltx_text" id="S4.T4.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.3" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.3.1" style="font-size:70%;background-color:#F3F3F3;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.4" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.4.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.5.1" style="font-size:70%;background-color:#F3F3F3;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.7.6.6.1" style="font-size:70%;background-color:#F3F3F3;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.7" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.7.1" style="font-size:70%;background-color:#F3F3F3;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.8.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.9" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.9.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.10" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.10.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.11"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.11.1" style="font-size:70%;background-color:#F3F3F3;">0.75</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.12" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.12.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.13" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.13.1" style="font-size:70%;background-color:#F3F3F3;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.7.6.14"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.6.14.1" style="font-size:70%;background-color:#F3F3F3;">0.83</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.8.7" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T4.3.8.7.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.2"><span class="ltx_text" id="S4.T4.3.8.7.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.3.1" style="font-size:70%;background-color:#F3F3F3;">0.63</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.4.1" style="font-size:70%;background-color:#F3F3F3;">0.81</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.5"><span class="ltx_text" id="S4.T4.3.8.7.5.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.6" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.8.7.6.1" style="font-size:70%;background-color:#F3F3F3;">0.46</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.7.1" style="font-size:70%;background-color:#F3F3F3;">0.63</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.8"><span class="ltx_text" id="S4.T4.3.8.7.8.1" style="font-size:70%;background-color:#F3F3F3;">0.80</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.9.1" style="font-size:70%;background-color:#F3F3F3;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.10.1" style="font-size:70%;background-color:#F3F3F3;">0.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.11"><span class="ltx_text" id="S4.T4.3.8.7.11.1" style="font-size:70%;background-color:#F3F3F3;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.12.1" style="font-size:70%;background-color:#F3F3F3;">0.45</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T4.3.8.7.13.1" style="font-size:70%;background-color:#F3F3F3;">0.58</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.3.8.7.14"><span class="ltx_text" id="S4.T4.3.8.7.14.1" style="font-size:70%;background-color:#F3F3F3;">0.76</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F8">
<p class="ltx_p ltx_align_center" id="S4.F8.1"><span class="ltx_text" id="S4.F8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="322" id="S4.F8.1.1.g1" src="extracted/5860915/figures/table_bar_4.png" width="449"/></span></p>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Comparison of AUC-ROC Scores for Multi-OCT-SelfNet-SwinV2 Classifier Without and With Self-Supervised Pretraining Phase Across Three Datasets and Test Sets.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 5. </span>Analyzing Model’s Generalization Performance with or without SSL Pre-training: Comparing Our Proposed Framework with SwinV2 Network which is not pre-trained with SSL, Assessing Penalty-Based Performance Scores Across Three Test Sets for Different Datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.3.1.1.1"><span class="ltx_text" id="S4.T5.3.1.1.1.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.3.1.1.2"><span class="ltx_text" id="S4.T5.3.1.1.2.1" style="font-size:70%;">Classifier Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.3.1.1.3"><span class="ltx_text" id="S4.T5.3.1.1.3.1" style="font-size:70%;">P-Index of AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.3.1.1.4"><span class="ltx_text" id="S4.T5.3.1.1.4.1" style="font-size:70%;">P-Index of Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.3.1.1.5"><span class="ltx_text" id="S4.T5.3.1.1.5.1" style="font-size:70%;">P-Index of AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.3.1.1.6"><span class="ltx_text" id="S4.T5.3.1.1.6.1" style="font-size:70%;">P-Index of F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.3.2.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.3.2.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.2.1.2"><span class="ltx_text" id="S4.T5.3.2.1.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.2.1.3"><span class="ltx_text" id="S4.T5.3.2.1.3.1" style="font-size:70%;background-color:#F3F3F3;">16.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.3.2.1.4.1" style="font-size:70%;background-color:#F3F3F3;">26.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.3.2.1.5.1" style="font-size:70%;background-color:#F3F3F3;">23.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T5.3.2.1.6.1" style="font-size:70%;background-color:#F3F3F3;">10.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3.2" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T5.3.3.2.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T5.3.3.2.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.3.2.2"><span class="ltx_text" id="S4.T5.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">15.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.3.2.4"><span class="ltx_text" id="S4.T5.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">28.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.3.2.5"><span class="ltx_text" id="S4.T5.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">24.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.3.2.6"><span class="ltx_text" id="S4.T5.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">14.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.4.3">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.3.4.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.4.3.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T5.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.3.3.1" style="font-size:70%;">11.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.3.4.1" style="font-size:70%;">34.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.3.5.1" style="font-size:70%;">26.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.3.6.1" style="font-size:70%;">30.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T5.3.5.4.1"><span class="ltx_text" id="S4.T5.3.5.4.1.1" style="font-size:70%;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T5.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.5.4.3"><span class="ltx_text" id="S4.T5.3.5.4.3.1" style="font-size:70%;">25.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.5.4.4"><span class="ltx_text" id="S4.T5.3.5.4.4.1" style="font-size:70%;">41.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.5.4.5"><span class="ltx_text" id="S4.T5.3.5.4.5.1" style="font-size:70%;">43.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.5.4.6"><span class="ltx_text" id="S4.T5.3.5.4.6.1" style="font-size:70%;">36.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.6.5">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.3.6.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.6.5.2"><span class="ltx_text" id="S4.T5.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.6.5.3.1" style="font-size:70%;background-color:#F3F3F3;">16.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T5.3.6.5.4.1" style="font-size:70%;background-color:#F3F3F3;">28.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T5.3.6.5.5.1" style="font-size:70%;background-color:#F3F3F3;">27.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.3.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T5.3.6.5.6.1" style="font-size:70%;background-color:#F3F3F3;">28.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.7.6" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T5.3.7.6.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T5.3.7.6.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.3.7.6.2"><span class="ltx_text" id="S4.T5.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">SwinV2-without-SSL</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.3.7.6.3"><span class="ltx_text" id="S4.T5.3.7.6.3.1" style="font-size:70%;background-color:#F3F3F3;">23.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.3.7.6.4"><span class="ltx_text" id="S4.T5.3.7.6.4.1" style="font-size:70%;background-color:#F3F3F3;">36.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.3.7.6.5"><span class="ltx_text" id="S4.T5.3.7.6.5.1" style="font-size:70%;background-color:#F3F3F3;">39.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.3.7.6.6"><span class="ltx_text" id="S4.T5.3.7.6.6.1" style="font-size:70%;background-color:#F3F3F3;">40.33</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4. </span>Performance Comparison in Limited Data Settings</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p" id="S4.SS4.SSS4.p1.1">In this experiment, we conducted an in-depth analysis to assess the effectiveness of our proposed framework in scenarios characterized by limited data availability, where acquiring a larger labeled dataset is not feasible. We compared the result to the baseline model. We fine-tuned the model during the fine-tuning stage using only 50% of the available training data for each dataset, as opposed to utilizing the entire dataset. Subsequently, we evaluated the performance of our proposed framework on the respective test sets. This experimental setup enabled us to gain valuable insights into the practical applicability of our model in clinical contexts, where data limitations are common and resource constraints pose significant challenges. The analysis from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T6" title="Table 6 ‣ 4.4.4. Performance Comparison in Limited Data Settings ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">6</span></a> shows notable distinctions across different datasets. Dataset-1 demonstrates better performance with the baseline model, both in on-domain and off-domain evaluations. This outcome is attributed to the dataset’s ample size, facilitating robust training. Conversely, Dataset-2 and Dataset-3, which have smaller sample sizes, experienced a significant reduction in performance with the baseline model when the training set size was halved. In these two instances, our proposed method exhibited substantial improvement over the baseline model in both on-domain and off-domain evaluations.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p2">
<p class="ltx_p" id="S4.SS4.SSS4.p2.1">The grouped bar chart in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F9" title="Figure 9 ‣ 4.4.4. Performance Comparison in Limited Data Settings ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">9</span></a> compares AUC-ROC scores between the baseline model and our proposed model in this limited data-settings experiment. The chart further validates the superior performance of our proposed Multi-OCT-SelfNet-SwinV2 model, as it consistently shows higher bar heights compared to the baseline model in most cases.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p3">
<p class="ltx_p" id="S4.SS4.SSS4.p3.1">To further quantify the model’s generalization capabilities, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T7" title="Table 7 ‣ 4.4.4. Performance Comparison in Limited Data Settings ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">7</span></a> provides insights into the penalty scores. Remarkably, our proposed model demonstrated significantly lower penalty scores compared to the baseline model for Dataset-2 and Dataset-3, underscoring its superior ability to generalize across datasets even with limited samples.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p4">
<p class="ltx_p" id="S4.SS4.SSS4.p4.1">These findings highlight the efficacy of our proposed method, particularly in scenarios with smaller datasets, where it outperforms the baseline model and showcases enhanced generalization capabilities.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 6. </span>Analyzing the Performance in Limited Data Settings: Comparison of our work with the baseline methods on test sets from three datasets, using only 50% of the training set from each dataset in finetuning. The evaluation focuses on the classification AUC-ROC, accuracy, AUC-PR, and F1-score.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.3.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T6.3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T6.3.1.1.3"><span class="ltx_text" id="S4.T6.3.1.1.3.1" style="font-size:70%;">AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T6.3.1.1.4"><span class="ltx_text" id="S4.T6.3.1.1.4.1" style="font-size:70%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T6.3.1.1.5"><span class="ltx_text" id="S4.T6.3.1.1.5.1" style="font-size:70%;">AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T6.3.1.1.6"><span class="ltx_text" id="S4.T6.3.1.1.6.1" style="font-size:70%;">F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T6.3.2.1.1"><span class="ltx_text" id="S4.T6.3.2.1.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.2.1.2"><span class="ltx_text" id="S4.T6.3.2.1.2.1" style="font-size:70%;">Classifier Name</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.3"><span class="ltx_text" id="S4.T6.3.2.1.3.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.4"><span class="ltx_text" id="S4.T6.3.2.1.4.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.5"><span class="ltx_text" id="S4.T6.3.2.1.5.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.6"><span class="ltx_text" id="S4.T6.3.2.1.6.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.7"><span class="ltx_text" id="S4.T6.3.2.1.7.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.8"><span class="ltx_text" id="S4.T6.3.2.1.8.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.9"><span class="ltx_text" id="S4.T6.3.2.1.9.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.10"><span class="ltx_text" id="S4.T6.3.2.1.10.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.11"><span class="ltx_text" id="S4.T6.3.2.1.11.1" style="font-size:70%;">Test3</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.12"><span class="ltx_text" id="S4.T6.3.2.1.12.1" style="font-size:70%;">Test1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.13"><span class="ltx_text" id="S4.T6.3.2.1.13.1" style="font-size:70%;">Test2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.2.1.14"><span class="ltx_text" id="S4.T6.3.2.1.14.1" style="font-size:70%;">Test3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.2">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.3.3.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.3"><span class="ltx_text" id="S4.T6.3.3.2.3.1" style="font-size:70%;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.4"><span class="ltx_text" id="S4.T6.3.3.2.4.1" style="font-size:70%;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.5"><span class="ltx_text" id="S4.T6.3.3.2.5.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.6"><span class="ltx_text" id="S4.T6.3.3.2.6.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.7"><span class="ltx_text" id="S4.T6.3.3.2.7.1" style="font-size:70%;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.8"><span class="ltx_text" id="S4.T6.3.3.2.8.1" style="font-size:70%;">0.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.9"><span class="ltx_text" id="S4.T6.3.3.2.9.1" style="font-size:70%;">0.96</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.10"><span class="ltx_text" id="S4.T6.3.3.2.10.1" style="font-size:70%;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.11"><span class="ltx_text" id="S4.T6.3.3.2.11.1" style="font-size:70%;">0.44</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.12"><span class="ltx_text" id="S4.T6.3.3.2.12.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.13"><span class="ltx_text" id="S4.T6.3.3.2.13.1" style="font-size:70%;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.3.2.14"><span class="ltx_text" id="S4.T6.3.3.2.14.1" style="font-size:70%;">0.61</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T6.3.4.3.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.4.3.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.2"><span class="ltx_text" id="S4.T6.3.4.3.2.1" style="font-size:70%;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.3"><span class="ltx_text" id="S4.T6.3.4.3.3.1" style="font-size:70%;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.4"><span class="ltx_text" id="S4.T6.3.4.3.4.1" style="font-size:70%;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.5"><span class="ltx_text" id="S4.T6.3.4.3.5.1" style="font-size:70%;">0.62</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.6"><span class="ltx_text" id="S4.T6.3.4.3.6.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.7"><span class="ltx_text" id="S4.T6.3.4.3.7.1" style="font-size:70%;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.8"><span class="ltx_text" id="S4.T6.3.4.3.8.1" style="font-size:70%;">0.46</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.9"><span class="ltx_text" id="S4.T6.3.4.3.9.1" style="font-size:70%;">0.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.10"><span class="ltx_text" id="S4.T6.3.4.3.10.1" style="font-size:70%;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.11"><span class="ltx_text" id="S4.T6.3.4.3.11.1" style="font-size:70%;">0.44</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.12"><span class="ltx_text" id="S4.T6.3.4.3.12.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.13"><span class="ltx_text" id="S4.T6.3.4.3.13.1" style="font-size:70%;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.4.3.14"><span class="ltx_text" id="S4.T6.3.4.3.14.1" style="font-size:70%;">0.61</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.5.4">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.3.5.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.3"><span class="ltx_text" id="S4.T6.3.5.4.3.1" style="font-size:70%;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.4"><span class="ltx_text" id="S4.T6.3.5.4.4.1" style="font-size:70%;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.5"><span class="ltx_text" id="S4.T6.3.5.4.5.1" style="font-size:70%;">0.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.6"><span class="ltx_text" id="S4.T6.3.5.4.6.1" style="font-size:70%;">0.30</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.7"><span class="ltx_text" id="S4.T6.3.5.4.7.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.8"><span class="ltx_text" id="S4.T6.3.5.4.8.1" style="font-size:70%;">0.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.9"><span class="ltx_text" id="S4.T6.3.5.4.9.1" style="font-size:70%;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.10"><span class="ltx_text" id="S4.T6.3.5.4.10.1" style="font-size:70%;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.11"><span class="ltx_text" id="S4.T6.3.5.4.11.1" style="font-size:70%;">0.59</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.12"><span class="ltx_text" id="S4.T6.3.5.4.12.1" style="font-size:70%;">0.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.13"><span class="ltx_text" id="S4.T6.3.5.4.13.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.4.14"><span class="ltx_text" id="S4.T6.3.5.4.14.1" style="font-size:70%;">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T6.3.6.5.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.6.5.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.2"><span class="ltx_text" id="S4.T6.3.6.5.2.1" style="font-size:70%;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.3"><span class="ltx_text" id="S4.T6.3.6.5.3.1" style="font-size:70%;">0.77</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.4"><span class="ltx_text" id="S4.T6.3.6.5.4.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.5"><span class="ltx_text" id="S4.T6.3.6.5.5.1" style="font-size:70%;">0.73</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.6"><span class="ltx_text" id="S4.T6.3.6.5.6.1" style="font-size:70%;">0.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.7"><span class="ltx_text" id="S4.T6.3.6.5.7.1" style="font-size:70%;">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.8"><span class="ltx_text" id="S4.T6.3.6.5.8.1" style="font-size:70%;">0.41</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.9"><span class="ltx_text" id="S4.T6.3.6.5.9.1" style="font-size:70%;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.10"><span class="ltx_text" id="S4.T6.3.6.5.10.1" style="font-size:70%;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.11"><span class="ltx_text" id="S4.T6.3.6.5.11.1" style="font-size:70%;">0.48</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.12"><span class="ltx_text" id="S4.T6.3.6.5.12.1" style="font-size:70%;">0.71</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.13"><span class="ltx_text" id="S4.T6.3.6.5.13.1" style="font-size:70%;">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.6.5.14"><span class="ltx_text" id="S4.T6.3.6.5.14.1" style="font-size:70%;">0.62</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.7.6">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.3.7.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.3"><span class="ltx_text" id="S4.T6.3.7.6.3.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.4"><span class="ltx_text" id="S4.T6.3.7.6.4.1" style="font-size:70%;">0.47</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.5"><span class="ltx_text" id="S4.T6.3.7.6.5.1" style="font-size:70%;">0.88</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.6"><span class="ltx_text" id="S4.T6.3.7.6.6.1" style="font-size:70%;">0.37</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.7"><span class="ltx_text" id="S4.T6.3.7.6.7.1" style="font-size:70%;">0.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.8"><span class="ltx_text" id="S4.T6.3.7.6.8.1" style="font-size:70%;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.9"><span class="ltx_text" id="S4.T6.3.7.6.9.1" style="font-size:70%;">0.43</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.10"><span class="ltx_text" id="S4.T6.3.7.6.10.1" style="font-size:70%;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.11"><span class="ltx_text" id="S4.T6.3.7.6.11.1" style="font-size:70%;">0.72</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.12"><span class="ltx_text" id="S4.T6.3.7.6.12.1" style="font-size:70%;">0.20</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.13"><span class="ltx_text" id="S4.T6.3.7.6.13.1" style="font-size:70%;">0.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.7.6.14"><span class="ltx_text" id="S4.T6.3.7.6.14.1" style="font-size:70%;">0.87</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.8.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T6.3.8.7.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.8.7.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T6.3.8.7.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.3"><span class="ltx_text" id="S4.T6.3.8.7.3.1" style="font-size:70%;">0.76</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.4"><span class="ltx_text" id="S4.T6.3.8.7.4.1" style="font-size:70%;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.5"><span class="ltx_text" id="S4.T6.3.8.7.5.1" style="font-size:70%;">0.88</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.6"><span class="ltx_text" id="S4.T6.3.8.7.6.1" style="font-size:70%;">0.47</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.7"><span class="ltx_text" id="S4.T6.3.8.7.7.1" style="font-size:70%;">0.80</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.8"><span class="ltx_text" id="S4.T6.3.8.7.8.1" style="font-size:70%;">0.81</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.9"><span class="ltx_text" id="S4.T6.3.8.7.9.1" style="font-size:70%;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.10"><span class="ltx_text" id="S4.T6.3.8.7.10.1" style="font-size:70%;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.11"><span class="ltx_text" id="S4.T6.3.8.7.11.1" style="font-size:70%;">0.73</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.12"><span class="ltx_text" id="S4.T6.3.8.7.12.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.13"><span class="ltx_text" id="S4.T6.3.8.7.13.1" style="font-size:70%;">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.3.8.7.14"><span class="ltx_text" id="S4.T6.3.8.7.14.1" style="font-size:70%;">0.78</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F9">
<p class="ltx_p ltx_align_center" id="S4.F9.1"><span class="ltx_text" id="S4.F9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="324" id="S4.F9.1.1.g1" src="extracted/5860915/figures/table_bar_6.png" width="449"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Comparison of AUC-ROC Scores Between Our Method (Multi-OCT-SelfNet-SwinV2) and Baseline (ResNet-50) on Test Sets from Three Datasets, when only 50% of Training Data has been used for Fine-Tuning. </figcaption>
</figure>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 7. </span>Analyzing Model’s Generalization Performance in Limited Data Settings: Comparing Our Proposed Framework with SwinV2 Network With the Baseline Model on test
sets from three datasets, using only 50% of the training set from each dataset in fine-tuning. Assessing Penalty-Based Performance Scores Across Three Test Sets for Different Datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.3.1.1.1"><span class="ltx_text" id="S4.T7.3.1.1.1.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T7.3.1.1.2"><span class="ltx_text" id="S4.T7.3.1.1.2.1" style="font-size:70%;">Classifier Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T7.3.1.1.3"><span class="ltx_text" id="S4.T7.3.1.1.3.1" style="font-size:70%;">P-Index of AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T7.3.1.1.4"><span class="ltx_text" id="S4.T7.3.1.1.4.1" style="font-size:70%;">P-Index of Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T7.3.1.1.5"><span class="ltx_text" id="S4.T7.3.1.1.5.1" style="font-size:70%;">P-Index of AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T7.3.1.1.6"><span class="ltx_text" id="S4.T7.3.1.1.6.1" style="font-size:70%;">P-Index of F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.3.2.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.3.2.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.2.1.2"><span class="ltx_text" id="S4.T7.3.2.1.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.3.2.1.3.1" style="font-size:70%;background-color:#F3F3F3;">11.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.3.2.1.4.1" style="font-size:70%;background-color:#F3F3F3;">16.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T7.3.2.1.5.1" style="font-size:70%;background-color:#F3F3F3;">20.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T7.3.2.1.6.1" style="font-size:70%;background-color:#F3F3F3;">18.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.3.2" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T7.3.3.2.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T7.3.3.2.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.3.2.2"><span class="ltx_text" id="S4.T7.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.3.2.3"><span class="ltx_text" id="S4.T7.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">14.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.3.2.4"><span class="ltx_text" id="S4.T7.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">25.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.3.2.5"><span class="ltx_text" id="S4.T7.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">23.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.3.2.6"><span class="ltx_text" id="S4.T7.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">18.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.4.3">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.3.4.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.4.3.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T7.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.4.3.3"><span class="ltx_text" id="S4.T7.3.4.3.3.1" style="font-size:70%;">22.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.4.3.4"><span class="ltx_text" id="S4.T7.3.4.3.4.1" style="font-size:70%;">60.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T7.3.4.3.5.1" style="font-size:70%;">34.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.4.3.6"><span class="ltx_text" id="S4.T7.3.4.3.6.1" style="font-size:70%;">59.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T7.3.5.4.1"><span class="ltx_text" id="S4.T7.3.5.4.1.1" style="font-size:70%;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T7.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T7.3.5.4.3.1" style="font-size:70%;">18.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T7.3.5.4.4.1" style="font-size:70%;">35.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.5.4.5"><span class="ltx_text" id="S4.T7.3.5.4.5.1" style="font-size:70%;">35.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T7.3.5.4.6.1" style="font-size:70%;">28.00</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.6.5">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.3.6.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.6.5.2"><span class="ltx_text" id="S4.T7.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.6.5.3"><span class="ltx_text" id="S4.T7.3.6.5.3.1" style="font-size:70%;background-color:#F3F3F3;">38.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.6.5.4"><span class="ltx_text" id="S4.T7.3.6.5.4.1" style="font-size:70%;background-color:#F3F3F3;">57.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.6.5.5"><span class="ltx_text" id="S4.T7.3.6.5.5.1" style="font-size:70%;background-color:#F3F3F3;">43.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.3.6.5.6"><span class="ltx_text" id="S4.T7.3.6.5.6.1" style="font-size:70%;background-color:#F3F3F3;">64.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.7.6" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T7.3.7.6.1" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T7.3.7.6.1.1" style="font-size:70%;background-color:#F3F3F3;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T7.3.7.6.2"><span class="ltx_text" id="S4.T7.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T7.3.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T7.3.7.6.3.1" style="font-size:70%;background-color:#F3F3F3;">14.00</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T7.3.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T7.3.7.6.4.1" style="font-size:70%;background-color:#F3F3F3;">30.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T7.3.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T7.3.7.6.5.1" style="font-size:70%;background-color:#F3F3F3;">26.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T7.3.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T7.3.7.6.6.1" style="font-size:70%;background-color:#F3F3F3;">30.0</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5. </span>Performance Evaluation on the Effect of Correcting Data Imbalance</h4>
<div class="ltx_para" id="S4.SS4.SSS5.p1">
<p class="ltx_p" id="S4.SS4.SSS5.p1.1"><span class="ltx_text" id="S4.SS4.SSS5.p1.1.1" style="color:#000000;">In this experiment, we have addressed the data imbalance issue by assigning different weights to each class in the loss function. This strategy ensures that the model focuses more on the minority class during training, reducing the bias towards the majority class. We compared the results of our model, using this weight-adjusting method, to the baseline model. The goal was to observe how our model performs compared to the baseline model when correcting data imbalance issues. Table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T8" style="color:#000000;" title="Table 8 ‣ 4.4.5. Performance Evaluation on the Effect of Correcting Data Imbalance ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S4.SS4.SSS5.p1.1.2" style="color:#000000;"> provides detailed results with accuracy, AUC-ROC, AUC-PR, and F1-score. The bar chart in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.F10" style="color:#000000;" title="Figure 10 ‣ 4.4.5. Performance Evaluation on the Effect of Correcting Data Imbalance ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S4.SS4.SSS5.p1.1.3" style="color:#000000;"> illustrates a similar trend to the previous experiments, when the dataset is sufficiently large (as with DS-1), the performance of both models is comparable. However, as the dataset size decreases, a more significant performance difference emerges, with our proposed model outperforming the baseline in most cases, particularly in the smallest dataset, DS-3. Additionally, Table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T9" style="color:#000000;" title="Table 9 ‣ 4.4.5. Performance Evaluation on the Effect of Correcting Data Imbalance ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" id="S4.SS4.SSS5.p1.1.4" style="color:#000000;"> shows that the P-Index is lower for our proposed model in most cases, indicating its superior generalization capability compared to the baseline. </span><span class="ltx_text" id="S4.SS4.SSS5.p1.1.5"></span></p>
</div>
<div class="ltx_para" id="S4.SS4.SSS5.p2">
<p class="ltx_p" id="S4.SS4.SSS5.p2.1"><span class="ltx_text" id="S4.SS4.SSS5.p2.1.1" style="color:#000000;">When comparing these results to the previous experiment, where class imbalance was not addressed, as shown in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.11375v1#S4.T1" style="color:#000000;" title="Table 1 ‣ 4.4.1. Performance Comparison with Different Encoder Network ‣ 4.4. Supervised Fine-tuning Result ‣ 4. Experiments ‣ Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS4.SSS5.p2.1.2" style="color:#000000;">, it is evident that correcting the imbalance causes a slight decline in performance for both models. Despite this reduction, our proposed model still outperforms the baseline in most cases, achieving a higher score between the two. This indicates that even with the trade-off of a minor performance drop, our model has a notable advantage in overall performance.</span><span class="ltx_text" id="S4.SS4.SSS5.p2.1.3"></span></p>
</div>
<figure class="ltx_table" id="S4.T8">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 8. </span>Analyzing the Performance Class Imbalance Correction: Comparison of our framework with baseline methods (ResNet-50) on the test sets from three datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T8.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.3.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.3.1.1.1"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T8.3.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T8.3.1.1.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.1.1.3.1" style="font-size:70%;background-color:#FFFFFF;">AUC-ROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T8.3.1.1.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.1.1.4.1" style="font-size:70%;background-color:#FFFFFF;">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T8.3.1.1.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.1.1.5.1" style="font-size:70%;background-color:#FFFFFF;">AUC-PR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T8.3.1.1.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.1.1.6.1" style="font-size:70%;background-color:#FFFFFF;">F1-Score</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.2.2" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T8.3.2.2.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.1.1" style="font-size:70%;background-color:#FFFFFF;">Train Set</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.3.2.2.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.2.1" style="font-size:70%;background-color:#FFFFFF;">Classifier Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.3.1" style="font-size:70%;background-color:#FFFFFF;">Test-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.4.1" style="font-size:70%;background-color:#FFFFFF;">Test-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.5.1" style="font-size:70%;background-color:#FFFFFF;">Test-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.6.1" style="font-size:70%;background-color:#FFFFFF;">Test-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.7.1" style="font-size:70%;background-color:#FFFFFF;">Test-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.8" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.8.1" style="font-size:70%;background-color:#FFFFFF;">Test-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.9" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.9.1" style="font-size:70%;background-color:#FFFFFF;">Test-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.10" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.10.1" style="font-size:70%;background-color:#FFFFFF;">Test-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.11" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.11.1" style="font-size:70%;background-color:#FFFFFF;">Test-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.12" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.12.1" style="font-size:70%;background-color:#FFFFFF;">Test-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.13" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.13.1" style="font-size:70%;background-color:#FFFFFF;">Test-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T8.3.2.2.14" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.2.2.14.1" style="font-size:70%;background-color:#FFFFFF;">Test-3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.3.3">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.3.3.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.3.3.2.1" style="font-size:70%;background-color:#FFFFFF;">Resnet50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.3.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.5"><span class="ltx_text" id="S4.T8.3.3.3.5.1" style="font-size:70%;background-color:#F3F3F3;">0.62</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.6.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.7.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.8"><span class="ltx_text" id="S4.T8.3.3.3.8.1" style="font-size:70%;background-color:#F3F3F3;">0.14</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.9.1" style="font-size:70%;background-color:#F3F3F3;">0.96</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.10.1" style="font-size:70%;background-color:#F3F3F3;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.11"><span class="ltx_text" id="S4.T8.3.3.3.11.1" style="font-size:70%;background-color:#F3F3F3;">0.41</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.12.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.3.3.13.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.3.3.14"><span class="ltx_text" id="S4.T8.3.3.3.14.1" style="font-size:70%;background-color:#F3F3F3;">0.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.4.4" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T8.3.4.4.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.4.4.1.1" style="font-size:70%;background-color:#FFFFFF;">DS-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.4.4.2.1" style="font-size:70%;background-color:#FFFFFF;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.3.1" style="font-size:70%;background-color:#F3F3F3;">0.97</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.4.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.5"><span class="ltx_text" id="S4.T8.3.4.4.5.1" style="font-size:70%;background-color:#F3F3F3;">0.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.6.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.7.1" style="font-size:70%;background-color:#F3F3F3;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.8"><span class="ltx_text" id="S4.T8.3.4.4.8.1" style="font-size:70%;background-color:#F3F3F3;">0.31</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.9.1" style="font-size:70%;background-color:#F3F3F3;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.10.1" style="font-size:70%;background-color:#F3F3F3;">0.99</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.11"><span class="ltx_text" id="S4.T8.3.4.4.11.1" style="font-size:70%;background-color:#F3F3F3;">0.44</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.12.1" style="font-size:70%;background-color:#F3F3F3;">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.4.4.13.1" style="font-size:70%;background-color:#F3F3F3;">0.93</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.4.4.14"><span class="ltx_text" id="S4.T8.3.4.4.14.1" style="font-size:70%;background-color:#F3F3F3;">0.61</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.5.5">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.3.5.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.2"><span class="ltx_text" id="S4.T8.3.5.5.2.1" style="font-size:70%;background-color:#FFFFFF;">Resnet50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.3.1" style="font-size:70%;background-color:#FFFFFF;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.4.1" style="font-size:70%;background-color:#FFFFFF;">0.98</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.5"><span class="ltx_text" id="S4.T8.3.5.5.5.1" style="font-size:70%;background-color:#FFFFFF;">0.51</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.6.1" style="font-size:70%;background-color:#FFFFFF;">0.35</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.7.1" style="font-size:70%;background-color:#FFFFFF;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.8"><span class="ltx_text" id="S4.T8.3.5.5.8.1" style="font-size:70%;background-color:#FFFFFF;">0.04</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.9" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.9.1" style="font-size:70%;background-color:#FFFFFF;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.10" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.10.1" style="font-size:70%;background-color:#FFFFFF;">0.95</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.11"><span class="ltx_text" id="S4.T8.3.5.5.11.1" style="font-size:70%;background-color:#FFFFFF;">0.52</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.12" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.12.1" style="font-size:70%;background-color:#FFFFFF;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.13" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.5.5.13.1" style="font-size:70%;background-color:#FFFFFF;">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.5.5.14"><span class="ltx_text" id="S4.T8.3.5.5.14.1" style="font-size:70%;background-color:#FFFFFF;">0.02</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.6.6" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T8.3.6.6.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.1.1" style="font-size:70%;background-color:#FFFFFF;">DS-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.2"><span class="ltx_text" id="S4.T8.3.6.6.2.1" style="font-size:70%;background-color:#FFFFFF;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.3.1" style="font-size:70%;background-color:#FFFFFF;">0.78</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.4.1" style="font-size:70%;background-color:#FFFFFF;">0.92</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.5"><span class="ltx_text" id="S4.T8.3.6.6.5.1" style="font-size:70%;background-color:#FFFFFF;">0.90</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.6.1" style="font-size:70%;background-color:#FFFFFF;">0.65</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.7.1" style="font-size:70%;background-color:#FFFFFF;">0.78</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.8"><span class="ltx_text" id="S4.T8.3.6.6.8.1" style="font-size:70%;background-color:#FFFFFF;">0.37</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.9" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.9.1" style="font-size:70%;background-color:#FFFFFF;">0.56</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.10" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.10.1" style="font-size:70%;background-color:#FFFFFF;">0.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.11"><span class="ltx_text" id="S4.T8.3.6.6.11.1" style="font-size:70%;background-color:#FFFFFF;">0.69</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.12" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.12.1" style="font-size:70%;background-color:#FFFFFF;">0.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.13" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.6.6.13.1" style="font-size:70%;background-color:#FFFFFF;">0.78</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.6.6.14"><span class="ltx_text" id="S4.T8.3.6.6.14.1" style="font-size:70%;background-color:#FFFFFF;">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.7.7">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.3.7.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.7.7.2.1" style="font-size:70%;background-color:#FFFFFF;">Resnet50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.3.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.4.1" style="font-size:70%;background-color:#F3F3F3;">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.5"><span class="ltx_text" id="S4.T8.3.7.7.5.1" style="font-size:70%;background-color:#F3F3F3;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.6.1" style="font-size:70%;background-color:#F3F3F3;">0.37</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.7.1" style="font-size:70%;background-color:#F3F3F3;">0.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.8"><span class="ltx_text" id="S4.T8.3.7.7.8.1" style="font-size:70%;background-color:#F3F3F3;">0.66</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.9.1" style="font-size:70%;background-color:#F3F3F3;">0.42</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.10.1" style="font-size:70%;background-color:#F3F3F3;">0.63</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.11"><span class="ltx_text" id="S4.T8.3.7.7.11.1" style="font-size:70%;background-color:#F3F3F3;">0.57</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.12.1" style="font-size:70%;background-color:#F3F3F3;">0.20</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.7.7.13.1" style="font-size:70%;background-color:#F3F3F3;">0.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.3.7.7.14"><span class="ltx_text" id="S4.T8.3.7.7.14.1" style="font-size:70%;background-color:#F3F3F3;">0.65</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.3.8.8" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T8.3.8.8.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.8.8.1.1" style="font-size:70%;background-color:#FFFFFF;">DS-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T8.3.8.8.2.1" style="font-size:70%;background-color:#FFFFFF;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.3.1" style="font-size:70%;background-color:#F3F3F3;">0.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.4.1" style="font-size:70%;background-color:#F3F3F3;">0.94</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.5"><span class="ltx_text" id="S4.T8.3.8.8.5.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.6.1" style="font-size:70%;background-color:#F3F3F3;">0.32</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.7" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.7.1" style="font-size:70%;background-color:#F3F3F3;">0.62</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.8"><span class="ltx_text" id="S4.T8.3.8.8.8.1" style="font-size:70%;background-color:#F3F3F3;">0.76</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.9" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.9.1" style="font-size:70%;background-color:#F3F3F3;">0.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.10" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.10.1" style="font-size:70%;background-color:#F3F3F3;">0.91</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.11"><span class="ltx_text" id="S4.T8.3.8.8.11.1" style="font-size:70%;background-color:#F3F3F3;">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.12" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.12.1" style="font-size:70%;background-color:#F3F3F3;">0.55</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.13" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T8.3.8.8.13.1" style="font-size:70%;background-color:#F3F3F3;">0.80</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T8.3.8.8.14"><span class="ltx_text" id="S4.T8.3.8.8.14.1" style="font-size:70%;background-color:#F3F3F3;">0.76</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F10">
<p class="ltx_p ltx_align_center" id="S4.F10.1"><span class="ltx_text" id="S4.F10.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="328" id="S4.F10.1.1.g1" src="extracted/5860915/figures/table_bar_8.png" width="449"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Comparison of AUC-ROC Scores Between Our Method (Multi-OCT-SelfNet-SwinV2) and Baseline (ResNet-50) on Test Sets from Three Datasets with class imbalance correction.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T9">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 9. </span>Analyzing Model’s Generalization Performance with Class Imbalance Correction: Comparison of our framework with baseline methods (ResNet-50) on the test sets from three datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T9.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.3.1.1.1"><span class="ltx_text" id="S4.T9.3.1.1.1.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T9.3.1.1.2"><span class="ltx_text" id="S4.T9.3.1.1.2.1" style="font-size:70%;">Classifier Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T9.3.1.1.3"><span class="ltx_text" id="S4.T9.3.1.1.3.1" style="font-size:70%;">P-Index of Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T9.3.1.1.4"><span class="ltx_text" id="S4.T9.3.1.1.4.1" style="font-size:70%;">P-Index of AUC-ROC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T9.3.1.1.5"><span class="ltx_text" id="S4.T9.3.1.1.5.1" style="font-size:70%;">P-Index of AUC-PR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T9.3.1.1.6"><span class="ltx_text" id="S4.T9.3.1.1.6.1" style="font-size:70%;">P-Index of F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.3.2.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.3.2.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.2.1.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.2.1.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.2.1.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.2.1.3.1" style="font-size:70%;background-color:#F3F3F3;">33.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.2.1.4" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.2.1.4.1" style="font-size:70%;background-color:#F3F3F3;">13.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.2.1.5" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.2.1.5.1" style="font-size:70%;background-color:#F3F3F3;">21.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.2.1.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.2.1.6.1" style="font-size:70%;background-color:#F3F3F3;">34.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T9.3.3.2.1"><span class="ltx_text" id="S4.T9.3.3.2.1.1" style="font-size:70%;">Dataset-1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.3.2.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.3.2.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.3.2.3" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.3.2.3.1" style="font-size:70%;background-color:#F3F3F3;">30.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.3.2.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.3.2.4.1" style="font-size:70%;background-color:#F3F3F3;">14.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.3.2.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.3.2.5.1" style="font-size:70%;background-color:#F3F3F3;">23.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.3.2.6" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.3.2.6.1" style="font-size:70%;background-color:#F3F3F3;">19.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.3.4.3">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.3.4.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.4.3.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.4.3.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.4.3.3"><span class="ltx_text" id="S4.T9.3.4.3.3.1" style="font-size:70%;">27.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.4.3.4"><span class="ltx_text" id="S4.T9.3.4.3.4.1" style="font-size:70%;">58.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.4.3.5"><span class="ltx_text" id="S4.T9.3.4.3.5.1" style="font-size:70%;">37.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.4.3.6"><span class="ltx_text" id="S4.T9.3.4.3.6.1" style="font-size:70%;">57.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T9.3.5.4.1"><span class="ltx_text" id="S4.T9.3.5.4.1.1" style="font-size:70%;">Dataset-2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.5.4.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.5.4.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T9.3.5.4.3.1" style="font-size:70%;">13.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T9.3.5.4.4.1" style="font-size:70%;">40.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T9.3.5.4.5.1" style="font-size:70%;">30.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T9.3.5.4.6.1" style="font-size:70%;">25.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.3.6.5">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.3.6.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.6.5.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.6.5.2.1" style="font-size:70%;background-color:#F3F3F3;">Resnet-50</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.6.5.3" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.6.5.3.1" style="font-size:70%;background-color:#F3F3F3;">44.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.6.5.4" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.6.5.4.1" style="font-size:70%;background-color:#F3F3F3;">65.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.6.5.5" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.6.5.5.1" style="font-size:70%;background-color:#F3F3F3;">46.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T9.3.6.5.6" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.6.5.6.1" style="font-size:70%;background-color:#F3F3F3;">71.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T9.3.7.6.1"><span class="ltx_text" id="S4.T9.3.7.6.1.1" style="font-size:70%;">Dataset-3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T9.3.7.6.2" style="background-color:#F3F3F3;"><span class="ltx_text" id="S4.T9.3.7.6.2.1" style="font-size:70%;background-color:#F3F3F3;">Multi-OCT-SelfNet-SwinV2</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T9.3.7.6.3" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.7.6.3.1" style="font-size:70%;background-color:#F3F3F3;">15.67</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T9.3.7.6.4" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.7.6.4.1" style="font-size:70%;background-color:#F3F3F3;">43.33</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T9.3.7.6.5" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.7.6.5.1" style="font-size:70%;background-color:#F3F3F3;">27.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T9.3.7.6.6" style="background-color:#F3F3F3;"><span class="ltx_text ltx_font_bold" id="S4.T9.3.7.6.6.1" style="font-size:70%;background-color:#F3F3F3;">29.67</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="color:#000000;">While our framework has shown significant potential in the automated classification of retinal diseases using OCT images, its complexity poses challenges in terms of interpretability. As we move forward, enhancing the transparency of our model is a key priority. By improving interpretability, we aim to build AI-based diagnostic tools that clinicians can trust and readily understand, ensuring that these models are not only powerful but also transparent in their operations. Additionally, we recognize the importance of continuous improvement in AI systems, particularly in the context of their application in diverse clinical environments. Our future work will focus on integrating a human-in-the-loop system to complement the model’s capabilities. By involving human expertise, we can enhance the model’s adaptability, enabling it to learn and improve from real-world feedback continuously. This approach will help mitigate issues related to cross-domain generalization, ensuring that our model maintains high performance across different clinical settings. </span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text" id="S5.p2.1.1" style="color:#000000;">Inconsistent labeling or errors in the dataset can introduce biases and negatively affect the model’s performance, particularly when combining datasets from multiple sources. In this process, the domain experts will periodically review the model’s predictions, especially in examples where the model is uncertain. This iterative feedback loop will help to continuously refine label quality and enhance model accuracy by effectively managing labeling inconsistencies and errors in the dataset.</span></p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The proposed work aims to leverage innovative techniques such as multi-source data fusion, self-supervised learning, and transformer networks to overcome the challenges posed by limited data availability in retinal disease diagnosis. During the self-supervised phase, the model is pre-trained using publicly available datasets, ensuring that sensitive clinical data is not exposed. In the downstream classification task specific to individual clinical settings, the model is fine-tuned solely on the local dataset, eliminating the need to share any private data. This approach preserves data privacy while allowing the model to adapt effectively to diverse clinical environments. This framework will be beneficial in situations where access to extensive datasets is limited offering a scalable and practical approach to implementing medical AI solutions. The results of our study showcase the effectiveness of our proposed framework, Multi-OCT-SelfNet, which consistently surpasses the baseline performance. Our approach demonstrates superior performance scores across all datasets, particularly excelling with smaller datasets. Through meticulous experimentation, we’ve validated the efficacy of our methodology. The incorporation of data fusion and self-supervised pre-training significantly enhances performance, as evidenced by our ablation study. This highlights the significance of these components in improving both the resilience and accuracy of our model, thereby facilitating robust generalization.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Minhaj Alam, David Le, Taeyoon Son, Jennifer I. Lim, and Xincheng Yao. 2020a.

</span>
<span class="ltx_bibblock">AV-Net: deep learning for fully automated artery-vein classification in optical coherence tomography angiography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Biomed. Opt. Express</em> 11, 9 (Sep 2020), 5249–5257.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1364/BOE.399514" title="">https://doi.org/10.1364/BOE.399514</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Minhaj Alam, Yue Zhang, Jennifer I Lim, RVP Chan, Min Yang, and Xincheng Yao. 2020b.

</span>
<span class="ltx_bibblock">Quantitative OCT angiography features for objective classification and staging of diabetic retinopathy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Retina (Philadelphia, Pa.)</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alshammari et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hamoud Alshammari, Karim Gasmi, Ibtihel Ben Ltaifa, Moez Krichen, Lassaad Ben Ammar, and Mahmood A Mahmood. 2022.

</span>
<span class="ltx_bibblock">Olive disease classification based on vision transformer and CNN models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Computational Intelligence and Neuroscience</em> 2022 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awais et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Muhammad Awais, Henning Müller, Tong B. Tang, and Fabrice Meriaudeau. 2017.

</span>
<span class="ltx_bibblock">Classification of SD-OCT images using a Deep learning approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">2017 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)</em>. 489–492.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICSIPA.2017.8120661" title="">https://doi.org/10.1109/ICSIPA.2017.8120661</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ayana et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gelan Ayana, Kokeb Dese, Yisak Dereje, Yonas Kebede, Hika Barki, Dechassa Amdissa, Nahimiya Husen, Fikadu Mulugeta, Bontu Habtamu, and Se-Woon Choe. 2023.

</span>
<span class="ltx_bibblock">Vision-Transformer-Based Transfer Learning for Mammogram Classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Diagnostics</em> 13, 2 (2023), 178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bressem et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Keno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L Vahldiek. 2020.

</span>
<span class="ltx_bibblock">Comparing different deep learning architectures for classification of chest radiographs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Scientific reports</em> 10, 1 (2020), 13590.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020.

</span>
<span class="ltx_bibblock">End-to-End Object Detection with Transformers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.12872 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kyung Jun Choi, Jung Eun Choi, Hyeon Cheol Roh, Jun Soo Eun, Jong Min Kim, Yong Kyun Shin, Min Chae Kang, Joon Kyo Chung, Chaeyeon Lee, Dongyoung Lee, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Deep learning models for screening of high myopia using optical coherence tomography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">Scientific reports</em> 11, 1 (2021), 21663.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020.

</span>
<span class="ltx_bibblock">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">CoRR</em> abs/2010.11929 (2020).

</span>
<span class="ltx_bibblock">arXiv:2010.11929

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2010.11929" title="">https://arxiv.org/abs/2010.11929</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Leyuan Fang, Jiahuan Guo, Xingxin He, and Muxing Li. 2022.

</span>
<span class="ltx_bibblock">Self-supervised patient-specific features learning for OCT image classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Medical &amp; Biological Engineering &amp; Computing</em> 60, 10 (2022), 2851–2863.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friberg et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Thomas R Friberg, Richard A Bilonick, and Peter M Brennen. 2011.

</span>
<span class="ltx_bibblock">Analysis of the relationship between drusen size and drusen area in eyes with age-related macular degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Ophthalmic Surgery, Lasers and Imaging Retina</em> 42, 5 (2011), 369–375.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gholami et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sina Gholami, Jennifer I. Lim, Theodore Leng, Sally Shin Yee Ong, Atalie Carina Thompson, and Minhaj Nur Alam. 2023.

</span>
<span class="ltx_bibblock">Federated learning for diagnosis of age-related macular degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Frontiers in Medicine</em> 10 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3389/fmed.2023.1259017" title="">https://doi.org/10.3389/fmed.2023.1259017</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 16000–16009.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock">Deep Residual Learning for Image Recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">CoRR</em> abs/1512.03385 (2015).

</span>
<span class="ltx_bibblock">arXiv:1512.03385

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1512.03385" title="">http://arxiv.org/abs/1512.03385</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Islam (2022)</span>
<span class="ltx_bibblock">
Khawar Islam. 2022.

</span>
<span class="ltx_bibblock">Recent advances in vision transformer: A survey and outlook of recent work.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2203.01536</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jannat et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Fatema-E Jannat, Sina Gholami, Minhaj Nur Alam, and Hamed Tabkhi. 2024.

</span>
<span class="ltx_bibblock">OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.12344 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jing and Tian (2020)</span>
<span class="ltx_bibblock">
Longlong Jing and Yingli Tian. 2020.

</span>
<span class="ltx_bibblock">Self-supervised visual feature learning with deep neural networks: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE transactions on pattern analysis and machine intelligence</em> 43, 11 (2020), 4037–4058.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kermany et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al<span class="ltx_text" id="bib.bib20.3.1">.</span> 2018.

</span>
<span class="ltx_bibblock">Identifying medical diagnoses and treatable diseases by image-based deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.4.1">cell</em> 172, 5 (2018), 1122–1131.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kihara et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuka Kihara, Mengxi Shen, Yingying Shi, Xiaoshuang Jiang, Liang Wang, Rita Laiginhas, Cancan Lyu, Jin Yang, Jeremy Liu, Rosalyn Morin, et al<span class="ltx_text" id="bib.bib21.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Detection of nonexudative macular neovascularization on structural oct images using vision transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.4.1">Ophthalmology Science</em> 2, 4 (2022), 100197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnapriya and Karuna (2023)</span>
<span class="ltx_bibblock">
Srigiri Krishnapriya and Yepuganti Karuna. 2023.

</span>
<span class="ltx_bibblock">Pre-trained deep learning models for brain MRI image classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Frontiers in Human Neuroscience</em> 17 (2023), 1150120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012.

</span>
<span class="ltx_bibblock">ImageNet Classification with Deep Convolutional Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Advances in Neural Information Processing Systems</em>, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (Eds.), Vol. 25. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
David Le, Minhaj Alam, Cham K. Yao, Jennifer I. Lim, Yi-Ting Hsieh, Robison V. P. Chan, Devrim Toslak, and Xincheng Yao. 2020.

</span>
<span class="ltx_bibblock">Transfer Learning for Automated OCTA Detection of Diabetic Retinopathy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Translational Vision Science &amp; Technology</em> 9, 2 (07 2020), 35–35.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1167/tvst.9.2.35" title="">https://doi.org/10.1167/tvst.9.2.35</a>
arXiv:https://arvojournals.org/arvo/content_public/journal/tvst/938366/i2164-2591-656-1-1965.pdf

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leandro et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Inferrera Leandro, Borsatti Lorenzo, Miladinovic Aleksandar, Giglio Rosa, Accardo Agostino, and Tognetto Daniele. 2023.

</span>
<span class="ltx_bibblock">OCT-based deep-learning models for the identification of retinal key signs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Scientific Reports</em> 13, 1 (2023), 14628.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
CS Lee, DM Baughman, and AY Lee. [n. d.].

</span>
<span class="ltx_bibblock">Deep learning is effective for the classification of OCT images of normal versus age-related macular degeneration. Ophthalmol Retina. 2017; 1 (4): 322–7.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Cecilia S. Lee, Doug M. Baughman, and Aaron Y. Lee. 2017.

</span>
<span class="ltx_bibblock">Deep Learning Is Effective for Classifying Normal versus Age-Related Macular Degeneration OCT Images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Ophthalmology Retina</em> 1, 4 (2017), 322–327.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.oret.2016.12.009" title="">https://doi.org/10.1016/j.oret.2016.12.009</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mingchao Li, Kun Huang, Qiuzhuo Xu, Jiadong Yang, Yuhan Zhang, Zexuan Ji, Keren Xie, Songtao Yuan, Qinghuai Liu, and Qiang Chen. 2020.

</span>
<span class="ltx_bibblock">OCTA-500: A Retinal Dataset for Optical Coherence Tomography Angiography Study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv e-prints</em> (2020), arXiv–2012.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al<span class="ltx_text" id="bib.bib29.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Swin transformer v2: Scaling up capacity and resolution. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 12009–12019.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021.

</span>
<span class="ltx_bibblock">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2103.14030 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1711.05101 [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei Lu, Yan Tong, Yue Yu, Yiqiao Xing, Changzheng Chen, and Yin Shen. 2018.

</span>
<span class="ltx_bibblock">Deep learning-based automated classification of multi-categorical abnormalities from optical coherence tomography images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Translational vision science &amp; technology</em> 7, 6 (2018), 41–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Okolo et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gabriel Iluebe Okolo, Stamos Katsigiannis, and Naeem Ramzan. 2022.

</span>
<span class="ltx_bibblock">IEViT: An enhanced vision transformer architecture for chest X-ray image classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Computer Methods and Programs in Biomedicine</em> 226 (2022), 107141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu and Sun (2019)</span>
<span class="ltx_bibblock">
Jiaming Qiu and Yankui Sun. 2019.

</span>
<span class="ltx_bibblock">Self-supervised iterative refinement learning for macular OCT volumetric data classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Computers in biology and medicine</em> 111 (2019), 103327.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidt-Erfurth et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ursula Schmidt-Erfurth, Sebastian M Waldstein, Sophie Klimscha, Amir Sadeghipour, Xiaofeng Hu, Bianca S Gerendas, Aaron Osborne, and Hrvoje Bogunović. 2018.

</span>
<span class="ltx_bibblock">Prediction of individual disease conversion in early AMD using artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Investigative ophthalmology &amp; visual science</em> 59, 8 (2018), 3199–3208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scott and Bressler (2013)</span>
<span class="ltx_bibblock">
Adrienne W Scott and Susan B Bressler. 2013.

</span>
<span class="ltx_bibblock">Long-term follow-up of vascular endothelial growth factor inhibitor therapy for neovascular age-related macular degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Current opinion in ophthalmology</em> 24, 3 (2013), 190–196.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazia et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Anis Shazia, Tan Zi Xuan, Joon Huang Chuah, Juliana Usman, Pengjiang Qian, and Khin Wee Lai. 2021.

</span>
<span class="ltx_bibblock">A comparative study of multiple neural network for detection of COVID-19 on chest X-ray.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">EURASIP journal on advances in signal processing</em> 2021 (2021), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:1409.1556</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivas et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chetana Srinivas, Nandini Prasad KS, Mohammed Zakariah, Yousef Ajmi Alothaibi, Kamran Shaukat, B Partibane, Halifa Awal, et al<span class="ltx_text" id="bib.bib39.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Deep transfer learning approaches in performance analysis of brain tumor classification using MRI images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.4.1">Journal of Healthcare Engineering</em> 2022 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivasan et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Pratul P Srinivasan, Leo A Kim, Priyatham S Mettu, Scott W Cousins, Grant M Comer, Joseph A Izatt, and Sina Farsiu. 2014.

</span>
<span class="ltx_bibblock">Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Biomedical optics express</em> 5, 10 (2014), 3568–3577.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.

</span>
<span class="ltx_bibblock">Going deeper with convolutions. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yucheng Tang, Dong Yang, Wenqi Li, Holger Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. 2022.

</span>
<span class="ltx_bibblock">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2111.14791 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021.

</span>
<span class="ltx_bibblock">Training data-efficient image transformers &amp; distillation through attention.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2012.12877 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsuji et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Takumasa Tsuji, Yuta Hirose, Kohei Fujimori, Takuya Hirose, Asuka Oyama, Yusuke Saikawa, Tatsuya Mimura, Kenshiro Shiraishi, Takenori Kobayashi, Atsushi Mizota, et al<span class="ltx_text" id="bib.bib44.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Classification of optical coherence tomography images using a capsule network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.4.1">BMC ophthalmology</em> 20, 1 (2020), 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. 2023.

</span>
<span class="ltx_bibblock">VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.11175 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Wenqiu Wang, Katarzyna Gawlik, Joe Lopez, Cindy Wen, Jie Zhu, Frances Wu, William Shi, Samuel Scheibler, Huimin Cai, Ram Vairavan, et al<span class="ltx_text" id="bib.bib47.3.1">.</span> 2016.

</span>
<span class="ltx_bibblock">Genetic and environmental factors strongly influence risk, severity and progression of age-related macular degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.4.1">Signal transduction and targeted therapy</em> 1, 1 (2016), 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Wei Wang, Ran Jiang, Ning Cui, Qian Li, Feng Yuan, and Zhifeng Xiao. 2022.

</span>
<span class="ltx_bibblock">Semi-supervised vision transformer with adaptive token sampling for breast cancer classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Frontiers in Pharmacology</em> 13 (2022), 929755.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">World Health Organization (2023)</span>
<span class="ltx_bibblock">
World Health Organization. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Blindness and visual impairment</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment" title="">https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianfang Wu, Ruo Hu, Zhenghong Xiao, Jiaxu Chen, and Jingwei Liu. 2021.

</span>
<span class="ltx_bibblock">Vision Transformer-based recognition of diabetic retinopathy grade.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Medical Physics</em> 48, 12 (2021), 7850–7863.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 2022.

</span>
<span class="ltx_bibblock">Simmim: A simple framework for masked image modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9653–9663.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dandi Yang, Cristhian Martinez, Lara Visuña, Hardev Khandhar, Chintan Bhatt, and Jesus Carretero. 2021.

</span>
<span class="ltx_bibblock">Detection and analysis of COVID-19 in medical images using deep learning techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Scientific Reports</em> 11, 1 (2021), 19638.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Kayoung Yi, Mircea Mujat, Boris H Park, Wei Sun, Joan W Miller, Johanna M Seddon, Lucy H Young, Johannes F de Boer, and Teresa C Chen. 2009.

</span>
<span class="ltx_bibblock">Spectral domain optical coherence tomography for quantitative evaluation of drusen and associated structural changes in non-neovascular age-related macular degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">British Journal of Ophthalmology</em> 93, 2 (2009), 176–181.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 17:22:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
