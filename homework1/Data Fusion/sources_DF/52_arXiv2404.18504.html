<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)</title>
<!--Generated on Tue Apr 30 17:01:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.18504v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S1" title="In Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2" title="In Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Multisensor System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2.SS1" title="In 2 Multisensor System ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Camera System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2.SS2" title="In 2 Multisensor System ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Wingbeat Sensor</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S3" title="In Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data fusion, ML-Methods and results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S3.SS1" title="In 3 Data fusion, ML-Methods and results ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data set</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S3.SS2" title="In 3 Data fusion, ML-Methods and results ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>ML-Methods</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\authorinfo</span>
<p class="ltx_p" id="p1.2">Send correspondence to Frank Haußer or Ingeborg Beckers 
<br class="ltx_break"/>E-mail: hausser@bht-berlin.de, beckers@bht-berlin.de</p>
</div>
<h1 class="ltx_title ltx_title_document">Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Tschaikner
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danja Brandt
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henning Schmidt
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Bießmann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Teodor Chiaburu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ilona Schrimpf
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">UBZ Listhof, Friedrich-List-Hof 1, 72770 Reutlingen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Schrimpf
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">UBZ Listhof, Friedrich-List-Hof 1, 72770 Reutlingen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexandra Stadel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">UBZ Listhof, Friedrich-List-Hof 1, 72770 Reutlingen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frank Haußer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ingeborg Beckers
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Berliner Hochschule für Technik, Luxemburger Str.10, 13353 Berlin, Germany
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Insect populations are declining globally, making systematic monitoring essential for conservation. Most classical methods involve death traps and counter insect conservation. This paper presents a multisensor approach that uses AI-based data fusion for insect classification. The system is designed as low-cost setup and consists of a camera module and an optical wing beat sensor as well as environmental sensors to measure temperature, irradiance or daytime as prior information. The system has been tested in the laboratory and in the field. First tests on a small very unbalanced data set with 7 species show promising results for species classification. The multisensor system will support biodiversity and agriculture studies.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>data fusion, insect monitoring, optical wingbeat sensor, machine learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Insects play a crucial role in various ecological and economic interactions with their environment, e.g. as a food source for various animals and due to their pollination services. They also play an important part in composting for soil fertility and in cleaning water. However, the number of insect species and individuals is in sharp decline worldwide. In Germany, for example, the Krefeld study recorded a 75% decline in insect biomass between 1989 and 2015 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib1" title="">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib2" title="">2</a>]</cite>. The reasons for this decline are manifold. Systematic monitoring of population, occurrence and distribution is therefore of great importance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Conventional insect monitoring systems usually use death traps to measure the absolute biomass of dried insects. Only a few species are classified at the species level. In this paper, a live monitoring system is presented that can complement and extend conventional long-term monitoring. Especially by including citizen scientists, a data set can be obtained that is important for systematic correlation studies. That way, blind spots on the monitoring map can be prevented.
We developed a standardized multisensor system that is easy to use, combined with a Web application for data collection and community networking. To bring monitoring to the public, an automated system based on AI algorithms is used, multiplying the valuable and necessary expert knowledge that is essential for serious statistics in scientific studies. The system counts and classifies insects according to the GBIF taxonomic system (order, family, genus, species). The main focus in the development of the system is the reproducibility of the data. Second, a cost-efficient selection of the components was important. Since some species are rare and the data base is strongly unbalanced, good data are essential for a robust automatic classification.
Current research in this area focuses on three partially overlapping use cases: Unstandardized mobile and open-source applications for citizen science <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib3" title="">3</a>]</cite>, monitoring for pest control and identification of beneficial insects in agriculture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib5" title="">5</a>]</cite>, and biodiversity monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper presents a system that catches up with traditional monitoring by
providing a live monitoring system, which can be connected to various established insect traps but may also allow for modified trap systems.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Automated monitoring uses a variety of technologies to detect, count, and distinguish insect species. Previous monitoring approaches have relied mainly on capturing image data. Others used wingbeat frequencies measured by acoustic sensors, multispectral analysis of reflected light, capacitance changes, and optoacoustic sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib10" title="">10</a>]</cite>. For imaging, insects must be at rest for the camera to take well-focused snapshots. Those studies that perform training of machine learning algorithms based on data from live insects, rather than image data from museums and other collections of dead insects, use yellow fields or light traps to image insects at rest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The multisensory design of the monitoring system is presented below. It consists of a camera system, a wing beat detector, and environmental sensors to measure temperature, humidity, and spectral irradiance. We use supervised learning for the training of the neural network on the basis of experts labeling a ground truth. The classification is done for hierarchical according to GBIF. In a prove of concept, it is demonstrated that combining different sensory signals via timestamps the prediction accuracy is improved compared to classifying on the base of the response of individual sensors only.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Multisensor System</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The multisensor system of the present work consists of a camera system with a resolving power of 10 micrometers, an optoacoustic wing beat sensor, environmental sensors and a real-time clock to correlate the data. The center of the sensor system is a Raspberry Pi4 minicomputer which is used for controlling and data processing. The insects first enter a camera system from a user-definable trap, from where they leave the system alive after passing the wingbeat sensor (fig.<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2.F1" title="Figure 1 ‣ 2 Multisensor System ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S2.F1.1.g1" src="extracted/2404.18504v1/figures/monitoring_principle.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>  Principle of Multisensorsystem including the path of an insects exemplarily</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The whole setup is open source and the components are chosen to be cost efficient according to the performance requirements. The instructions can be found on Gitlab (<a class="ltx_ref ltx_url" href="https://gitlab.com/kinsecta/sensorik_dev/sensorcluster" title="">https://gitlab.com/kinsecta/sensorik˙dev/sensorcluster</a>). The data can be exchanged via a WebApp. There is also a forum for networking among participating monitoring professional scientists and Citizen Scientists. To increase the accuracy of the classification, the following environmental data are collected to make probability predictions about the occurrence of specific species. These include measurements of humidity, pressure and temperature using and Adafruit BME280 I2C sensor and the photometric brightness with an Adafruit BH1750.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The spectral distribution of light intensity, measured with a Adafruit AS7341 10-Channel Light sensor, can be used to indirectly infer the cloudage. Hyperspectral data are detected for the wavelength channels 415nm, 445nm, 480nm, 515nm, 550nm, 590nm, 660nm, 690nm with bandwidth of <math alttext="\pm" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mo id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><csymbol cd="latexml" id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">±</annotation></semantics></math> 10nm respectively and an infrared sensor sensitive in the range of (850-1050)nm. An additional clear sensors measures the complete spectrum. Due to scattering by water droplets, the spectral distribution changes at different degrees of cloud coverage. A real time clock (Adafruit PCF8523) ensures the co-registration of data. We preferred commercially available sensors that communicate via I2C, that have a STEMMA QT interface and hence can be connected in series.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Camera System</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Classification of insects based on images considers features such as color, contrast, patterning, size, and shape relationships. Entomological features such as pubescence and wing vascular pattern can be decisive for certain species. The developed system is standardized for homogeneous illumination, motion blur suppression, resolution, and depth of field, which are critical for accurate classification. Homogeneous diffuse illumination and resolution adjusted to the requirement of insect differentiation are crucial for classification. For example, background shadows make segmentation difficult. For this reason, diffusing PMMA glass provides uniform lightning from both lateral directions. With an effective aperture of f/8, a resolution of 10 micrometers is achieved, which is e.g. sufficient for differentiating vein segments in the wings of insects and also to identify single hairs. At the same time, a sufficient basis of depth of field is achieved in the so-called insect arena. The arena area measures 60x45x20 <math alttext="\mathrm{mm}^{3}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><msup id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">mm</mi><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">mm</ci><cn id="S2.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\mathrm{mm}^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">roman_mm start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A custom circuit board controls the lighting, which starts as soon as the insects pass a light barrier in the center of the arena. Two light barriers are integrated at the top and the bottom of the arena respectively, using two photoelectric lock-in amplifiers (IS471FE Sharp Microlectronics) to trigger the homogeneous LED illumination. A 500-microsecond flash with an exposure time of 23.5 milliseconds simulates a global shutter. Thus, the motion blur from the CMOS rolling shutter is minimized. The camera system is presented in detail in a previous paper from Brandt et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib12" title="">12</a>]</cite>.
Fluttering Wings and moving limbs show some motion blur, but the fuselage remains unblurred and sharp. The self-designed board is connected to a Raspberry Pi 4, which drives an HQ camera with a 10-MP teleobjective and stores an H264 video stream in a ring buffer. Simultaneously, with the triggering via the photoelectric sensor, three png-images are automatically extracted from the video stream selected via brightness. They are used for classification. The video sequence is also stored. A Python-based Bokeh-Application is running on a local server for controlling the sensors, data processing and storing information on SD card.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S2.F2.1.g1" src="extracted/2404.18504v1/figures/Eristalis_tenax.png" width="240"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>  Image taken by the camera system: An insect, species Eristalis tenax, is passing the arena.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Wingbeat Sensor</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">An optoacoustic sensor records the variation of infrared light intensity to record the wingbeat frequency of insects.
As soon as an insect passes the active zone of the sensor, the light intensity decreases due to shading by the insect’s body. The signal is modulated by the wingbeat frequency (fig.<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2.F3" title="Figure 3 ‣ 2.2 Wingbeat Sensor ‣ 2 Multisensor System ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S2.F3.1.g1" src="extracted/2404.18504v1/figures/wingbeat_sensor.png" width="389"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>  Wingbeat sensor including Emitter, Receiver and Fresnel Lenses</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.3">In contrast to acoustic measurements, here the wingbeat signals are detected free of background noise. The method based on an idea by Potamitis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib9" title="">9</a>]</cite>, is adapted for multisensor monitoring and allows for co-relation with camera images and other measurements via timestamp. The challenge in insect monitoring is the classification of diverse insects with a large variance in different wingbeat frequencies and flight velocities. The range of wingbeats can vary greatly depending on species, size, the physical condition of the insect, and temperature. But also the flight behavior like hovering, fast flight or navigation influences the frequency. The design of our wingbeat sensor focuses on the typical frequencies between 200-600 Hz, so that at reasonable flight speeds between 2 and 30 km/h the minimum active path through the sensor must be sufficient to measure 2-50 wing beats, which requires an active length of 6 cm. For this reason, we use Fresnel lenses that provide homogeneous illumination over the length along the direction of flight path. The sensor area is (100 x 100) mm<sup class="ltx_sup" id="S2.SS2.p2.3.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.3.1.1">2</span></sup>. Light of two IR LEDs (YSMY12940, light intensity 16mW/sr, wavelength 940nm and an aperture angle of <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><mo id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><csymbol cd="latexml" id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.1d">±</annotation></semantics></math> 40° FWHM) is collimated by cylindrical Fresnel lenses (f=50mm) and focused behind the active area onto 12 highly sensitive PIN photo diodes (QSB34GR, aperture angle <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.1"><semantics id="S2.SS2.p2.3.m3.1a"><mo id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.1d">±</annotation></semantics></math> 60° FWHM, rise and fall times are 50ns), ensuring a sampling rate of up to 2.5MHz. The wing beat frequencies can thus be sampled in detail.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The photo diodes are connected in parallel. The signals are pre-amplified using a transimpedance amplifier before being AD converted through a sound card (DELOCK 63926 24bit/ 96kHz), where the signals pass a high pass filter with a cutoff frequency of 8Hz and the DC signal, caused by the insect body, is subtracted. For pre-amplifying we used the OPV TS971 that is suitable for portable devices with a very low noise level of 4ns/<math alttext="\sqrt{\textrm{Hz}}" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><msqrt id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mtext id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2a.cmml">Hz</mtext></msqrt><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><root id="S2.SS2.p3.1.m1.1.1a.cmml" xref="S2.SS2.p3.1.m1.1.1"></root><ci id="S2.SS2.p3.1.m1.1.1.2a.cmml" xref="S2.SS2.p3.1.m1.1.1.2"><mtext id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">Hz</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\sqrt{\textrm{Hz}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">square-root start_ARG Hz end_ARG</annotation></semantics></math> and a high dynamic bandwidth of 12MHz.
The frequency spectra are Fourier transformed via Welch’s method to estimate the spectral density in a Power Spectrum Density (PSD), see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S2.F4" title="Figure 4 ‣ 2.2 Wingbeat Sensor ‣ 2 Multisensor System ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_tag">4</span></a>. The main frequency so as the higher harmonics indicating significant differences between species can be analysed from the PSD spectra. They are used for the machine learning algorithm.
Additionally, Spectrograms allow following changes of the wing beat frequencies over time.</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="195" id="S2.F4.1.g1" src="extracted/2404.18504v1/figures/WingbeatEristalisTenax.png" width="293"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="196" id="S2.F4.2.g2" src="extracted/2404.18504v1/figures/WingbeatDrosophilaDrosophila.png" width="293"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>  Examples of wingbeat signals of two different insect species: (left) Eristalis tenax, (right) Drosophila drosophila. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data fusion, ML-Methods and results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As a first proof of concept, we use a small data set of multisensor data and a simple neural network architecture in order to investigate the potential of data fusion of wingbeat and camera data for improving the predictive power.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data set</h3>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">order</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">family</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">genus</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">species</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.2.2.1" rowspan="4"><span class="ltx_text" id="S3.T1.1.2.2.1.1">Hymenoptera</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.2.2.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.2.2.2.1">Apidae</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.2.2.3">Apis</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.2.2.4">mellifera</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.3.3.1"><span class="ltx_text" id="S3.T1.1.3.3.1.1">Bombus</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.3.3.2">terrestris</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.4.1" rowspan="2"><span class="ltx_text" id="S3.T1.1.4.4.1.1">Vespidae</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.4.2">Vespa</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.4.3">crabro</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.5.1">Polistes</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.5.2">dominula</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.6.6.1">Mecoptera</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.6.6.2">Panorpidae</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.6.6.3">Panorpa</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.6.6.4">communis</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.7.7.1" rowspan="2"><span class="ltx_text" id="S3.T1.1.7.7.1.1">Diptera</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.7.7.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.7.7.2.1">Syrphidae</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.7.7.3">Eristalis</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.7.7.4">tenax</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.1.8.8.1">Episyrphus</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.1.8.8.2">balteatus</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A small data set of 7 insect species spread over the taxonomic tree is used, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S3.F5" title="Figure 5 ‣ 3.1 Data set ‣ 3 Data fusion, ML-Methods and results ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_tag">5</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="239" id="S3.F5.1.g1" src="extracted/2404.18504v1/figures/histogramm.png" width="299"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.2.g2" src="extracted/2404.18504v1/figures/Apis_mellifera_crop.png" width="78"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.3.g3" src="extracted/2404.18504v1/figures/Bombus_terrestris_crop.png" width="78"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.4.g4" src="extracted/2404.18504v1/figures/Panorpa_communis_crop.png" width="78"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.5.g5" src="extracted/2404.18504v1/figures/Vespa_cabro_crop.png" width="78"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.6.g6" src="extracted/2404.18504v1/figures/Polistes_dominula_crop.png" width="78"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.7.g7" src="extracted/2404.18504v1/figures/Eristalis_tenax_crop.png" width="78"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="78" id="S3.F5.8.g8" src="extracted/2404.18504v1/figures/Episyrphus_balteatus_crop.png" width="78"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>  A highly unbalanced data set of 7 insect species has been obtained from measurements with the multi sensor system. The presented images have been cropped for better visualisation. Note, that some insects are located at the border of the arena. Also only fast flattering wings show some blurring, but still the image quality is very good.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ML-Methods</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since not all multisensor systems in the field will measure camera and wingbeat signals, we implemented a prediction system, that may deal with three different input data: only camera image, only wingbeat signal or combined camera and wingbeat data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Thus, a straight forward approach proceeds as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">Implement and train a predictor (NN) for insect classification based on camera images. Use all available camera data for training.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">Implement and train a predictor (NN) for insect classification based on wing beat signals. Use all available wingbeat signals for training.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">Use the trained NNs in (a) and (b) as feature extractors and implement and train a predictor on pairs of camera/wingbeat data.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.p2.2">A schematic overview of this architecture is given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#S3.F6" title="Figure 6 ‣ 3.2 ML-Methods ‣ 3 Data fusion, ML-Methods and results ‣ Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.F6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.F6.1.1.1">
<td class="ltx_td ltx_align_center" id="S3.F6.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="233" id="S3.F6.1.1.1.1.g1" src="extracted/2404.18504v1/figures/NNArchitectur.png" width="479"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> 
Schematic overview of our neural network architecture for fusion of image and wingbeat data.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">At the point of writing this paper, we achieved an overall prediction accuracy of 0.92 on the test set for case (a), using camera images only. Here a mobile net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib14" title="">14</a>]</cite> was trained. For wingbeat signals, case (b), the current data set proved to be to small for training the CNNs as implemented and tested successfully in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.18504v1#bib.bib15" title="">15</a>]</cite> for larger data sets. However, a linear support vector machine achieved already an accuracy of 0.68.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">We are confident to report more results in the near future as soon as more data are available.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>Funded by Federal Ministry for the Environment, Nature Conservation, Nuclear Safety and Consumer protection, Project <span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">KInsecta: KI-based Insect Monitoring with Citizen Science</span>, Funding Nr. 67KI2079.

</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Hallmann, C. A., Sorg, M., Jongejans, E., Siepel, H., Hofland, N., Schwan, H., Stenmans, W., Müller, A., Sumser, H., Hörren, T., Goulson, D., and Kroon, H. D., “More than 75 percent decline over 27 years in total flying insect biomass in protected areas,” <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">PLoS ONE</span> <span class="ltx_text ltx_font_bold" id="bib.bib1.2.2">12</span> (10 2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Sánchez-Bayo, F. and Wyckhuys, K. A., “Worldwide decline of the entomofauna: A review of its drivers,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Biological conservation</span> <span class="ltx_text ltx_font_bold" id="bib.bib2.2.2">232</span>, 8–27 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S., “The inaturalist species classification and detection dataset,” in [<span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span> ], 8769–8778 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Lima, M. C. F., de Almeida Leandro, M. E. D., Valero, C., Coronel, L. C. P., and Bazzo, C. O. G., “Automatic detection and monitoring of insect pests—a review,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Agriculture</span> <span class="ltx_text ltx_font_bold" id="bib.bib4.2.2">10</span>(5), 161 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Wu, X., Zhan, C., Lai, Y.-K., Cheng, M.-M., and Yang, J., “Ip102: A large-scale benchmark dataset for insect pest recognition,” in [<span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span> ], 8787–8796 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
van Klink, R., August, T., Bas, Y., Bodesheim, P., Bonn, A., Fossøy, F., Høye, T. T., Jongejans, E., Menz, M. H., Miraldo, A., Roslin, T., Roy, H. E., Ruczyński, I., Schigel, D., Schäffler, L., Sheard, J. K., Svenningsen, C., Tschan, G. F., Wäldchen, J., Zizka, V. M., Åström, J., and Bowler, D. E., “Emerging technologies revolutionise insect ecology and monitoring,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Trends in Ecology and Evolution</span> <span class="ltx_text ltx_font_bold" id="bib.bib6.2.2">37</span>, 872–885 (10 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Mankin, R., Hagstrum, D., Guo, M., Eliopoulos, P., and Njoroge, A., “Automated applications of acoustics for stored product insect detection, monitoring, and management,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Insects</span> <span class="ltx_text ltx_font_bold" id="bib.bib7.2.2">12</span> (3 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Rigakis, I., Potamitis, I., Tatlas, N. A., Livadaras, I., and Ntalampiras, S., “A multispectral backscattered light recorder of insects’ wingbeats,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Electronics (Switzerland)</span> <span class="ltx_text ltx_font_bold" id="bib.bib8.2.2">8</span> (3 2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Potamitis, I., Rigakis, I., Vidakis, N., Petousis, M., and Weber, M., “Affordable bimodal optical sensors to spread the use of automated insect monitoring,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Journal of Sensors</span> <span class="ltx_text ltx_font_bold" id="bib.bib9.2.2">2018</span> (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Wang, J., Zhu, S., Lin, Y., Svanberg, S., and Zhao, G., “Mosquito counting system based on optical sensing,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Applied Physics B: Lasers and Optics</span> <span class="ltx_text ltx_font_bold" id="bib.bib10.2.2">126</span> (2 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Sittinger, “Insect detect.” <a class="ltx_ref ltx_url" href="https://maxsitt.github.io/insect-detect-docs" title="">https://maxsitt.github.io/insect-detect-docs</a> (2023).

</span>
<span class="ltx_bibblock">Online; accessed 30 January 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Brandt, D., Tschaikner, M., Chiaburu, T., Schmidt, H., Schrimpf, I., Stadel, A., Beckers, I. E., and Haußer, F., “Low cost machine vision for insect classification,” in [<span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Intelligent Systems and Applications</span> ], Arai, K., ed., 18–34, Springer Nature Switzerland, Cham (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Potamitis, I. and Rigakis, I., “Large aperture optoelectronic devices to record and time-stamp insects’ wingbeats,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Sensors Journal</span> <span class="ltx_text ltx_font_bold" id="bib.bib13.2.2">16</span>(15), 6053–6061 (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H., “Mobilenets: Efficient convolutional neural networks for mobile vision applications,” (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Chiaburu, T. (2021).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 17:01:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
