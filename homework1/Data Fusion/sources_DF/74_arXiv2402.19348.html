<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook</title>
<!--Generated on Sun Jun 16 10:13:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.19348v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S2" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Taxonomy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data Perspective</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS1" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS2" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Geographical Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS3" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Traffic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS4" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Social Media Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS5" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Demographic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.SS6" title="In 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Environment Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology Perspective</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS1" title="In 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2" title="In 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Feature-based Data Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS1" title="In 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Feature Addition and Multiplication</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS2" title="In 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Feature Concatenation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS3" title="In 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Graph-based Data Fusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3" title="In 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Alignment-based Data Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS1" title="In 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Attention-based Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS2" title="In 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Encoder-based Alignment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4" title="In 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Contrast-based Data Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS1" title="In 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Instance Contrast-based Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS2" title="In 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Batch Contrast-based Fusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5" title="In 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Generation-based Data Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS1" title="In 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Autoregressive Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS2" title="In 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Mask Modeling-based Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS3" title="In 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Diffusion-based Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS4" title="In 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.4 </span>LLM-enhanced Data Fusion</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Application Perspective</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS1" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Urban Planning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS1.SSS1" title="In 5.1 Urban Planning ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>City-level Planning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS1.SSS2" title="In 5.1 Urban Planning ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Region-level Planning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS2" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Transportation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS2.SSS1" title="In 5.2 Transportation ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Public Transportation Service</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS2.SSS2" title="In 5.2 Transportation ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Transportation Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS3" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Economy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS4" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Public Safety and Security</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS5" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Social</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS6" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Environment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS7" title="In 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Energy</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS7.SSS1" title="In 5.7 Energy ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.1 </span>Energy Efficient Urban Planning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.SS7.SSS2" title="In 5.7 Energy ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.2 </span>Energy Efficient Urban Transportation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S6" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Challenges and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S7" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#A1" title="In Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Github Link</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Deep Learning for Cross-Domain Data Fusion in Urban Computing: 
<br class="ltx_break"/>Taxonomy, Advances, and Outlook</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingchen Zou<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>These authors contributed equally to this work.</span></span></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yibo Yan<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>These authors contributed equally to this work.</span></span></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xixuan Hao
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuehong Hu
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haomin Wen
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Erdong Liu
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Junbo Zhang
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Li
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianrui Li
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yu Zheng
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuxuan Liang
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">As cities continue to burgeon, <span class="ltx_text ltx_font_italic" id="id1.id1.1">Urban Computing</span> emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: <span class="ltx_text ltx_font_italic" id="id1.id1.2">feature-based</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.3">alignment-based</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.4">contrast-based</span>, and <span class="ltx_text ltx_font_italic" id="id1.id1.5">generation-based</span> fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: <span class="ltx_text ltx_font_italic" id="id1.id1.6">urban planning</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.7">transportation</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.8">economy</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.9">public safety</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.10">society</span>, <span class="ltx_text ltx_font_italic" id="id1.id1.11">environment</span>, and <span class="ltx_text ltx_font_italic" id="id1.id1.12">energy</span>. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between <span class="ltx_text ltx_font_italic" id="id1.id1.13">Large Language Models (LLMs)</span> and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoshall/Awesome-Multimodal-Urban-Computing" title="">https://github.com/yoshall/Awesome-Multimodal-Urban-Computing</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Urban Computing , Data fusion , Deep learning, Multi-modal data , Large language models, Sustainable development

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span></span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\affiliation</span>
<p class="ltx_p" id="p1.2">[ustgz]organization=The Hong Kong University of Science and Technology (Guangzhou),
city=Guangzhou,
country=China
<span class="ltx_ERROR undefined" id="p1.2.1">\affiliation</span>[jdt]organization=JD Technology &amp; JD Intelligent Cities Research,
city=Beijing,
country=China
<span class="ltx_ERROR undefined" id="p1.2.2">\affiliation</span>[thu]organization=Tsinghua University,
city=Beijing,
country=China
<span class="ltx_ERROR undefined" id="p1.2.3">\affiliation</span>[swjtu]organization=Southwest Jiaotong University,
city=Chengdu,
country=China

</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Cities, indispensable components of modern civilization, have undergone transformative trajectories propelled by human advancements in cultural, financial, political, and technological domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib321" title="">321</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib300" title="">300</a>]</cite>. Despite their pivotal role in societal progress, the unprecedented surge in global urbanization since the 19th century has precipitated formidable sustainability challenges including energy consumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib281" title="">281</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib303" title="">303</a>]</cite>, environmental pollution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib161" title="">161</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib135" title="">135</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib274" title="">274</a>]</cite>, socio-economic disparities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib330" title="">330</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib150" title="">150</a>]</cite>, and urban traffic issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib140" title="">140</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib150" title="">150</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib212" title="">212</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib167" title="">167</a>]</cite>. In the 21st century, the profound strides achieved in machine learning and spatio-tempral data mining have manifested in myriad successful applications across diverse domains, such as finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib215" title="">215</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib19" title="">19</a>]</cite>, biology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib194" title="">194</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib257" title="">257</a>]</cite>, and healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib251" title="">251</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib238" title="">238</a>]</cite>. This surge in technological prowess has incited a notable shift in research focus, with scholars now directing their attention toward harnessing these advancements to optimize the intricate facets of urban planning, operations, management, etc. A pivotal contribution to this evolving discourse is the pioneering work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>]</cite>, which encapsulated and elucidated these endeavors by introducing the concept of <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">Urban Computing</span>. This paradigm leverages sensing technologies and expansive computing infrastructure to scrutinize voluminous data emanating from urban spaces. The fundamental objective is to gain profound insights into the dynamics of cities, thereby addressing challenges such as traffic congestion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib119" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib375" title="">375</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib218" title="">218</a>]</cite>, energy consumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib128" title="">128</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib223" title="">223</a>]</cite>, and air quality pollution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib335" title="">335</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib334" title="">334</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Urban computing necessitates the integration of extensive and diverse datasets sourced from various sources and modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib67" title="">67</a>]</cite>, also referred to as <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">Cross-Domain Data Fusion</span>, which arises from the recognition that relying solely on a singular data source or modality may prove inadequate for the holistic implementation of urban tasks. <span class="ltx_text" id="S1.p2.1.2" style="color:#000000;">For example, in the realm of traffic prediction, it becomes imperative to assimilate meteorological forecast data with geographical information. This involves taking into account the congestion induced by rainfall and the influence of school and business hours on traffic flow during peak periods. In the context of urban planning, one must combine population density and economic activity data. This includes evaluating factors such as population density and income levels when devising plans for new commercial districts to ascertain their viability. Furthermore, in the field of public safety management, integrating crime data with socio-economic data is crucial. This entails considering unemployment rates and education levels in high-crime areas when strategizing police deployment plans.</span> In recent years, a growing number of studies in urban computing are expanding cross-domain data fusion to encompass diverse sources like sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib121" title="">121</a>]</cite>, satellites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>]</cite>, social media <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib380" title="">380</a>]</cite>, and citizen-generated data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>]</cite>. Additionally, there is a trend towards introducing new data modalities, including text (e.g., social media posts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>]</cite> and geographic information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib43" title="">43</a>]</cite>) and images (e.g., satellite <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite> and street-view images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>]</cite>). Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the cross-domain data fusion in urban computing from the views of data modality and source.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S1.F1.g1" src="extracted/5670403/Images_zxc/introduction_image.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A sketch of cross-domain urban computing. <span class="ltx_text ltx_font_bold" id="S1.F1.3.1">Left</span>: It involves the integration of urban data from diverse modalities, including spatio-temporal, visual, textual, and other modalities, through the process of data fusion. <span class="ltx_text ltx_font_bold" id="S1.F1.4.2">Right</span>: Generally, these urban data derive from multiple sources, such as geographical data, transportation, social media, demography, and environment.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Prior research, exemplified by <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>]</cite>, emphasized the critical role of cross-domain data fusion in amalgamating information from multiple sources. With the emergence of data fusion studies in urban computing, <cite class="ltx_cite ltx_citemacro_citet">Zheng [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>]</cite> classified the related fusion methodologies into three types: stage-based, feature level-based, and semantic-based data fusion. Furthermore, within the purview of semantic-based methods, a more intricate taxonomy emerges, delineating four subtypes: multi-view learning-based, similarity-based, probabilistic dependency-based, and transfer learning-based methods.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="132" id="S1.F2.g1" src="extracted/5670403/Images_zxc/11.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text" id="S1.F2.2.1" style="color:#000000;">Times cited and publications over time for Deep Learning in Urban Computing on prestigious venues(Source: Web of Science)</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Over the past decade, the emergence of Deep Learning (DL) has asserted dominance in processing spatio-temporal data in urban computing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib282" title="">282</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib125" title="">125</a>]</cite>. In contrast to traditional machine learning methods, these models present distinct superiority, characterized by larger model capacity, automated feature extraction capabilities, and inherent compatibility with cross-domain data fusion. <span class="ltx_text" id="S1.p4.1.1" style="color:#000000;">As Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates, there has been a significant uptick in both the number of papers published and the citations received for research related to deep learning in urban computing since 2015.</span> The paradigm shift derived from deep learning renders previous surveys, especially <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>]</cite>, on urban data fusion somewhat obsolete, as traditional taxonomy may not aptly capture the nuances and differences among these advanced methodologies. In light of this issue, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">our survey is dedicated to bridging this gap and provides a contemporary perspective by offering a comprehensive and updated taxonomy that aligns with the era of deep learning.</em> Through a thorough examination of deep learning-based cross-domain data fusion methods, we seek to establish a robust foundation for understanding and navigating the landscape of urban computing.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Specifically, we commence by presenting a novel taxonomy that classifies existing urban data sources into five distinct types, while concurrently categorizing fusion methods into four types. This systematic classification offers valuable insights into the intricate integration of diverse modalities within urban computing. Secondly, we systematically categorize widely used datasets and outline common application scenarios for urban computing fusion models. Thirdly, inspired by the breakthrough of Large Language Models (LLMs) in financial time series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib306" title="">306</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib345" title="">345</a>]</cite>, medicine <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib262" title="">262</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib244" title="">244</a>]</cite>, law <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib51" title="">51</a>]</cite>, and climate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib236" title="">236</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib146" title="">146</a>]</cite>, we further summarize inspiring research that integrates LLMs into urban computing, which complements the basic taxonomy of cross-domain data fusion in urban computing.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Related Surveys</span>. Several surveys have recently explored the application of deep learning-based data fusion across various domains. Notably, <cite class="ltx_cite ltx_citemacro_citet">Zheng [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>]</cite> conducted a comprehensive investigation into the methodologies proposed for cross-domain big data fusion prior to 2015. Their research reveals that machine learning-based data fusion was the de-facto dominant approach in urban computing during that period, albeit with challenges in comprehending cross-modal relations. Subsequently, with the remarkable success of deep learning models such as recurrent neural networks (RNN) and convolutional neural networks (CNN) in representation learning, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib282" title="">282</a>]</cite> conducted an exhaustive review on the utilization of deep learning for spatio-temporal data mining, with a special focus on the fusion of multi-source spatio-temporal data. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite> further provided a summary of deep learning-based data fusion methodologies for urban big data fusion before 2020. Though these surveys shed light on the advancements in deep learning to their respective fields, they primarily concentrated on specific aspects and do not extensively cover cross-modal data fusion or the latest paradigms, including contrast-based and generation-based approaches which have gained significant popularity since 2021. In other words, none of them have provided a comprehensive and up-to-date taxonomy framework for data fusion methodologies in the context of urban computing.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Furthermore, existing surveys lack a specific focus on the data and application perspectives of these DL models within the field of urban computing. For instance, <cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib76" title="">76</a>]</cite> summarized the fusion models for spatio-temporal data based on generative adversarial networks (GAN), while <cite class="ltx_cite ltx_citemacro_citet">Deldari et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>]</cite> concentrated on self-supervised representation learning on the fusion of multi-modal data in the general domain. Besides, there are a couple of surveys investigating the deep learning-based data fusion in specific applications (i.e., a subdomain) of urban computing, such as crowd flow prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib315" title="">315</a>]</cite>, intelligent transportation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib348" title="">348</a>]</cite>, and social event detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib2" title="">2</a>]</cite>. These facts highlight the need for a new survey to serve as a guide for future endeavors in data fusion in urban computing concepts.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To this end, this paper aims to provide a <span class="ltx_text ltx_font_italic" id="S1.p8.1.1">comprehensive</span> and <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">up-to-date</span> review of deep learning-based data fusion methodologies, explicitly tailored for cross-domain data fusion in urban computing. Our intention is not only documenting the latest advancements but also illuminating available resources and practical applications, and identifying potential research directions. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a> succinctly summarizes the key differences between our survey and other relevant surveys in the field. Through our exploration, we seek to provide a valuable resource for researchers and stakeholders, fostering an enhanced understanding of the intricacies surrounding the integration of diverse urban data modalities via deep learning approaches.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>A thorough comparison between related surveys and ours, focusing on scopes (i.e., Specific versus Urban Computing), relevant modalities (e.g., general spatio-temporal data, image, text, and others), and primary topics of focus (i.e., data sources and modalities utilized for urban computing (Data), data fusion models and techniques (Fusion Model), application domains and downstream tasks in urban computing (Application) and LLMs for urban computing).</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S1.T1.1" style="width:433.6pt;height:86.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-279.7pt,55.8pt) scale(0.436649007749989,0.436649007749989) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1">
<tr class="ltx_tr" id="S1.T1.1.1.2">
<td class="ltx_td ltx_border_tt" id="S1.T1.1.1.2.1"></td>
<td class="ltx_td ltx_border_tt" id="S1.T1.1.1.2.2"></td>
<td class="ltx_td ltx_border_tt" id="S1.T1.1.1.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S1.T1.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.4.1">Scope</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S1.T1.1.1.2.5"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.5.1">Modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S1.T1.1.1.2.6"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.6.1">Focus</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.1">
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.2.1">Survey</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.3.1">Year</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.4.1">Venue</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.5.1">Specific</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.6.1">Urban Computing</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.7.1">Spatio-temporal</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.8.1">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.9.1">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.10.1">Others</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.11.1">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.12.1">Fusion Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.13"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.13.1">Application</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.1">
<span class="ltx_text" id="S1.T1.1.1.1.1.1" style="color:#BF0040;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="17" id="S1.T1.1.1.1.1.1.g1" src="extracted/5670403/fire.png" width="12"/></span> <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.T1.1.1.1.1.2">LLM</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.3.1"><cite class="ltx_cite ltx_citemacro_citet">Zheng [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.2">2015</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.3">IEEE Trans. Big Data</td>
<td class="ltx_td ltx_border_t" id="S1.T1.1.1.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.6"><span class="ltx_text" id="S1.T1.1.1.3.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.7"><span class="ltx_text" id="S1.T1.1.1.3.7.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.8"><span class="ltx_text" id="S1.T1.1.1.3.8.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.9"><span class="ltx_text" id="S1.T1.1.1.3.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.10"><span class="ltx_text" id="S1.T1.1.1.3.10.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.11"><span class="ltx_text" id="S1.T1.1.1.3.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.12"><span class="ltx_text" id="S1.T1.1.1.3.12.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.3.13"><span class="ltx_text" id="S1.T1.1.1.3.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.4">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.4.1"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib282" title="">282</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.2">2020</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.3">IEEE TKDE</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.4.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.6"><span class="ltx_text" id="S1.T1.1.1.4.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.7"><span class="ltx_text" id="S1.T1.1.1.4.7.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.8"><span class="ltx_text" id="S1.T1.1.1.4.8.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.9"><span class="ltx_text" id="S1.T1.1.1.4.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.10"><span class="ltx_text" id="S1.T1.1.1.4.10.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.11"><span class="ltx_text" id="S1.T1.1.1.4.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.12"><span class="ltx_text" id="S1.T1.1.1.4.12.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.13"><span class="ltx_text" id="S1.T1.1.1.4.13.1" style="color:#E7524C;"> ✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.5">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.5.1"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.2">2020</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.3">Information Fusion</td>
<td class="ltx_td" id="S1.T1.1.1.5.4"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.5">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.6"><span class="ltx_text" id="S1.T1.1.1.5.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.7"><span class="ltx_text" id="S1.T1.1.1.5.7.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.8"><span class="ltx_text" id="S1.T1.1.1.5.8.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.9"><span class="ltx_text" id="S1.T1.1.1.5.9.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.10"><span class="ltx_text" id="S1.T1.1.1.5.10.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.11"><span class="ltx_text" id="S1.T1.1.1.5.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.12"><span class="ltx_text" id="S1.T1.1.1.5.12.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.5.13"><span class="ltx_text" id="S1.T1.1.1.5.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.6">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.6.1"><cite class="ltx_cite ltx_citemacro_citet">Xie et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib315" title="">315</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.2">2020</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.3">Information Fusion</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.6.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.6"><span class="ltx_text" id="S1.T1.1.1.6.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.7"><span class="ltx_text" id="S1.T1.1.1.6.7.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.8"><span class="ltx_text" id="S1.T1.1.1.6.8.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.9"><span class="ltx_text" id="S1.T1.1.1.6.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.10"><span class="ltx_text" id="S1.T1.1.1.6.10.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.11"><span class="ltx_text" id="S1.T1.1.1.6.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.12"><span class="ltx_text" id="S1.T1.1.1.6.12.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.6.13"><span class="ltx_text" id="S1.T1.1.1.6.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.7">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.7.1"><cite class="ltx_cite ltx_citemacro_citet">Yuan and Li [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib348" title="">348</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.2">2021</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.3">Springer Data Sci. Eng.</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.7.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.6"><span class="ltx_text" id="S1.T1.1.1.7.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.7"><span class="ltx_text" id="S1.T1.1.1.7.7.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.8"><span class="ltx_text" id="S1.T1.1.1.7.8.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.9"><span class="ltx_text" id="S1.T1.1.1.7.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.10"><span class="ltx_text" id="S1.T1.1.1.7.10.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.11"><span class="ltx_text" id="S1.T1.1.1.7.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.12"><span class="ltx_text" id="S1.T1.1.1.7.12.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.7.13"><span class="ltx_text" id="S1.T1.1.1.7.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.8">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.8.1"><cite class="ltx_cite ltx_citemacro_citet">Afyouni et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib2" title="">2</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.2">2022</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.3">Information Fusion</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.8.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.6"><span class="ltx_text" id="S1.T1.1.1.8.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.7"><span class="ltx_text" id="S1.T1.1.1.8.7.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.8"><span class="ltx_text" id="S1.T1.1.1.8.8.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.9"><span class="ltx_text" id="S1.T1.1.1.8.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.10"><span class="ltx_text" id="S1.T1.1.1.8.10.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.11"><span class="ltx_text" id="S1.T1.1.1.8.11.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.12"><span class="ltx_text" id="S1.T1.1.1.8.12.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.8.13"><span class="ltx_text" id="S1.T1.1.1.8.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.9">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.9.1"><cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib76" title="">76</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.2">2022</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.3">ACM TIST</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.9.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.6"><span class="ltx_text" id="S1.T1.1.1.9.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.7"><span class="ltx_text" id="S1.T1.1.1.9.7.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.8"><span class="ltx_text" id="S1.T1.1.1.9.8.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.9"><span class="ltx_text" id="S1.T1.1.1.9.9.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.10"><span class="ltx_text" id="S1.T1.1.1.9.10.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.11"><span class="ltx_text" id="S1.T1.1.1.9.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.12"><span class="ltx_text" id="S1.T1.1.1.9.12.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.9.13"><span class="ltx_text" id="S1.T1.1.1.9.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.10">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.10.1"><cite class="ltx_cite ltx_citemacro_citet">Deldari et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.2">2022</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.3">Unpublished</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.4">✓</td>
<td class="ltx_td" id="S1.T1.1.1.10.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.6"><span class="ltx_text" id="S1.T1.1.1.10.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.7"><span class="ltx_text" id="S1.T1.1.1.10.7.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.8"><span class="ltx_text" id="S1.T1.1.1.10.8.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.9"><span class="ltx_text" id="S1.T1.1.1.10.9.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.10"><span class="ltx_text" id="S1.T1.1.1.10.10.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.11"><span class="ltx_text" id="S1.T1.1.1.10.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.12"><span class="ltx_text" id="S1.T1.1.1.10.12.1" style="color:#E7524C;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.10.13"><span class="ltx_text" id="S1.T1.1.1.10.13.1" style="color:#E7524C;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.11.1.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.2">2024</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.3">-</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.6"><span class="ltx_text" id="S1.T1.1.1.11.6.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.7"><span class="ltx_text" id="S1.T1.1.1.11.7.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.8"><span class="ltx_text" id="S1.T1.1.1.11.8.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.9"><span class="ltx_text" id="S1.T1.1.1.11.9.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.10"><span class="ltx_text" id="S1.T1.1.1.11.10.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.11"><span class="ltx_text" id="S1.T1.1.1.11.11.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.12"><span class="ltx_text" id="S1.T1.1.1.11.12.1" style="color:#2191A8;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.1.1.11.13"><span class="ltx_text" id="S1.T1.1.1.11.13.1" style="color:#2191A8;">✓</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text ltx_font_bold" id="S1.p9.1.1">Our Contributions</span>. Compared to previous surveys, the contributions of our survey can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p10">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Comprehensive and Up-to-Date Survey.</span> To the best of our knowledge, this is the first comprehensive survey that systematically reviews studies on deep learning techniques for cross-domain data fusion models in urban computing. We firmly believe that the taxonomy, progress, and prospects introduced in this paper can significantly promote the development of this field.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Novel and Structured Taxonomy.</span> We present a novel taxonomy, which organizes current research efforts from three perspectives: i) data sources, which mainly include spatio-temporal, visual, and textual modalities; ii) fusion methods, consisting of feature-based, alignment-based, contrast-based, and generation-based fusion methods; iii) applications, spanning diverse domains such as urban planning, transportation, economy, public safety, society, environment, and energy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Extensive Dataset Compilation.</span> Our survey thoroughly compiles and categorizes popular datasets in urban computing, taking into consideration their sources, temporal coverage, and spatial distribution characteristics. Additionally, we outline the prevailing common application scenarios for urban computing fusion models, examining their practical contributions and acknowledging their limitations in a series of downstream applications, respectively.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Future Research Outlook.</span> We endeavor to identify and provide detailed explanations of several promising directions for future research, covering various aspects, including data privacy protection, the establishment of open benchmarks, the diversification of applications, and the optimization of efficiency. Moreover, capitalizing on the progress of LLMs (e.g., GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib211" title="">211</a>]</cite> and Sora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib22" title="">22</a>]</cite>) and their notable benefits in fusing multi-modal and multi-source data, we further investigate their innovative applications and propose potential approaches for urban computing.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1"><span class="ltx_text ltx_font_bold" id="S1.p11.1.1">Organization</span>.
The structure of the remaining sections of this paper is as follows:
in Section 2, we present the overall taxonomy of deep-learning-based data fusion methods in urban computing, providing a broad overview of the field before delving into the specific perspectives and intricacies.
Section 3 offers a comprehensive and detailed overview of the data utilized in urban computing, covering various modalities and sources.
In Section 4, we elaborate on the fusion methods employed in urban computing, discussing their approaches and techniques.
Section 5 encapsulates the extensive applications we have compiled, highlighting the practical implementations and contributions of data fusion models in urban computing.
Section 7 outlines the challenges and promising avenues for future research in this domain, identifying potential areas of improvement and exploration.
Finally, we conclude our paper in Section 8.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Taxonomy</h2>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="926" id="S2.F3.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The taxonomy framework for deep learning-based cross-domain data fusion in urban computing in our survey. The framework is structured around three dimensions: data, fusion method, and application. Within each perspective, we categorize existing research into different categories to provide a comprehensive and well-organized review.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section provides a taxonomy of deep learning for multi-source and multi-modal data fusion in urban computing. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S2.F3" title="Figure 3 ‣ 2 Taxonomy ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>, our survey is structured along three dimensions: data in cross-domain fusion in urban computing, modality fusion methods, and applications based on data fusion. A detailed synopsis of the related works can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3" title="3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>, from the data perspective, we divided the data normally utilized in urban computing into five categories according to data sources: <span class="ltx_text ltx_font_italic" id="S2.p2.1.1">geographical data</span>, <span class="ltx_text ltx_font_italic" id="S2.p2.1.2">traffic data</span>, <span class="ltx_text ltx_font_italic" id="S2.p2.1.3">social media data</span>, <span class="ltx_text ltx_font_italic" id="S2.p2.1.4">demographic data</span> and <span class="ltx_text ltx_font_italic" id="S2.p2.1.5">environmental data</span>. Additionally, we have also classified the data from a modality perspective, including <span class="ltx_text ltx_font_italic" id="S2.p2.1.6">spatio-temporal data</span>, <span class="ltx_text ltx_font_italic" id="S2.p2.1.7">visual data</span>, <span class="ltx_text ltx_font_italic" id="S2.p2.1.8">textual data</span>, and other types of data. These two categorizations allow for a systematic understanding and analysis of the different types of data used in urban computing research. We further present a comprehensive overview of public datasets used in cross-domain data fusion in urban computing in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.T2" title="Table 2 ‣ 3.1 Overview ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">From the fusion methodology perspective, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4" title="4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4</span></a> covers a comprehensive review of existing data fusion methods in urban computing, categorized into <span class="ltx_text ltx_font_italic" id="S2.p3.1.1">feature-based</span>, <span class="ltx_text ltx_font_italic" id="S2.p3.1.2">alignment-based</span>, <span class="ltx_text ltx_font_italic" id="S2.p3.1.3">contrast-based</span>, and <span class="ltx_text ltx_font_italic" id="S2.p3.1.4">generation-based fusion</span>. In each category, we subdivide the existing literature into several types based on the models’ properties. A detailed taxonomy from a modality fusion perspective can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S2.F3" title="Figure 3 ‣ 2 Taxonomy ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>. Additionally, in the generation-based fusion section, we also focus on the recent application of LLM for data fusion in urban computing which offers valuable insights for the research community.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5" title="5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">5</span></a>, we divide the multi-modal application in urban computing into seven categories: <span class="ltx_text ltx_font_italic" id="S2.p4.1.1">urban planning</span>, <span class="ltx_text ltx_font_italic" id="S2.p4.1.2">transportation</span>, <span class="ltx_text ltx_font_italic" id="S2.p4.1.3">economy</span>, <span class="ltx_text ltx_font_italic" id="S2.p4.1.4">public safety and security</span>, <span class="ltx_text ltx_font_italic" id="S2.p4.1.5">social</span>, <span class="ltx_text ltx_font_italic" id="S2.p4.1.6">environment</span> and <span class="ltx_text ltx_font_italic" id="S2.p4.1.7">energy</span>. We explore the superiority of multi-modal data fusion methodologies in each type of downstream task.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data Perspective</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section delves into the datasets used in cross-domain data fusion in urban computing. Based on the literature available since 2015, we categorize diverse urban data and perform a statistical analysis of their distributions. Furthermore, we discuss how each type of data is incorporated into different research and real-world scenarios.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Based on the characteristics of the data and their diverse sources from various domains, this survey categorizes datasets utilized in the field of cross-domain data fusion in urban computing into six segments, including geographical data, traffic data, social network data, demographic data, environment data, and other data (i.e., data that cannot be categorized into the aforementioned types, such as healthcare data). As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S2.F3" title="Figure 3 ‣ 2 Taxonomy ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>, these categories are defined as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Geographical data</span> refers to geographical information of specific locations on the Earth’s surface, such as coordinates (i.e., latitude and longitude). This category extends to spatial attributes, including but not limited to topography, land use, and physical features.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Traffic data</span> encompasses information related to the movement of vehicles and pedestrians, including factors like traffic flow, congestion, speed, and road conditions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Social media data</span> comprises user-generated content from online platforms, encompassing geo-tagged text, images, and videos, offering insights into user behaviors, sentiment, and emerging trends.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Demographic data</span> involves statistical information about populations, including characteristics such as age, gender, ethnicity, income, and education.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Environment data</span> incorporates information about the natural world, covering aspects like climate, air quality, biodiversity, and pollution levels.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="161" id="S3.F4.g1" src="extracted/5670403/Images_zxc/proportion_dataset.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Proportion of dataset type among highly related papers within the scope of cross-domain data fusion in urban computing.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Based on the aforementioned categorization, Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.T2" title="Table 2 ‣ 3.1 Overview ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">2</span></a> summarized open-sourced datasets commonly used for cross-domain data fusion in urban computing. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.F4" title="Figure 4 ‣ 3.1 Overview ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4</span></a> presents the distribution of dataset modalities across all investigated papers in this survey. From the pie chart, it is evident that geographical and transportation data are the most crucial datasets in urban computing, with a majority of papers (approaching 70%) opting for them as a primary modality. Following closely is social media data, which plays a significant role in urban computing, often combined with other modal datasets to address the complex and dynamic challenges of urban environments. Demographic data and environment data are also common datasets, however, only around 11% of studies choose to incorporate these two types of data in their research.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="130" id="S3.F5.g1" src="extracted/5670403/Images_zxc/city_data_bar.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of dataset usage frequency from different cities (i.e., bars) and countries (i.e., colors) across highly related papers within the scope of this survey. Note that the cities with less than four paper usage are omitted in this illustration for simplicity.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Furthermore, we also investigate the geographic distribution of the datasets across various cities and countries. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.F5" title="Figure 5 ‣ 3.1 Overview ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">5</span></a>, the bar chart indicates the frequency of dataset utilization in different cities (i.e., bars) and countries (i.e., colors). Distinctive colors of the bars denote different countries, indicating the popularity of the data focus in certain countries. Notably, datasets from Beijing and New York emerged as extensively utilized, followed closely by cities such as Chicago, Singapore, and Shanghai. Overall, the majority of datasets in the domain of cross-domain data fusion in urban computing originate from China and the United States.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Taxonomy and summary of open-sourced dataset used for cross-domain data fusion in urban computing.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:706.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-349.8pt,570.0pt) scale(0.38261452378274,0.38261452378274) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Content</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Format</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.5.1">Link</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.6.1">Reference</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.2.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.2.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.4">ArcGIS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.2.5"><span class="ltx_text" id="S3.T2.1.1.2.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.arcgis.com" title="">https://developers.arcgis.com</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.2.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3">
<td class="ltx_td" id="S3.T2.1.1.3.1"></td>
<td class="ltx_td" id="S3.T2.1.1.3.2"></td>
<td class="ltx_td" id="S3.T2.1.1.3.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.4">PlanetScope</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.3.5"><span class="ltx_text" id="S3.T2.1.1.3.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.planet.com/docs/data/planetscope/" title="">https://developers.planet.com/docs/data/planetscope/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.3.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4">
<td class="ltx_td" id="S3.T2.1.1.4.1"></td>
<td class="ltx_td" id="S3.T2.1.1.4.2"></td>
<td class="ltx_td" id="S3.T2.1.1.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.4">Google Earth</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.5"><span class="ltx_text" id="S3.T2.1.1.4.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.google.com/maps/documentation/" title="">https://developers.google.com/maps/documentation/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.4.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.4.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.4.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib116" title="">116</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5">
<td class="ltx_td" id="S3.T2.1.1.5.1"></td>
<td class="ltx_td" id="S3.T2.1.1.5.2"></td>
<td class="ltx_td" id="S3.T2.1.1.5.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.4">OpenStreetMap</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.5"><span class="ltx_text" id="S3.T2.1.1.5.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.openstreetmap.org/" title="">https://www.openstreetmap.org/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6">
<td class="ltx_td" id="S3.T2.1.1.6.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.2"><span class="ltx_text" id="S3.T2.1.1.6.2.1">Satellite Image</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.3"><span class="ltx_text" id="S3.T2.1.1.6.3.1">Image</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.4">Baidu Maps</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.5"><span class="ltx_text" id="S3.T2.1.1.6.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbsyun.baidu.com" title="">https://lbsyun.baidu.com</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.6.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.6.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.6.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib324" title="">324</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib313" title="">313</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7">
<td class="ltx_td" id="S3.T2.1.1.7.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.7.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.7.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.4">Baidu Map</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.7.5"><span class="ltx_text" id="S3.T2.1.1.7.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbsyun.baidu.com" title="">https://lbsyun.baidu.com</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.7.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.7.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.7.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.7.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib124" title="">124</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8">
<td class="ltx_td" id="S3.T2.1.1.8.1"></td>
<td class="ltx_td" id="S3.T2.1.1.8.2"></td>
<td class="ltx_td" id="S3.T2.1.1.8.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.4">Google Street</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.8.5"><span class="ltx_text" id="S3.T2.1.1.8.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.google.com/maps/" title="">https://developers.google.com/maps/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.8.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.8.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.8.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.8.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib4" title="">4</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9">
<td class="ltx_td" id="S3.T2.1.1.9.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.2"><span class="ltx_text" id="S3.T2.1.1.9.2.1"><span class="ltx_text" id="S3.T2.1.1.9.2.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.9.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.9.2.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.9.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.9.2.1.2.1.1.1">Street-View</span></span>
<span class="ltx_tr" id="S3.T2.1.1.9.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.9.2.1.2.1.2.1">Image</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.9.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.3"><span class="ltx_text" id="S3.T2.1.1.9.3.1">Image</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.4">Tencent Map</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.9.5"><span class="ltx_text" id="S3.T2.1.1.9.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbs.qq.com/tool/streetview/index.html" title="">https://lbs.qq.com/tool/streetview/index.html</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.9.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10">
<td class="ltx_td" id="S3.T2.1.1.10.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.10.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.10.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.10.4">Tencent Map Service</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.10.5"><span class="ltx_text" id="S3.T2.1.1.10.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbs.qq.com/getPoint/" title="">https://lbs.qq.com/getPoint/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.10.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.10.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.10.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.10.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib235" title="">235</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11">
<td class="ltx_td" id="S3.T2.1.1.11.1"></td>
<td class="ltx_td" id="S3.T2.1.1.11.2"></td>
<td class="ltx_td" id="S3.T2.1.1.11.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.11.4">WeChat POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.11.5"><span class="ltx_text" id="S3.T2.1.1.11.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://open.weixin.qq.com" title="">https://open.weixin.qq.com</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.11.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib277" title="">277</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.12">
<td class="ltx_td" id="S3.T2.1.1.12.1"></td>
<td class="ltx_td" id="S3.T2.1.1.12.2"></td>
<td class="ltx_td" id="S3.T2.1.1.12.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.12.4">Baidu Map POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.12.5"><span class="ltx_text" id="S3.T2.1.1.12.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbsyun.baidu.com" title="">https://lbsyun.baidu.com</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.12.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.12.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.12.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.12.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib313" title="">313</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.13">
<td class="ltx_td" id="S3.T2.1.1.13.1"></td>
<td class="ltx_td" id="S3.T2.1.1.13.2"></td>
<td class="ltx_td" id="S3.T2.1.1.13.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.13.4">NYC Open POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.13.5"><span class="ltx_text" id="S3.T2.1.1.13.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opendata.cityofnewyork.us/" title="">https://opendata.cityofnewyork.us/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.13.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.13.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.13.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.13.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib288" title="">288</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.14">
<td class="ltx_td" id="S3.T2.1.1.14.1"></td>
<td class="ltx_td" id="S3.T2.1.1.14.2"></td>
<td class="ltx_td" id="S3.T2.1.1.14.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.4">Foursquare</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.14.5"><span class="ltx_text" id="S3.T2.1.1.14.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.foursquare.com/docs/checkins/checkins" title="">https://developer.foursquare.com/docs/checkins/checkins</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.14.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.14.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.14.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.14.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib381" title="">381</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib116" title="">116</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.15">
<td class="ltx_td" id="S3.T2.1.1.15.1"></td>
<td class="ltx_td" id="S3.T2.1.1.15.2"></td>
<td class="ltx_td" id="S3.T2.1.1.15.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.4">Wikipedia POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.15.5"><span class="ltx_text" id="S3.T2.1.1.15.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wikipedia.org" title="">https://www.wikipedia.org</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.15.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.16">
<td class="ltx_td" id="S3.T2.1.1.16.1"></td>
<td class="ltx_td" id="S3.T2.1.1.16.2"></td>
<td class="ltx_td" id="S3.T2.1.1.16.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.4">AMap Service</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.16.5"><span class="ltx_text" id="S3.T2.1.1.16.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lbs.amap.com" title="">https://lbs.amap.com</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.16.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.17">
<td class="ltx_td" id="S3.T2.1.1.17.1"></td>
<td class="ltx_td" id="S3.T2.1.1.17.2"></td>
<td class="ltx_td" id="S3.T2.1.1.17.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.17.4">Yelp POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.17.5"><span class="ltx_text" id="S3.T2.1.1.17.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.yelp.com/developers" title="">https://www.yelp.com/developers</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.17.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.17.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.17.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.17.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib380" title="">380</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.18">
<td class="ltx_td" id="S3.T2.1.1.18.1"></td>
<td class="ltx_td" id="S3.T2.1.1.18.2"></td>
<td class="ltx_td" id="S3.T2.1.1.18.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.18.4">Dianping POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.18.5"><span class="ltx_text" id="S3.T2.1.1.18.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.dianping.com/" title="">https://api.dianping.com/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.18.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.18.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.18.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.18.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.19">
<td class="ltx_td" id="S3.T2.1.1.19.1"></td>
<td class="ltx_td" id="S3.T2.1.1.19.2"></td>
<td class="ltx_td" id="S3.T2.1.1.19.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.4">Weibo POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.19.5"><span class="ltx_text" id="S3.T2.1.1.19.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://open.weibo.com/wiki/API" title="">https://open.weibo.com/wiki/API</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.19.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.19.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.19.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.19.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib77" title="">77</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.20">
<td class="ltx_td" id="S3.T2.1.1.20.1"></td>
<td class="ltx_td" id="S3.T2.1.1.20.2"></td>
<td class="ltx_td" id="S3.T2.1.1.20.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.20.4">Flickr POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.20.5"><span class="ltx_text" id="S3.T2.1.1.20.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.flickr.com/services/developer/api/" title="">https://www.flickr.com/services/developer/api/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.20.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib99" title="">99</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.21">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.21.1"><span class="ltx_text" id="S3.T2.1.1.21.1.1"><span class="ltx_text" id="S3.T2.1.1.21.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.21.1.1.2"> <span class="ltx_text" id="S3.T2.1.1.21.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.21.1.1.2.1.1">
<span class="ltx_tr" id="S3.T2.1.1.21.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.21.1.1.2.1.1.1.1">Geographical</span></span>
<span class="ltx_tr" id="S3.T2.1.1.21.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.21.1.1.2.1.1.2.1">Data</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.21.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.21.2"><span class="ltx_text" id="S3.T2.1.1.21.2.1">POIs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.21.3"><span class="ltx_text" id="S3.T2.1.1.21.3.1"><span class="ltx_text" id="S3.T2.1.1.21.3.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.21.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.21.3.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.21.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.21.3.1.2.1.1.1">Point Vector</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.21.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.21.4">Bing Map POIs</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.21.5"><span class="ltx_text" id="S3.T2.1.1.21.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bingmapsportal.com" title="">https://www.bingmapsportal.com</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.21.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.21.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.21.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.21.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib37" title="">37</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.22">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.22.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.22.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.22.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.22.4">Shenzhou UCar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.22.5"><span class="ltx_text" id="S3.T2.1.1.22.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bit.ly/2MG47xz" title="">https://bit.ly/2MG47xz</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.22.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib93" title="">93</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.23">
<td class="ltx_td" id="S3.T2.1.1.23.1"></td>
<td class="ltx_td" id="S3.T2.1.1.23.2"></td>
<td class="ltx_td" id="S3.T2.1.1.23.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.23.4">Chicago Transportation</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.23.5"><span class="ltx_text" id="S3.T2.1.1.23.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.cityofchicago.org/" title="">https://data.cityofchicago.org/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.23.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.23.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.23.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.23.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib288" title="">288</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib116" title="">116</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.24">
<td class="ltx_td" id="S3.T2.1.1.24.1"></td>
<td class="ltx_td" id="S3.T2.1.1.24.2"></td>
<td class="ltx_td" id="S3.T2.1.1.24.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.24.4">VED</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.24.5"><span class="ltx_text" id="S3.T2.1.1.24.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/gsoh/VED" title="">https://github.com/gsoh/VED</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.24.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib209" title="">209</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib372" title="">372</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.25">
<td class="ltx_td" id="S3.T2.1.1.25.1"></td>
<td class="ltx_td" id="S3.T2.1.1.25.2"></td>
<td class="ltx_td" id="S3.T2.1.1.25.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.25.4">Taxi Shenzhen</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.25.5"><span class="ltx_text" id="S3.T2.1.1.25.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/cbdog94/STL" title="">https://github.com/cbdog94/STL</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.25.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.25.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.25.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.25.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib302" title="">302</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.26">
<td class="ltx_td" id="S3.T2.1.1.26.1"></td>
<td class="ltx_td" id="S3.T2.1.1.26.2"></td>
<td class="ltx_td" id="S3.T2.1.1.26.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.26.4">NYC Open Taxi Data</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.26.5"><span class="ltx_text" id="S3.T2.1.1.26.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opendata.cityofnewyork.us/how-to/" title="">https://opendata.cityofnewyork.us/how-to/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.26.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.26.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.26.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.26.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.27">
<td class="ltx_td" id="S3.T2.1.1.27.1"></td>
<td class="ltx_td" id="S3.T2.1.1.27.2"></td>
<td class="ltx_td" id="S3.T2.1.1.27.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.27.4">GeoLife</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.27.5"><span class="ltx_text" id="S3.T2.1.1.27.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://urban-computing.com/index-893.htm" title="">http://urban-computing.com/index-893.htm</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.27.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.27.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.27.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.27.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib398" title="">398</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib400" title="">400</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib394" title="">394</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib347" title="">347</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.28">
<td class="ltx_td" id="S3.T2.1.1.28.1"></td>
<td class="ltx_td" id="S3.T2.1.1.28.2"></td>
<td class="ltx_td" id="S3.T2.1.1.28.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.28.4">T-Drive Taxi</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.28.5"><span class="ltx_text" id="S3.T2.1.1.28.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://urban-computing.com/index-58.htm" title="">http://urban-computing.com/index-58.htm</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.28.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.28.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.28.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.28.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib350" title="">350</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib351" title="">351</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib217" title="">217</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib191" title="">191</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.29">
<td class="ltx_td" id="S3.T2.1.1.29.1"></td>
<td class="ltx_td" id="S3.T2.1.1.29.2"></td>
<td class="ltx_td" id="S3.T2.1.1.29.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.29.4">DiDi Traffic</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.29.5"><span class="ltx_text" id="S3.T2.1.1.29.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://outreach.didichuxing.com/research/opendata/" title="">https://outreach.didichuxing.com/research/opendata/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.29.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib188" title="">188</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib228" title="">228</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib328" title="">328</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib261" title="">261</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.30">
<td class="ltx_td" id="S3.T2.1.1.30.1"></td>
<td class="ltx_td" id="S3.T2.1.1.30.2"></td>
<td class="ltx_td" id="S3.T2.1.1.30.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.30.4">Xiamen Taxi</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.30.5"><span class="ltx_text" id="S3.T2.1.1.30.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.mendeley.com/datasets/6xg39x9vgd/1" title="">https://data.mendeley.com/datasets/6xg39x9vgd/1</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.30.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.30.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.30.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.30.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib39" title="">39</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.31">
<td class="ltx_td" id="S3.T2.1.1.31.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.31.2"><span class="ltx_text" id="S3.T2.1.1.31.2.1"><span class="ltx_text" id="S3.T2.1.1.31.2.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.31.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.31.2.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.31.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.31.2.1.2.1.1.1">Traffic</span></span>
<span class="ltx_tr" id="S3.T2.1.1.31.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.31.2.1.2.1.2.1">Trajectory</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.31.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.31.3"><span class="ltx_text" id="S3.T2.1.1.31.3.1"><span class="ltx_text" id="S3.T2.1.1.31.3.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.31.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.31.3.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.31.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.31.3.1.2.1.1.1">Spatio-temporal</span></span>
<span class="ltx_tr" id="S3.T2.1.1.31.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.31.3.1.2.1.2.1">Trajectory</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.31.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.31.4">Grab-Posisi</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.31.5"><span class="ltx_text" id="S3.T2.1.1.31.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/W3yD5m" title="">https://goo.su/W3yD5m</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.31.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.31.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.31.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.31.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.32">
<td class="ltx_td" id="S3.T2.1.1.32.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.32.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.32.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.32.4">California-PEMS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.32.5"><span class="ltx_text" id="S3.T2.1.1.32.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://pems.dot.ca.gov" title="">http://pems.dot.ca.gov</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.32.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib254" title="">254</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.33">
<td class="ltx_td" id="S3.T2.1.1.33.1"></td>
<td class="ltx_td" id="S3.T2.1.1.33.2"></td>
<td class="ltx_td" id="S3.T2.1.1.33.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.33.4">METR-LA</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.33.5"><span class="ltx_text" id="S3.T2.1.1.33.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.metro.net" title="">https://www.metro.net</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.33.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib143" title="">143</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib171" title="">171</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.34">
<td class="ltx_td" id="S3.T2.1.1.34.1"></td>
<td class="ltx_td" id="S3.T2.1.1.34.2"></td>
<td class="ltx_td" id="S3.T2.1.1.34.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.34.4">Large-ST</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.34.5"><span class="ltx_text" id="S3.T2.1.1.34.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/liuxu77/LargeST" title="">https://github.com/liuxu77/LargeST</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.34.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib182" title="">182</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.35">
<td class="ltx_td" id="S3.T2.1.1.35.1"></td>
<td class="ltx_td" id="S3.T2.1.1.35.2"></td>
<td class="ltx_td" id="S3.T2.1.1.35.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.35.4">MobileBJ</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.35.5"><span class="ltx_text" id="S3.T2.1.1.35.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/FIBLAB/DeepSTN/issues/4" title="">https://github.com/FIBLAB/DeepSTN/issues/4</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.35.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.36">
<td class="ltx_td" id="S3.T2.1.1.36.1"></td>
<td class="ltx_td" id="S3.T2.1.1.36.2"></td>
<td class="ltx_td" id="S3.T2.1.1.36.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.36.4">TaxiBJ</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.36.5"><span class="ltx_text" id="S3.T2.1.1.36.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/aQyjTAz" title="">https://goo.su/aQyjTAz</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.36.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.37">
<td class="ltx_td" id="S3.T2.1.1.37.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.37.2"><span class="ltx_text" id="S3.T2.1.1.37.2.1">Taffic Flow</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.37.3"><span class="ltx_text" id="S3.T2.1.1.37.3.1"><span class="ltx_text" id="S3.T2.1.1.37.3.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.37.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.37.3.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.37.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.37.3.1.2.1.1.1">Spatio-temporal</span></span>
<span class="ltx_tr" id="S3.T2.1.1.37.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.37.3.1.2.1.2.1">Graph</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.37.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.37.4">BikeNYC</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.37.5"><span class="ltx_text" id="S3.T2.1.1.37.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://citibikenyc.com/" title="">https://citibikenyc.com/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.37.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib120" title="">120</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.38">
<td class="ltx_td" id="S3.T2.1.1.38.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.38.2" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.38.2.1">Road Network</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.38.3" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.38.3.1">Spatial Graph</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.38.4">
<span class="ltx_text" id="S3.T2.1.1.38.4.1"></span> <span class="ltx_text" id="S3.T2.1.1.38.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.38.4.2.1">
<span class="ltx_tr" id="S3.T2.1.1.38.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.38.4.2.1.1.1">OpenStreetMap</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.38.4.3"></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.38.5"><span class="ltx_text" id="S3.T2.1.1.38.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.openstreetmap.org" title="">https://www.openstreetmap.org</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.38.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib188" title="">188</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib84" title="">84</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.39">
<td class="ltx_td" id="S3.T2.1.1.39.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.39.2">US Census Bureau</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.39.3"><span class="ltx_text" id="S3.T2.1.1.39.3.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.census.gov/data.html" title="">https://www.census.gov/data.html</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.39.4"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.40">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.40.1"><span class="ltx_text" id="S3.T2.1.1.40.1.1"><span class="ltx_text" id="S3.T2.1.1.40.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.40.1.1.2"> <span class="ltx_text" id="S3.T2.1.1.40.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.40.1.1.2.1.1">
<span class="ltx_tr" id="S3.T2.1.1.40.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.40.1.1.2.1.1.1.1">Traffic</span></span>
<span class="ltx_tr" id="S3.T2.1.1.40.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.40.1.1.2.1.1.2.1">Data</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.40.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.40.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.40.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.40.4">LaDe</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.40.5"><span class="ltx_text" id="S3.T2.1.1.40.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cainiaotechai.github.io/LaDe-website/" title="">https://cainiaotechai.github.io/LaDe-website/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.40.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib305" title="">305</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.41">
<td class="ltx_td" id="S3.T2.1.1.41.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.41.2"><span class="ltx_text" id="S3.T2.1.1.41.2.1">Logistics</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.41.3"><span class="ltx_text" id="S3.T2.1.1.41.3.1"><span class="ltx_text" id="S3.T2.1.1.41.3.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.41.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.41.3.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.41.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.41.3.1.2.1.1.1">Spatio-temporal</span></span>
<span class="ltx_tr" id="S3.T2.1.1.41.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.41.3.1.2.1.2.1">Trajectory</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.41.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.41.4">JD Logistics</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.41.5"><span class="ltx_text" id="S3.T2.1.1.41.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://corporate.jd.com/ourBusiness#jdLogistics" title="">https://corporate.jd.com/ourBusiness#jdLogistics</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.41.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib235" title="">235</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.42">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.42.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.42.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.42.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.42.4" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.42.4.1"><span class="ltx_text" id="S3.T2.1.1.42.4.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.42.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.42.4.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.42.4.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.42.4.1.2.1.1.1">Twitter</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.42.4.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.42.5" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.42.5.1"><span class="ltx_text" id="S3.T2.1.1.42.5.1.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.twitter.com/en/docs" title="">https://developer.twitter.com/en/docs</a></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.42.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.42.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.42.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.42.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib381" title="">381</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib352" title="">352</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib270" title="">270</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib301" title="">301</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib240" title="">240</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.43">
<td class="ltx_td" id="S3.T2.1.1.43.1"></td>
<td class="ltx_td" id="S3.T2.1.1.43.2"></td>
<td class="ltx_td" id="S3.T2.1.1.43.3"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.43.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.43.4.1">
<tr class="ltx_tr" id="S3.T2.1.1.43.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.43.4.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib289" title="">289</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib283" title="">283</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib285" title="">285</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib284" title="">284</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib200" title="">200</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib184" title="">184</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.44">
<td class="ltx_td" id="S3.T2.1.1.44.1"></td>
<td class="ltx_td" id="S3.T2.1.1.44.2"></td>
<td class="ltx_td" id="S3.T2.1.1.44.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.44.4">Common Crawl</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.44.5"><span class="ltx_text" id="S3.T2.1.1.44.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://registry.opendata.aws/commoncrawl/" title="">https://registry.opendata.aws/commoncrawl/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.44.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib380" title="">380</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.45">
<td class="ltx_td" id="S3.T2.1.1.45.1"></td>
<td class="ltx_td" id="S3.T2.1.1.45.2"></td>
<td class="ltx_td" id="S3.T2.1.1.45.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.45.4">Yelp Reviews</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.45.5"><span class="ltx_text" id="S3.T2.1.1.45.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.yelp.com/dataset" title="">https://www.yelp.com/dataset</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.45.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.45.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.45.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.45.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib380" title="">380</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.46">
<td class="ltx_td" id="S3.T2.1.1.46.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.46.2"><span class="ltx_text" id="S3.T2.1.1.46.2.1">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.46.3"><span class="ltx_text" id="S3.T2.1.1.46.3.1">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.46.4">Weibo Traffic Police</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.46.5"><span class="ltx_text" id="S3.T2.1.1.46.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://open.weibo.com/developers/" title="">http://open.weibo.com/developers/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.46.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.47">
<td class="ltx_td" id="S3.T2.1.1.47.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.47.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.47.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.47.4"><span class="ltx_text" id="S3.T2.1.1.47.4.1">YFCC100M</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.47.5"><span class="ltx_text" id="S3.T2.1.1.47.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/jzaDU" title="">https://goo.su/jzaDU</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.47.6"><span class="ltx_text" id="S3.T2.1.1.47.6.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.47.6.1.1">
<span class="ltx_tr" id="S3.T2.1.1.47.6.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.47.6.1.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib340" title="">340</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib99" title="">99</a>]</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.48">
<td class="ltx_td" id="S3.T2.1.1.48.1"></td>
<td class="ltx_td" id="S3.T2.1.1.48.2"></td>
<td class="ltx_td" id="S3.T2.1.1.48.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.48.4"><span class="ltx_text" id="S3.T2.1.1.48.4.1">NUS-WIDE</span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.48.5"><span class="ltx_text" id="S3.T2.1.1.48.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/dWPQZcD" title="">https://goo.su/dWPQZcD</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.48.6"><span class="ltx_text" id="S3.T2.1.1.48.6.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.48.6.1.1">
<span class="ltx_tr" id="S3.T2.1.1.48.6.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.48.6.1.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib340" title="">340</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib338" title="">338</a>]</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.49">
<td class="ltx_td" id="S3.T2.1.1.49.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.49.2"><span class="ltx_text" id="S3.T2.1.1.49.2.1"><span class="ltx_text" id="S3.T2.1.1.49.2.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.49.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.49.2.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.49.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.49.2.1.2.1.1.1">Geo-tagged</span></span>
<span class="ltx_tr" id="S3.T2.1.1.49.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.49.2.1.2.1.2.1">Image &amp; Video</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.49.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.49.3"><span class="ltx_text" id="S3.T2.1.1.49.3.1">Image&amp;Video</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.49.4">GeoUGV</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.49.5"><span class="ltx_text" id="S3.T2.1.1.49.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qualinet.github.io/databases/video/" title="">https://qualinet.github.io/databases/video/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.49.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib187" title="">187</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.50">
<td class="ltx_td" id="S3.T2.1.1.50.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.50.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.50.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.50.4">Jiepang User Check-in</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.50.5"><span class="ltx_text" id="S3.T2.1.1.50.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jiepang.app/" title="">https://jiepang.app/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.50.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.51">
<td class="ltx_td" id="S3.T2.1.1.51.1"></td>
<td class="ltx_td" id="S3.T2.1.1.51.2"></td>
<td class="ltx_td" id="S3.T2.1.1.51.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.51.4">Gowalla User Location</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.51.5"><span class="ltx_text" id="S3.T2.1.1.51.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://konect.cc/networks/loc-gowalla_edges/" title="">http://konect.cc/networks/loc-gowalla_edges/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.51.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.51.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.51.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.51.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib352" title="">352</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.52">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.52.1"><span class="ltx_text" id="S3.T2.1.1.52.1.1"><span class="ltx_text" id="S3.T2.1.1.52.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.52.1.1.2"> <span class="ltx_text" id="S3.T2.1.1.52.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.52.1.1.2.1.1">
<span class="ltx_tr" id="S3.T2.1.1.52.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.52.1.1.2.1.1.1.1">Social</span></span>
<span class="ltx_tr" id="S3.T2.1.1.52.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.52.1.1.2.1.1.2.1">Media</span></span>
<span class="ltx_tr" id="S3.T2.1.1.52.1.1.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.52.1.1.2.1.1.3.1">Data</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.52.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.52.2"><span class="ltx_text" id="S3.T2.1.1.52.2.1">Users’ Info</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.52.3"><span class="ltx_text" id="S3.T2.1.1.52.3.1">Time Series</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.52.4">WeChat Mobility</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.52.5"><span class="ltx_text" id="S3.T2.1.1.52.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://open.weixin.qq.com/" title="">https://open.weixin.qq.com/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.52.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib277" title="">277</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.53">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.53.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.53.2">Crime</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.53.3">Time Series</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.53.4">NYC Crime</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.53.5"><span class="ltx_text" id="S3.T2.1.1.53.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opendata.cityofnewyork.us/" title="">https://opendata.cityofnewyork.us/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.53.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.54">
<td class="ltx_td" id="S3.T2.1.1.54.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.54.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.54.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.54.4">Land Use SG</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.54.5"><span class="ltx_text" id="S3.T2.1.1.54.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ura.gov.sg/Corporate/Planning/Master-Plan" title="">https://www.ura.gov.sg/Corporate/Planning/Master-Plan</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.54.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.55">
<td class="ltx_td" id="S3.T2.1.1.55.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.55.2"><span class="ltx_text" id="S3.T2.1.1.55.2.1">Land Use</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.55.3"><span class="ltx_text" id="S3.T2.1.1.55.3.1">Time Series</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.55.4">Land Use NYC</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.55.5"><span class="ltx_text" id="S3.T2.1.1.55.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/puTuG" title="">https://goo.su/puTuG</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.55.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.56">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.56.1"><span class="ltx_text" id="S3.T2.1.1.56.1.1"><span class="ltx_text" id="S3.T2.1.1.56.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.56.1.1.2"> <span class="ltx_text" id="S3.T2.1.1.56.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.56.1.1.2.1.1">
<span class="ltx_tr" id="S3.T2.1.1.56.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.56.1.1.2.1.1.1.1">Demographic</span></span>
<span class="ltx_tr" id="S3.T2.1.1.56.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.56.1.1.2.1.1.2.1">Data</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.56.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.56.2">Population</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.56.3">Time Series</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.56.4">WorldPop</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.56.5"><span class="ltx_text" id="S3.T2.1.1.56.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.worldpop.org/" title="">https://www.worldpop.org/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.56.6">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.56.6.1">
<tr class="ltx_tr" id="S3.T2.1.1.56.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.1.56.6.1.1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.57">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.57.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.57.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.57.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.57.4">TipDM China Weather</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.57.5"><span class="ltx_text" id="S3.T2.1.1.57.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tipdm.org/" title="">https://www.tipdm.org/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.57.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib178" title="">178</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.58">
<td class="ltx_td" id="S3.T2.1.1.58.1"></td>
<td class="ltx_td" id="S3.T2.1.1.58.2"></td>
<td class="ltx_td" id="S3.T2.1.1.58.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.58.4">DarkSky Weather</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.58.5"><span class="ltx_text" id="S3.T2.1.1.58.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://support.apple.com/en-us/102594" title="">https://support.apple.com/en-us/102594</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.58.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.59">
<td class="ltx_td" id="S3.T2.1.1.59.1"></td>
<td class="ltx_td" id="S3.T2.1.1.59.2"></td>
<td class="ltx_td" id="S3.T2.1.1.59.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.59.4">WeatherNY</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.59.5"><span class="ltx_text" id="S3.T2.1.1.59.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opendata.cityofnewyork.us/" title="">https://opendata.cityofnewyork.us/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.59.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.60">
<td class="ltx_td" id="S3.T2.1.1.60.1"></td>
<td class="ltx_td" id="S3.T2.1.1.60.2"></td>
<td class="ltx_td" id="S3.T2.1.1.60.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.60.4">WeatherChicago</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.60.5"><span class="ltx_text" id="S3.T2.1.1.60.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.cityofchicago.org/" title="">https://data.cityofchicago.org/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.60.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.61">
<td class="ltx_td" id="S3.T2.1.1.61.1"></td>
<td class="ltx_td" id="S3.T2.1.1.61.2"></td>
<td class="ltx_td" id="S3.T2.1.1.61.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.61.4">Weather Underground</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.61.5"><span class="ltx_text" id="S3.T2.1.1.61.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wunderground.com/" title="">https://www.wunderground.com/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.61.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.62">
<td class="ltx_td" id="S3.T2.1.1.62.1"></td>
<td class="ltx_td" id="S3.T2.1.1.62.2"></td>
<td class="ltx_td" id="S3.T2.1.1.62.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.62.4">
<span class="ltx_text" id="S3.T2.1.1.62.4.1"></span> <span class="ltx_text" id="S3.T2.1.1.62.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.62.4.2.1">
<span class="ltx_tr" id="S3.T2.1.1.62.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.62.4.2.1.1.1">DidiSY</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.62.4.3"></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.62.5"><span class="ltx_text" id="S3.T2.1.1.62.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.didiglobal.com/" title="">https://www.didiglobal.com/</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.62.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.63">
<td class="ltx_td" id="S3.T2.1.1.63.1"></td>
<td class="ltx_td" id="S3.T2.1.1.63.2"></td>
<td class="ltx_td" id="S3.T2.1.1.63.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.63.4">WD_BJ weather</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.63.5"><span class="ltx_text" id="S3.T2.1.1.63.5.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/DmHFHd" title="">https://goo.su/DmHFHd</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.63.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib192" title="">192</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.64">
<td class="ltx_td" id="S3.T2.1.1.64.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.64.2"><span class="ltx_text" id="S3.T2.1.1.64.2.1">Meteorology</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.64.3"><span class="ltx_text" id="S3.T2.1.1.64.3.1">Time Series</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.64.4">WD_USA weather</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.64.5"><span class="ltx_text" id="S3.T2.1.1.64.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/RVhBA" title="">https://goo.su/RVhBA</a></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.64.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib192" title="">192</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.65">
<td class="ltx_td" id="S3.T2.1.1.65.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.65.2">Greenery</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.65.3">Time Series</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.65.4">Google Earth</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.65.5"><span class="ltx_text" id="S3.T2.1.1.65.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://earth.google.com/" title="">https://earth.google.com/</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.65.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.66">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.66.1"><span class="ltx_text" id="S3.T2.1.1.66.1.1"><span class="ltx_text" id="S3.T2.1.1.66.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.66.1.1.2"> <span class="ltx_text" id="S3.T2.1.1.66.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.66.1.1.2.1.1">
<span class="ltx_tr" id="S3.T2.1.1.66.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.66.1.1.2.1.1.1.1">Environment</span></span>
<span class="ltx_tr" id="S3.T2.1.1.66.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.66.1.1.2.1.1.2.1">Data</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.66.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.1.1.66.2" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.66.2.1">Air Quality</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.1.1.66.3" rowspan="2"><span class="ltx_text" id="S3.T2.1.1.66.3.1">Time Series</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.66.4">UrbanAir</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.66.5"><span class="ltx_text" id="S3.T2.1.1.66.5.1" style="color:#2E86C1;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://goo.su/hfzNB53" title="">https://goo.su/hfzNB53</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.66.6"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib399" title="">399</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib396" title="">396</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib392" title="">392</a>]</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.67">
<td class="ltx_td ltx_border_bb" id="S3.T2.1.1.67.1"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.67.2">KnowAir</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.67.3"><span class="ltx_text" id="S3.T2.1.1.67.3.1" style="color:#2E86C1;"> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/shuowang-ai/PM2.5-GNN" title="">https://github.com/shuowang-ai/PM2.5-GNN</a></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.67.4"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib286" title="">286</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib346" title="">346</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib370" title="">370</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib318" title="">318</a>]</cite></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Geographical Data</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Geographical data plays a crucial role in modeling spatial relationships, contributing to the enhancement of cross-domain data fusion in urban computing by providing valuable insights into the geospatial context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib379" title="">379</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib377" title="">377</a>]</cite>. Tobler’s First Law of Geography, as stated by <cite class="ltx_cite ltx_citemacro_citet">Miller [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib198" title="">198</a>]</cite>, indicates that ”Everything is related to everything else, but near things are more related than distant things.” This emphasizes the importance of geographical data, which serve as the basis for spatial modeling-based urban computing research. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.F6" title="Figure 6 ‣ 3.2 Geographical Data ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">6</span></a> shows the main four types of geographical data through a visualization approach.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Points of Interest (POI) data</span>, as a cornerstone of cross-domain data fusion in urban computing, command significant attention and utility in the realm of geographical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib107" title="">107</a>]</cite>.
POI data refers to a collection of data representing specific locations or sites that hold significance or interest, often encompassing businesses, landmarks, or other notable entities in a given geographical area <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite>. Conventionally, POI data may include the location coordinates (i.e., longitude and latitude), address, categorization (e.g., restaurants, hotels, parks, etc.), address, phone number, operating hours, and so on. Recent cross-domain urban datasets may also contain user reviews, photos, and other individual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib380" title="">380</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>]</cite>. Therefore, POI data is capable of representing the semantics of a specific area, encapsulating key locations and entities to provide a comprehensive understanding of the geographic context. For example, <cite class="ltx_cite ltx_citemacro_citet">Bing et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib20" title="">20</a>]</cite> collected POI data of two cities: New York City and Tokyo from the Foursquare platform and observed that POIs with higher spatial similarity values often have similar semantics. Through AMaps Service Platform in Wuhan and Shanghai, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite> collected approximately 2 million POI records and fused them with satellite visual embedding for socioeconomic downstream tasks. To model the spatial correlations between POIs, <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite> developed a spatial network based on the multi-relation data among POIs, such as spatial distance and mobility connectivity.</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="160" id="S3.F6.g1" src="extracted/5670403/Images_zxc/geographical_data.png" width="275"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of four types of geographical data collected at Central Park, New York, USA: (a) POI data and digital map; (b) street-view image; (c) satellite image.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F6.1">.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Satellite image data</span> holds significant importance in representing geo-context by providing a visual depiction of Earth’s surface, and offering advantages such as global coverage, real-time monitoring, and the ability to capture fine-scale details <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib353" title="">353</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib207" title="">207</a>]</cite>. Satellite imagery is readily accessible and amenable to processing through various publicly available platforms (e.g., Google Earth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib116" title="">116</a>]</cite>, OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite>, Baidu Map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib324" title="">324</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib313" title="">313</a>]</cite>, and PlanetScope <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite>), facilitating multi-modal research in urban computing domain.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Moreover, <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">street-view image data</span> plays a vital role in depicting geo-contextual information by providing immersive, ground-level perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib130" title="">130</a>]</cite>. A significant amount of cross-domain urban research has collected data from public platforms such as Google Street <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib4" title="">4</a>]</cite>, Baidu Map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib124" title="">124</a>]</cite>, and Tencent Map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib113" title="">113</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Traffic Data</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Traffic data accounts for the second largest proportion in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.F4" title="Figure 4 ‣ 3.1 Overview ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4</span></a>. Different from geographical data, traffic data is generated through human activities and is directly associated with socio-economic factors, making it a distinct data type with significant implications.
Traffic data finds application in diverse downstream tasks, encompassing multiple domains within urban computing, which involve spatial and temporal dimensions, as well as dynamic and static aspects. Therefore, we classify traffic data into four distinct types, taking into account their characteristics and usage scenarios: trajectory data, traffic flow data, road network data, and other miscellaneous data. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S3.F7" title="Figure 7 ‣ 3.3 Traffic Data ‣ 3 Data Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">7</span></a> indicates the visualization of the first three types of traffic data.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="103" id="S3.F7.g1" src="extracted/5670403/Images_zxc/trafficdata.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of three primary types of traffic data: (a) traffic trajectory; (b) traffic flow; (c) road network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib307" title="">307</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Human trajectory data</span> can be conceptualized as a sequential trace produced by an object in motion within geographical spaces. This trajectory is typically represented as an array of chronologically sequenced points, delineated as <math alttext="p_{1}\rightarrow p_{2}\rightarrow\cdots\rightarrow p_{n}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><msub id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">p</mi><mn id="S3.SS3.p2.1.m1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p2.1.m1.1.1.3" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.3.cmml">→</mo><msub id="S3.SS3.p2.1.m1.1.1.4" xref="S3.SS3.p2.1.m1.1.1.4.cmml"><mi id="S3.SS3.p2.1.m1.1.1.4.2" xref="S3.SS3.p2.1.m1.1.1.4.2.cmml">p</mi><mn id="S3.SS3.p2.1.m1.1.1.4.3" xref="S3.SS3.p2.1.m1.1.1.4.3.cmml">2</mn></msub><mo id="S3.SS3.p2.1.m1.1.1.5" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.5.cmml">→</mo><mi id="S3.SS3.p2.1.m1.1.1.6" mathvariant="normal" xref="S3.SS3.p2.1.m1.1.1.6.cmml">⋯</mi><mo id="S3.SS3.p2.1.m1.1.1.7" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.7.cmml">→</mo><msub id="S3.SS3.p2.1.m1.1.1.8" xref="S3.SS3.p2.1.m1.1.1.8.cmml"><mi id="S3.SS3.p2.1.m1.1.1.8.2" xref="S3.SS3.p2.1.m1.1.1.8.2.cmml">p</mi><mi id="S3.SS3.p2.1.m1.1.1.8.3" xref="S3.SS3.p2.1.m1.1.1.8.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><and id="S3.SS3.p2.1.m1.1.1a.cmml" xref="S3.SS3.p2.1.m1.1.1"></and><apply id="S3.SS3.p2.1.m1.1.1b.cmml" xref="S3.SS3.p2.1.m1.1.1"><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">→</ci><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">𝑝</ci><cn id="S3.SS3.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.2.3">1</cn></apply><apply id="S3.SS3.p2.1.m1.1.1.4.cmml" xref="S3.SS3.p2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.4.1.cmml" xref="S3.SS3.p2.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.4.2.cmml" xref="S3.SS3.p2.1.m1.1.1.4.2">𝑝</ci><cn id="S3.SS3.p2.1.m1.1.1.4.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.4.3">2</cn></apply></apply><apply id="S3.SS3.p2.1.m1.1.1c.cmml" xref="S3.SS3.p2.1.m1.1.1"><ci id="S3.SS3.p2.1.m1.1.1.5.cmml" xref="S3.SS3.p2.1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2402.19348v2#S3.SS3.p2.1.m1.1.1.4.cmml" id="S3.SS3.p2.1.m1.1.1d.cmml" xref="S3.SS3.p2.1.m1.1.1"></share><ci id="S3.SS3.p2.1.m1.1.1.6.cmml" xref="S3.SS3.p2.1.m1.1.1.6">⋯</ci></apply><apply id="S3.SS3.p2.1.m1.1.1e.cmml" xref="S3.SS3.p2.1.m1.1.1"><ci id="S3.SS3.p2.1.m1.1.1.7.cmml" xref="S3.SS3.p2.1.m1.1.1.7">→</ci><share href="https://arxiv.org/html/2402.19348v2#S3.SS3.p2.1.m1.1.1.6.cmml" id="S3.SS3.p2.1.m1.1.1f.cmml" xref="S3.SS3.p2.1.m1.1.1"></share><apply id="S3.SS3.p2.1.m1.1.1.8.cmml" xref="S3.SS3.p2.1.m1.1.1.8"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.8.1.cmml" xref="S3.SS3.p2.1.m1.1.1.8">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.8.2.cmml" xref="S3.SS3.p2.1.m1.1.1.8.2">𝑝</ci><ci id="S3.SS3.p2.1.m1.1.1.8.3.cmml" xref="S3.SS3.p2.1.m1.1.1.8.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">p_{1}\rightarrow p_{2}\rightarrow\cdots\rightarrow p_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT → italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT → ⋯ → italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. Each point in this sequence comprises a set of geospatial coordinates coupled with a corresponding timestamp, formalized as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{P}=(\mathbf{x},\mathbf{y},\mathbf{t})," class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">𝐏</mi><mo id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.1.cmml"><mo id="S3.E1.m1.4.4.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.1.1.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐱</mi><mo id="S3.E1.m1.4.4.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝐲</mi><mo id="S3.E1.m1.4.4.1.1.3.2.3" xref="S3.E1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">𝐭</mi><mo id="S3.E1.m1.4.4.1.1.3.2.4" stretchy="false" xref="S3.E1.m1.4.4.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"></eq><ci id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2">𝐏</ci><vector id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐱</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐲</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝐭</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\mathbf{P}=(\mathbf{x},\mathbf{y},\mathbf{t}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">bold_P = ( bold_x , bold_y , bold_t ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.4">where <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m1.1"><semantics id="S3.SS3.p2.2.m1.1a"><mi id="S3.SS3.p2.2.m1.1.1" xref="S3.SS3.p2.2.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m1.1b"><ci id="S3.SS3.p2.2.m1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m1.1d">bold_x</annotation></semantics></math> and <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m2.1"><semantics id="S3.SS3.p2.3.m2.1a"><mi id="S3.SS3.p2.3.m2.1.1" xref="S3.SS3.p2.3.m2.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m2.1b"><ci id="S3.SS3.p2.3.m2.1.1.cmml" xref="S3.SS3.p2.3.m2.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m2.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m2.1d">bold_y</annotation></semantics></math> denote the spatial coordinates, typically representing longitude and latitude in a geodetic framework. <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m3.1"><semantics id="S3.SS3.p2.4.m3.1a"><mi id="S3.SS3.p2.4.m3.1.1" xref="S3.SS3.p2.4.m3.1.1.cmml">𝐭</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m3.1b"><ci id="S3.SS3.p2.4.m3.1.1.cmml" xref="S3.SS3.p2.4.m3.1.1">𝐭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m3.1c">\mathbf{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m3.1d">bold_t</annotation></semantics></math> represents the timestamp. This format facilitates the precise tracking and analysis of spatial dynamics over time.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Nowadays, location-based services, such as taxi and ride-hailing services, have generated enormous trajectory data.
Additionally, many service providers, such as Uber and DiDi, have made their datasets publicly available to support research in this field.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite> utilized taxi trip data during one month as a modality to assist in profiling urban regions.
PANDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> demonstrated the significance of taxi trajectory in predicting road risk, which can aid in the extraction of road segments.
<cite class="ltx_cite ltx_citemacro_citet">Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib84" title="">84</a>]</cite> forecasted ride-hailing demand on large-scale ride-hailing datasets in Beijing and Shanghai.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Traffic flow data</span> provides critical insights into the movement patterns of vehicles and pedestrians within urban environments, essential for understanding and optimizing city dynamics. In the realm of urban computing, three datasets — <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2">MobileBJ</span>, <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.3">BikeNYC</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.4">TaxiBJ</span> — emerge as key sources of insight for traffic mobility. Specifically, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.5">MobileBJ</span> dataset, collected from a Chinese social network, has significantly contributed to three key areas of urban computing. Firstly, it enabled the development of DeepSTN+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>]</cite>, a deep learning model for predicting urban crowd flows, which integrates spatial dependencies, POIs, and temporal data. Additionally, it supported studies on high-dimensional spatio-temporal data in urban communities, employing advanced representation learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">The <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.1">BikeNYC</span> dataset, derived from New York City’s bike-sharing system in 2014, provides rich data on trip durations, station locations, and times. It focuses on the last 14 days for testing purposes and includes 9 POI types. This dataset has been instrumental in developing a model for multi-step passenger demand forecasting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib11" title="">11</a>]</cite>, leveraging its detailed trip data to address complex spatio-temporal challenges in urban transportation planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib367" title="">367</a>]</cite>. Meanwhile, <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.2">TaxiBJ</span> collects GPS traces from Beijing taxis, encompassing trip details, travel times, speeds, and specific pick-up and drop-off points. This dataset allowed for the development of a novel embedding strategy for urban regions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite>, providing deeper insights into city dynamics and supporting sustainable urban development. It also enhanced traffic prediction accuracy by applying spatiotemporal graph neural networks and integrating self-supervised learning for improved long-term forecasting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib120" title="">120</a>]</cite>. Furthermore, <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.3">TaxiBJ</span> demonstrated the effectiveness of a contrastive self-supervision method in inferring fine-grained urban flows, especially in environments with limited resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>]</cite>. An innovative approach for computing trajectory similarities, <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.4">TrajGAT</span>, was introduced using <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.5">TaxiBJ</span>, significantly advancing the analysis of long trajectory data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib328" title="">328</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p6.1.1">Road network data</span> is intimately related to human daily lives, as it serves as the foundation for various services such as navigation and food delivery.
The acquisition of road network data can be achieved through various methods.
The earliest on-site manual surveying was labor-intensive and required a large amount of resources. With the development of remote sensing technology, which can effectively reduce costs, the proportion of road network data drawing relying on on-site collection has been decreasing.
Another method to collect road network data is through UGC (User Generated Content), which collects road network information through anonymous terminal devices. The collected route can effectively help with updating road attributes and refinement of the road network.
Generally, road network data can be downloaded in shapefile format from different open-sourced platforms such as Open Street Map, GRIP global roads database, DIVA-GIS, etc.
For example, <cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite> predicted travel demand and traffic flow based on taxi orders which is associated with trajectories on the road network.
<cite class="ltx_cite ltx_citemacro_citet">Zhu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib405" title="">405</a>]</cite> ranked region significance taking into account multi-source spatial data including trajectories on road networks.
Besides, the acquisition of street-view image datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib68" title="">68</a>]</cite> also necessitated the sampling of collection points on road networks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">Other miscellaneous data include logistic data, transportation safety data, and transportation recommendation data. In the logistics field, LaDe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib305" title="">305</a>]</cite> introduced the first industry-scale last-mile delivery dataset. LaDe includes detailed information about the courier trajectories and waybill information, which can support massive spatio-temporal data mining tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib235" title="">235</a>]</cite>. Transportation safety data is also of great significance. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib39" title="">39</a>]</cite>, road obstacle data was used to develop RADAR, a real-time system for identifying road obstacles in urban areas during typhoon seasons. <cite class="ltx_cite ltx_citemacro_citet">You et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> used a comprehensive event dataset, including data on road accidents, fallen trees, and ponding water, to successfully develop a framework for predicting road risks in post-disaster urban settings with high accuracy. Transportation recommendations have become an integral part of our daily lives, contributing significantly to various service enhancements. For instance, <cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib78" title="">78</a>]</cite> utilized this data to develop GraphTrip, a groundbreaking framework that leverages spatio-temporal graph representation learning for trip recommendations. This framework was rigorously tested using datasets from Edinburgh, Glasgow, Osaka, Toronto, and Melbourne, showcasing notable improvements in travel planning accuracy. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Guo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib93" title="">93</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">He et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib99" title="">99</a>]</cite> employed multi-source urban data, encompassing operational data, taxi GPS trajectories, and public transportation information, to create advanced recommendation models.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Social Media Data</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Twitter, an online social media and social networking service allows registered users to post <span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">geo-textual data</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib50" title="">50</a>]</cite>. Hence, the Tweet data, owing to its inherent geo-tagged information, is utilized by researchers as a modality representing user social states, and it is integrated with other models in multi-modal learning. For instance, <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib381" title="">381</a>]</cite> collected English tweets using the Twitter API in two cities, New York City and Singapore, and then annotated whether a tweet was POI-related. Finally, they can model the association between the tweet and its most semantically related POI. To study periodic human mobility patterns, <cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib352" title="">352</a>]</cite> collected geo-annotated tweets from the most recent 3,200 tweets of Twitter users, and mapped them to the corresponding city by reverse geocoding. With a similar goal of understanding urban dynamics, <cite class="ltx_cite ltx_citemacro_citet">Miyazawa et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib200" title="">200</a>]</cite> focused only on tweets regarding mobility and social activity; while <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib284" title="">284</a>]</cite> are more interested in tweets concerning traffic events in Chicago. Furthermore, geo-textual data such as tweets can be used to learn user preferences to support personalized maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib383" title="">383</a>]</cite>. Some work, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib270" title="">270</a>]</cite> working on POI boundary estimation, also removed the content automatically created by other services like Twimight, Tweetbot, and so forth.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Geo-tagged photos</span> are crucial in multi-modal learning as they provide spatial context, enriching the understanding of content by incorporating location information into the broader spectrum of data modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib203" title="">203</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib25" title="">25</a>]</cite>. The two most commonly used geo-tagged photo datasets are the Yahoo Flickr Creative Commons (YFCC) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib263" title="">263</a>]</cite> as well as the NUS-WIDE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib47" title="">47</a>]</cite>. It is noteworthy that the YFCC dataset is well-known as the largest public multimedia collection released, which consists of 100 million photos posted on Flickr with relevant meta information such as geo-location coordinates and the date taken. The NUS-WIDE dataset is a well-known web image dataset (with 269,648 images and the associated tags from Flickr) created by Lab for Media Search in the National University of Singapore. Both <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">He et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib99" title="">99</a>]</cite> leveraged the geo-tagged photos to jointly learn a context-aware embedding for personalized tour recommendation. <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib338" title="">338</a>]</cite> derived training labels from the geo-tagged documents from the NUS-WIDE dataset, and therefore generated GPS embeddings. In the work of <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib340" title="">340</a>]</cite> where multi context-aware location representations need to be learned, the geo-tagged photos from the NUS-WIDE dataset are used for image classification evaluation, whereas those from the YFCC dataset are leveraged to learn the semantic context.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">With the ubiquity of sensor-rich smartphones, acquiring continuous video frames with spatial metadata has become practical. The public <span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">geo-tagged mobile video data</span> comes mainly from two mobile platforms, MediaQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib137" title="">137</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib197" title="">197</a>]</cite> and GeoVid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib85" title="">85</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Lu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib187" title="">187</a>]</cite> collected the geo-tagged video data from the aforementioned data platforms, and proposed the GeoUGV dataset consisting of two sets, videos and their geospatial metadata. Moreover, many social media platforms (e.g., WeChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib277" title="">277</a>]</cite>, Gowalla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib352" title="">352</a>]</cite>, Baidu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>]</cite>, Jiepang <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite>) encompass valuable user information, serving as a fundamental modality for fusion.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Demographic Data</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The inclusion of demographic datasets in spatio-temporal multi-modal learning is pivotal as it enhances the contextual understanding of a given human group <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib246" title="">246</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib271" title="">271</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib252" title="">252</a>]</cite>. The WorldPop organization plays a crucial role in global demographic research by providing high-resolution <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.1">population data</span>, enabling informed decision-making, and addressing various socio-economic and public health challenges worldwide <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib189" title="">189</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib233" title="">233</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Xi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite> collected Beijing population and population density statistics from the WorldPop platform as one of the predicted socioeconomic indicators; whereas <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite> extracted those from Singapore and New York City. In terms of data post-processing, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite> further estimated the population density per grid cell through the Random Forest-based redistribution method.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.1">Crime data</span> holds immense significance as it serves as a critical resource for understanding patterns and factors influencing criminal activities, enabling policymakers and researchers to develop effective strategies for crime prevention and public safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib376" title="">376</a>]</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite> collected crime data from the NYC Open Data website as ground truth values to be predicted by region embeddings, and there are 40 thousand crime records during one year in New York City.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Besides, <span class="ltx_text ltx_font_bold" id="S3.SS5.p3.1.1">land use data</span> is crucial in urban planning, providing valuable insights into the spatial distribution of human activities, and informing decision-makers to optimize land resources for various purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib250" title="">250</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib231" title="">231</a>]</cite>. It consists of property data (e.g., private residential property transactions), residential data (e.g., buying and renting properties), business data (e.g., renewal of business use), and so on. For instance, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite> collected land use data of Singapore and New York City from Singapore Master Plan 2019 and NYC MapPLUTO, respectively, as a region representation evaluation benchmark.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Environment Data</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Environment data, particularly <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.1">meteorological data</span>, offers essential insights into dynamic weather patterns and environmental conditions that are integral for understanding complex interactions in various urban domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib266" title="">266</a>]</cite>. The publicly available APIs can be utilized to gather meteorological data. For example, <cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite> utilized the Dark Sky API integrated into Apple Weather to extract diverse weather characteristics for each region and used one-hot encoding to represent categorical attributes. <cite class="ltx_cite ltx_citemacro_citet">You et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> extracted temporal contextual features including rainfall, temperature, humidity, dew point, and wind speed using Weather Underground API. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite> obtained temperature and sky condition data for both NYC and Chicago from NYC Open Data and Chicago Data Portal, respectively. By doing so, they were able to analyze the spatio-temporal correlations to predict the risk of traffic accidents. To precisely forecast the weather, <cite class="ltx_cite ltx_citemacro_citet">Ma et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib192" title="">192</a>]</cite> employed three real-world weather datasets: the WD_BJ dataset, which was gathered from 10 ground automatic weather stations in Beijing and contains nine meteorological variables; and the WD_ISR dataset and WD_USA dataset, both collected from OpenWeather and providing information on four weather conditions (namely temperature, humidity, wind speed, and atmospheric pressure) for Israel and the USA, respectively. In addition, enterprises also gather their own meteorological data, such as the DidiSY dataset which includes information on weather conditions, temperature, and wind speed specifically in Shenyang, a major city in China <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite>. Besides, open-source datasets from competitions can also be utilized, such as the Chinese weather dataset from the 7th Teddy Cup Data Mining Challenge, which includes 12 different types of weather <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib178" title="">178</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS6.p2.6.1">Greenery data</span> holds significance in sustainable urban planning, providing valuable information about the distribution and density of vegetation, which is essential for assessing biodiversity, urban green spaces, and their impact on overall ecological health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib333" title="">333</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib208" title="">208</a>]</cite>. In particular, <cite class="ltx_cite ltx_citemacro_citet">You et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> extracted the degree of tree coverage using AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib139" title="">139</a>]</cite> based on satellite imagery obtained from the Google Earth platform. <span class="ltx_text ltx_font_bold" id="S3.SS6.p2.6.2">Air quality data</span> is of paramount importance for environmental management, facilitating the identification of sources, and enabling the development of effective strategies to mitigate the adverse impacts of air pollution on both human health and the environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib374" title="">374</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib260" title="">260</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib32" title="">32</a>]</cite>. For example, the UrbanAir system offers air quality data every hour from 2,296 stations in 302 Chinese cities, where each record consists of the concentration of six pollutants: <math alttext="NO_{2}" class="ltx_Math" display="inline" id="S3.SS6.p2.1.m1.1"><semantics id="S3.SS6.p2.1.m1.1a"><mrow id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml"><mi id="S3.SS6.p2.1.m1.1.1.2" xref="S3.SS6.p2.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS6.p2.1.m1.1.1.1" xref="S3.SS6.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS6.p2.1.m1.1.1.3" xref="S3.SS6.p2.1.m1.1.1.3.cmml"><mi id="S3.SS6.p2.1.m1.1.1.3.2" xref="S3.SS6.p2.1.m1.1.1.3.2.cmml">O</mi><mn id="S3.SS6.p2.1.m1.1.1.3.3" xref="S3.SS6.p2.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><apply id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1"><times id="S3.SS6.p2.1.m1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1.1"></times><ci id="S3.SS6.p2.1.m1.1.1.2.cmml" xref="S3.SS6.p2.1.m1.1.1.2">𝑁</ci><apply id="S3.SS6.p2.1.m1.1.1.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.1.1.3.1.cmml" xref="S3.SS6.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS6.p2.1.m1.1.1.3.2.cmml" xref="S3.SS6.p2.1.m1.1.1.3.2">𝑂</ci><cn id="S3.SS6.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS6.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">NO_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.1.m1.1d">italic_N italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, S<math alttext="O_{2}" class="ltx_Math" display="inline" id="S3.SS6.p2.2.m2.1"><semantics id="S3.SS6.p2.2.m2.1a"><msub id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml"><mi id="S3.SS6.p2.2.m2.1.1.2" xref="S3.SS6.p2.2.m2.1.1.2.cmml">O</mi><mn id="S3.SS6.p2.2.m2.1.1.3" xref="S3.SS6.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><apply id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.2.m2.1.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p2.2.m2.1.1.2.cmml" xref="S3.SS6.p2.2.m2.1.1.2">𝑂</ci><cn id="S3.SS6.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS6.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">O_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.2.m2.1d">italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="O_{3}" class="ltx_Math" display="inline" id="S3.SS6.p2.3.m3.1"><semantics id="S3.SS6.p2.3.m3.1a"><msub id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml"><mi id="S3.SS6.p2.3.m3.1.1.2" xref="S3.SS6.p2.3.m3.1.1.2.cmml">O</mi><mn id="S3.SS6.p2.3.m3.1.1.3" xref="S3.SS6.p2.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b"><apply id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS6.p2.3.m3.1.1.2.cmml" xref="S3.SS6.p2.3.m3.1.1.2">𝑂</ci><cn id="S3.SS6.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS6.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">O_{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.3.m3.1d">italic_O start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="CO" class="ltx_Math" display="inline" id="S3.SS6.p2.4.m4.1"><semantics id="S3.SS6.p2.4.m4.1a"><mrow id="S3.SS6.p2.4.m4.1.1" xref="S3.SS6.p2.4.m4.1.1.cmml"><mi id="S3.SS6.p2.4.m4.1.1.2" xref="S3.SS6.p2.4.m4.1.1.2.cmml">C</mi><mo id="S3.SS6.p2.4.m4.1.1.1" xref="S3.SS6.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS6.p2.4.m4.1.1.3" xref="S3.SS6.p2.4.m4.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.4.m4.1b"><apply id="S3.SS6.p2.4.m4.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1"><times id="S3.SS6.p2.4.m4.1.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1.1"></times><ci id="S3.SS6.p2.4.m4.1.1.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2">𝐶</ci><ci id="S3.SS6.p2.4.m4.1.1.3.cmml" xref="S3.SS6.p2.4.m4.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.4.m4.1c">CO</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.4.m4.1d">italic_C italic_O</annotation></semantics></math>, <math alttext="PM2.5" class="ltx_Math" display="inline" id="S3.SS6.p2.5.m5.1"><semantics id="S3.SS6.p2.5.m5.1a"><mrow id="S3.SS6.p2.5.m5.1.1" xref="S3.SS6.p2.5.m5.1.1.cmml"><mi id="S3.SS6.p2.5.m5.1.1.2" xref="S3.SS6.p2.5.m5.1.1.2.cmml">P</mi><mo id="S3.SS6.p2.5.m5.1.1.1" xref="S3.SS6.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.SS6.p2.5.m5.1.1.3" xref="S3.SS6.p2.5.m5.1.1.3.cmml">M</mi><mo id="S3.SS6.p2.5.m5.1.1.1a" xref="S3.SS6.p2.5.m5.1.1.1.cmml">⁢</mo><mn id="S3.SS6.p2.5.m5.1.1.4" xref="S3.SS6.p2.5.m5.1.1.4.cmml">2.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.5.m5.1b"><apply id="S3.SS6.p2.5.m5.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1"><times id="S3.SS6.p2.5.m5.1.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1.1"></times><ci id="S3.SS6.p2.5.m5.1.1.2.cmml" xref="S3.SS6.p2.5.m5.1.1.2">𝑃</ci><ci id="S3.SS6.p2.5.m5.1.1.3.cmml" xref="S3.SS6.p2.5.m5.1.1.3">𝑀</ci><cn id="S3.SS6.p2.5.m5.1.1.4.cmml" type="float" xref="S3.SS6.p2.5.m5.1.1.4">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.5.m5.1c">PM2.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.5.m5.1d">italic_P italic_M 2.5</annotation></semantics></math> and <math alttext="PM10" class="ltx_Math" display="inline" id="S3.SS6.p2.6.m6.1"><semantics id="S3.SS6.p2.6.m6.1a"><mrow id="S3.SS6.p2.6.m6.1.1" xref="S3.SS6.p2.6.m6.1.1.cmml"><mi id="S3.SS6.p2.6.m6.1.1.2" xref="S3.SS6.p2.6.m6.1.1.2.cmml">P</mi><mo id="S3.SS6.p2.6.m6.1.1.1" xref="S3.SS6.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.SS6.p2.6.m6.1.1.3" xref="S3.SS6.p2.6.m6.1.1.3.cmml">M</mi><mo id="S3.SS6.p2.6.m6.1.1.1a" xref="S3.SS6.p2.6.m6.1.1.1.cmml">⁢</mo><mn id="S3.SS6.p2.6.m6.1.1.4" xref="S3.SS6.p2.6.m6.1.1.4.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.6.m6.1b"><apply id="S3.SS6.p2.6.m6.1.1.cmml" xref="S3.SS6.p2.6.m6.1.1"><times id="S3.SS6.p2.6.m6.1.1.1.cmml" xref="S3.SS6.p2.6.m6.1.1.1"></times><ci id="S3.SS6.p2.6.m6.1.1.2.cmml" xref="S3.SS6.p2.6.m6.1.1.2">𝑃</ci><ci id="S3.SS6.p2.6.m6.1.1.3.cmml" xref="S3.SS6.p2.6.m6.1.1.3">𝑀</ci><cn id="S3.SS6.p2.6.m6.1.1.4.cmml" type="integer" xref="S3.SS6.p2.6.m6.1.1.4">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.6.m6.1c">PM10</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p2.6.m6.1d">italic_P italic_M 10</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib399" title="">399</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology Perspective</h2>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F8.1" style="width:633.8pt;height:741.7pt;vertical-align:-734.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S4.F8.1.1"><span class="ltx_text" id="S4.F8.1.1.1"><span class="ltx_ERROR undefined" id="S4.F8.1.1.1.1">{forest}</span>
forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=center,
font=<span class="ltx_text" id="S4.F8.1.1.1.2" style="font-size:120%;">,
rectangle,
draw=hidden-draw,
rounded corners,
align=left,
text centered,
minimum width=5em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
line width=0.8pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=1text width=10em,font=,,
where level=2text width=12em,font=,,
where level=3text width=10em,font=,,
where level=4text width=7em,font=,,
where level=5text width=7em,font=,,
[Cross-Domain 
<br class="ltx_break"/>Data Fusion in 
<br class="ltx_break"/>Urban Computing
[Feature-based Data 
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2" title="4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2</span></a>)
[Feature Addition &amp; 
<br class="ltx_break"/>Multiplication (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS1" title="4.2.1 Feature Addition and Multiplication ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>)
[DeepST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib363" title="">363</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite>, ROD-Revenue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib94" title="">94</a>]</cite>, leaf, text width=20em]
]
[Feature Concatenation 
<br class="ltx_break"/>(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS2" title="4.2.2 Feature Concatenation ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>)
[ST-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib362" title="">362</a>]</cite>, DeepCrime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib108" title="">108</a>]</cite>, DeepSTN+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>]</cite>,  
<br class="ltx_break"/>ST-MetaNet+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib218" title="">218</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Liang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib164" title="">164</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>]</cite>, 
<br class="ltx_break"/>ST-SHN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib311" title="">311</a>]</cite>,  <cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>]</cite>,  <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite>, leaf, text width=20em]
]
[Graph-based Feature
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS3" title="4.2.3 Graph-based Data Fusion ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>)
[Multi-view Graph
[<cite class="ltx_cite ltx_citemacro_citet">Du et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>], Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib82" title="">82</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>], Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite>, leaf, text width=16em]
]
[Heterogeneous Graph
[<cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib176" title="">176</a>], Keerthi Chandra et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib278" title="">278</a>], Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib365" title="">365</a>]</cite>, leaf, text width=16em]
]
]
]
[Alignment-based Data 
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3" title="4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.3</span></a>)
[Attention-based Alignment 
<br class="ltx_break"/>(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS1" title="4.3.1 Attention-based Alignment ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>)
[<cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>], Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite>, GSNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite>, 
<br class="ltx_break"/>ERNIE-GeoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Xi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>]</cite>, RankETPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib298" title="">298</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>], Yan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite>, MVMT-STNcite <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib288" title="">288</a>]</cite>, 
<br class="ltx_break"/>SAInf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib193" title="">193</a>]</cite>, leaf, text width=22em]
]
[Encoder-based Alignment
<br class="ltx_break"/>(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS2" title="4.3.2 Encoder-based Alignment ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>)
[DeepUrbanEvent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>]</cite>, DeepTransport <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib247" title="">247</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Jiang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>], Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib28" title="">28</a>]</cite>, RegionDCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite>, leaf, text width=22em]
]
]
[Contrast-based Data 
<br class="ltx_break"/>Fusion(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4" title="4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.4</span></a>)
[Instance Contrast-based
<br class="ltx_break"/>Fusion
(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS1" title="4.4.1 Instance Contrast-based Fusion ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>)
[<cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>], Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>], Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Mao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib196" title="">196</a>]</cite>, leaf, text width=18em]
]
[Batch Contrast-based 
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS2" title="4.4.2 Batch Contrast-based Fusion ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>)
[<cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>], Mao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib196" title="">196</a>]</cite>, RecMVC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite>, 
<br class="ltx_break"/><cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite>, UrbanCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite>, leaf, text width=18em]
]
]
[Generation-based Data 
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5" title="4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5</span></a>)
[Autoregressive Model
<br class="ltx_break"/>(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS1" title="4.5.1 Autoregressive Model ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>)
[GeoMAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib162" title="">162</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib382" title="">382</a>]</cite>, leaf, text width=22em
]
]
[Mask Modeling-based
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS2" title="4.5.2 Mask Modeling-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>)
[SatMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib49" title="">49</a>]</cite>, ERNIE-GeoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>]</cite>, QUERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib314" title="">314</a>]</cite>, G2PTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib304" title="">304</a>]</cite>,  
<br class="ltx_break"/>MGeo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib61" title="">61</a>]</cite>, Scale-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib230" title="">230</a>]</cite>, leaf, text width=22em
]
]
[Diffusion-based Fusion
<br class="ltx_break"/>(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS3" title="4.5.3 Diffusion-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.3</span></a>)
[DiffusionSat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib136" title="">136</a>]</cite>, ChatTraffic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib358" title="">358</a>]</cite>, DiffUFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>]</cite>, 
<br class="ltx_break"/>DiffTraj <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib356" title="">356</a>]</cite>, leaf, text width=22em
]
]
[LLM-enhanced Data
<br class="ltx_break"/>Fusion (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS4" title="4.5.4 LLM-enhanced Data Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.4</span></a>)
[GeoLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib195" title="">195</a>]</cite>, LLM-Mob <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib291" title="">291</a>]</cite>, RSGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib105" title="">105</a>]</cite>, GeoChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib141" title="">141</a>]</cite>, 
<br class="ltx_break"/>LLM4TS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib34" title="">34</a>]</cite>, Time-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib126" title="">126</a>]</cite>, UniTime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib181" title="">181</a>]</cite>, leaf, text width=22em
]
]
]
]</span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Taxonomy of deep learning-based cross-domain data fusion methods in urban computing.</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we begin by providing definitions and explanations of four types of multi-modal fusion. Then, we explore each type in detail, offering more specific categorizations and providing illustrative examples for each case. The comprehensive summary of fusion models can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.T3" title="Table 3 ‣ 4.1 Overview ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overview</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The integration of deep learning techniques into urban computing has facilitated the development of various deep-learning-based data fusion methods. These techniques aim to leverage the inherent connections within diverse urban data streams. To better understand the differences in the underlying concepts of data fusion in these studies, <span class="ltx_text" id="S4.SS1.p1.1.1" style="color:#000000;">we consult prior surveys on methodologies in Urban Computing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib390" title="">390</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite> and taxonomies of deep learning methods in other fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib282" title="">282</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib348" title="">348</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib76" title="">76</a>]</cite> and categorize the lasted researches into four distinct groups based on their underlying techniques,</span> as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F8" title="Figure 8 ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">8</span></a>. The definitions of each category are outlined as follows:</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The summary of deep learning-based cross-domain data fusion models in urban computing. We denote different data source as follows: Traffic Data <span class="ltx_text" id="S4.T3.6.1" style="color:#A0E0E0;">✤</span>; Geographical Data <span class="ltx_text" id="S4.T3.7.2" style="color:#959595;">▲</span>; Social Media Data <span class="ltx_text" id="S4.T3.8.3" style="color:#F5C9C8;">◆</span>; Demographical Data <span class="ltx_text" id="S4.T3.9.4" style="color:#FBEB65;">❖</span>; Environmental Data <span class="ltx_text" id="S4.T3.10.5" style="color:#3C9898;">🍀</span>. Notice that method names are assigned based on original reference model names if available; otherwise, they are named after the first authors.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.11" style="width:433.6pt;height:472.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-447.8pt,488.1pt) scale(0.3262339947116,0.3262339947116) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.11.1">
<tr class="ltx_tr" id="S4.T3.11.1.1">
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.1"></td>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.2"></td>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="9" id="S4.T3.11.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.1.4.1">Modality</span></td>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.5"></td>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.6"></td>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.11.1.1.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.2">
<td class="ltx_td" id="S4.T3.11.1.2.1"></td>
<td class="ltx_td" id="S4.T3.11.1.2.2"></td>
<td class="ltx_td" id="S4.T3.11.1.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S4.T3.11.1.2.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.2.4.1">General Spatio-temporal</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T3.11.1.2.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.2.5.1">Visual</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T3.11.1.2.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.2.6.1">Textual</span></td>
<td class="ltx_td" id="S4.T3.11.1.2.7"></td>
<td class="ltx_td" id="S4.T3.11.1.2.8"></td>
<td class="ltx_td" id="S4.T3.11.1.2.9"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.3">
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.1.1">Category</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.2.1">Method</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.3"><span class="ltx_text" id="S4.T3.11.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.3.1.1">Data Sourc</span>e</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.4">
<span class="ltx_text" id="S4.T3.11.1.3.4.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.4.2"> <span class="ltx_text" id="S4.T3.11.1.3.4.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.4.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.4.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.4.2.1.1.1.1">Time</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.4.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.4.2.1.1.2.1">series</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.4.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.5">
<span class="ltx_text" id="S4.T3.11.1.3.5.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.5.2"> <span class="ltx_text" id="S4.T3.11.1.3.5.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.5.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.5.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.5.2.1.1.1.1">POI /</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.5.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.5.2.1.1.2.1">Location</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.5.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.6">
<span class="ltx_text" id="S4.T3.11.1.3.6.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.6.2"> <span class="ltx_text" id="S4.T3.11.1.3.6.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.6.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.6.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.6.2.1.1.1.1">Trajectory/</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.6.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.6.2.1.1.2.1">Road network</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.6.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.7"><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.7.1">Mobility</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.8">
<span class="ltx_text" id="S4.T3.11.1.3.8.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.8.2"> <span class="ltx_text" id="S4.T3.11.1.3.8.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.8.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.8.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.8.2.1.1.1.1">ST</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.8.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.8.2.1.1.2.1">events</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.8.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.9"><span class="ltx_text" id="S4.T3.11.1.3.9.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.9.2"> <span class="ltx_text" id="S4.T3.11.1.3.9.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.9.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.9.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.9.2.1.1.1.1">Satellite</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.9.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.9.2.1.1.2.1">image</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.9.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.10"><span class="ltx_text" id="S4.T3.11.1.3.10.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.10.2"> <span class="ltx_text" id="S4.T3.11.1.3.10.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.10.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.10.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.10.2.1.1.1.1">Street-view</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.10.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.10.2.1.1.2.1">image</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.10.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.11"><span class="ltx_text" id="S4.T3.11.1.3.11.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.11.2"> <span class="ltx_text" id="S4.T3.11.1.3.11.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.11.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.11.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.11.2.1.1.1.1">Social</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.11.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.11.2.1.1.2.1">media text</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.11.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.3.12"><span class="ltx_text" id="S4.T3.11.1.3.12.1"></span><span class="ltx_text ltx_font_italic" id="S4.T3.11.1.3.12.2"> <span class="ltx_text" id="S4.T3.11.1.3.12.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.3.12.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.3.12.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.12.2.1.1.1.1">Geo-</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.12.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.12.2.1.1.2.1">imformation</span></span>
<span class="ltx_tr" id="S4.T3.11.1.3.12.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.3.12.2.1.1.3.1">text</span></span>
</span></span><span class="ltx_text" id="S4.T3.11.1.3.12.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.13"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.13.1">Application</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.14"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.14.1">Institution</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.3.15"><span class="ltx_text ltx_font_bold" id="S4.T3.11.1.3.15.1">Year</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.4">
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.2">DeepST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib363" title="">363</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.3"><span class="ltx_text" id="S4.T3.11.1.4.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.4.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.4.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.4"><span class="ltx_text" id="S4.T3.11.1.4.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.6"><span class="ltx_text" id="S4.T3.11.1.4.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.8"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.9"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.10"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.11"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.4.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.13">Transportation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.14">Microsoft</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.4.15">2016</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.5">
<td class="ltx_td" id="S4.T3.11.1.5.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.2">ST-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib364" title="">364</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.3"><span class="ltx_text" id="S4.T3.11.1.5.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.5.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.5.3.1.1.1" style="color:#FBEB65;">❖<span class="ltx_text" id="S4.T3.11.1.5.3.1.1.1.1" style="color:#3C9898;">🍀</span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.4"><span class="ltx_text" id="S4.T3.11.1.5.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.5"><span class="ltx_text" id="S4.T3.11.1.5.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.6"><span class="ltx_text" id="S4.T3.11.1.5.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.5.7"></td>
<td class="ltx_td" id="S4.T3.11.1.5.8"></td>
<td class="ltx_td" id="S4.T3.11.1.5.9"></td>
<td class="ltx_td" id="S4.T3.11.1.5.10"></td>
<td class="ltx_td" id="S4.T3.11.1.5.11"></td>
<td class="ltx_td" id="S4.T3.11.1.5.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.14">Microsoft</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.5.15">2018</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.6">
<td class="ltx_td" id="S4.T3.11.1.6.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.2">ST-MetaNet+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib218" title="">218</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.3"><span class="ltx_text" id="S4.T3.11.1.6.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.6.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.4"><span class="ltx_text" id="S4.T3.11.1.6.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.5"><span class="ltx_text" id="S4.T3.11.1.6.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.6"><span class="ltx_text" id="S4.T3.11.1.6.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.6.7"></td>
<td class="ltx_td" id="S4.T3.11.1.6.8"></td>
<td class="ltx_td" id="S4.T3.11.1.6.9"></td>
<td class="ltx_td" id="S4.T3.11.1.6.10"></td>
<td class="ltx_td" id="S4.T3.11.1.6.11"></td>
<td class="ltx_td" id="S4.T3.11.1.6.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.14">JD Research</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.6.15">2020</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.7">
<td class="ltx_td" id="S4.T3.11.1.7.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.2">DeepCrime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib108" title="">108</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.3"><span class="ltx_text" id="S4.T3.11.1.7.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.7.3.1.1" style="color:#F5C9C8;">◆<span class="ltx_text" id="S4.T3.11.1.7.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.4"><span class="ltx_text" id="S4.T3.11.1.7.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.5"><span class="ltx_text" id="S4.T3.11.1.7.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.6"><span class="ltx_text" id="S4.T3.11.1.7.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.7.7"></td>
<td class="ltx_td" id="S4.T3.11.1.7.8"></td>
<td class="ltx_td" id="S4.T3.11.1.7.9"></td>
<td class="ltx_td" id="S4.T3.11.1.7.10"></td>
<td class="ltx_td" id="S4.T3.11.1.7.11"></td>
<td class="ltx_td" id="S4.T3.11.1.7.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.13">Social</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.14">JD Research</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.7.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.8">
<td class="ltx_td" id="S4.T3.11.1.8.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.2">STUKG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib278" title="">278</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.3"><span class="ltx_text" id="S4.T3.11.1.8.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.8.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.8.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.5"><span class="ltx_text" id="S4.T3.11.1.8.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.6"><span class="ltx_text" id="S4.T3.11.1.8.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.8.7"></td>
<td class="ltx_td" id="S4.T3.11.1.8.8"></td>
<td class="ltx_td" id="S4.T3.11.1.8.9"></td>
<td class="ltx_td" id="S4.T3.11.1.8.10"></td>
<td class="ltx_td" id="S4.T3.11.1.8.11"></td>
<td class="ltx_td" id="S4.T3.11.1.8.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.8.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.9">
<td class="ltx_td" id="S4.T3.11.1.9.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.2">DeepSTN+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.3"><span class="ltx_text" id="S4.T3.11.1.9.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.9.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.4"><span class="ltx_text" id="S4.T3.11.1.9.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.5"><span class="ltx_text" id="S4.T3.11.1.9.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.9.6"></td>
<td class="ltx_td" id="S4.T3.11.1.9.7"></td>
<td class="ltx_td" id="S4.T3.11.1.9.8"></td>
<td class="ltx_td" id="S4.T3.11.1.9.9"></td>
<td class="ltx_td" id="S4.T3.11.1.9.10"></td>
<td class="ltx_td" id="S4.T3.11.1.9.11"></td>
<td class="ltx_td" id="S4.T3.11.1.9.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.9.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.10">
<td class="ltx_td" id="S4.T3.11.1.10.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.2">DeepTP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.3"><span class="ltx_text" id="S4.T3.11.1.10.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.10.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.10.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.5"><span class="ltx_text" id="S4.T3.11.1.10.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.10.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.7"><span class="ltx_text" id="S4.T3.11.1.10.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.10.8"></td>
<td class="ltx_td" id="S4.T3.11.1.10.9"></td>
<td class="ltx_td" id="S4.T3.11.1.10.10"></td>
<td class="ltx_td" id="S4.T3.11.1.10.11"></td>
<td class="ltx_td" id="S4.T3.11.1.10.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.10.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.11">
<td class="ltx_td" id="S4.T3.11.1.11.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.2"><cite class="ltx_cite ltx_citemacro_citet">Guo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib94" title="">94</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.3"><span class="ltx_text" id="S4.T3.11.1.11.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.11.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.4"><span class="ltx_text" id="S4.T3.11.1.11.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.5"><span class="ltx_text" id="S4.T3.11.1.11.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.6"><span class="ltx_text" id="S4.T3.11.1.11.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.11.7"></td>
<td class="ltx_td" id="S4.T3.11.1.11.8"></td>
<td class="ltx_td" id="S4.T3.11.1.11.9"></td>
<td class="ltx_td" id="S4.T3.11.1.11.10"></td>
<td class="ltx_td" id="S4.T3.11.1.11.11"></td>
<td class="ltx_td" id="S4.T3.11.1.11.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.14">BUAA</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.11.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.12">
<td class="ltx_td" id="S4.T3.11.1.12.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.2">Photo2Trip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.3"><span class="ltx_text" id="S4.T3.11.1.12.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.12.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.12.3.1.1.1" style="color:#F5C9C8;">◆</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.12.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.5"><span class="ltx_text" id="S4.T3.11.1.12.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.12.6"></td>
<td class="ltx_td" id="S4.T3.11.1.12.7"></td>
<td class="ltx_td" id="S4.T3.11.1.12.8"></td>
<td class="ltx_td" id="S4.T3.11.1.12.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.10"><span class="ltx_text" id="S4.T3.11.1.12.10.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.12.11"></td>
<td class="ltx_td" id="S4.T3.11.1.12.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.14">SU/RU/UCA</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.12.15">2017</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.13">
<td class="ltx_td" id="S4.T3.11.1.13.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.2">ST-SHN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib311" title="">311</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.3"><span class="ltx_text" id="S4.T3.11.1.13.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.13.3.1.1" style="color:#FBEB65;">❖</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.4"><span class="ltx_text" id="S4.T3.11.1.13.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.5"><span class="ltx_text" id="S4.T3.11.1.13.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.13.6"></td>
<td class="ltx_td" id="S4.T3.11.1.13.7"></td>
<td class="ltx_td" id="S4.T3.11.1.13.8"></td>
<td class="ltx_td" id="S4.T3.11.1.13.9"></td>
<td class="ltx_td" id="S4.T3.11.1.13.10"></td>
<td class="ltx_td" id="S4.T3.11.1.13.11"></td>
<td class="ltx_td" id="S4.T3.11.1.13.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.13">Public Safety</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.14">SCUT/HKU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.13.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.14">
<td class="ltx_td" id="S4.T3.11.1.14.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.2">GeoMAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib162" title="">162</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.3"><span class="ltx_text" id="S4.T3.11.1.14.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.14.3.1.1" style="color:#3C9898;">🍀</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.4"><span class="ltx_text" id="S4.T3.11.1.14.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.5"><span class="ltx_text" id="S4.T3.11.1.14.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.14.6"></td>
<td class="ltx_td" id="S4.T3.11.1.14.7"></td>
<td class="ltx_td" id="S4.T3.11.1.14.8"></td>
<td class="ltx_td" id="S4.T3.11.1.14.9"></td>
<td class="ltx_td" id="S4.T3.11.1.14.10"></td>
<td class="ltx_td" id="S4.T3.11.1.14.11"></td>
<td class="ltx_td" id="S4.T3.11.1.14.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.14">XDU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.14.15">2018</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.15">
<td class="ltx_td" id="S4.T3.11.1.15.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.2"><cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.3"><span class="ltx_text" id="S4.T3.11.1.15.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.15.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.15.4"></td>
<td class="ltx_td" id="S4.T3.11.1.15.5"></td>
<td class="ltx_td" id="S4.T3.11.1.15.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.7"><span class="ltx_text" id="S4.T3.11.1.15.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.15.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.9"><span class="ltx_text" id="S4.T3.11.1.15.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.10"><span class="ltx_text" id="S4.T3.11.1.15.10.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.15.11"></td>
<td class="ltx_td" id="S4.T3.11.1.15.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.13">Urban planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.14">PKU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.15.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.16">
<td class="ltx_td" id="S4.T3.11.1.16.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.2"><cite class="ltx_cite ltx_citemacro_citet">Liang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib164" title="">164</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.3"><span class="ltx_text" id="S4.T3.11.1.16.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.4"><span class="ltx_text" id="S4.T3.11.1.16.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.5"><span class="ltx_text" id="S4.T3.11.1.16.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.6"><span class="ltx_text" id="S4.T3.11.1.16.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.16.7"></td>
<td class="ltx_td" id="S4.T3.11.1.16.8"></td>
<td class="ltx_td" id="S4.T3.11.1.16.9"></td>
<td class="ltx_td" id="S4.T3.11.1.16.10"></td>
<td class="ltx_td" id="S4.T3.11.1.16.11"></td>
<td class="ltx_td" id="S4.T3.11.1.16.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.14">NUS</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.16.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.17">
<td class="ltx_td" id="S4.T3.11.1.17.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.2"><cite class="ltx_cite ltx_citemacro_citet">Balsebre et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.3"><span class="ltx_text" id="S4.T3.11.1.17.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td" id="S4.T3.11.1.17.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.5"><span class="ltx_text" id="S4.T3.11.1.17.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.17.6"></td>
<td class="ltx_td" id="S4.T3.11.1.17.7"></td>
<td class="ltx_td" id="S4.T3.11.1.17.8"></td>
<td class="ltx_td" id="S4.T3.11.1.17.9"></td>
<td class="ltx_td" id="S4.T3.11.1.17.10"></td>
<td class="ltx_td" id="S4.T3.11.1.17.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.12"><span class="ltx_text" id="S4.T3.11.1.17.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.17.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.18">
<td class="ltx_td" id="S4.T3.11.1.18.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.2"><cite class="ltx_cite ltx_citemacro_citet">Ruan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib235" title="">235</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.3"><span class="ltx_text" id="S4.T3.11.1.18.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.18.3.1.1" style="color:#A0E0E0;">✤</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.4"><span class="ltx_text" id="S4.T3.11.1.18.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.5"><span class="ltx_text" id="S4.T3.11.1.18.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.18.6"></td>
<td class="ltx_td" id="S4.T3.11.1.18.7"></td>
<td class="ltx_td" id="S4.T3.11.1.18.8"></td>
<td class="ltx_td" id="S4.T3.11.1.18.9"></td>
<td class="ltx_td" id="S4.T3.11.1.18.10"></td>
<td class="ltx_td" id="S4.T3.11.1.18.11"></td>
<td class="ltx_td" id="S4.T3.11.1.18.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.18.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.19">
<td class="ltx_td" id="S4.T3.11.1.19.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.2"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.3"><span class="ltx_text" id="S4.T3.11.1.19.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.19.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.19.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.5"><span class="ltx_text" id="S4.T3.11.1.19.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.19.6"></td>
<td class="ltx_td" id="S4.T3.11.1.19.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.8"><span class="ltx_text" id="S4.T3.11.1.19.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.19.9"></td>
<td class="ltx_td" id="S4.T3.11.1.19.10"></td>
<td class="ltx_td" id="S4.T3.11.1.19.11"></td>
<td class="ltx_td" id="S4.T3.11.1.19.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.13">Economy</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.14">HKUST(GZ)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.19.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.20">
<td class="ltx_td" id="S4.T3.11.1.20.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.2">PANDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.3"><span class="ltx_text" id="S4.T3.11.1.20.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.20.3.1.1" style="color:#F5C9C8;">◆<span class="ltx_text" id="S4.T3.11.1.20.3.1.1.1" style="color:#FBEB65;">❖<span class="ltx_text" id="S4.T3.11.1.20.3.1.1.1.1" style="color:#3C9898;">🍀</span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.4"><span class="ltx_text" id="S4.T3.11.1.20.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.20.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.6"><span class="ltx_text" id="S4.T3.11.1.20.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.20.7"></td>
<td class="ltx_td" id="S4.T3.11.1.20.8"></td>
<td class="ltx_td" id="S4.T3.11.1.20.9"></td>
<td class="ltx_td" id="S4.T3.11.1.20.10"></td>
<td class="ltx_td" id="S4.T3.11.1.20.11"></td>
<td class="ltx_td" id="S4.T3.11.1.20.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.13">Public Safety</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.14">XMU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.20.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.21">
<td class="ltx_td" id="S4.T3.11.1.21.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.2">UVLens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.3"><span class="ltx_text" id="S4.T3.11.1.21.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.21.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.21.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.21.4"></td>
<td class="ltx_td" id="S4.T3.11.1.21.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.6"><span class="ltx_text" id="S4.T3.11.1.21.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.7"><span class="ltx_text" id="S4.T3.11.1.21.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.21.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.9"><span class="ltx_text" id="S4.T3.11.1.21.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.21.10"></td>
<td class="ltx_td" id="S4.T3.11.1.21.11"></td>
<td class="ltx_td" id="S4.T3.11.1.21.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.14">XMU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.21.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.22">
<td class="ltx_td" id="S4.T3.11.1.22.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.2"><cite class="ltx_cite ltx_citemacro_citet">Miyazawa et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib200" title="">200</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.3"><span class="ltx_text" id="S4.T3.11.1.22.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.22.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.22.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.5"><span class="ltx_text" id="S4.T3.11.1.22.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.6"><span class="ltx_text" id="S4.T3.11.1.22.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.22.7"></td>
<td class="ltx_td" id="S4.T3.11.1.22.8"></td>
<td class="ltx_td" id="S4.T3.11.1.22.9"></td>
<td class="ltx_td" id="S4.T3.11.1.22.10"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.11"><span class="ltx_text" id="S4.T3.11.1.22.11.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.22.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.14">SUSTech</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.22.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.23">
<td class="ltx_td" id="S4.T3.11.1.23.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.2">NodeSense2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.3"><span class="ltx_text" id="S4.T3.11.1.23.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.23.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.23.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.5"><span class="ltx_text" id="S4.T3.11.1.23.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.23.6"></td>
<td class="ltx_td" id="S4.T3.11.1.23.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.8"><span class="ltx_text" id="S4.T3.11.1.23.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.23.9"></td>
<td class="ltx_td" id="S4.T3.11.1.23.10"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.11"><span class="ltx_text" id="S4.T3.11.1.23.11.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.23.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.13">Social</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.14">UCF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.23.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.24">
<td class="ltx_td" id="S4.T3.11.1.24.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.2"><cite class="ltx_cite ltx_citemacro_citet">Keerthi Chandra et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.3"><span class="ltx_text" id="S4.T3.11.1.24.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.24.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.24.3.1.1.1" style="color:#F5C9C8;">◆</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.24.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.5"><span class="ltx_text" id="S4.T3.11.1.24.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.24.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.7"><span class="ltx_text" id="S4.T3.11.1.24.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.24.8"></td>
<td class="ltx_td" id="S4.T3.11.1.24.9"></td>
<td class="ltx_td" id="S4.T3.11.1.24.10"></td>
<td class="ltx_td" id="S4.T3.11.1.24.11"></td>
<td class="ltx_td" id="S4.T3.11.1.24.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.14">UCF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.24.15">2020</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.25">
<td class="ltx_td" id="S4.T3.11.1.25.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.2"><cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.3"><span class="ltx_text" id="S4.T3.11.1.25.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.25.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.25.3.1.1.1" style="color:#F5C9C8;">◆</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.25.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.5"><span class="ltx_text" id="S4.T3.11.1.25.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.25.6"></td>
<td class="ltx_td" id="S4.T3.11.1.25.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.8"><span class="ltx_text" id="S4.T3.11.1.25.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.25.9"></td>
<td class="ltx_td" id="S4.T3.11.1.25.10"></td>
<td class="ltx_td" id="S4.T3.11.1.25.11"></td>
<td class="ltx_td" id="S4.T3.11.1.25.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.14">UCF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.25.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.26">
<td class="ltx_td" id="S4.T3.11.1.26.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.2"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib184" title="">184</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.3"><span class="ltx_text" id="S4.T3.11.1.26.3.1" style="color:#F5C9C8;">◆</span></td>
<td class="ltx_td" id="S4.T3.11.1.26.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.5"><span class="ltx_text" id="S4.T3.11.1.26.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.26.6"></td>
<td class="ltx_td" id="S4.T3.11.1.26.7"></td>
<td class="ltx_td" id="S4.T3.11.1.26.8"></td>
<td class="ltx_td" id="S4.T3.11.1.26.9"></td>
<td class="ltx_td" id="S4.T3.11.1.26.10"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.11"><span class="ltx_text" id="S4.T3.11.1.26.11.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.26.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.13">Social</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.14">Gatech</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.26.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.27">
<td class="ltx_td" id="S4.T3.11.1.27.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.2"><cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.3"><span class="ltx_text" id="S4.T3.11.1.27.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.4"><span class="ltx_text" id="S4.T3.11.1.27.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.5"><span class="ltx_text" id="S4.T3.11.1.27.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.27.6"></td>
<td class="ltx_td" id="S4.T3.11.1.27.7"></td>
<td class="ltx_td" id="S4.T3.11.1.27.8"></td>
<td class="ltx_td" id="S4.T3.11.1.27.9"></td>
<td class="ltx_td" id="S4.T3.11.1.27.10"></td>
<td class="ltx_td" id="S4.T3.11.1.27.11"></td>
<td class="ltx_td" id="S4.T3.11.1.27.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.14">RMIT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.27.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.28">
<td class="ltx_td" id="S4.T3.11.1.28.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.2"><cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.3"><span class="ltx_text" id="S4.T3.11.1.28.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.28.3.1.1" style="color:#3C9898;">🍀</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.4"><span class="ltx_text" id="S4.T3.11.1.28.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.28.5"></td>
<td class="ltx_td" id="S4.T3.11.1.28.6"></td>
<td class="ltx_td" id="S4.T3.11.1.28.7"></td>
<td class="ltx_td" id="S4.T3.11.1.28.8"></td>
<td class="ltx_td" id="S4.T3.11.1.28.9"></td>
<td class="ltx_td" id="S4.T3.11.1.28.10"></td>
<td class="ltx_td" id="S4.T3.11.1.28.11"></td>
<td class="ltx_td" id="S4.T3.11.1.28.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.14">Shanghai AI Lab</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.28.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.29">
<td class="ltx_td" id="S4.T3.11.1.29.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.2"><cite class="ltx_cite ltx_citemacro_citet">Ke et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib132" title="">132</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.3"><span class="ltx_text" id="S4.T3.11.1.29.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.4"><span class="ltx_text" id="S4.T3.11.1.29.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.5"><span class="ltx_text" id="S4.T3.11.1.29.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.29.6"></td>
<td class="ltx_td" id="S4.T3.11.1.29.7"></td>
<td class="ltx_td" id="S4.T3.11.1.29.8"></td>
<td class="ltx_td" id="S4.T3.11.1.29.9"></td>
<td class="ltx_td" id="S4.T3.11.1.29.10"></td>
<td class="ltx_td" id="S4.T3.11.1.29.11"></td>
<td class="ltx_td" id="S4.T3.11.1.29.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.14">Alibaba</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.29.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.30">
<td class="ltx_td" id="S4.T3.11.1.30.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.2"><cite class="ltx_cite ltx_citemacro_citet">Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib84" title="">84</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.3"><span class="ltx_text" id="S4.T3.11.1.30.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td" id="S4.T3.11.1.30.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.5"><span class="ltx_text" id="S4.T3.11.1.30.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.6"><span class="ltx_text" id="S4.T3.11.1.30.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.30.7"></td>
<td class="ltx_td" id="S4.T3.11.1.30.8"></td>
<td class="ltx_td" id="S4.T3.11.1.30.9"></td>
<td class="ltx_td" id="S4.T3.11.1.30.10"></td>
<td class="ltx_td" id="S4.T3.11.1.30.11"></td>
<td class="ltx_td" id="S4.T3.11.1.30.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.14">Alibaba</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.30.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.31">
<td class="ltx_td" id="S4.T3.11.1.31.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.2"><cite class="ltx_cite ltx_citemacro_citet">Yao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib329" title="">329</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.3"><span class="ltx_text" id="S4.T3.11.1.31.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.4"><span class="ltx_text" id="S4.T3.11.1.31.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.5"><span class="ltx_text" id="S4.T3.11.1.31.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.31.6"></td>
<td class="ltx_td" id="S4.T3.11.1.31.7"></td>
<td class="ltx_td" id="S4.T3.11.1.31.8"></td>
<td class="ltx_td" id="S4.T3.11.1.31.9"></td>
<td class="ltx_td" id="S4.T3.11.1.31.10"></td>
<td class="ltx_td" id="S4.T3.11.1.31.11"></td>
<td class="ltx_td" id="S4.T3.11.1.31.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.14">PSU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.31.15">2018</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.32">
<td class="ltx_td" id="S4.T3.11.1.32.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.2"><cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib79" title="">79</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.3"><span class="ltx_text" id="S4.T3.11.1.32.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.4"><span class="ltx_text" id="S4.T3.11.1.32.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.5"><span class="ltx_text" id="S4.T3.11.1.32.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.6"><span class="ltx_text" id="S4.T3.11.1.32.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.7"><span class="ltx_text" id="S4.T3.11.1.32.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.8"><span class="ltx_text" id="S4.T3.11.1.32.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.32.9"></td>
<td class="ltx_td" id="S4.T3.11.1.32.10"></td>
<td class="ltx_td" id="S4.T3.11.1.32.11"></td>
<td class="ltx_td" id="S4.T3.11.1.32.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.14">SWJTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.32.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.33">
<td class="ltx_td" id="S4.T3.11.1.33.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.2">DeepMob <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib249" title="">249</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.3"><span class="ltx_text" id="S4.T3.11.1.33.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.33.3.1.1" style="color:#FBEB65;">❖</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.4"><span class="ltx_text" id="S4.T3.11.1.33.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.5"><span class="ltx_text" id="S4.T3.11.1.33.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.6"><span class="ltx_text" id="S4.T3.11.1.33.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.7"><span class="ltx_text" id="S4.T3.11.1.33.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.33.8"></td>
<td class="ltx_td" id="S4.T3.11.1.33.9"></td>
<td class="ltx_td" id="S4.T3.11.1.33.10"></td>
<td class="ltx_td" id="S4.T3.11.1.33.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.12"><span class="ltx_text" id="S4.T3.11.1.33.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.13">Public Safety</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.14">SUSTech</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.33.15">2017</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.34">
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T3.11.1.34.1.1" style="background-color:#FFFFFF;">
<span class="ltx_text" id="S4.T3.11.1.34.1.1.1"><span class="ltx_text" id="S4.T3.11.1.34.1.1.1.1"></span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.34.1.1.1.2"> <span class="ltx_text" id="S4.T3.11.1.34.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.34.1.1.1.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.34.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.34.1.1.1.2.1.1.1.1">Feature</span></span>
<span class="ltx_tr" id="S4.T3.11.1.34.1.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.34.1.1.1.2.1.1.2.1">Based</span></span>
<span class="ltx_tr" id="S4.T3.11.1.34.1.1.1.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.34.1.1.1.2.1.1.3.1">Data</span></span>
<span class="ltx_tr" id="S4.T3.11.1.34.1.1.1.2.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.34.1.1.1.2.1.1.4.1">Fusion</span></span>
</span></span> <span class="ltx_text" id="S4.T3.11.1.34.1.1.1.2.2"></span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.2"><cite class="ltx_cite ltx_citemacro_citet">Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib82" title="">82</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.3"><span class="ltx_text" id="S4.T3.11.1.34.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.34.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.34.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.5"><span class="ltx_text" id="S4.T3.11.1.34.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.6"><span class="ltx_text" id="S4.T3.11.1.34.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.7"><span class="ltx_text" id="S4.T3.11.1.34.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.34.8"></td>
<td class="ltx_td" id="S4.T3.11.1.34.9"></td>
<td class="ltx_td" id="S4.T3.11.1.34.10"></td>
<td class="ltx_td" id="S4.T3.11.1.34.11"></td>
<td class="ltx_td" id="S4.T3.11.1.34.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.14">HKUST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.34.15">2019</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.35">
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.2"><cite class="ltx_cite ltx_citemacro_citet">Xi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.3"><span class="ltx_text" id="S4.T3.11.1.35.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.5"><span class="ltx_text" id="S4.T3.11.1.35.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.6"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.9"><span class="ltx_text" id="S4.T3.11.1.35.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.10"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.11"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.35.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.13">General</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.14">THU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.35.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.36">
<td class="ltx_td" id="S4.T3.11.1.36.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.2"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.3"><span class="ltx_text" id="S4.T3.11.1.36.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.36.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.36.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.36.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.5"><span class="ltx_text" id="S4.T3.11.1.36.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.36.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.7"><span class="ltx_text" id="S4.T3.11.1.36.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.36.8"></td>
<td class="ltx_td" id="S4.T3.11.1.36.9"></td>
<td class="ltx_td" id="S4.T3.11.1.36.10"></td>
<td class="ltx_td" id="S4.T3.11.1.36.11"></td>
<td class="ltx_td" id="S4.T3.11.1.36.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.36.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.37">
<td class="ltx_td" id="S4.T3.11.1.37.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.2"><cite class="ltx_cite ltx_citemacro_citet">Yuan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.3"><span class="ltx_text" id="S4.T3.11.1.37.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.37.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.37.3.1.1.1" style="color:#3C9898;">🍀</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.37.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.5"><span class="ltx_text" id="S4.T3.11.1.37.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.37.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.7"><span class="ltx_text" id="S4.T3.11.1.37.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.37.8"></td>
<td class="ltx_td" id="S4.T3.11.1.37.9"></td>
<td class="ltx_td" id="S4.T3.11.1.37.10"></td>
<td class="ltx_td" id="S4.T3.11.1.37.11"></td>
<td class="ltx_td" id="S4.T3.11.1.37.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.37.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.38">
<td class="ltx_td" id="S4.T3.11.1.38.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.2"><cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.3"><span class="ltx_text" id="S4.T3.11.1.38.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.38.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.38.3.1.1.1" style="color:#FBEB65;">❖<span class="ltx_text" id="S4.T3.11.1.38.3.1.1.1.1" style="color:#3C9898;">🍀</span></span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.38.4"></td>
<td class="ltx_td" id="S4.T3.11.1.38.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.6"><span class="ltx_text" id="S4.T3.11.1.38.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.38.7"></td>
<td class="ltx_td" id="S4.T3.11.1.38.8"></td>
<td class="ltx_td" id="S4.T3.11.1.38.9"></td>
<td class="ltx_td" id="S4.T3.11.1.38.10"></td>
<td class="ltx_td" id="S4.T3.11.1.38.11"></td>
<td class="ltx_td" id="S4.T3.11.1.38.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.14">NUS</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.38.15">2020</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.39">
<td class="ltx_td" id="S4.T3.11.1.39.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.2">GSNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.3"><span class="ltx_text" id="S4.T3.11.1.39.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.39.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.39.3.1.1.1" style="color:#3C9898;">🍀</span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.4"><span class="ltx_text" id="S4.T3.11.1.39.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.5"><span class="ltx_text" id="S4.T3.11.1.39.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.6"><span class="ltx_text" id="S4.T3.11.1.39.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.39.7"></td>
<td class="ltx_td" id="S4.T3.11.1.39.8"></td>
<td class="ltx_td" id="S4.T3.11.1.39.9"></td>
<td class="ltx_td" id="S4.T3.11.1.39.10"></td>
<td class="ltx_td" id="S4.T3.11.1.39.11"></td>
<td class="ltx_td" id="S4.T3.11.1.39.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.13">Public Safety</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.14">BJTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.39.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.40">
<td class="ltx_td" id="S4.T3.11.1.40.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.2"><cite class="ltx_cite ltx_citemacro_citet">Hashem et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib98" title="">98</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.3"><span class="ltx_text" id="S4.T3.11.1.40.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.40.3.1.1" style="color:#FBEB65;">❖</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.40.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.5"><span class="ltx_text" id="S4.T3.11.1.40.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.40.6"></td>
<td class="ltx_td" id="S4.T3.11.1.40.7"></td>
<td class="ltx_td" id="S4.T3.11.1.40.8"></td>
<td class="ltx_td" id="S4.T3.11.1.40.9"></td>
<td class="ltx_td" id="S4.T3.11.1.40.10"></td>
<td class="ltx_td" id="S4.T3.11.1.40.11"></td>
<td class="ltx_td" id="S4.T3.11.1.40.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.40.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.41">
<td class="ltx_td" id="S4.T3.11.1.41.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.2">TrajGAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib328" title="">328</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.3"><span class="ltx_text" id="S4.T3.11.1.41.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td" id="S4.T3.11.1.41.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.5"><span class="ltx_text" id="S4.T3.11.1.41.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.6"><span class="ltx_text" id="S4.T3.11.1.41.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.41.7"></td>
<td class="ltx_td" id="S4.T3.11.1.41.8"></td>
<td class="ltx_td" id="S4.T3.11.1.41.9"></td>
<td class="ltx_td" id="S4.T3.11.1.41.10"></td>
<td class="ltx_td" id="S4.T3.11.1.41.11"></td>
<td class="ltx_td" id="S4.T3.11.1.41.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.41.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.42">
<td class="ltx_td" id="S4.T3.11.1.42.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.2">RADAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib39" title="">39</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.3"><span class="ltx_text" id="S4.T3.11.1.42.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.4"><span class="ltx_text" id="S4.T3.11.1.42.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.42.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.6"><span class="ltx_text" id="S4.T3.11.1.42.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.42.7"></td>
<td class="ltx_td" id="S4.T3.11.1.42.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.9"><span class="ltx_text" id="S4.T3.11.1.42.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.42.10"></td>
<td class="ltx_td" id="S4.T3.11.1.42.11"></td>
<td class="ltx_td" id="S4.T3.11.1.42.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.14">XMU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.42.15">2018</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.43">
<td class="ltx_td" id="S4.T3.11.1.43.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.2"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib287" title="">287</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.3"><span class="ltx_text" id="S4.T3.11.1.43.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.43.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.43.3.1.1.1" style="color:#3C9898;">🍀</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.43.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.5"><span class="ltx_text" id="S4.T3.11.1.43.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.6"><span class="ltx_text" id="S4.T3.11.1.43.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.43.7"></td>
<td class="ltx_td" id="S4.T3.11.1.43.8"></td>
<td class="ltx_td" id="S4.T3.11.1.43.9"></td>
<td class="ltx_td" id="S4.T3.11.1.43.10"></td>
<td class="ltx_td" id="S4.T3.11.1.43.11"></td>
<td class="ltx_td" id="S4.T3.11.1.43.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.14">CSU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.43.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.44">
<td class="ltx_td" id="S4.T3.11.1.44.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.2"><cite class="ltx_cite ltx_citemacro_citet">Tedjopurnomo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib261" title="">261</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.3"><span class="ltx_text" id="S4.T3.11.1.44.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.4"><span class="ltx_text" id="S4.T3.11.1.44.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.5"><span class="ltx_text" id="S4.T3.11.1.44.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.44.6"></td>
<td class="ltx_td" id="S4.T3.11.1.44.7"></td>
<td class="ltx_td" id="S4.T3.11.1.44.8"></td>
<td class="ltx_td" id="S4.T3.11.1.44.9"></td>
<td class="ltx_td" id="S4.T3.11.1.44.10"></td>
<td class="ltx_td" id="S4.T3.11.1.44.11"></td>
<td class="ltx_td" id="S4.T3.11.1.44.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.14">RMIT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.44.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.45">
<td class="ltx_td" id="S4.T3.11.1.45.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.2">ERNIE-GeoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.3"><span class="ltx_text" id="S4.T3.11.1.45.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td" id="S4.T3.11.1.45.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.5"><span class="ltx_text" id="S4.T3.11.1.45.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.45.6"></td>
<td class="ltx_td" id="S4.T3.11.1.45.7"></td>
<td class="ltx_td" id="S4.T3.11.1.45.8"></td>
<td class="ltx_td" id="S4.T3.11.1.45.9"></td>
<td class="ltx_td" id="S4.T3.11.1.45.10"></td>
<td class="ltx_td" id="S4.T3.11.1.45.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.12"><span class="ltx_text" id="S4.T3.11.1.45.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.14">Baidu</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.45.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.46">
<td class="ltx_td" id="S4.T3.11.1.46.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.2">SAInf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib193" title="">193</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.3"><span class="ltx_text" id="S4.T3.11.1.46.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.46.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.46.3.1.1.1" style="color:#3C9898;">🍀</span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.4"><span class="ltx_text" id="S4.T3.11.1.46.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.46.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.6"><span class="ltx_text" id="S4.T3.11.1.46.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.46.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.8"><span class="ltx_text" id="S4.T3.11.1.46.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.46.9"></td>
<td class="ltx_td" id="S4.T3.11.1.46.10"></td>
<td class="ltx_td" id="S4.T3.11.1.46.11"></td>
<td class="ltx_td" id="S4.T3.11.1.46.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.14">JD Research</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.46.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.47">
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T3.11.1.47.1.1" style="background-color:#FFFFFF;">
<span class="ltx_text" id="S4.T3.11.1.47.1.1.1"><span class="ltx_text" id="S4.T3.11.1.47.1.1.1.1"></span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.47.1.1.1.2"> <span class="ltx_text" id="S4.T3.11.1.47.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.47.1.1.1.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.47.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.47.1.1.1.2.1.1.1.1">Alignment</span></span>
<span class="ltx_tr" id="S4.T3.11.1.47.1.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.47.1.1.1.2.1.1.2.1">Based</span></span>
<span class="ltx_tr" id="S4.T3.11.1.47.1.1.1.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.47.1.1.1.2.1.1.3.1">Data</span></span>
<span class="ltx_tr" id="S4.T3.11.1.47.1.1.1.2.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.47.1.1.1.2.1.1.4.1">Fusion</span></span>
</span></span> <span class="ltx_text" id="S4.T3.11.1.47.1.1.1.2.2"></span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.2"><cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib77" title="">77</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.3"><span class="ltx_text" id="S4.T3.11.1.47.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.47.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.4"><span class="ltx_text" id="S4.T3.11.1.47.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.5"><span class="ltx_text" id="S4.T3.11.1.47.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.47.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.7"><span class="ltx_text" id="S4.T3.11.1.47.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.8"><span class="ltx_text" id="S4.T3.11.1.47.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.47.9"></td>
<td class="ltx_td" id="S4.T3.11.1.47.10"></td>
<td class="ltx_td" id="S4.T3.11.1.47.11"></td>
<td class="ltx_td" id="S4.T3.11.1.47.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.14">SWJTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.47.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.48">
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.2"> KnowCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.3"><span class="ltx_text" id="S4.T3.11.1.48.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.48.3.1.1" style="color:#F5C9C8;">◆<span class="ltx_text" id="S4.T3.11.1.48.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.5"><span class="ltx_text" id="S4.T3.11.1.48.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.6"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.9"><span class="ltx_text" id="S4.T3.11.1.48.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.10"><span class="ltx_text" id="S4.T3.11.1.48.10.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.11"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.48.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.13">Economy</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.14">THU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.48.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.49">
<td class="ltx_td" id="S4.T3.11.1.49.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.2"> <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.3"><span class="ltx_text" id="S4.T3.11.1.49.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.49.3.1.1" style="color:#F5C9C8;">◆<span class="ltx_text" id="S4.T3.11.1.49.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.49.4"></td>
<td class="ltx_td" id="S4.T3.11.1.49.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.6"><span class="ltx_text" id="S4.T3.11.1.49.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.49.7"></td>
<td class="ltx_td" id="S4.T3.11.1.49.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.9"><span class="ltx_text" id="S4.T3.11.1.49.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.10"><span class="ltx_text" id="S4.T3.11.1.49.10.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.49.11"></td>
<td class="ltx_td" id="S4.T3.11.1.49.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.13">Economy</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.49.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.50">
<td class="ltx_td" id="S4.T3.11.1.50.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.2"> MMGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.3"><span class="ltx_text" id="S4.T3.11.1.50.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.50.3.1.1" style="color:#F5C9C8;">◆<span class="ltx_text" id="S4.T3.11.1.50.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.50.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.5"><span class="ltx_text" id="S4.T3.11.1.50.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.50.6"></td>
<td class="ltx_td" id="S4.T3.11.1.50.7"></td>
<td class="ltx_td" id="S4.T3.11.1.50.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.9"><span class="ltx_text" id="S4.T3.11.1.50.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.50.10"></td>
<td class="ltx_td" id="S4.T3.11.1.50.11"></td>
<td class="ltx_td" id="S4.T3.11.1.50.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.50.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.51">
<td class="ltx_td" id="S4.T3.11.1.51.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.2"> ReMVC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.3"><span class="ltx_text" id="S4.T3.11.1.51.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.51.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.51.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.51.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.5"><span class="ltx_text" id="S4.T3.11.1.51.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.51.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.7"><span class="ltx_text" id="S4.T3.11.1.51.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.51.8"></td>
<td class="ltx_td" id="S4.T3.11.1.51.9"></td>
<td class="ltx_td" id="S4.T3.11.1.51.10"></td>
<td class="ltx_td" id="S4.T3.11.1.51.11"></td>
<td class="ltx_td" id="S4.T3.11.1.51.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.14">NTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.51.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.52">
<td class="ltx_td" id="S4.T3.11.1.52.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.2"> HMTRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib173" title="">173</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.3"><span class="ltx_text" id="S4.T3.11.1.52.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.4"><span class="ltx_text" id="S4.T3.11.1.52.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.52.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.6"><span class="ltx_text" id="S4.T3.11.1.52.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.52.7"></td>
<td class="ltx_td" id="S4.T3.11.1.52.8"></td>
<td class="ltx_td" id="S4.T3.11.1.52.9"></td>
<td class="ltx_td" id="S4.T3.11.1.52.10"></td>
<td class="ltx_td" id="S4.T3.11.1.52.11"></td>
<td class="ltx_td" id="S4.T3.11.1.52.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.14">UCF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.52.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.53">
<td class="ltx_td" id="S4.T3.11.1.53.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.2"> <cite class="ltx_cite ltx_citemacro_citet">Mao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib196" title="">196</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.3"><span class="ltx_text" id="S4.T3.11.1.53.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.4"><span class="ltx_text" id="S4.T3.11.1.53.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.53.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.6"><span class="ltx_text" id="S4.T3.11.1.53.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.53.7"></td>
<td class="ltx_td" id="S4.T3.11.1.53.8"></td>
<td class="ltx_td" id="S4.T3.11.1.53.9"></td>
<td class="ltx_td" id="S4.T3.11.1.53.10"></td>
<td class="ltx_td" id="S4.T3.11.1.53.11"></td>
<td class="ltx_td" id="S4.T3.11.1.53.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.14">Shanghai AI Lab</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.53.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.54">
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T3.11.1.54.1.1" style="background-color:#FFFFFF;">
<span class="ltx_text" id="S4.T3.11.1.54.1.1.1"><span class="ltx_text" id="S4.T3.11.1.54.1.1.1.1"></span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.54.1.1.1.2"> <span class="ltx_text" id="S4.T3.11.1.54.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.54.1.1.1.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.54.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.54.1.1.1.2.1.1.1.1">Contrast</span></span>
<span class="ltx_tr" id="S4.T3.11.1.54.1.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.54.1.1.1.2.1.1.2.1">Baed</span></span>
<span class="ltx_tr" id="S4.T3.11.1.54.1.1.1.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.54.1.1.1.2.1.1.3.1">Data</span></span>
<span class="ltx_tr" id="S4.T3.11.1.54.1.1.1.2.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.54.1.1.1.2.1.1.4.1">Fusion</span></span>
</span></span> <span class="ltx_text" id="S4.T3.11.1.54.1.1.1.2.2"></span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.2"> UrbanSTC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.3"><span class="ltx_text" id="S4.T3.11.1.54.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.4"><span class="ltx_text" id="S4.T3.11.1.54.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.54.5"></td>
<td class="ltx_td" id="S4.T3.11.1.54.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.7"><span class="ltx_text" id="S4.T3.11.1.54.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.54.8"></td>
<td class="ltx_td" id="S4.T3.11.1.54.9"></td>
<td class="ltx_td" id="S4.T3.11.1.54.10"></td>
<td class="ltx_td" id="S4.T3.11.1.54.11"></td>
<td class="ltx_td" id="S4.T3.11.1.54.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.14">JD Research</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.54.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.55">
<td class="ltx_td" id="S4.T3.11.1.55.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.2"> UrbanCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.3"><span class="ltx_text" id="S4.T3.11.1.55.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.55.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.55.4"></td>
<td class="ltx_td" id="S4.T3.11.1.55.5"></td>
<td class="ltx_td" id="S4.T3.11.1.55.6"></td>
<td class="ltx_td" id="S4.T3.11.1.55.7"></td>
<td class="ltx_td" id="S4.T3.11.1.55.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.9"><span class="ltx_text" id="S4.T3.11.1.55.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.55.10"></td>
<td class="ltx_td" id="S4.T3.11.1.55.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.12"><span class="ltx_text" id="S4.T3.11.1.55.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.14">HKUST(GZ)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.55.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.56">
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.2"> SG-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib378" title="">378</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.3"><span class="ltx_text" id="S4.T3.11.1.56.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.5"><span class="ltx_text" id="S4.T3.11.1.56.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.6"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.9"><span class="ltx_text" id="S4.T3.11.1.56.9.1" style="color:#AADCE0;">●</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.10"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.11"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.11.1.56.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.13">Urban Planning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.14">NUS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.1.56.15">2020</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.57">
<td class="ltx_td" id="S4.T3.11.1.57.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.2"> ActSTD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib354" title="">354</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.3"><span class="ltx_text" id="S4.T3.11.1.57.3.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.57.3.1.1" style="color:#F5C9C8;">◆</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.4"><span class="ltx_text" id="S4.T3.11.1.57.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.57.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.6"><span class="ltx_text" id="S4.T3.11.1.57.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.57.7"></td>
<td class="ltx_td" id="S4.T3.11.1.57.8"></td>
<td class="ltx_td" id="S4.T3.11.1.57.9"></td>
<td class="ltx_td" id="S4.T3.11.1.57.10"></td>
<td class="ltx_td" id="S4.T3.11.1.57.11"></td>
<td class="ltx_td" id="S4.T3.11.1.57.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.14">THU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.57.15">2022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.58">
<td class="ltx_td" id="S4.T3.11.1.58.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.2"> DiffSTG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib299" title="">299</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.3"><span class="ltx_text" id="S4.T3.11.1.58.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.58.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.4"><span class="ltx_text" id="S4.T3.11.1.58.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.58.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.6"><span class="ltx_text" id="S4.T3.11.1.58.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.58.7"></td>
<td class="ltx_td" id="S4.T3.11.1.58.8"></td>
<td class="ltx_td" id="S4.T3.11.1.58.9"></td>
<td class="ltx_td" id="S4.T3.11.1.58.10"></td>
<td class="ltx_td" id="S4.T3.11.1.58.11"></td>
<td class="ltx_td" id="S4.T3.11.1.58.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.13">General</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.14">BJTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.58.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.59">
<td class="ltx_td" id="S4.T3.11.1.59.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.2"> CP-Route <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib297" title="">297</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.3"><span class="ltx_text" id="S4.T3.11.1.59.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.59.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.4"><span class="ltx_text" id="S4.T3.11.1.59.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.59.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.6"><span class="ltx_text" id="S4.T3.11.1.59.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.59.7"></td>
<td class="ltx_td" id="S4.T3.11.1.59.8"></td>
<td class="ltx_td" id="S4.T3.11.1.59.9"></td>
<td class="ltx_td" id="S4.T3.11.1.59.10"></td>
<td class="ltx_td" id="S4.T3.11.1.59.11"></td>
<td class="ltx_td" id="S4.T3.11.1.59.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.14">BJTU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.59.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.60">
<td class="ltx_td" id="S4.T3.11.1.60.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.2"> G2PTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib304" title="">304</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.3"><span class="ltx_text" id="S4.T3.11.1.60.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.60.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.60.3.1.1.1" style="color:#F5C9C8;">◆</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.60.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.5"><span class="ltx_text" id="S4.T3.11.1.60.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.6"><span class="ltx_text" id="S4.T3.11.1.60.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.60.7"></td>
<td class="ltx_td" id="S4.T3.11.1.60.8"></td>
<td class="ltx_td" id="S4.T3.11.1.60.9"></td>
<td class="ltx_td" id="S4.T3.11.1.60.10"></td>
<td class="ltx_td" id="S4.T3.11.1.60.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.12"><span class="ltx_text" id="S4.T3.11.1.60.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.14">Cainiao</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.60.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.61">
<td class="ltx_td" id="S4.T3.11.1.61.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.2"> DiffUFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.3"><span class="ltx_text" id="S4.T3.11.1.61.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.61.3.1.1" style="color:#959595;">▲</span></span></td>
<td class="ltx_td" id="S4.T3.11.1.61.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.5"><span class="ltx_text" id="S4.T3.11.1.61.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.6"><span class="ltx_text" id="S4.T3.11.1.61.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.61.7"></td>
<td class="ltx_td" id="S4.T3.11.1.61.8"></td>
<td class="ltx_td" id="S4.T3.11.1.61.9"></td>
<td class="ltx_td" id="S4.T3.11.1.61.10"></td>
<td class="ltx_td" id="S4.T3.11.1.61.11"></td>
<td class="ltx_td" id="S4.T3.11.1.61.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.14">CSU</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.61.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.62">
<td class="ltx_td" id="S4.T3.11.1.62.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.2"> DP-TFI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib320" title="">320</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.3"><span class="ltx_text" id="S4.T3.11.1.62.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.62.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.62.3.1.1.1" style="color:#3C9898;">🍀</span></span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.4"><span class="ltx_text" id="S4.T3.11.1.62.4.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.62.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.6"><span class="ltx_text" id="S4.T3.11.1.62.6.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.62.7"></td>
<td class="ltx_td" id="S4.T3.11.1.62.8"></td>
<td class="ltx_td" id="S4.T3.11.1.62.9"></td>
<td class="ltx_td" id="S4.T3.11.1.62.10"></td>
<td class="ltx_td" id="S4.T3.11.1.62.11"></td>
<td class="ltx_td" id="S4.T3.11.1.62.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.14">UESTC</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.62.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.63">
<td class="ltx_td" id="S4.T3.11.1.63.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.2"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib273" title="">273</a>]</cite></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.3"><span class="ltx_text" id="S4.T3.11.1.63.3.1" style="color:#A0E0E0;">✤<span class="ltx_text" id="S4.T3.11.1.63.3.1.1" style="color:#959595;">▲<span class="ltx_text" id="S4.T3.11.1.63.3.1.1.1" style="color:#FBEB65;">❖</span></span></span></td>
<td class="ltx_td" id="S4.T3.11.1.63.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.5"><span class="ltx_text" id="S4.T3.11.1.63.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.63.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.7"><span class="ltx_text" id="S4.T3.11.1.63.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.8"><span class="ltx_text" id="S4.T3.11.1.63.8.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.63.9"></td>
<td class="ltx_td" id="S4.T3.11.1.63.10"></td>
<td class="ltx_td" id="S4.T3.11.1.63.11"></td>
<td class="ltx_td" id="S4.T3.11.1.63.12"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.13">Urban Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.14">UCF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.63.15">2021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.64">
<td class="ltx_td" id="S4.T3.11.1.64.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.2">Chattraffc<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib358" title="">358</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.3"><span class="ltx_text" id="S4.T3.11.1.64.3.1" style="color:#A0E0E0;">✤</span></td>
<td class="ltx_td" id="S4.T3.11.1.64.4"></td>
<td class="ltx_td" id="S4.T3.11.1.64.5"></td>
<td class="ltx_td" id="S4.T3.11.1.64.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.7"><span class="ltx_text" id="S4.T3.11.1.64.7.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td" id="S4.T3.11.1.64.8"></td>
<td class="ltx_td" id="S4.T3.11.1.64.9"></td>
<td class="ltx_td" id="S4.T3.11.1.64.10"></td>
<td class="ltx_td" id="S4.T3.11.1.64.11"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.12"><span class="ltx_text" id="S4.T3.11.1.64.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.13">Transportation</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.14">BJUT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.1.64.15">2023</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.1.65">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T3.11.1.65.1.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S4.T3.11.1.65.1.1.1"></span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.11.1.65.1.1.2"> <span class="ltx_text" id="S4.T3.11.1.65.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.11.1.65.1.1.2.1.1">
<span class="ltx_tr" id="S4.T3.11.1.65.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.65.1.1.2.1.1.1.1">Generation</span></span>
<span class="ltx_tr" id="S4.T3.11.1.65.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.65.1.1.2.1.1.2.1">Based</span></span>
<span class="ltx_tr" id="S4.T3.11.1.65.1.1.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.65.1.1.2.1.1.3.1">Data</span></span>
<span class="ltx_tr" id="S4.T3.11.1.65.1.1.2.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.11.1.65.1.1.2.1.1.4.1">Fusion</span></span>
</span></span> <span class="ltx_text" id="S4.T3.11.1.65.1.1.2.2"></span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.2"> MGEO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib61" title="">61</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.3"><span class="ltx_text" id="S4.T3.11.1.65.3.1" style="color:#959595;">▲</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.5"><span class="ltx_text" id="S4.T3.11.1.65.5.1" style="color:#FFE6B7;">●</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.6"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.7"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.8"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.9"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.10"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.11.1.65.11"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.12"><span class="ltx_text" id="S4.T3.11.1.65.12.1" style="color:#528FAD;">●</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.13">General</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.14">Alibaba</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.1.65.15">2023</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.2.3">Feature-Based Data Fusion</span> is a straightforward fusion strategy, involving the combination of features from diverse sources such as sensor, visual, and textual data. The core idea is to consolidate raw or processed data features from various sources, forming comprehensive characteristics of studied urban objects. This integration is crucial for enhancing the predictive capabilities of urban models, facilitating complex analyses such as traffic projections and socioeconomic pattern recognition. The distinctive feature of this category lies in directly merging features using methods such as addition, multiplication, concatenation, or graph-based operations. <span class="ltx_text" id="S4.SS1.p2.2.2" style="color:#000000;">This type of fusion has relatively low computational complexity, primarily depending on the feature dimensions and the specific fusion operation, typically linear <math alttext="O(n)" class="ltx_Math" display="inline" id="S4.SS1.p2.1.1.m1.1"><semantics id="S4.SS1.p2.1.1.m1.1a"><mrow id="S4.SS1.p2.1.1.m1.1.2" xref="S4.SS1.p2.1.1.m1.1.2.cmml"><mi id="S4.SS1.p2.1.1.m1.1.2.2" mathcolor="#000000" xref="S4.SS1.p2.1.1.m1.1.2.2.cmml">O</mi><mo id="S4.SS1.p2.1.1.m1.1.2.1" xref="S4.SS1.p2.1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p2.1.1.m1.1.2.3.2" xref="S4.SS1.p2.1.1.m1.1.2.cmml"><mo id="S4.SS1.p2.1.1.m1.1.2.3.2.1" mathcolor="#000000" stretchy="false" xref="S4.SS1.p2.1.1.m1.1.2.cmml">(</mo><mi id="S4.SS1.p2.1.1.m1.1.1" mathcolor="#000000" xref="S4.SS1.p2.1.1.m1.1.1.cmml">n</mi><mo id="S4.SS1.p2.1.1.m1.1.2.3.2.2" mathcolor="#000000" stretchy="false" xref="S4.SS1.p2.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.1.m1.1b"><apply id="S4.SS1.p2.1.1.m1.1.2.cmml" xref="S4.SS1.p2.1.1.m1.1.2"><times id="S4.SS1.p2.1.1.m1.1.2.1.cmml" xref="S4.SS1.p2.1.1.m1.1.2.1"></times><ci id="S4.SS1.p2.1.1.m1.1.2.2.cmml" xref="S4.SS1.p2.1.1.m1.1.2.2">𝑂</ci><ci id="S4.SS1.p2.1.1.m1.1.1.cmml" xref="S4.SS1.p2.1.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math>. Here, <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p2.2.2.m2.1"><semantics id="S4.SS1.p2.2.2.m2.1a"><mi id="S4.SS1.p2.2.2.m2.1.1" mathcolor="#000000" xref="S4.SS1.p2.2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.2.m2.1b"><ci id="S4.SS1.p2.2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.2.m2.1d">italic_n</annotation></semantics></math> represents the number of feature dimensions involved in the fusion process, indicating total number of features being combined from various sources.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.2.3">Alignment-based Data Fusion</span> aims to identify a shared feature space or structure among diverse data representations, enabling their alignment or integration. This involves the model learning to transform information from one source to another for semantic consistency. For instance, in tasks merging images and text, the model must align visual features with textual descriptions, often achieved through a multi-modal embedding space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib229" title="">229</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib14" title="">14</a>]</cite>. The widely used attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib268" title="">268</a>]</cite> exemplifies alignment-based data fusion, assigning weights to different input parts to prioritize them. In tasks like image captioning, cross-modal attention aligns and weights specific image regions with the textual description <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib190" title="">190</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib361" title="">361</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib406" title="">406</a>]</cite>. <span class="ltx_text" id="S4.SS1.p3.2.2" style="color:#000000;">The computational complexity could be higher due to extensive matrix operations, typically quadratic <math alttext="O(n^{2})" class="ltx_Math" display="inline" id="S4.SS1.p3.1.1.m1.1"><semantics id="S4.SS1.p3.1.1.m1.1a"><mrow id="S4.SS1.p3.1.1.m1.1.1" xref="S4.SS1.p3.1.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.SS1.p3.1.1.m1.1.1.3.cmml">O</mi><mo id="S4.SS1.p3.1.1.m1.1.1.2" xref="S4.SS1.p3.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p3.1.1.m1.1.1.1.1" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS1.p3.1.1.m1.1.1.1.1.2" mathcolor="#000000" stretchy="false" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p3.1.1.m1.1.1.1.1.1" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p3.1.1.m1.1.1.1.1.1.2" mathcolor="#000000" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p3.1.1.m1.1.1.1.1.1.3" mathcolor="#000000" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S4.SS1.p3.1.1.m1.1.1.1.1.3" mathcolor="#000000" stretchy="false" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.1.m1.1b"><apply id="S4.SS1.p3.1.1.m1.1.1.cmml" xref="S4.SS1.p3.1.1.m1.1.1"><times id="S4.SS1.p3.1.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.1.m1.1.1.2"></times><ci id="S4.SS1.p3.1.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.1.m1.1.1.3">𝑂</ci><apply id="S4.SS1.p3.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p3.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.2">𝑛</ci><cn id="S4.SS1.p3.1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.1.m1.1c">O(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.1.m1.1d">italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p3.2.2.m2.1"><semantics id="S4.SS1.p3.2.2.m2.1a"><mi id="S4.SS1.p3.2.2.m2.1.1" mathcolor="#000000" xref="S4.SS1.p3.2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.2.m2.1b"><ci id="S4.SS1.p3.2.2.m2.1.1.cmml" xref="S4.SS1.p3.2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.2.m2.1d">italic_n</annotation></semantics></math> is the input sequence length.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.2.3">Contrast-based Data Fusion</span> employs the contrastive learning framework to enhance feature discriminability at the sample level, contrasting with alignment-based data fusion that focuses on feature-level alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib202" title="">202</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib147" title="">147</a>]</cite>. By training the model to differentiate categories or samples, it identifies key distinguishing features. In urban computing, this involves comparing traffic patterns or environmental variables across different areas, time points, and modalities. The pairing of positive and negative samples enables the model to learn and excel in distinguishing complex urban situations, refining the acuity of computational tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib373" title="">373</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib258" title="">258</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib216" title="">216</a>]</cite>. <span class="ltx_text" id="S4.SS1.p4.2.2" style="color:#000000;">Utilizing contrastive learning to enhance feature discriminability, this method requires significant computation for sample pairing and similarity calculations, with complexity often quadratic <math alttext="O(n^{2})" class="ltx_Math" display="inline" id="S4.SS1.p4.1.1.m1.1"><semantics id="S4.SS1.p4.1.1.m1.1a"><mrow id="S4.SS1.p4.1.1.m1.1.1" xref="S4.SS1.p4.1.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.SS1.p4.1.1.m1.1.1.3.cmml">O</mi><mo id="S4.SS1.p4.1.1.m1.1.1.2" xref="S4.SS1.p4.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p4.1.1.m1.1.1.1.1" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS1.p4.1.1.m1.1.1.1.1.2" mathcolor="#000000" stretchy="false" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p4.1.1.m1.1.1.1.1.1" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p4.1.1.m1.1.1.1.1.1.2" mathcolor="#000000" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p4.1.1.m1.1.1.1.1.1.3" mathcolor="#000000" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S4.SS1.p4.1.1.m1.1.1.1.1.3" mathcolor="#000000" stretchy="false" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.1.m1.1b"><apply id="S4.SS1.p4.1.1.m1.1.1.cmml" xref="S4.SS1.p4.1.1.m1.1.1"><times id="S4.SS1.p4.1.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.1.m1.1.1.2"></times><ci id="S4.SS1.p4.1.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.1.m1.1.1.3">𝑂</ci><apply id="S4.SS1.p4.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p4.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.2">𝑛</ci><cn id="S4.SS1.p4.1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p4.1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.1.m1.1c">O(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.1.m1.1d">italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>. Here, <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p4.2.2.m2.1"><semantics id="S4.SS1.p4.2.2.m2.1a"><mi id="S4.SS1.p4.2.2.m2.1.1" mathcolor="#000000" xref="S4.SS1.p4.2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.2.m2.1b"><ci id="S4.SS1.p4.2.2.m2.1.1.cmml" xref="S4.SS1.p4.2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.2.m2.1d">italic_n</annotation></semantics></math> represents the number of samples involved in the contrastive learning process, indicating the total number of data points being compared.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.2.3">Generation-based Data Fusion</span> utilizes deep learning’s creative capacity to generate one urban modality under the condition of the same or other modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib265" title="">265</a>]</cite>. In urban computing, this approach proves beneficial for simulating diverse scenarios, such as crafting traffic patterns under various circumstances and assessing urban planning outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib355" title="">355</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib5" title="">5</a>]</cite>. The generative methods involved in urban multi-modal fusion encompass mask modeling, diffusion, and LLM-enhanced techniques. <span class="ltx_text" id="S4.SS1.p5.2.2" style="color:#000000;">Employing generative models such as GANs and VAEs to simulate urban scenarios, this approach involves highly complex training and optimization processes, leading to very high computational complexity, also generally quadratic <math alttext="O(n^{2})" class="ltx_Math" display="inline" id="S4.SS1.p5.1.1.m1.1"><semantics id="S4.SS1.p5.1.1.m1.1a"><mrow id="S4.SS1.p5.1.1.m1.1.1" xref="S4.SS1.p5.1.1.m1.1.1.cmml"><mi id="S4.SS1.p5.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.SS1.p5.1.1.m1.1.1.3.cmml">O</mi><mo id="S4.SS1.p5.1.1.m1.1.1.2" xref="S4.SS1.p5.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p5.1.1.m1.1.1.1.1" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS1.p5.1.1.m1.1.1.1.1.2" mathcolor="#000000" stretchy="false" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p5.1.1.m1.1.1.1.1.1" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p5.1.1.m1.1.1.1.1.1.2" mathcolor="#000000" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p5.1.1.m1.1.1.1.1.1.3" mathcolor="#000000" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S4.SS1.p5.1.1.m1.1.1.1.1.3" mathcolor="#000000" stretchy="false" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.1.m1.1b"><apply id="S4.SS1.p5.1.1.m1.1.1.cmml" xref="S4.SS1.p5.1.1.m1.1.1"><times id="S4.SS1.p5.1.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.1.m1.1.1.2"></times><ci id="S4.SS1.p5.1.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.1.m1.1.1.3">𝑂</ci><apply id="S4.SS1.p5.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p5.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.2">𝑛</ci><cn id="S4.SS1.p5.1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p5.1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.1.m1.1c">O(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.1.m1.1d">italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> or higher. Specifically, <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p5.2.2.m2.1"><semantics id="S4.SS1.p5.2.2.m2.1a"><mi id="S4.SS1.p5.2.2.m2.1.1" mathcolor="#000000" xref="S4.SS1.p5.2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.2.m2.1b"><ci id="S4.SS1.p5.2.2.m2.1.1.cmml" xref="S4.SS1.p5.2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.2.2.m2.1d">italic_n</annotation></semantics></math> indicates the number of data points or parameters involved in the training and optimization process of the generative model, indicating the scale of the data used to simulate urban scenarios.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Feature-based Data Fusion</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Feature-based fusion integrates information from different modalities through methods like <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">feature addition</span>, emphasizing equal importance, and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">feature multiplication</span>, highlighting joint significance (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS1" title="4.2.1 Feature Addition and Multiplication ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>). Additionally, techniques such as <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">feature concatenation</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS2" title="4.2.2 Feature Concatenation ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>) and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">graph-based fusion</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS2.SSS3" title="4.2.3 Graph-based Data Fusion ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>) enhance multi-modal understanding by combining feature vectors or leveraging graph structures to represent inter-modal connections.
<span class="ltx_text" id="S4.SS2.p1.1.5" style="color:#000000;">This fusion approach is characterized by its simplicity and efficiency, making it particularly advantageous for applications requiring high real-time performance, such as transportation. However, it is relatively coarse and lacks the ability to capture and align details accurately, which may lead to underperformance in tasks necessitating the amalgamation of diverse datasets, such as urban planning.</span></p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Feature Addition and Multiplication</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.5">Element-wise addition is a fundamental operation among multi-modal fusion techniques, where corresponding elements of vectors or matrices from different modalities are summed to create a fused representation. Given two vectors <math alttext="\mathbf{X}=[x_{1},x_{2},\ldots,x_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.4"><semantics id="S4.SS2.SSS1.p1.1.m1.4a"><mrow id="S4.SS2.SSS1.p1.1.m1.4.4" xref="S4.SS2.SSS1.p1.1.m1.4.4.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.4.4.5" xref="S4.SS2.SSS1.p1.1.m1.4.4.5.cmml">𝐗</mi><mo id="S4.SS2.SSS1.p1.1.m1.4.4.4" xref="S4.SS2.SSS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.1.m1.4.4.3.3" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml"><mo id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">[</mo><msub id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.5" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.2" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.3" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.6" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS1.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.7" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.2" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.3" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.4b"><apply id="S4.SS2.SSS1.p1.1.m1.4.4.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4"><eq id="S4.SS2.SSS1.p1.1.m1.4.4.4.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.4"></eq><ci id="S4.SS2.SSS1.p1.1.m1.4.4.5.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.5">𝐗</ci><list id="S4.SS2.SSS1.p1.1.m1.4.4.3.4.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3"><apply id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">…</ci><apply id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.4c">\mathbf{X}=[x_{1},x_{2},\ldots,x_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.4d">bold_X = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="\mathbf{Y}=[y_{1},y_{2},\ldots,y_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.2.m2.4"><semantics id="S4.SS2.SSS1.p1.2.m2.4a"><mrow id="S4.SS2.SSS1.p1.2.m2.4.4" xref="S4.SS2.SSS1.p1.2.m2.4.4.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.4.4.5" xref="S4.SS2.SSS1.p1.2.m2.4.4.5.cmml">𝐘</mi><mo id="S4.SS2.SSS1.p1.2.m2.4.4.4" xref="S4.SS2.SSS1.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.2.m2.4.4.3.3" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml"><mo id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml">[</mo><msub id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.2" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.3" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.5" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.2" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.2.cmml">y</mi><mn id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.3" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.6" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS1.p1.2.m2.1.1" mathvariant="normal" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">…</mi><mo id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.7" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.2" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.2.cmml">y</mi><mi id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.3" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.4b"><apply id="S4.SS2.SSS1.p1.2.m2.4.4.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4"><eq id="S4.SS2.SSS1.p1.2.m2.4.4.4.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.4"></eq><ci id="S4.SS2.SSS1.p1.2.m2.4.4.5.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.5">𝐘</ci><list id="S4.SS2.SSS1.p1.2.m2.4.4.3.4.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3"><apply id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.2">𝑦</ci><cn id="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.2">𝑦</ci><cn id="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">…</ci><apply id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.2">𝑦</ci><ci id="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.4c">\mathbf{Y}=[y_{1},y_{2},\ldots,y_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.2.m2.4d">bold_Y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> representing features from different modalities, the element-wise addition operation results in a new vector <math alttext="\mathbf{Z}=[z_{1},z_{2},\ldots,z_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.3.m3.4"><semantics id="S4.SS2.SSS1.p1.3.m3.4a"><mrow id="S4.SS2.SSS1.p1.3.m3.4.4" xref="S4.SS2.SSS1.p1.3.m3.4.4.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.4.4.5" xref="S4.SS2.SSS1.p1.3.m3.4.4.5.cmml">𝐙</mi><mo id="S4.SS2.SSS1.p1.3.m3.4.4.4" xref="S4.SS2.SSS1.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.3.m3.4.4.3.3" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml"><mo id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml">[</mo><msub id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.2" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.2.cmml">z</mi><mn id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.3" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.5" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.2" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.2.cmml">z</mi><mn id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.3" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.6" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS1.p1.3.m3.1.1" mathvariant="normal" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">…</mi><mo id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.7" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.2" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.2.cmml">z</mi><mi id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.3" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.4b"><apply id="S4.SS2.SSS1.p1.3.m3.4.4.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4"><eq id="S4.SS2.SSS1.p1.3.m3.4.4.4.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.4"></eq><ci id="S4.SS2.SSS1.p1.3.m3.4.4.5.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.5">𝐙</ci><list id="S4.SS2.SSS1.p1.3.m3.4.4.3.4.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3"><apply id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.2">𝑧</ci><cn id="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.2">𝑧</ci><cn id="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">…</ci><apply id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.2">𝑧</ci><ci id="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS1.p1.3.m3.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.4c">\mathbf{Z}=[z_{1},z_{2},\ldots,z_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.3.m3.4d">bold_Z = [ italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, where <math alttext="z_{i}=x_{i}+y_{i}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.4.m4.1"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mrow id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml"><msub id="S4.SS2.SSS1.p1.4.m4.1.1.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.2.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.2.cmml">z</mi><mi id="S4.SS2.SSS1.p1.4.m4.1.1.2.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS1.p1.4.m4.1.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.4.m4.1.1.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml"><msub id="S4.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.2.cmml">x</mi><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS1.p1.4.m4.1.1.3.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">+</mo><msub id="S4.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.2.cmml">y</mi><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><apply id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1"><eq id="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.1"></eq><apply id="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.2">𝑧</ci><ci id="S4.SS2.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3"><plus id="S4.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.1"></plus><apply id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.2">𝑥</ci><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.3">𝑖</ci></apply><apply id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.2">𝑦</ci><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">z_{i}=x_{i}+y_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.4.m4.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="i=1,2,\ldots,n" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.5.m5.4"><semantics id="S4.SS2.SSS1.p1.5.m5.4a"><mrow id="S4.SS2.SSS1.p1.5.m5.4.5" xref="S4.SS2.SSS1.p1.5.m5.4.5.cmml"><mi id="S4.SS2.SSS1.p1.5.m5.4.5.2" xref="S4.SS2.SSS1.p1.5.m5.4.5.2.cmml">i</mi><mo id="S4.SS2.SSS1.p1.5.m5.4.5.1" xref="S4.SS2.SSS1.p1.5.m5.4.5.1.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.5.m5.4.5.3.2" xref="S4.SS2.SSS1.p1.5.m5.4.5.3.1.cmml"><mn id="S4.SS2.SSS1.p1.5.m5.1.1" xref="S4.SS2.SSS1.p1.5.m5.1.1.cmml">1</mn><mo id="S4.SS2.SSS1.p1.5.m5.4.5.3.2.1" xref="S4.SS2.SSS1.p1.5.m5.4.5.3.1.cmml">,</mo><mn id="S4.SS2.SSS1.p1.5.m5.2.2" xref="S4.SS2.SSS1.p1.5.m5.2.2.cmml">2</mn><mo id="S4.SS2.SSS1.p1.5.m5.4.5.3.2.2" xref="S4.SS2.SSS1.p1.5.m5.4.5.3.1.cmml">,</mo><mi id="S4.SS2.SSS1.p1.5.m5.3.3" mathvariant="normal" xref="S4.SS2.SSS1.p1.5.m5.3.3.cmml">…</mi><mo id="S4.SS2.SSS1.p1.5.m5.4.5.3.2.3" xref="S4.SS2.SSS1.p1.5.m5.4.5.3.1.cmml">,</mo><mi id="S4.SS2.SSS1.p1.5.m5.4.4" xref="S4.SS2.SSS1.p1.5.m5.4.4.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.5.m5.4b"><apply id="S4.SS2.SSS1.p1.5.m5.4.5.cmml" xref="S4.SS2.SSS1.p1.5.m5.4.5"><eq id="S4.SS2.SSS1.p1.5.m5.4.5.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.4.5.1"></eq><ci id="S4.SS2.SSS1.p1.5.m5.4.5.2.cmml" xref="S4.SS2.SSS1.p1.5.m5.4.5.2">𝑖</ci><list id="S4.SS2.SSS1.p1.5.m5.4.5.3.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.4.5.3.2"><cn id="S4.SS2.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS2.SSS1.p1.5.m5.1.1">1</cn><cn id="S4.SS2.SSS1.p1.5.m5.2.2.cmml" type="integer" xref="S4.SS2.SSS1.p1.5.m5.2.2">2</cn><ci id="S4.SS2.SSS1.p1.5.m5.3.3.cmml" xref="S4.SS2.SSS1.p1.5.m5.3.3">…</ci><ci id="S4.SS2.SSS1.p1.5.m5.4.4.cmml" xref="S4.SS2.SSS1.p1.5.m5.4.4">𝑛</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.5.m5.4c">i=1,2,\ldots,n</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.5.m5.4d">italic_i = 1 , 2 , … , italic_n</annotation></semantics></math>. Mathematically, the element-wise addition operation is expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{Z}=\mathbf{X}+\mathbf{Y}." class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">𝐙</mi><mo id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml">𝐗</mi><mo id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><mi id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml">𝐘</mi></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" lspace="0em" xref="S4.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"></eq><ci id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2">𝐙</ci><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><plus id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></plus><ci id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2">𝐗</ci><ci id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3">𝐘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\mathbf{Z}=\mathbf{X}+\mathbf{Y}.</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">bold_Z = bold_X + bold_Y .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS1.p1.6">Element-wise multiplication varies from the resulting new vector <math alttext="\mathbf{Z}=[x_{1}y_{1},x_{2}y_{2},\ldots,x_{n}y_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.6.m1.4"><semantics id="S4.SS2.SSS1.p1.6.m1.4a"><mrow id="S4.SS2.SSS1.p1.6.m1.4.4" xref="S4.SS2.SSS1.p1.6.m1.4.4.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.4.4.5" xref="S4.SS2.SSS1.p1.6.m1.4.4.5.cmml">𝐙</mi><mo id="S4.SS2.SSS1.p1.6.m1.4.4.4" xref="S4.SS2.SSS1.p1.6.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS1.p1.6.m1.4.4.3.3" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml"><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml">[</mo><mrow id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.cmml"><msub id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.2" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.2.cmml">x</mi><mn id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.3" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.1" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.1.cmml">⁢</mo><msub id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.2" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.2.cmml">y</mi><mn id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.3" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.5" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml">,</mo><mrow id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.cmml"><msub id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.2" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.2.cmml">x</mi><mn id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.3" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.1" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.1.cmml">⁢</mo><msub id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.2" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.2.cmml">y</mi><mn id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.3" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.3.cmml">2</mn></msub></mrow><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.6" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS1.p1.6.m1.1.1" mathvariant="normal" xref="S4.SS2.SSS1.p1.6.m1.1.1.cmml">…</mi><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.7" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml">,</mo><mrow id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.cmml"><msub id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.2" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.2.cmml">x</mi><mi id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.3" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.3.cmml">n</mi></msub><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.1" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.1.cmml">⁢</mo><msub id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.cmml"><mi id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.2" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.2.cmml">y</mi><mi id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.3" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.3.cmml">n</mi></msub></mrow><mo id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.6.m1.4b"><apply id="S4.SS2.SSS1.p1.6.m1.4.4.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4"><eq id="S4.SS2.SSS1.p1.6.m1.4.4.4.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.4"></eq><ci id="S4.SS2.SSS1.p1.6.m1.4.4.5.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.5">𝐙</ci><list id="S4.SS2.SSS1.p1.6.m1.4.4.3.4.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3"><apply id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1"><times id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.1"></times><apply id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.2">𝑥</ci><cn id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.2.3">1</cn></apply><apply id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.2">𝑦</ci><cn id="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.6.m1.2.2.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2"><times id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.1"></times><apply id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.2">𝑥</ci><cn id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.2.3">2</cn></apply><apply id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.2">𝑦</ci><cn id="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.6.m1.3.3.2.2.2.3.3">2</cn></apply></apply><ci id="S4.SS2.SSS1.p1.6.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.1.1">…</ci><apply id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3"><times id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.1"></times><apply id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.2">𝑥</ci><ci id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.2.3">𝑛</ci></apply><apply id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.1.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.2.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.2">𝑦</ci><ci id="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.3.cmml" xref="S4.SS2.SSS1.p1.6.m1.4.4.3.3.3.3.3">𝑛</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.6.m1.4c">\mathbf{Z}=[x_{1}y_{1},x_{2}y_{2},\ldots,x_{n}y_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.6.m1.4d">bold_Z = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>. Mathematically, such an operation can be expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{Z}=\mathbf{X}\odot\mathbf{Y}," class="ltx_Math" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml">𝐙</mi><mo id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.3.2.cmml">𝐗</mi><mo id="S4.E3.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E3.m1.1.1.1.1.3.1.cmml">⊙</mo><mi id="S4.E3.m1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.3.3.cmml">𝐘</mi></mrow></mrow><mo id="S4.E3.m1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"></eq><ci id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2">𝐙</ci><apply id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"><csymbol cd="latexml" id="S4.E3.m1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.3.1">direct-product</csymbol><ci id="S4.E3.m1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.3.2">𝐗</ci><ci id="S4.E3.m1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.3.3">𝐘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\mathbf{Z}=\mathbf{X}\odot\mathbf{Y},</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">bold_Z = bold_X ⊙ bold_Y ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS1.p1.7">where <math alttext="\odot" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.7.m1.1"><semantics id="S4.SS2.SSS1.p1.7.m1.1a"><mo id="S4.SS2.SSS1.p1.7.m1.1.1" xref="S4.SS2.SSS1.p1.7.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.7.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.7.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.7.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.7.m1.1c">\odot</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.7.m1.1d">⊙</annotation></semantics></math> denotes element-wise multiplication. These fusion approaches provide a simple yet effective way to integrate information, preserving the individual contributions of each modality in the combined representation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">For example, <cite class="ltx_cite ltx_citemacro_citet">Guo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib94" title="">94</a>]</cite> developed the ROD-Revenue framework to predict driver revenue using a linear regression model, given features extracted from multi-source urban data. The basic features from multiple datasets, including ride-on-demand (RoD) service, taxi service, and POI information, are used to construct composite features in a product form. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite> fused the historical passenger demand, meteorological data, and time meta to learn a joint representation for citywide passenger demand prediction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Feature Concatenation</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.3">Concatenation is a widely used technique for multi-modal fusion, involving the combination of feature vectors or matrices from different modalities by appending them along a specified axis. Let vectors <math alttext="\mathbf{X}=[x_{1},x_{2},\ldots,x_{m}]" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.4"><semantics id="S4.SS2.SSS2.p1.1.m1.4a"><mrow id="S4.SS2.SSS2.p1.1.m1.4.4" xref="S4.SS2.SSS2.p1.1.m1.4.4.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.4.4.5" xref="S4.SS2.SSS2.p1.1.m1.4.4.5.cmml">𝐗</mi><mo id="S4.SS2.SSS2.p1.1.m1.4.4.4" xref="S4.SS2.SSS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS2.p1.1.m1.4.4.3.3" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml"><mo id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">[</mo><msub id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.5" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.2" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.3" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.6" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS2.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.7" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.2" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.3" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.4b"><apply id="S4.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4"><eq id="S4.SS2.SSS2.p1.1.m1.4.4.4.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.4"></eq><ci id="S4.SS2.SSS2.p1.1.m1.4.4.5.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.5">𝐗</ci><list id="S4.SS2.SSS2.p1.1.m1.4.4.3.4.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3"><apply id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">…</ci><apply id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.4.4.3.3.3.3">𝑚</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.4c">\mathbf{X}=[x_{1},x_{2},\ldots,x_{m}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.4d">bold_X = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="\mathbf{Y}=[y_{1},y_{2},\ldots,y_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.4"><semantics id="S4.SS2.SSS2.p1.2.m2.4a"><mrow id="S4.SS2.SSS2.p1.2.m2.4.4" xref="S4.SS2.SSS2.p1.2.m2.4.4.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.4.4.5" xref="S4.SS2.SSS2.p1.2.m2.4.4.5.cmml">𝐘</mi><mo id="S4.SS2.SSS2.p1.2.m2.4.4.4" xref="S4.SS2.SSS2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS2.SSS2.p1.2.m2.4.4.3.3" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml"><mo id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.4" stretchy="false" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml">[</mo><msub id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.5" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.2" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.2.cmml">y</mi><mn id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.3" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.6" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><mi id="S4.SS2.SSS2.p1.2.m2.1.1" mathvariant="normal" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">…</mi><mo id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.7" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.2" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.2.cmml">y</mi><mi id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.3" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.8" stretchy="false" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.4b"><apply id="S4.SS2.SSS2.p1.2.m2.4.4.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4"><eq id="S4.SS2.SSS2.p1.2.m2.4.4.4.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.4"></eq><ci id="S4.SS2.SSS2.p1.2.m2.4.4.5.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.5">𝐘</ci><list id="S4.SS2.SSS2.p1.2.m2.4.4.3.4.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3"><apply id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.2">𝑦</ci><cn id="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.2">𝑦</ci><cn id="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">…</ci><apply id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.2">𝑦</ci><ci id="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.4c">\mathbf{Y}=[y_{1},y_{2},\ldots,y_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.4d">bold_Y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> represent features from two modalities. Concatenating these vectors along the concatenation axis results in a new vector <math alttext="\mathbf{Z}=[x_{1},x_{2},\ldots,x_{m},y_{1},y_{2},\ldots,y_{n}]" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.3.m3.8"><semantics id="S4.SS2.SSS2.p1.3.m3.8a"><mrow id="S4.SS2.SSS2.p1.3.m3.8.8" xref="S4.SS2.SSS2.p1.3.m3.8.8.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.8.8.8" xref="S4.SS2.SSS2.p1.3.m3.8.8.8.cmml">𝐙</mi><mo id="S4.SS2.SSS2.p1.3.m3.8.8.7" xref="S4.SS2.SSS2.p1.3.m3.8.8.7.cmml">=</mo><mrow id="S4.SS2.SSS2.p1.3.m3.8.8.6.6" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml"><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.7" stretchy="false" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">[</mo><msub id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.2" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.2.cmml">x</mi><mn id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.3" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.8" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><msub id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.2" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.2.cmml">x</mi><mn id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.3" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.9" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><mi id="S4.SS2.SSS2.p1.3.m3.1.1" mathvariant="normal" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">…</mi><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.10" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><msub id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.2" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.2.cmml">x</mi><mi id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.3" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.3.cmml">m</mi></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.11" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><msub id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.2" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.2.cmml">y</mi><mn id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.3" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.3.cmml">1</mn></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.12" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><msub id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.2" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.2.cmml">y</mi><mn id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.3" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.3.cmml">2</mn></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.13" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><mi id="S4.SS2.SSS2.p1.3.m3.2.2" mathvariant="normal" xref="S4.SS2.SSS2.p1.3.m3.2.2.cmml">…</mi><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.14" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">,</mo><msub id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.2" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.2.cmml">y</mi><mi id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.3" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.3.cmml">n</mi></msub><mo id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.15" stretchy="false" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.8b"><apply id="S4.SS2.SSS2.p1.3.m3.8.8.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8"><eq id="S4.SS2.SSS2.p1.3.m3.8.8.7.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.7"></eq><ci id="S4.SS2.SSS2.p1.3.m3.8.8.8.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.8">𝐙</ci><list id="S4.SS2.SSS2.p1.3.m3.8.8.6.7.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6"><apply id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.2">𝑥</ci><cn id="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.3.m3.3.3.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.2">𝑥</ci><cn id="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.3.m3.4.4.2.2.2.3">2</cn></apply><ci id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">…</ci><apply id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.cmml" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.2">𝑥</ci><ci id="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.3.cmml" xref="S4.SS2.SSS2.p1.3.m3.5.5.3.3.3.3">𝑚</ci></apply><apply id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.cmml" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.2">𝑦</ci><cn id="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.3.m3.6.6.4.4.4.3">1</cn></apply><apply id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.cmml" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.2">𝑦</ci><cn id="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.3.m3.7.7.5.5.5.3">2</cn></apply><ci id="S4.SS2.SSS2.p1.3.m3.2.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.2.2">…</ci><apply id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6">subscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.2">𝑦</ci><ci id="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.3.cmml" xref="S4.SS2.SSS2.p1.3.m3.8.8.6.6.6.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.8c">\mathbf{Z}=[x_{1},x_{2},\ldots,x_{m},y_{1},y_{2},\ldots,y_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.3.m3.8d">bold_Z = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, forming a fused representation. Mathematically, the concatenation operation is denoted as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{Z}=\text{concat}(\mathbf{X},\mathbf{Y})." class="ltx_Math" display="block" id="S4.E4.m1.3"><semantics id="S4.E4.m1.3a"><mrow id="S4.E4.m1.3.3.1" xref="S4.E4.m1.3.3.1.1.cmml"><mrow id="S4.E4.m1.3.3.1.1" xref="S4.E4.m1.3.3.1.1.cmml"><mi id="S4.E4.m1.3.3.1.1.2" xref="S4.E4.m1.3.3.1.1.2.cmml">𝐙</mi><mo id="S4.E4.m1.3.3.1.1.1" xref="S4.E4.m1.3.3.1.1.1.cmml">=</mo><mrow id="S4.E4.m1.3.3.1.1.3" xref="S4.E4.m1.3.3.1.1.3.cmml"><mtext id="S4.E4.m1.3.3.1.1.3.2" xref="S4.E4.m1.3.3.1.1.3.2a.cmml">concat</mtext><mo id="S4.E4.m1.3.3.1.1.3.1" xref="S4.E4.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S4.E4.m1.3.3.1.1.3.3.2" xref="S4.E4.m1.3.3.1.1.3.3.1.cmml"><mo id="S4.E4.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S4.E4.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">𝐗</mi><mo id="S4.E4.m1.3.3.1.1.3.3.2.2" xref="S4.E4.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">𝐘</mi><mo id="S4.E4.m1.3.3.1.1.3.3.2.3" stretchy="false" xref="S4.E4.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E4.m1.3.3.1.2" lspace="0em" xref="S4.E4.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.3b"><apply id="S4.E4.m1.3.3.1.1.cmml" xref="S4.E4.m1.3.3.1"><eq id="S4.E4.m1.3.3.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1"></eq><ci id="S4.E4.m1.3.3.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.2">𝐙</ci><apply id="S4.E4.m1.3.3.1.1.3.cmml" xref="S4.E4.m1.3.3.1.1.3"><times id="S4.E4.m1.3.3.1.1.3.1.cmml" xref="S4.E4.m1.3.3.1.1.3.1"></times><ci id="S4.E4.m1.3.3.1.1.3.2a.cmml" xref="S4.E4.m1.3.3.1.1.3.2"><mtext id="S4.E4.m1.3.3.1.1.3.2.cmml" xref="S4.E4.m1.3.3.1.1.3.2">concat</mtext></ci><interval closure="open" id="S4.E4.m1.3.3.1.1.3.3.1.cmml" xref="S4.E4.m1.3.3.1.1.3.3.2"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">𝐗</ci><ci id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">𝐘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.3c">\mathbf{Z}=\text{concat}(\mathbf{X},\mathbf{Y}).</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.3d">bold_Z = concat ( bold_X , bold_Y ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS2.p1.4">This method allows the preservation of individual modality information while creating a comprehensive representation for downstream tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">For instance, ST-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib362" title="">362</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib364" title="">364</a>]</cite> dynamically aggregated the outputs of three different residual temporal networks, assigning different weights to different branches and regions. The aggregation was further combined with external factors such as meteorological information (illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F9" title="Figure 9 ‣ 4.2.2 Feature Concatenation ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">9</span></a>). Similarly, <cite class="ltx_cite ltx_citemacro_citet">Lin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>]</cite> proposed a DeepSTN+ framework to forecast urban crowd flows via long-range spatial dependence modeling and the introduction of prior knowledge such as POI distribution. The three categories of historical crowd flow maps - closeness, period and trend, are concatenated firstly, followed by convolution as the fusion of different kinds of information. Some studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib213" title="">213</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib253" title="">253</a>]</cite> followed this concatenation scheme to fuse multimodal information (i.e., external factors) as well. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Liang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib164" title="">164</a>]</cite> appended the same three temporal sequences with meta features representing POI and road network (illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F10" title="Figure 10 ‣ 4.2.2 Feature Concatenation ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">10</span></a>), which serve as fine-grained knowledge for urban flow prediction.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="219" id="S4.F9.g1" src="extracted/5670403/Images_zxc/STResNet.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>ST-ResNet dynamically fused the results from three distinct residual temporal networks, allocating varied weights to diverse branches and regions. This fusion process was additionally integrated with external factors, including meteorological information. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib362" title="">362</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib364" title="">364</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">In addition, concatenation operation is often used in visual feature fusion. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib113" title="">113</a>]</cite> concatenated the high-dimensional features of satellite imagery, street-level imagery, and taxi trajectory time series together, and then passed them through a softmax layer to distinguish between urban and non-urban villages. <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite> also concatenated latent representations of satellite imagery and GPS trace as a fusion method for missing road attribute inference. In addition, the features of household capacity, human mobility, and commercial hotness can be merged together to estimate the population <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S4.F10.g1" src="extracted/5670403/Images_zxc/YLiang.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The feature-based data fusion framework proposed by <cite class="ltx_cite ltx_citemacro_citet">Liang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib164" title="">164</a>]</cite>. Features extracted from the temporal sequence are concatenated together with external features from POI and road networks.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Graph-based Data Fusion</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.5">Traditional deep learning-based feature extraction and fusion methods have brought revolutionary advancements to various urban computing tasks in recent years. These tasks typically involve data represented in the Euclidean space. However, there is a growing need for addressing tasks where data is generated from non-Euclidean domains and represented as graphs with intricate relationships between objects such as POI networks. In urban computing, graphs serve as representations of intricate networks that encompass various elements such as roads, buildings, and social interactions. In general, a graph can be represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{G}=(\mathbf{V},\mathbf{E})\text{ (General form),}" class="ltx_Math" display="block" id="S4.E5.m1.2"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2.3" xref="S4.E5.m1.2.3.cmml"><mi id="S4.E5.m1.2.3.2" xref="S4.E5.m1.2.3.2.cmml">𝐆</mi><mo id="S4.E5.m1.2.3.1" xref="S4.E5.m1.2.3.1.cmml">=</mo><mrow id="S4.E5.m1.2.3.3" xref="S4.E5.m1.2.3.3.cmml"><mrow id="S4.E5.m1.2.3.3.2.2" xref="S4.E5.m1.2.3.3.2.1.cmml"><mo id="S4.E5.m1.2.3.3.2.2.1" stretchy="false" xref="S4.E5.m1.2.3.3.2.1.cmml">(</mo><mi id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml">𝐕</mi><mo id="S4.E5.m1.2.3.3.2.2.2" xref="S4.E5.m1.2.3.3.2.1.cmml">,</mo><mi id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml">𝐄</mi><mo id="S4.E5.m1.2.3.3.2.2.3" stretchy="false" xref="S4.E5.m1.2.3.3.2.1.cmml">)</mo></mrow><mo id="S4.E5.m1.2.3.3.1" xref="S4.E5.m1.2.3.3.1.cmml">⁢</mo><mtext id="S4.E5.m1.2.3.3.3" xref="S4.E5.m1.2.3.3.3a.cmml"> (General form),</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.3.cmml" xref="S4.E5.m1.2.3"><eq id="S4.E5.m1.2.3.1.cmml" xref="S4.E5.m1.2.3.1"></eq><ci id="S4.E5.m1.2.3.2.cmml" xref="S4.E5.m1.2.3.2">𝐆</ci><apply id="S4.E5.m1.2.3.3.cmml" xref="S4.E5.m1.2.3.3"><times id="S4.E5.m1.2.3.3.1.cmml" xref="S4.E5.m1.2.3.3.1"></times><interval closure="open" id="S4.E5.m1.2.3.3.2.1.cmml" xref="S4.E5.m1.2.3.3.2.2"><ci id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1">𝐕</ci><ci id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2">𝐄</ci></interval><ci id="S4.E5.m1.2.3.3.3a.cmml" xref="S4.E5.m1.2.3.3.3"><mtext id="S4.E5.m1.2.3.3.3.cmml" xref="S4.E5.m1.2.3.3.3"> (General form),</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\mathbf{G}=(\mathbf{V},\mathbf{E})\text{ (General form),}</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.2d">bold_G = ( bold_V , bold_E ) (General form),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS3.p1.6">or</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{G}^{(t)}=(\mathbf{V},\mathbf{E},\mathbf{X}^{(t)})\text{ (Spatio-%
temporal form),}" class="ltx_Math" display="block" id="S4.E6.m1.5"><semantics id="S4.E6.m1.5a"><mrow id="S4.E6.m1.5.5" xref="S4.E6.m1.5.5.cmml"><msup id="S4.E6.m1.5.5.3" xref="S4.E6.m1.5.5.3.cmml"><mi id="S4.E6.m1.5.5.3.2" xref="S4.E6.m1.5.5.3.2.cmml">𝐆</mi><mrow id="S4.E6.m1.1.1.1.3" xref="S4.E6.m1.5.5.3.cmml"><mo id="S4.E6.m1.1.1.1.3.1" stretchy="false" xref="S4.E6.m1.5.5.3.cmml">(</mo><mi id="S4.E6.m1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.cmml">t</mi><mo id="S4.E6.m1.1.1.1.3.2" stretchy="false" xref="S4.E6.m1.5.5.3.cmml">)</mo></mrow></msup><mo id="S4.E6.m1.5.5.2" xref="S4.E6.m1.5.5.2.cmml">=</mo><mrow id="S4.E6.m1.5.5.1" xref="S4.E6.m1.5.5.1.cmml"><mrow id="S4.E6.m1.5.5.1.1.1" xref="S4.E6.m1.5.5.1.1.2.cmml"><mo id="S4.E6.m1.5.5.1.1.1.2" stretchy="false" xref="S4.E6.m1.5.5.1.1.2.cmml">(</mo><mi id="S4.E6.m1.3.3" xref="S4.E6.m1.3.3.cmml">𝐕</mi><mo id="S4.E6.m1.5.5.1.1.1.3" xref="S4.E6.m1.5.5.1.1.2.cmml">,</mo><mi id="S4.E6.m1.4.4" xref="S4.E6.m1.4.4.cmml">𝐄</mi><mo id="S4.E6.m1.5.5.1.1.1.4" xref="S4.E6.m1.5.5.1.1.2.cmml">,</mo><msup id="S4.E6.m1.5.5.1.1.1.1" xref="S4.E6.m1.5.5.1.1.1.1.cmml"><mi id="S4.E6.m1.5.5.1.1.1.1.2" xref="S4.E6.m1.5.5.1.1.1.1.2.cmml">𝐗</mi><mrow id="S4.E6.m1.2.2.1.3" xref="S4.E6.m1.5.5.1.1.1.1.cmml"><mo id="S4.E6.m1.2.2.1.3.1" stretchy="false" xref="S4.E6.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S4.E6.m1.2.2.1.1" xref="S4.E6.m1.2.2.1.1.cmml">t</mi><mo id="S4.E6.m1.2.2.1.3.2" stretchy="false" xref="S4.E6.m1.5.5.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.E6.m1.5.5.1.1.1.5" stretchy="false" xref="S4.E6.m1.5.5.1.1.2.cmml">)</mo></mrow><mo id="S4.E6.m1.5.5.1.2" xref="S4.E6.m1.5.5.1.2.cmml">⁢</mo><mtext id="S4.E6.m1.5.5.1.3" xref="S4.E6.m1.5.5.1.3a.cmml"> (Spatio-temporal form),</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.5b"><apply id="S4.E6.m1.5.5.cmml" xref="S4.E6.m1.5.5"><eq id="S4.E6.m1.5.5.2.cmml" xref="S4.E6.m1.5.5.2"></eq><apply id="S4.E6.m1.5.5.3.cmml" xref="S4.E6.m1.5.5.3"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.3.1.cmml" xref="S4.E6.m1.5.5.3">superscript</csymbol><ci id="S4.E6.m1.5.5.3.2.cmml" xref="S4.E6.m1.5.5.3.2">𝐆</ci><ci id="S4.E6.m1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1">𝑡</ci></apply><apply id="S4.E6.m1.5.5.1.cmml" xref="S4.E6.m1.5.5.1"><times id="S4.E6.m1.5.5.1.2.cmml" xref="S4.E6.m1.5.5.1.2"></times><vector id="S4.E6.m1.5.5.1.1.2.cmml" xref="S4.E6.m1.5.5.1.1.1"><ci id="S4.E6.m1.3.3.cmml" xref="S4.E6.m1.3.3">𝐕</ci><ci id="S4.E6.m1.4.4.cmml" xref="S4.E6.m1.4.4">𝐄</ci><apply id="S4.E6.m1.5.5.1.1.1.1.cmml" xref="S4.E6.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.1.1.1.1.1.cmml" xref="S4.E6.m1.5.5.1.1.1.1">superscript</csymbol><ci id="S4.E6.m1.5.5.1.1.1.1.2.cmml" xref="S4.E6.m1.5.5.1.1.1.1.2">𝐗</ci><ci id="S4.E6.m1.2.2.1.1.cmml" xref="S4.E6.m1.2.2.1.1">𝑡</ci></apply></vector><ci id="S4.E6.m1.5.5.1.3a.cmml" xref="S4.E6.m1.5.5.1.3"><mtext id="S4.E6.m1.5.5.1.3.cmml" xref="S4.E6.m1.5.5.1.3"> (Spatio-temporal form),</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.5c">\mathbf{G}^{(t)}=(\mathbf{V},\mathbf{E},\mathbf{X}^{(t)})\text{ (Spatio-%
temporal form),}</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.5d">bold_G start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = ( bold_V , bold_E , bold_X start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) (Spatio-temporal form),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS3.p1.4">where <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p1.4.1">V</span> represents a set of nodes and <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p1.4.2">E</span> is the set of edges. For a spatio-temporal graph, where the node attributes vary dynamically over time, <math alttext="\textbf{G}^{(t)}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.1.m1.1"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><msup id="S4.SS2.SSS3.p1.1.m1.1.2" xref="S4.SS2.SSS3.p1.1.m1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p1.1.m1.1.2.2" xref="S4.SS2.SSS3.p1.1.m1.1.2.2a.cmml">G</mtext><mrow id="S4.SS2.SSS3.p1.1.m1.1.1.1.3" xref="S4.SS2.SSS3.p1.1.m1.1.2.cmml"><mo id="S4.SS2.SSS3.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S4.SS2.SSS3.p1.1.m1.1.2.cmml">(</mo><mi id="S4.SS2.SSS3.p1.1.m1.1.1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.1.1.cmml">t</mi><mo id="S4.SS2.SSS3.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S4.SS2.SSS3.p1.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.p1.1.m1.1.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p1.1.m1.1.2.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.2">superscript</csymbol><ci id="S4.SS2.SSS3.p1.1.m1.1.2.2a.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p1.1.m1.1.2.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.2.2">G</mtext></ci><ci id="S4.SS2.SSS3.p1.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">\textbf{G}^{(t)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.1.m1.1d">G start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the graph representation at time step <math alttext="t" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.2.m2.1"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mi id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><ci id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.2.m2.1d">italic_t</annotation></semantics></math> and <math alttext="\textbf{X}^{(t)}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.3.m3.1"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><msup id="S4.SS2.SSS3.p1.3.m3.1.2" xref="S4.SS2.SSS3.p1.3.m3.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p1.3.m3.1.2.2" xref="S4.SS2.SSS3.p1.3.m3.1.2.2a.cmml">X</mtext><mrow id="S4.SS2.SSS3.p1.3.m3.1.1.1.3" xref="S4.SS2.SSS3.p1.3.m3.1.2.cmml"><mo id="S4.SS2.SSS3.p1.3.m3.1.1.1.3.1" stretchy="false" xref="S4.SS2.SSS3.p1.3.m3.1.2.cmml">(</mo><mi id="S4.SS2.SSS3.p1.3.m3.1.1.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.1.1.cmml">t</mi><mo id="S4.SS2.SSS3.p1.3.m3.1.1.1.3.2" stretchy="false" xref="S4.SS2.SSS3.p1.3.m3.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><apply id="S4.SS2.SSS3.p1.3.m3.1.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p1.3.m3.1.2.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.2">superscript</csymbol><ci id="S4.SS2.SSS3.p1.3.m3.1.2.2a.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p1.3.m3.1.2.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.2.2">X</mtext></ci><ci id="S4.SS2.SSS3.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">\textbf{X}^{(t)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.3.m3.1d">X start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the node feature matrix of graph <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.SS2.SSS3.p1.4.3">G</span> at the same time step. Deep learning models based on spatio-temporal graphs have gained significant importance and achieved remarkable success across various applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib343" title="">343</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib158" title="">158</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib308" title="">308</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.4">An intuitive approach for graph-based data fusion is <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.4.1">multi-view graph network-based data fusion</span>, which represents diverse data through a set of multi-view graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib82" title="">82</a>]</cite>. In this framework, each view is associated with different edge vectors, denoted as <math alttext="\mathbf{E^{i}}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.1.m1.1"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><msup id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml">𝐄</mi><mi id="S4.SS2.SSS3.p2.1.m1.1.1.3" xref="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml">𝐢</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><apply id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.2">𝐄</ci><ci id="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.3">𝐢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">\mathbf{E^{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.1.m1.1d">bold_E start_POSTSUPERSCRIPT bold_i end_POSTSUPERSCRIPT</annotation></semantics></math>, which represent the relationships between each node (such POI), denoted as <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.2.m2.1"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mi id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><ci id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.2.m2.1d">bold_V</annotation></semantics></math>, under a particular feature view <math alttext="\mathbf{i}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.3.m3.1"><semantics id="S4.SS2.SSS3.p2.3.m3.1a"><mi id="S4.SS2.SSS3.p2.3.m3.1.1" xref="S4.SS2.SSS3.p2.3.m3.1.1.cmml">𝐢</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.3.m3.1b"><ci id="S4.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1">𝐢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.3.m3.1c">\mathbf{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.3.m3.1d">bold_i</annotation></semantics></math> (distance, mobility, semantic, etc.). For graphs from different views, graph nodes <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.4.m4.1"><semantics id="S4.SS2.SSS3.p2.4.m4.1a"><mi id="S4.SS2.SSS3.p2.4.m4.1.1" xref="S4.SS2.SSS3.p2.4.m4.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.4.m4.1b"><ci id="S4.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.4.m4.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.4.m4.1d">bold_V</annotation></semantics></math> usually serve as connectors, linking them together for feature fusion. Each view in these frameworks captures a unique perspective or aspect of the underlying data, allowing for a comprehensive understanding of a more complex system.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Based on the conception of a multi-view graph, <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite> proposed multi-view POI graph networks that incorporate various geo-features including regions, distances, and human mobility connectivity. <cite class="ltx_cite ltx_citemacro_citet">Du et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>]</cite> developed a group of POI networks to characterize both static and dynamic features through the variable graph edges. Both of these work effectively integrate and fuse information from various data sources and achieve great success in downstream tasks. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib172" title="">172</a>]</cite> conducted research on characterizing urban vibrancy by employing a multi-view graph framework and demonstrated the effectiveness of multi-view graphs in capturing the intricate relationship between urban vibrancy and urban spatiotemporal dynamics. In addition, different regions in a city can be represented through multi-view graphs as well. <cite class="ltx_cite ltx_citemacro_citet">Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib82" title="">82</a>]</cite> proposed a multiple graph framework to encode pair-wise correlations between regions in urban areas. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F11" title="Figure 11 ‣ 4.2.3 Graph-based Data Fusion ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">11</span></a>, each region square, measuring (<math alttext="1km\times 1km" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p3.1.m1.1"><semantics id="S4.SS2.SSS3.p3.1.m1.1a"><mrow id="S4.SS2.SSS3.p3.1.m1.1.1" xref="S4.SS2.SSS3.p3.1.m1.1.1.cmml"><mrow id="S4.SS2.SSS3.p3.1.m1.1.1.2" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.cmml"><mrow id="S4.SS2.SSS3.p3.1.m1.1.1.2.2" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.cmml"><mn id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.2" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.2.cmml">1</mn><mo id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.3" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.3.cmml">k</mi><mo id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1a" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.4" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.4.cmml">m</mi></mrow><mo id="S4.SS2.SSS3.p3.1.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.1.cmml">×</mo><mn id="S4.SS2.SSS3.p3.1.m1.1.1.2.3" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.SS2.SSS3.p3.1.m1.1.1.1" xref="S4.SS2.SSS3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p3.1.m1.1.1.3" xref="S4.SS2.SSS3.p3.1.m1.1.1.3.cmml">k</mi><mo id="S4.SS2.SSS3.p3.1.m1.1.1.1a" xref="S4.SS2.SSS3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p3.1.m1.1.1.4" xref="S4.SS2.SSS3.p3.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p3.1.m1.1b"><apply id="S4.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1"><times id="S4.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.1"></times><apply id="S4.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2"><times id="S4.SS2.SSS3.p3.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.1"></times><apply id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2"><times id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.1"></times><cn id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.2.cmml" type="integer" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.2">1</cn><ci id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.3.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.3">𝑘</ci><ci id="S4.SS2.SSS3.p3.1.m1.1.1.2.2.4.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.2.4">𝑚</ci></apply><cn id="S4.SS2.SSS3.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.3">1</cn></apply><ci id="S4.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.3">𝑘</ci><ci id="S4.SS2.SSS3.p3.1.m1.1.1.4.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p3.1.m1.1c">1km\times 1km</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p3.1.m1.1d">1 italic_k italic_m × 1 italic_k italic_m</annotation></semantics></math>), is represented as a node in the graphs. By utilizing graph convolutional networks to observe and fuse these graphs, the framework can capture various region correlations and achieve impressive results in forecasting ride-hailing demand.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="160" id="S4.F11.g1" src="extracted/5670403/Images_zxc/geospatio.png" width="281"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The overall architecture of the spatio-temporal multi-graph convolution network proposed by <cite class="ltx_cite ltx_citemacro_citet">Geng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib82" title="">82</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">While it is efficient to incorporate features from different data sources using multi-view graphs, describing high-dimensional cross-modal correlation between different items can be challenging as there is no direct interaction between nodes from each graph. To address these difficulties, researchers proposed spatiotemporal heterogeneous networks (SHNs) to encapsulate the complex cross-modal relationships among different urban entities. Subsequently, <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p4.1.1">heterogeneous graph-based data fusion</span> is considered as a promising approach to the fusion of various data in one graph and tackles the above challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib176" title="">176</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib365" title="">365</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib278" title="">278</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib134" title="">134</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib408" title="">408</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib292" title="">292</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib184" title="">184</a>]</cite> proposed a hierarchical embedding framework that aimed to establish connections between nodes in the city activity graph and user interaction graph. Although the specific emphasis on SHNs may not be explicitly stressed, this attempt, along with the successful cross-modal node connection, has demonstrated significant success in capturing and representing cross-modal connections. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib365" title="">365</a>]</cite> proposed a compound multi-linear relationship graph network to converge various edge connectivity with interaction between multi-view graphs. In their methodology, which is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F12" title="Figure 12 ‣ 4.2.3 Graph-based Data Fusion ‣ 4.2 Feature-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">12</span></a>, any random walk path in the compound graph is a compound of various kinds of relationships for different views. This work strongly develops the generality of graph neural networks in cross-domain data fusion.</p>
</div>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S4.F12.g1" src="x2.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>In (a), the multi-view graph connectivity is shown for each individual graph. Vertices represent regions, and the weighted edges represent region-wise relationships. There is no interaction between the graphs.
In (b), the compound graph connectivity is represented as a multi-linear graph. Vertices are connected if there is an edge in any of the traditional multi-view graphs. This allows for a more comprehensive understanding of the interdependencies among regions across all graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib365" title="">365</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.3"><cite class="ltx_cite ltx_citemacro_citet">Chandra et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib33" title="">33</a>]</cite> designed a SHNs embedding framework that takes the POIs as nodes and human mobility between POIs as weighted links. The SHNs in their research are represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathbf{G}=(\mathbf{V},\mathbf{E},\mathbf{\phi},\mathbf{\psi})," class="ltx_Math" display="block" id="S4.E7.m1.5"><semantics id="S4.E7.m1.5a"><mrow id="S4.E7.m1.5.5.1" xref="S4.E7.m1.5.5.1.1.cmml"><mrow id="S4.E7.m1.5.5.1.1" xref="S4.E7.m1.5.5.1.1.cmml"><mi id="S4.E7.m1.5.5.1.1.2" xref="S4.E7.m1.5.5.1.1.2.cmml">𝐆</mi><mo id="S4.E7.m1.5.5.1.1.1" xref="S4.E7.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E7.m1.5.5.1.1.3.2" xref="S4.E7.m1.5.5.1.1.3.1.cmml"><mo id="S4.E7.m1.5.5.1.1.3.2.1" stretchy="false" xref="S4.E7.m1.5.5.1.1.3.1.cmml">(</mo><mi id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml">𝐕</mi><mo id="S4.E7.m1.5.5.1.1.3.2.2" xref="S4.E7.m1.5.5.1.1.3.1.cmml">,</mo><mi id="S4.E7.m1.2.2" xref="S4.E7.m1.2.2.cmml">𝐄</mi><mo id="S4.E7.m1.5.5.1.1.3.2.3" xref="S4.E7.m1.5.5.1.1.3.1.cmml">,</mo><mi id="S4.E7.m1.3.3" xref="S4.E7.m1.3.3.cmml">ϕ</mi><mo id="S4.E7.m1.5.5.1.1.3.2.4" xref="S4.E7.m1.5.5.1.1.3.1.cmml">,</mo><mi id="S4.E7.m1.4.4" xref="S4.E7.m1.4.4.cmml">ψ</mi><mo id="S4.E7.m1.5.5.1.1.3.2.5" stretchy="false" xref="S4.E7.m1.5.5.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.5.5.1.2" xref="S4.E7.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.5b"><apply id="S4.E7.m1.5.5.1.1.cmml" xref="S4.E7.m1.5.5.1"><eq id="S4.E7.m1.5.5.1.1.1.cmml" xref="S4.E7.m1.5.5.1.1.1"></eq><ci id="S4.E7.m1.5.5.1.1.2.cmml" xref="S4.E7.m1.5.5.1.1.2">𝐆</ci><vector id="S4.E7.m1.5.5.1.1.3.1.cmml" xref="S4.E7.m1.5.5.1.1.3.2"><ci id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1">𝐕</ci><ci id="S4.E7.m1.2.2.cmml" xref="S4.E7.m1.2.2">𝐄</ci><ci id="S4.E7.m1.3.3.cmml" xref="S4.E7.m1.3.3">italic-ϕ</ci><ci id="S4.E7.m1.4.4.cmml" xref="S4.E7.m1.4.4">𝜓</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.5c">\mathbf{G}=(\mathbf{V},\mathbf{E},\mathbf{\phi},\mathbf{\psi}),</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.5d">bold_G = ( bold_V , bold_E , italic_ϕ , italic_ψ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS3.p5.2">where <math alttext="\phi:\textbf{V}\rightarrow\mathcal{L}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p5.1.m1.1"><semantics id="S4.SS2.SSS3.p5.1.m1.1a"><mrow id="S4.SS2.SSS3.p5.1.m1.1.1" xref="S4.SS2.SSS3.p5.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p5.1.m1.1.1.2" xref="S4.SS2.SSS3.p5.1.m1.1.1.2.cmml">ϕ</mi><mo id="S4.SS2.SSS3.p5.1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S4.SS2.SSS3.p5.1.m1.1.1.1.cmml">:</mo><mrow id="S4.SS2.SSS3.p5.1.m1.1.1.3" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p5.1.m1.1.1.3.2" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.2a.cmml">V</mtext><mo id="S4.SS2.SSS3.p5.1.m1.1.1.3.1" stretchy="false" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS3.p5.1.m1.1.1.3.3" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.3.cmml">ℒ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p5.1.m1.1b"><apply id="S4.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1"><ci id="S4.SS2.SSS3.p5.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.1">:</ci><ci id="S4.SS2.SSS3.p5.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.2">italic-ϕ</ci><apply id="S4.SS2.SSS3.p5.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.3"><ci id="S4.SS2.SSS3.p5.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.1">→</ci><ci id="S4.SS2.SSS3.p5.1.m1.1.1.3.2a.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p5.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.2">V</mtext></ci><ci id="S4.SS2.SSS3.p5.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS3.p5.1.m1.1.1.3.3">ℒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p5.1.m1.1c">\phi:\textbf{V}\rightarrow\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p5.1.m1.1d">italic_ϕ : V → caligraphic_L</annotation></semantics></math> is a mapping function for nodes and <math alttext="\psi:\textbf{E}\rightarrow\mathcal{R}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p5.2.m2.1"><semantics id="S4.SS2.SSS3.p5.2.m2.1a"><mrow id="S4.SS2.SSS3.p5.2.m2.1.1" xref="S4.SS2.SSS3.p5.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p5.2.m2.1.1.2" xref="S4.SS2.SSS3.p5.2.m2.1.1.2.cmml">ψ</mi><mo id="S4.SS2.SSS3.p5.2.m2.1.1.1" lspace="0.278em" rspace="0.278em" xref="S4.SS2.SSS3.p5.2.m2.1.1.1.cmml">:</mo><mrow id="S4.SS2.SSS3.p5.2.m2.1.1.3" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p5.2.m2.1.1.3.2" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.2a.cmml">E</mtext><mo id="S4.SS2.SSS3.p5.2.m2.1.1.3.1" stretchy="false" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS3.p5.2.m2.1.1.3.3" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.3.cmml">ℛ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p5.2.m2.1b"><apply id="S4.SS2.SSS3.p5.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1"><ci id="S4.SS2.SSS3.p5.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.1">:</ci><ci id="S4.SS2.SSS3.p5.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.2">𝜓</ci><apply id="S4.SS2.SSS3.p5.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.3"><ci id="S4.SS2.SSS3.p5.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.1">→</ci><ci id="S4.SS2.SSS3.p5.2.m2.1.1.3.2a.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.SSS3.p5.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.2">E</mtext></ci><ci id="S4.SS2.SSS3.p5.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS3.p5.2.m2.1.1.3.3">ℛ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p5.2.m2.1c">\psi:\textbf{E}\rightarrow\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p5.2.m2.1d">italic_ψ : E → caligraphic_R</annotation></semantics></math> is a edge type mapping function. Within this presentation based on SHNs, the framework could analyze urban mobility data from multiple sources in a uniform model space.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p6">
<p class="ltx_p" id="S4.SS2.SSS3.p6.1">In addition to the commonly researched SHNs, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib176" title="">176</a>]</cite> proposed a multi-modal transportation graph consisting of nodes for users, transport modes, and origin-destination pairs. This approach aimed to develop a multi-modal planning methodology in a transport recommend system. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib278" title="">278</a>]</cite> constructed an urban knowledge graph that effectively integrates features and knowledge from various trajectory data sources to model users’ mobility patterns.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Alignment-based Data Fusion</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Alignment fusion ensures that semantically related content across modalities is effectively combined. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F13" title="Figure 13 ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">13</span></a>, methods for multi-modal alignment span categories such as <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">cross-modal attention mechanism</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS1" title="4.3.1 Attention-based Alignment ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>) and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">multi-modal encoder-based fusion</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS3.SSS2" title="4.3.2 Encoder-based Alignment ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>), enabling understanding and collaboration between disparate sources of information.
<span class="ltx_text" id="S4.SS3.p1.1.3" style="color:#000000;">Alignment-based approaches can achieve more precise modal alignment and exhibit flexibility, making them suitable for various general scenarios. However, they generally require higher computational resources, which makes them more appropriate for offline tasks in transportation and urban planning.</span></p>
</div>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="234" id="S4.F13.g1" src="extracted/5670403/Images_zxc/alignment.png" width="228"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The general framework of alignment-based cross-domain data fusion in urban computing: (a) cross-attention framework for attention-based alignment; (b) unified encoder framework for encoder-based alignment.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Attention-based Alignment</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.3">Attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib268" title="">268</a>]</cite>, especially multi-modal cross-attention is a fusion technique crucial for integrating information across diverse modalities, such as text and images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib296" title="">296</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib122" title="">122</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib327" title="">327</a>]</cite>. The cross-attention mechanism can be summarized in the following three steps. First, project the feature vectors of modalities <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.3.1">X</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.3.2">Y</span> into query (<span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.3.3">Q</span>), key (<span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.3.4">K</span>), and value (<span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.3.5">V</span>) spaces using learnable parameter matrices <math alttext="\mathbf{W}_{Q}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><msub id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">𝐖</mi><mi id="S4.SS3.SSS1.p1.1.m1.1.1.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">𝐖</ci><ci id="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">\mathbf{W}_{Q}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">bold_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{W}_{K}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.m2.1"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><msub id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p1.2.m2.1.1.2" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.cmml">𝐖</mi><mi id="S4.SS3.SSS1.p1.2.m2.1.1.3" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><apply id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.2">𝐖</ci><ci id="S4.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">\mathbf{W}_{K}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.m2.1d">bold_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{W}_{V}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.3.m3.1"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><msub id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p1.3.m3.1.1.2" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.cmml">𝐖</mi><mi id="S4.SS3.SSS1.p1.3.m3.1.1.3" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><apply id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.2">𝐖</ci><ci id="S4.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">\mathbf{W}_{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.3.m3.1d">bold_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>, respectively:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\textbf{Q}_{X}=\textbf{X}\textbf{W}_{Q_{X}},\quad\textbf{K}_{Y}=\textbf{Y}%
\textbf{W}_{K_{Y}},\quad\textbf{V}_{Y}=\textbf{Y}\textbf{W}_{V_{Y}}." class="ltx_Math" display="block" id="S4.E8.m1.1"><semantics id="S4.E8.m1.1a"><mrow id="S4.E8.m1.1.1.1"><mrow id="S4.E8.m1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.cmml"><msub id="S4.E8.m1.1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.2.2" xref="S4.E8.m1.1.1.1.1.1.1.2.2a.cmml">Q</mtext><mi id="S4.E8.m1.1.1.1.1.1.1.2.3" xref="S4.E8.m1.1.1.1.1.1.1.2.3.cmml">X</mi></msub><mo id="S4.E8.m1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.cmml">=</mo><msub id="S4.E8.m1.1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.1.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.1.1.3.2c.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.3.2a" xref="S4.E8.m1.1.1.1.1.1.1.3.2c.cmml">X</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.3.2b" xref="S4.E8.m1.1.1.1.1.1.1.3.2c.cmml">W</mtext></mrow><msub id="S4.E8.m1.1.1.1.1.1.1.3.3" xref="S4.E8.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.3.3.2" xref="S4.E8.m1.1.1.1.1.1.1.3.3.2.cmml">Q</mi><mi id="S4.E8.m1.1.1.1.1.1.1.3.3.3" xref="S4.E8.m1.1.1.1.1.1.1.3.3.3.cmml">X</mi></msub></msub></mrow><mo id="S4.E8.m1.1.1.1.1.2.3" rspace="1.167em" xref="S4.E8.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S4.E8.m1.1.1.1.1.2.2.2" xref="S4.E8.m1.1.1.1.1.2.2.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.2.2.1.1" xref="S4.E8.m1.1.1.1.1.2.2.1.1.cmml"><msub id="S4.E8.m1.1.1.1.1.2.2.1.1.2" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.2.2" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.2a.cmml">K</mtext><mi id="S4.E8.m1.1.1.1.1.2.2.1.1.2.3" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.3.cmml">Y</mi></msub><mo id="S4.E8.m1.1.1.1.1.2.2.1.1.1" xref="S4.E8.m1.1.1.1.1.2.2.1.1.1.cmml">=</mo><msub id="S4.E8.m1.1.1.1.1.2.2.1.1.3" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2c.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2a" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2c.cmml">Y</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2b" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2c.cmml">W</mtext></mrow><msub id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.2" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.2.cmml">K</mi><mi id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.3" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.3.cmml">Y</mi></msub></msub></mrow><mo id="S4.E8.m1.1.1.1.1.2.2.2.3" rspace="1.167em" xref="S4.E8.m1.1.1.1.1.2.2.3a.cmml">,</mo><mrow id="S4.E8.m1.1.1.1.1.2.2.2.2" xref="S4.E8.m1.1.1.1.1.2.2.2.2.cmml"><msub id="S4.E8.m1.1.1.1.1.2.2.2.2.2" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.2.2" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.2a.cmml">V</mtext><mi id="S4.E8.m1.1.1.1.1.2.2.2.2.2.3" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml">Y</mi></msub><mo id="S4.E8.m1.1.1.1.1.2.2.2.2.1" xref="S4.E8.m1.1.1.1.1.2.2.2.2.1.cmml">=</mo><msub id="S4.E8.m1.1.1.1.1.2.2.2.2.3" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2c.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2a" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2c.cmml">Y</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2b" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2c.cmml">W</mtext></mrow><msub id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.2" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.2.cmml">V</mi><mi id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.3" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.3.cmml">Y</mi></msub></msub></mrow></mrow></mrow><mo id="S4.E8.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.1b"><apply id="S4.E8.m1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3a.cmml" xref="S4.E8.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E8.m1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1"><eq id="S4.E8.m1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1"></eq><apply id="S4.E8.m1.1.1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.2.2a.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2.2">Q</mtext></ci><ci id="S4.E8.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2.3">𝑋</ci></apply><apply id="S4.E8.m1.1.1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.3.2c.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.2"><mrow id="S4.E8.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.3.2a.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.2">X</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.1.1.3.2b.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.2">W</mtext></mrow></ci><apply id="S4.E8.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.3.2">𝑄</ci><ci id="S4.E8.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.3.3">𝑋</ci></apply></apply></apply><apply id="S4.E8.m1.1.1.1.1.2.2.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.3a.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E8.m1.1.1.1.1.2.2.1.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1"><eq id="S4.E8.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.1"></eq><apply id="S4.E8.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.1.1.2.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.1.1.2.2a.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.2.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.2">K</mtext></ci><ci id="S4.E8.m1.1.1.1.1.2.2.1.1.2.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.2.3">𝑌</ci></apply><apply id="S4.E8.m1.1.1.1.1.2.2.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2c.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2"><mrow id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2a.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2">Y</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.2b.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.2">W</mtext></mrow></ci><apply id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.2">𝐾</ci><ci id="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.1.1.3.3.3">𝑌</ci></apply></apply></apply><apply id="S4.E8.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2"><eq id="S4.E8.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.1"></eq><apply id="S4.E8.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.2.2.2.2a.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.2">V</mtext></ci><ci id="S4.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.2.3">𝑌</ci></apply><apply id="S4.E8.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2c.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2"><mrow id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2a.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2">Y</mtext><mtext class="ltx_mathvariant_bold" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.2b.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.2">W</mtext></mrow></ci><apply id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.2">𝑉</ci><ci id="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.2.2.2.2.3.3.3">𝑌</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.1c">\textbf{Q}_{X}=\textbf{X}\textbf{W}_{Q_{X}},\quad\textbf{K}_{Y}=\textbf{Y}%
\textbf{W}_{K_{Y}},\quad\textbf{V}_{Y}=\textbf{Y}\textbf{W}_{V_{Y}}.</annotation><annotation encoding="application/x-llamapun" id="S4.E8.m1.1d">Q start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = bold_X bold_W start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_POSTSUBSCRIPT , K start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT = bold_Y bold_W start_POSTSUBSCRIPT italic_K start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_POSTSUBSCRIPT , V start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT = bold_Y bold_W start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS1.p1.6">Second, compute the initial attention scores by taking the dot product of the query vectors of modality <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.6.1">X</span> (i.e., <math alttext="\mathbf{Q}_{X}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.4.m1.1"><semantics id="S4.SS3.SSS1.p1.4.m1.1a"><msub id="S4.SS3.SSS1.p1.4.m1.1.1" xref="S4.SS3.SSS1.p1.4.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.4.m1.1.1.2" xref="S4.SS3.SSS1.p1.4.m1.1.1.2.cmml">𝐐</mi><mi id="S4.SS3.SSS1.p1.4.m1.1.1.3" xref="S4.SS3.SSS1.p1.4.m1.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.4.m1.1b"><apply id="S4.SS3.SSS1.p1.4.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.4.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.4.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.4.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.4.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.4.m1.1.1.2">𝐐</ci><ci id="S4.SS3.SSS1.p1.4.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p1.4.m1.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.4.m1.1c">\mathbf{Q}_{X}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.4.m1.1d">bold_Q start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT</annotation></semantics></math>) and the key vectors of modality Y (i.e., <math alttext="\mathbf{K}_{Y}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.5.m2.1"><semantics id="S4.SS3.SSS1.p1.5.m2.1a"><msub id="S4.SS3.SSS1.p1.5.m2.1.1" xref="S4.SS3.SSS1.p1.5.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p1.5.m2.1.1.2" xref="S4.SS3.SSS1.p1.5.m2.1.1.2.cmml">𝐊</mi><mi id="S4.SS3.SSS1.p1.5.m2.1.1.3" xref="S4.SS3.SSS1.p1.5.m2.1.1.3.cmml">Y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.5.m2.1b"><apply id="S4.SS3.SSS1.p1.5.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.5.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.5.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p1.5.m2.1.1.2">𝐊</ci><ci id="S4.SS3.SSS1.p1.5.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p1.5.m2.1.1.3">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.5.m2.1c">\mathbf{K}_{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.5.m2.1d">bold_K start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT</annotation></semantics></math>), divided by the square root of the dimensionality of the key vectors <math alttext="\sqrt{d_{k}}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.6.m3.1"><semantics id="S4.SS3.SSS1.p1.6.m3.1a"><msqrt id="S4.SS3.SSS1.p1.6.m3.1.1" xref="S4.SS3.SSS1.p1.6.m3.1.1.cmml"><msub id="S4.SS3.SSS1.p1.6.m3.1.1.2" xref="S4.SS3.SSS1.p1.6.m3.1.1.2.cmml"><mi id="S4.SS3.SSS1.p1.6.m3.1.1.2.2" xref="S4.SS3.SSS1.p1.6.m3.1.1.2.2.cmml">d</mi><mi id="S4.SS3.SSS1.p1.6.m3.1.1.2.3" xref="S4.SS3.SSS1.p1.6.m3.1.1.2.3.cmml">k</mi></msub></msqrt><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.6.m3.1b"><apply id="S4.SS3.SSS1.p1.6.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1"><root id="S4.SS3.SSS1.p1.6.m3.1.1a.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1"></root><apply id="S4.SS3.SSS1.p1.6.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.6.m3.1.1.2.1.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p1.6.m3.1.1.2.2.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1.2.2">𝑑</ci><ci id="S4.SS3.SSS1.p1.6.m3.1.1.2.3.cmml" xref="S4.SS3.SSS1.p1.6.m3.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.6.m3.1c">\sqrt{d_{k}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.6.m3.1d">square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>. Subsequently, we apply the softmax function to obtain normalized attention weights, ensuring that the weights sum up to 1:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\textbf{A}=\text{Softmax}(\frac{\textbf{Q}_{X}\textbf{K}_{Y}^{T}}{\sqrt{d_{k}}%
})." class="ltx_Math" display="block" id="S4.E9.m1.2"><semantics id="S4.E9.m1.2a"><mrow id="S4.E9.m1.2.2.1" xref="S4.E9.m1.2.2.1.1.cmml"><mrow id="S4.E9.m1.2.2.1.1" xref="S4.E9.m1.2.2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.2.2.1.1.2" xref="S4.E9.m1.2.2.1.1.2a.cmml">A</mtext><mo id="S4.E9.m1.2.2.1.1.1" xref="S4.E9.m1.2.2.1.1.1.cmml">=</mo><mrow id="S4.E9.m1.2.2.1.1.3" xref="S4.E9.m1.2.2.1.1.3.cmml"><mtext id="S4.E9.m1.2.2.1.1.3.2" xref="S4.E9.m1.2.2.1.1.3.2a.cmml">Softmax</mtext><mo id="S4.E9.m1.2.2.1.1.3.1" xref="S4.E9.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S4.E9.m1.2.2.1.1.3.3.2" xref="S4.E9.m1.1.1.cmml"><mo id="S4.E9.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S4.E9.m1.1.1.cmml">(</mo><mfrac id="S4.E9.m1.1.1" xref="S4.E9.m1.1.1.cmml"><mrow id="S4.E9.m1.1.1.2" xref="S4.E9.m1.1.1.2.cmml"><msub id="S4.E9.m1.1.1.2.2" xref="S4.E9.m1.1.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.1.1.2.2.2" xref="S4.E9.m1.1.1.2.2.2a.cmml">Q</mtext><mi id="S4.E9.m1.1.1.2.2.3" xref="S4.E9.m1.1.1.2.2.3.cmml">X</mi></msub><mo id="S4.E9.m1.1.1.2.1" xref="S4.E9.m1.1.1.2.1.cmml">⁢</mo><msubsup id="S4.E9.m1.1.1.2.3" xref="S4.E9.m1.1.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.1.1.2.3.2.2" xref="S4.E9.m1.1.1.2.3.2.2a.cmml">K</mtext><mi id="S4.E9.m1.1.1.2.3.2.3" xref="S4.E9.m1.1.1.2.3.2.3.cmml">Y</mi><mi id="S4.E9.m1.1.1.2.3.3" xref="S4.E9.m1.1.1.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S4.E9.m1.1.1.3" xref="S4.E9.m1.1.1.3.cmml"><msub id="S4.E9.m1.1.1.3.2" xref="S4.E9.m1.1.1.3.2.cmml"><mi id="S4.E9.m1.1.1.3.2.2" xref="S4.E9.m1.1.1.3.2.2.cmml">d</mi><mi id="S4.E9.m1.1.1.3.2.3" xref="S4.E9.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S4.E9.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S4.E9.m1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E9.m1.2.2.1.2" lspace="0em" xref="S4.E9.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.2b"><apply id="S4.E9.m1.2.2.1.1.cmml" xref="S4.E9.m1.2.2.1"><eq id="S4.E9.m1.2.2.1.1.1.cmml" xref="S4.E9.m1.2.2.1.1.1"></eq><ci id="S4.E9.m1.2.2.1.1.2a.cmml" xref="S4.E9.m1.2.2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.2.2.1.1.2.cmml" xref="S4.E9.m1.2.2.1.1.2">A</mtext></ci><apply id="S4.E9.m1.2.2.1.1.3.cmml" xref="S4.E9.m1.2.2.1.1.3"><times id="S4.E9.m1.2.2.1.1.3.1.cmml" xref="S4.E9.m1.2.2.1.1.3.1"></times><ci id="S4.E9.m1.2.2.1.1.3.2a.cmml" xref="S4.E9.m1.2.2.1.1.3.2"><mtext id="S4.E9.m1.2.2.1.1.3.2.cmml" xref="S4.E9.m1.2.2.1.1.3.2">Softmax</mtext></ci><apply id="S4.E9.m1.1.1.cmml" xref="S4.E9.m1.2.2.1.1.3.3.2"><divide id="S4.E9.m1.1.1.1.cmml" xref="S4.E9.m1.2.2.1.1.3.3.2"></divide><apply id="S4.E9.m1.1.1.2.cmml" xref="S4.E9.m1.1.1.2"><times id="S4.E9.m1.1.1.2.1.cmml" xref="S4.E9.m1.1.1.2.1"></times><apply id="S4.E9.m1.1.1.2.2.cmml" xref="S4.E9.m1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.2.2.1.cmml" xref="S4.E9.m1.1.1.2.2">subscript</csymbol><ci id="S4.E9.m1.1.1.2.2.2a.cmml" xref="S4.E9.m1.1.1.2.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.1.1.2.2.2.cmml" xref="S4.E9.m1.1.1.2.2.2">Q</mtext></ci><ci id="S4.E9.m1.1.1.2.2.3.cmml" xref="S4.E9.m1.1.1.2.2.3">𝑋</ci></apply><apply id="S4.E9.m1.1.1.2.3.cmml" xref="S4.E9.m1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.2.3.1.cmml" xref="S4.E9.m1.1.1.2.3">superscript</csymbol><apply id="S4.E9.m1.1.1.2.3.2.cmml" xref="S4.E9.m1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.2.3.2.1.cmml" xref="S4.E9.m1.1.1.2.3">subscript</csymbol><ci id="S4.E9.m1.1.1.2.3.2.2a.cmml" xref="S4.E9.m1.1.1.2.3.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E9.m1.1.1.2.3.2.2.cmml" xref="S4.E9.m1.1.1.2.3.2.2">K</mtext></ci><ci id="S4.E9.m1.1.1.2.3.2.3.cmml" xref="S4.E9.m1.1.1.2.3.2.3">𝑌</ci></apply><ci id="S4.E9.m1.1.1.2.3.3.cmml" xref="S4.E9.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S4.E9.m1.1.1.3.cmml" xref="S4.E9.m1.1.1.3"><root id="S4.E9.m1.1.1.3a.cmml" xref="S4.E9.m1.1.1.3"></root><apply id="S4.E9.m1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.3.2.1.cmml" xref="S4.E9.m1.1.1.3.2">subscript</csymbol><ci id="S4.E9.m1.1.1.3.2.2.cmml" xref="S4.E9.m1.1.1.3.2.2">𝑑</ci><ci id="S4.E9.m1.1.1.3.2.3.cmml" xref="S4.E9.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.2c">\textbf{A}=\text{Softmax}(\frac{\textbf{Q}_{X}\textbf{K}_{Y}^{T}}{\sqrt{d_{k}}%
}).</annotation><annotation encoding="application/x-llamapun" id="S4.E9.m1.2d">A = Softmax ( divide start_ARG Q start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT K start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS1.p1.7">Third, use the attention weights to compute the weighted sum of values for each modality:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\textbf{Z}_{X}=\textbf{A}\textbf{V}_{Y},\quad\textbf{Z}_{Y}=\textbf{A}^{T}%
\textbf{V}_{Y}." class="ltx_Math" display="block" id="S4.E10.m1.1"><semantics id="S4.E10.m1.1a"><mrow id="S4.E10.m1.1.1.1"><mrow id="S4.E10.m1.1.1.1.1.2" xref="S4.E10.m1.1.1.1.1.3.cmml"><mrow id="S4.E10.m1.1.1.1.1.1.1" xref="S4.E10.m1.1.1.1.1.1.1.cmml"><msub id="S4.E10.m1.1.1.1.1.1.1.2" xref="S4.E10.m1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.2.2" xref="S4.E10.m1.1.1.1.1.1.1.2.2a.cmml">Z</mtext><mi id="S4.E10.m1.1.1.1.1.1.1.2.3" xref="S4.E10.m1.1.1.1.1.1.1.2.3.cmml">X</mi></msub><mo id="S4.E10.m1.1.1.1.1.1.1.1" xref="S4.E10.m1.1.1.1.1.1.1.1.cmml">=</mo><msub id="S4.E10.m1.1.1.1.1.1.1.3" xref="S4.E10.m1.1.1.1.1.1.1.3.cmml"><mrow id="S4.E10.m1.1.1.1.1.1.1.3.2" xref="S4.E10.m1.1.1.1.1.1.1.3.2c.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.3.2a" xref="S4.E10.m1.1.1.1.1.1.1.3.2c.cmml">A</mtext><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.3.2b" xref="S4.E10.m1.1.1.1.1.1.1.3.2c.cmml">V</mtext></mrow><mi id="S4.E10.m1.1.1.1.1.1.1.3.3" xref="S4.E10.m1.1.1.1.1.1.1.3.3.cmml">Y</mi></msub></mrow><mo id="S4.E10.m1.1.1.1.1.2.3" rspace="1.167em" xref="S4.E10.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S4.E10.m1.1.1.1.1.2.2" xref="S4.E10.m1.1.1.1.1.2.2.cmml"><msub id="S4.E10.m1.1.1.1.1.2.2.2" xref="S4.E10.m1.1.1.1.1.2.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.2.2" xref="S4.E10.m1.1.1.1.1.2.2.2.2a.cmml">Z</mtext><mi id="S4.E10.m1.1.1.1.1.2.2.2.3" xref="S4.E10.m1.1.1.1.1.2.2.2.3.cmml">Y</mi></msub><mo id="S4.E10.m1.1.1.1.1.2.2.1" xref="S4.E10.m1.1.1.1.1.2.2.1.cmml">=</mo><mrow id="S4.E10.m1.1.1.1.1.2.2.3" xref="S4.E10.m1.1.1.1.1.2.2.3.cmml"><msup id="S4.E10.m1.1.1.1.1.2.2.3.2" xref="S4.E10.m1.1.1.1.1.2.2.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.3.2.2" xref="S4.E10.m1.1.1.1.1.2.2.3.2.2a.cmml">A</mtext><mi id="S4.E10.m1.1.1.1.1.2.2.3.2.3" xref="S4.E10.m1.1.1.1.1.2.2.3.2.3.cmml">T</mi></msup><mo id="S4.E10.m1.1.1.1.1.2.2.3.1" xref="S4.E10.m1.1.1.1.1.2.2.3.1.cmml">⁢</mo><msub id="S4.E10.m1.1.1.1.1.2.2.3.3" xref="S4.E10.m1.1.1.1.1.2.2.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.3.3.2" xref="S4.E10.m1.1.1.1.1.2.2.3.3.2a.cmml">V</mtext><mi id="S4.E10.m1.1.1.1.1.2.2.3.3.3" xref="S4.E10.m1.1.1.1.1.2.2.3.3.3.cmml">Y</mi></msub></mrow></mrow></mrow><mo id="S4.E10.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E10.m1.1b"><apply id="S4.E10.m1.1.1.1.1.3.cmml" xref="S4.E10.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.3a.cmml" xref="S4.E10.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E10.m1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1"><eq id="S4.E10.m1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.1"></eq><apply id="S4.E10.m1.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.1.1.2.2a.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2.2">Z</mtext></ci><ci id="S4.E10.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2.3">𝑋</ci></apply><apply id="S4.E10.m1.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.1.1.3.2c.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2"><mrow id="S4.E10.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.3.2a.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2">A</mtext><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.1.1.3.2b.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2">V</mtext></mrow></ci><ci id="S4.E10.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3">𝑌</ci></apply></apply><apply id="S4.E10.m1.1.1.1.1.2.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2"><eq id="S4.E10.m1.1.1.1.1.2.2.1.cmml" xref="S4.E10.m1.1.1.1.1.2.2.1"></eq><apply id="S4.E10.m1.1.1.1.1.2.2.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.2.2.2.1.cmml" xref="S4.E10.m1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.2.2.2.2a.cmml" xref="S4.E10.m1.1.1.1.1.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2.2.2">Z</mtext></ci><ci id="S4.E10.m1.1.1.1.1.2.2.2.3.cmml" xref="S4.E10.m1.1.1.1.1.2.2.2.3">𝑌</ci></apply><apply id="S4.E10.m1.1.1.1.1.2.2.3.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3"><times id="S4.E10.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.1"></times><apply id="S4.E10.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.2.2.3.2.1.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.2">superscript</csymbol><ci id="S4.E10.m1.1.1.1.1.2.2.3.2.2a.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.3.2.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.2.2">A</mtext></ci><ci id="S4.E10.m1.1.1.1.1.2.2.3.2.3.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.2.3">𝑇</ci></apply><apply id="S4.E10.m1.1.1.1.1.2.2.3.3.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.3"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.2.2.3.3.1.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.3">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.2.2.3.3.2a.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E10.m1.1.1.1.1.2.2.3.3.2.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.3.2">V</mtext></ci><ci id="S4.E10.m1.1.1.1.1.2.2.3.3.3.cmml" xref="S4.E10.m1.1.1.1.1.2.2.3.3.3">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E10.m1.1c">\textbf{Z}_{X}=\textbf{A}\textbf{V}_{Y},\quad\textbf{Z}_{Y}=\textbf{A}^{T}%
\textbf{V}_{Y}.</annotation><annotation encoding="application/x-llamapun" id="S4.E10.m1.1d">Z start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = bold_A bold_V start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , Z start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT = A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT V start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS1.p1.8">In recent years, the urban computing community leveraged such fusion mode and its variants for comprehensive urban modality alignment. For example, to model the relations among the target road attributes, <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>]</cite> generated task-specific fused representations by applying attention-based feature fusion of location, bearing, speed, and map context. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite> applied a GAT-based attention mechanism in learning region representations from two views of the built correlations (i.e., human mobility view and region attribute view), and a joint learning module to fuse multi-view embeddings. In this proposed multi-view joint learning module shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F14" title="Figure 14 ‣ 4.3.1 Attention-based Alignment ‣ 4.3 Alignment-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">14</span></a>, the self-attention layer enables information sharing across all views and the fusion layer is responsible for combining multi-view representations via adaptive weights.</p>
</div>
<figure class="ltx_figure" id="S4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="149" id="S4.F14.g1" src="extracted/5670403/Images_zxc/liyong.png" width="198"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The architecture of multi-view joint learning module, consisting of a self-attention layer and a fusion layer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite>. <math alttext="\mathcal{E}_{i}^{\prime}" class="ltx_Math" display="inline" id="S4.F14.4.m1.1"><semantics id="S4.F14.4.m1.1b"><msubsup id="S4.F14.4.m1.1.1" xref="S4.F14.4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.F14.4.m1.1.1.2.2" mathcolor="#000000" xref="S4.F14.4.m1.1.1.2.2.cmml">ℰ</mi><mi id="S4.F14.4.m1.1.1.2.3" mathcolor="#000000" xref="S4.F14.4.m1.1.1.2.3.cmml">i</mi><mo id="S4.F14.4.m1.1.1.3" mathcolor="#000000" xref="S4.F14.4.m1.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.F14.4.m1.1c"><apply id="S4.F14.4.m1.1.1.cmml" xref="S4.F14.4.m1.1.1"><csymbol cd="ambiguous" id="S4.F14.4.m1.1.1.1.cmml" xref="S4.F14.4.m1.1.1">superscript</csymbol><apply id="S4.F14.4.m1.1.1.2.cmml" xref="S4.F14.4.m1.1.1"><csymbol cd="ambiguous" id="S4.F14.4.m1.1.1.2.1.cmml" xref="S4.F14.4.m1.1.1">subscript</csymbol><ci id="S4.F14.4.m1.1.1.2.2.cmml" xref="S4.F14.4.m1.1.1.2.2">ℰ</ci><ci id="S4.F14.4.m1.1.1.2.3.cmml" xref="S4.F14.4.m1.1.1.2.3">𝑖</ci></apply><ci id="S4.F14.4.m1.1.1.3.cmml" xref="S4.F14.4.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F14.4.m1.1d">\mathcal{E}_{i}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.F14.4.m1.1e">caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.F14.6.2" style="color:#000000;"> is the representation for the <math alttext="i" class="ltx_Math" display="inline" id="S4.F14.5.1.m1.1"><semantics id="S4.F14.5.1.m1.1b"><mi id="S4.F14.5.1.m1.1.1" mathcolor="#000000" xref="S4.F14.5.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.F14.5.1.m1.1c"><ci id="S4.F14.5.1.m1.1.1.cmml" xref="S4.F14.5.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F14.5.1.m1.1d">i</annotation><annotation encoding="application/x-llamapun" id="S4.F14.5.1.m1.1e">italic_i</annotation></semantics></math>-th view of global information, and <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.F14.6.2.m2.1"><semantics id="S4.F14.6.2.m2.1b"><mi id="S4.F14.6.2.m2.1.1" mathcolor="#000000" xref="S4.F14.6.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.F14.6.2.m2.1c"><ci id="S4.F14.6.2.m2.1.1.cmml" xref="S4.F14.6.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F14.6.2.m2.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.F14.6.2.m2.1e">italic_α</annotation></semantics></math> is the weight of global information.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">The MVMT-STN model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib288" title="">288</a>]</cite> included a multi-view GCN to capture the global semantic dependency between POI, road network, and risk information. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>]</cite> used a transformer-based aggregation layer to model the graph structure containing both toponym and spatial knowledge. <cite class="ltx_cite ltx_citemacro_citet">Qiang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib225" title="">225</a>]</cite> designed a transformer encoder to represent the features of each package by integrating those of other candidate packages. The RankETPA model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib298" title="">298</a>]</cite> was designed for package pick-up arrival time prediction, and there is an attention module to model the interaction between the package’s features and the courier’s features.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">The other similar attentional fusion work includes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>]</cite> (POI-view and geographic-view representations), <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite> (region-level and inter-traffic correlations), <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite> (geographical and semantic spatio-temporal representations) and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite> (satellite visual and textual representations).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Encoder-based Alignment</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Encoder-based fusion entails the integration of multiple modalities into a shared encoder architecture, as opposed to employing separate encoders for each modality. This approach leverages a unified deep learning model (e.g., RNN variants <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib248" title="">248</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>]</cite> and self-attention methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite>) to collectively process and extract meaningful representations from diverse sources of information. By cohesively injecting multi-modal data into a single encoder, the model is inherently encouraged to capture intricate inter-modal relationships and dependencies.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">For example, <cite class="ltx_cite ltx_citemacro_citet">Song et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib248" title="">248</a>]</cite> developed the DeepTransport model for mobility simulation and transportation mode prediction, where heterogeneous data including GPS records and transportation information are fed into a deep LSTM learning architecture. Such multi-layer LSTM has been demonstrated to be able to learn at different time scales over the input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib103" title="">103</a>]</cite>. The DeepUrbanEvent system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>]</cite> contained a ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib243" title="">243</a>]</cite> encoder module for simultaneous multi-step forecasting of crowd density and crowd flow. The ConvLSTM extends the fully connected LSTM (FC-LSTM) to have convolutional structures in both input-to-state and state-to-state transitions. The RegionDCL framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite> leveraged the Transformer encoder with average-pooling as the region-level encoder of building features and POI information. <cite class="ltx_cite ltx_citemacro_citet">Cai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib28" title="">28</a>]</cite> proposed a multi-level graph encoder equipped with a GAT encoding module to capture couriers’ both high-level transfer modes between AOIs and low-level transfer models between locations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Contrast-based Data Fusion</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Contrastive learning, a pivotal paradigm in machine learning, can be categorized based on contrast creation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib202" title="">202</a>]</cite>. The categorization encompasses methods such as instance contrast, batch contrast, and temporal contrast, which each focuses on contrasting representations through augmentations, batch comparisons, and temporal shifts, respectively.
<span class="ltx_text" id="S4.SS4.p1.1.1" style="color:#000000;">Compared to alignment-based methods, the contrast-based data fusion method enhances discrimination through negative sample augmentation. However, it imposes stringent requirements on the selection of negative samples and batch size. Due to its demand for large datasets, it is well-suited for downstream tasks where data is abundant or easily accessible, such as those related to transportation, environment and economy.</span></p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.4">The InfoNCE (Noise-Contrastive Estimation) loss, a common objective function in contrastive learning, aims to maximize the similarity between positive pairs and minimize the similarity between negative pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib210" title="">210</a>]</cite>. Mathematically, the InfoNCE loss is formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\small\mathcal{L}_{\text{InfoNCE}}(\mathbf{X},\mathbf{Y})=-\log\left(\frac{%
\exp(\text{sim}(\mathbf{X},\mathbf{Y}))}{\exp(\text{sim}(\mathbf{X},\mathbf{Y}%
))+\sum_{k=1}^{K}\exp(\text{sim}(\mathbf{X},\mathbf{N}_{k}))}\right)," class="ltx_Math" display="block" id="S4.E11.m1.15"><semantics id="S4.E11.m1.15a"><mrow id="S4.E11.m1.15.15.1" xref="S4.E11.m1.15.15.1.1.cmml"><mrow id="S4.E11.m1.15.15.1.1" xref="S4.E11.m1.15.15.1.1.cmml"><mrow id="S4.E11.m1.15.15.1.1.2" xref="S4.E11.m1.15.15.1.1.2.cmml"><msub id="S4.E11.m1.15.15.1.1.2.2" xref="S4.E11.m1.15.15.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E11.m1.15.15.1.1.2.2.2" mathsize="90%" xref="S4.E11.m1.15.15.1.1.2.2.2.cmml">ℒ</mi><mtext id="S4.E11.m1.15.15.1.1.2.2.3" mathsize="90%" xref="S4.E11.m1.15.15.1.1.2.2.3a.cmml">InfoNCE</mtext></msub><mo id="S4.E11.m1.15.15.1.1.2.1" xref="S4.E11.m1.15.15.1.1.2.1.cmml">⁢</mo><mrow id="S4.E11.m1.15.15.1.1.2.3.2" xref="S4.E11.m1.15.15.1.1.2.3.1.cmml"><mo id="S4.E11.m1.15.15.1.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S4.E11.m1.15.15.1.1.2.3.1.cmml">(</mo><mi id="S4.E11.m1.12.12" mathsize="90%" xref="S4.E11.m1.12.12.cmml">𝐗</mi><mo id="S4.E11.m1.15.15.1.1.2.3.2.2" mathsize="90%" xref="S4.E11.m1.15.15.1.1.2.3.1.cmml">,</mo><mi id="S4.E11.m1.13.13" mathsize="90%" xref="S4.E11.m1.13.13.cmml">𝐘</mi><mo id="S4.E11.m1.15.15.1.1.2.3.2.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.15.15.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E11.m1.15.15.1.1.1" mathsize="90%" xref="S4.E11.m1.15.15.1.1.1.cmml">=</mo><mrow id="S4.E11.m1.15.15.1.1.3" xref="S4.E11.m1.15.15.1.1.3.cmml"><mo id="S4.E11.m1.15.15.1.1.3a" mathsize="90%" rspace="0.167em" xref="S4.E11.m1.15.15.1.1.3.cmml">−</mo><mrow id="S4.E11.m1.15.15.1.1.3.2.2" xref="S4.E11.m1.15.15.1.1.3.2.1.cmml"><mi id="S4.E11.m1.14.14" mathsize="90%" xref="S4.E11.m1.14.14.cmml">log</mi><mo id="S4.E11.m1.15.15.1.1.3.2.2a" xref="S4.E11.m1.15.15.1.1.3.2.1.cmml">⁡</mo><mrow id="S4.E11.m1.15.15.1.1.3.2.2.1" xref="S4.E11.m1.15.15.1.1.3.2.1.cmml"><mo id="S4.E11.m1.15.15.1.1.3.2.2.1.1" xref="S4.E11.m1.15.15.1.1.3.2.1.cmml">(</mo><mfrac id="S4.E11.m1.11.11" xref="S4.E11.m1.11.11.cmml"><mrow id="S4.E11.m1.4.4.4.4" xref="S4.E11.m1.4.4.4.5.cmml"><mi id="S4.E11.m1.3.3.3.3" mathsize="90%" xref="S4.E11.m1.3.3.3.3.cmml">exp</mi><mo id="S4.E11.m1.4.4.4.4a" xref="S4.E11.m1.4.4.4.5.cmml">⁡</mo><mrow id="S4.E11.m1.4.4.4.4.1" xref="S4.E11.m1.4.4.4.5.cmml"><mo id="S4.E11.m1.4.4.4.4.1.2" maxsize="90%" minsize="90%" xref="S4.E11.m1.4.4.4.5.cmml">(</mo><mrow id="S4.E11.m1.4.4.4.4.1.1" xref="S4.E11.m1.4.4.4.4.1.1.cmml"><mtext id="S4.E11.m1.4.4.4.4.1.1.2" mathsize="90%" xref="S4.E11.m1.4.4.4.4.1.1.2a.cmml">sim</mtext><mo id="S4.E11.m1.4.4.4.4.1.1.1" xref="S4.E11.m1.4.4.4.4.1.1.1.cmml">⁢</mo><mrow id="S4.E11.m1.4.4.4.4.1.1.3.2" xref="S4.E11.m1.4.4.4.4.1.1.3.1.cmml"><mo id="S4.E11.m1.4.4.4.4.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S4.E11.m1.4.4.4.4.1.1.3.1.cmml">(</mo><mi id="S4.E11.m1.1.1.1.1" mathsize="90%" xref="S4.E11.m1.1.1.1.1.cmml">𝐗</mi><mo id="S4.E11.m1.4.4.4.4.1.1.3.2.2" mathsize="90%" xref="S4.E11.m1.4.4.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E11.m1.2.2.2.2" mathsize="90%" xref="S4.E11.m1.2.2.2.2.cmml">𝐘</mi><mo id="S4.E11.m1.4.4.4.4.1.1.3.2.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.4.4.4.4.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E11.m1.4.4.4.4.1.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.4.4.4.5.cmml">)</mo></mrow></mrow><mrow id="S4.E11.m1.11.11.11" xref="S4.E11.m1.11.11.11.cmml"><mrow id="S4.E11.m1.10.10.10.6.1" xref="S4.E11.m1.10.10.10.6.2.cmml"><mi id="S4.E11.m1.7.7.7.3" mathsize="90%" xref="S4.E11.m1.7.7.7.3.cmml">exp</mi><mo id="S4.E11.m1.10.10.10.6.1a" xref="S4.E11.m1.10.10.10.6.2.cmml">⁡</mo><mrow id="S4.E11.m1.10.10.10.6.1.1" xref="S4.E11.m1.10.10.10.6.2.cmml"><mo id="S4.E11.m1.10.10.10.6.1.1.2" maxsize="90%" minsize="90%" xref="S4.E11.m1.10.10.10.6.2.cmml">(</mo><mrow id="S4.E11.m1.10.10.10.6.1.1.1" xref="S4.E11.m1.10.10.10.6.1.1.1.cmml"><mtext id="S4.E11.m1.10.10.10.6.1.1.1.2" mathsize="90%" xref="S4.E11.m1.10.10.10.6.1.1.1.2a.cmml">sim</mtext><mo id="S4.E11.m1.10.10.10.6.1.1.1.1" xref="S4.E11.m1.10.10.10.6.1.1.1.1.cmml">⁢</mo><mrow id="S4.E11.m1.10.10.10.6.1.1.1.3.2" xref="S4.E11.m1.10.10.10.6.1.1.1.3.1.cmml"><mo id="S4.E11.m1.10.10.10.6.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S4.E11.m1.10.10.10.6.1.1.1.3.1.cmml">(</mo><mi id="S4.E11.m1.5.5.5.1" mathsize="90%" xref="S4.E11.m1.5.5.5.1.cmml">𝐗</mi><mo id="S4.E11.m1.10.10.10.6.1.1.1.3.2.2" mathsize="90%" xref="S4.E11.m1.10.10.10.6.1.1.1.3.1.cmml">,</mo><mi id="S4.E11.m1.6.6.6.2" mathsize="90%" xref="S4.E11.m1.6.6.6.2.cmml">𝐘</mi><mo id="S4.E11.m1.10.10.10.6.1.1.1.3.2.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.10.10.10.6.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E11.m1.10.10.10.6.1.1.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.10.10.10.6.2.cmml">)</mo></mrow></mrow><mo id="S4.E11.m1.11.11.11.8" mathsize="90%" rspace="0.055em" xref="S4.E11.m1.11.11.11.8.cmml">+</mo><mrow id="S4.E11.m1.11.11.11.7" xref="S4.E11.m1.11.11.11.7.cmml"><msubsup id="S4.E11.m1.11.11.11.7.2" xref="S4.E11.m1.11.11.11.7.2.cmml"><mo id="S4.E11.m1.11.11.11.7.2.2.2" maxsize="90%" minsize="90%" stretchy="true" xref="S4.E11.m1.11.11.11.7.2.2.2.cmml">∑</mo><mrow id="S4.E11.m1.11.11.11.7.2.2.3" xref="S4.E11.m1.11.11.11.7.2.2.3.cmml"><mi id="S4.E11.m1.11.11.11.7.2.2.3.2" mathsize="90%" xref="S4.E11.m1.11.11.11.7.2.2.3.2.cmml">k</mi><mo id="S4.E11.m1.11.11.11.7.2.2.3.1" mathsize="90%" xref="S4.E11.m1.11.11.11.7.2.2.3.1.cmml">=</mo><mn id="S4.E11.m1.11.11.11.7.2.2.3.3" mathsize="90%" xref="S4.E11.m1.11.11.11.7.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E11.m1.11.11.11.7.2.3" mathsize="90%" xref="S4.E11.m1.11.11.11.7.2.3.cmml">K</mi></msubsup><mrow id="S4.E11.m1.11.11.11.7.1.1" xref="S4.E11.m1.11.11.11.7.1.2.cmml"><mi id="S4.E11.m1.9.9.9.5" mathsize="90%" xref="S4.E11.m1.9.9.9.5.cmml">exp</mi><mo id="S4.E11.m1.11.11.11.7.1.1a" xref="S4.E11.m1.11.11.11.7.1.2.cmml">⁡</mo><mrow id="S4.E11.m1.11.11.11.7.1.1.1" xref="S4.E11.m1.11.11.11.7.1.2.cmml"><mo id="S4.E11.m1.11.11.11.7.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E11.m1.11.11.11.7.1.2.cmml">(</mo><mrow id="S4.E11.m1.11.11.11.7.1.1.1.1" xref="S4.E11.m1.11.11.11.7.1.1.1.1.cmml"><mtext id="S4.E11.m1.11.11.11.7.1.1.1.1.3" mathsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.3a.cmml">sim</mtext><mo id="S4.E11.m1.11.11.11.7.1.1.1.1.2" xref="S4.E11.m1.11.11.11.7.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.2.cmml"><mo id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E11.m1.8.8.8.4" mathsize="90%" xref="S4.E11.m1.8.8.8.4.cmml">𝐗</mi><mo id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.3" mathsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.2.cmml">,</mo><msub id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.cmml"><mi id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.2" mathsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.2.cmml">𝐍</mi><mi id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.3" mathsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.4" maxsize="90%" minsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E11.m1.11.11.11.7.1.1.1.3" maxsize="90%" minsize="90%" xref="S4.E11.m1.11.11.11.7.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac><mo id="S4.E11.m1.15.15.1.1.3.2.2.1.2" xref="S4.E11.m1.15.15.1.1.3.2.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E11.m1.15.15.1.2" mathsize="90%" xref="S4.E11.m1.15.15.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E11.m1.15b"><apply id="S4.E11.m1.15.15.1.1.cmml" xref="S4.E11.m1.15.15.1"><eq id="S4.E11.m1.15.15.1.1.1.cmml" xref="S4.E11.m1.15.15.1.1.1"></eq><apply id="S4.E11.m1.15.15.1.1.2.cmml" xref="S4.E11.m1.15.15.1.1.2"><times id="S4.E11.m1.15.15.1.1.2.1.cmml" xref="S4.E11.m1.15.15.1.1.2.1"></times><apply id="S4.E11.m1.15.15.1.1.2.2.cmml" xref="S4.E11.m1.15.15.1.1.2.2"><csymbol cd="ambiguous" id="S4.E11.m1.15.15.1.1.2.2.1.cmml" xref="S4.E11.m1.15.15.1.1.2.2">subscript</csymbol><ci id="S4.E11.m1.15.15.1.1.2.2.2.cmml" xref="S4.E11.m1.15.15.1.1.2.2.2">ℒ</ci><ci id="S4.E11.m1.15.15.1.1.2.2.3a.cmml" xref="S4.E11.m1.15.15.1.1.2.2.3"><mtext id="S4.E11.m1.15.15.1.1.2.2.3.cmml" mathsize="63%" xref="S4.E11.m1.15.15.1.1.2.2.3">InfoNCE</mtext></ci></apply><interval closure="open" id="S4.E11.m1.15.15.1.1.2.3.1.cmml" xref="S4.E11.m1.15.15.1.1.2.3.2"><ci id="S4.E11.m1.12.12.cmml" xref="S4.E11.m1.12.12">𝐗</ci><ci id="S4.E11.m1.13.13.cmml" xref="S4.E11.m1.13.13">𝐘</ci></interval></apply><apply id="S4.E11.m1.15.15.1.1.3.cmml" xref="S4.E11.m1.15.15.1.1.3"><minus id="S4.E11.m1.15.15.1.1.3.1.cmml" xref="S4.E11.m1.15.15.1.1.3"></minus><apply id="S4.E11.m1.15.15.1.1.3.2.1.cmml" xref="S4.E11.m1.15.15.1.1.3.2.2"><log id="S4.E11.m1.14.14.cmml" xref="S4.E11.m1.14.14"></log><apply id="S4.E11.m1.11.11.cmml" xref="S4.E11.m1.11.11"><divide id="S4.E11.m1.11.11.12.cmml" xref="S4.E11.m1.11.11"></divide><apply id="S4.E11.m1.4.4.4.5.cmml" xref="S4.E11.m1.4.4.4.4"><exp id="S4.E11.m1.3.3.3.3.cmml" xref="S4.E11.m1.3.3.3.3"></exp><apply id="S4.E11.m1.4.4.4.4.1.1.cmml" xref="S4.E11.m1.4.4.4.4.1.1"><times id="S4.E11.m1.4.4.4.4.1.1.1.cmml" xref="S4.E11.m1.4.4.4.4.1.1.1"></times><ci id="S4.E11.m1.4.4.4.4.1.1.2a.cmml" xref="S4.E11.m1.4.4.4.4.1.1.2"><mtext id="S4.E11.m1.4.4.4.4.1.1.2.cmml" mathsize="90%" xref="S4.E11.m1.4.4.4.4.1.1.2">sim</mtext></ci><interval closure="open" id="S4.E11.m1.4.4.4.4.1.1.3.1.cmml" xref="S4.E11.m1.4.4.4.4.1.1.3.2"><ci id="S4.E11.m1.1.1.1.1.cmml" xref="S4.E11.m1.1.1.1.1">𝐗</ci><ci id="S4.E11.m1.2.2.2.2.cmml" xref="S4.E11.m1.2.2.2.2">𝐘</ci></interval></apply></apply><apply id="S4.E11.m1.11.11.11.cmml" xref="S4.E11.m1.11.11.11"><plus id="S4.E11.m1.11.11.11.8.cmml" xref="S4.E11.m1.11.11.11.8"></plus><apply id="S4.E11.m1.10.10.10.6.2.cmml" xref="S4.E11.m1.10.10.10.6.1"><exp id="S4.E11.m1.7.7.7.3.cmml" xref="S4.E11.m1.7.7.7.3"></exp><apply id="S4.E11.m1.10.10.10.6.1.1.1.cmml" xref="S4.E11.m1.10.10.10.6.1.1.1"><times id="S4.E11.m1.10.10.10.6.1.1.1.1.cmml" xref="S4.E11.m1.10.10.10.6.1.1.1.1"></times><ci id="S4.E11.m1.10.10.10.6.1.1.1.2a.cmml" xref="S4.E11.m1.10.10.10.6.1.1.1.2"><mtext id="S4.E11.m1.10.10.10.6.1.1.1.2.cmml" mathsize="90%" xref="S4.E11.m1.10.10.10.6.1.1.1.2">sim</mtext></ci><interval closure="open" id="S4.E11.m1.10.10.10.6.1.1.1.3.1.cmml" xref="S4.E11.m1.10.10.10.6.1.1.1.3.2"><ci id="S4.E11.m1.5.5.5.1.cmml" xref="S4.E11.m1.5.5.5.1">𝐗</ci><ci id="S4.E11.m1.6.6.6.2.cmml" xref="S4.E11.m1.6.6.6.2">𝐘</ci></interval></apply></apply><apply id="S4.E11.m1.11.11.11.7.cmml" xref="S4.E11.m1.11.11.11.7"><apply id="S4.E11.m1.11.11.11.7.2.cmml" xref="S4.E11.m1.11.11.11.7.2"><csymbol cd="ambiguous" id="S4.E11.m1.11.11.11.7.2.1.cmml" xref="S4.E11.m1.11.11.11.7.2">superscript</csymbol><apply id="S4.E11.m1.11.11.11.7.2.2.cmml" xref="S4.E11.m1.11.11.11.7.2"><csymbol cd="ambiguous" id="S4.E11.m1.11.11.11.7.2.2.1.cmml" xref="S4.E11.m1.11.11.11.7.2">subscript</csymbol><sum id="S4.E11.m1.11.11.11.7.2.2.2.cmml" xref="S4.E11.m1.11.11.11.7.2.2.2"></sum><apply id="S4.E11.m1.11.11.11.7.2.2.3.cmml" xref="S4.E11.m1.11.11.11.7.2.2.3"><eq id="S4.E11.m1.11.11.11.7.2.2.3.1.cmml" xref="S4.E11.m1.11.11.11.7.2.2.3.1"></eq><ci id="S4.E11.m1.11.11.11.7.2.2.3.2.cmml" xref="S4.E11.m1.11.11.11.7.2.2.3.2">𝑘</ci><cn id="S4.E11.m1.11.11.11.7.2.2.3.3.cmml" type="integer" xref="S4.E11.m1.11.11.11.7.2.2.3.3">1</cn></apply></apply><ci id="S4.E11.m1.11.11.11.7.2.3.cmml" xref="S4.E11.m1.11.11.11.7.2.3">𝐾</ci></apply><apply id="S4.E11.m1.11.11.11.7.1.2.cmml" xref="S4.E11.m1.11.11.11.7.1.1"><exp id="S4.E11.m1.9.9.9.5.cmml" xref="S4.E11.m1.9.9.9.5"></exp><apply id="S4.E11.m1.11.11.11.7.1.1.1.1.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1"><times id="S4.E11.m1.11.11.11.7.1.1.1.1.2.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.2"></times><ci id="S4.E11.m1.11.11.11.7.1.1.1.1.3a.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.3"><mtext id="S4.E11.m1.11.11.11.7.1.1.1.1.3.cmml" mathsize="90%" xref="S4.E11.m1.11.11.11.7.1.1.1.1.3">sim</mtext></ci><interval closure="open" id="S4.E11.m1.11.11.11.7.1.1.1.1.1.2.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1"><ci id="S4.E11.m1.8.8.8.4.cmml" xref="S4.E11.m1.8.8.8.4">𝐗</ci><apply id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.1.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.2.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.2">𝐍</ci><ci id="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.3.cmml" xref="S4.E11.m1.11.11.11.7.1.1.1.1.1.1.1.3">𝑘</ci></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E11.m1.15c">\small\mathcal{L}_{\text{InfoNCE}}(\mathbf{X},\mathbf{Y})=-\log\left(\frac{%
\exp(\text{sim}(\mathbf{X},\mathbf{Y}))}{\exp(\text{sim}(\mathbf{X},\mathbf{Y}%
))+\sum_{k=1}^{K}\exp(\text{sim}(\mathbf{X},\mathbf{N}_{k}))}\right),</annotation><annotation encoding="application/x-llamapun" id="S4.E11.m1.15d">caligraphic_L start_POSTSUBSCRIPT InfoNCE end_POSTSUBSCRIPT ( bold_X , bold_Y ) = - roman_log ( divide start_ARG roman_exp ( sim ( bold_X , bold_Y ) ) end_ARG start_ARG roman_exp ( sim ( bold_X , bold_Y ) ) + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( sim ( bold_X , bold_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.p2.3">where <math alttext="K" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_K</annotation></semantics></math> is the total number of negative samples, <math alttext="\text{sim}(\mathbf{X},\mathbf{Y})" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.2"><semantics id="S4.SS4.p2.2.m2.2a"><mrow id="S4.SS4.p2.2.m2.2.3" xref="S4.SS4.p2.2.m2.2.3.cmml"><mtext id="S4.SS4.p2.2.m2.2.3.2" xref="S4.SS4.p2.2.m2.2.3.2a.cmml">sim</mtext><mo id="S4.SS4.p2.2.m2.2.3.1" xref="S4.SS4.p2.2.m2.2.3.1.cmml">⁢</mo><mrow id="S4.SS4.p2.2.m2.2.3.3.2" xref="S4.SS4.p2.2.m2.2.3.3.1.cmml"><mo id="S4.SS4.p2.2.m2.2.3.3.2.1" stretchy="false" xref="S4.SS4.p2.2.m2.2.3.3.1.cmml">(</mo><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">𝐗</mi><mo id="S4.SS4.p2.2.m2.2.3.3.2.2" xref="S4.SS4.p2.2.m2.2.3.3.1.cmml">,</mo><mi id="S4.SS4.p2.2.m2.2.2" xref="S4.SS4.p2.2.m2.2.2.cmml">𝐘</mi><mo id="S4.SS4.p2.2.m2.2.3.3.2.3" stretchy="false" xref="S4.SS4.p2.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.2b"><apply id="S4.SS4.p2.2.m2.2.3.cmml" xref="S4.SS4.p2.2.m2.2.3"><times id="S4.SS4.p2.2.m2.2.3.1.cmml" xref="S4.SS4.p2.2.m2.2.3.1"></times><ci id="S4.SS4.p2.2.m2.2.3.2a.cmml" xref="S4.SS4.p2.2.m2.2.3.2"><mtext id="S4.SS4.p2.2.m2.2.3.2.cmml" xref="S4.SS4.p2.2.m2.2.3.2">sim</mtext></ci><interval closure="open" id="S4.SS4.p2.2.m2.2.3.3.1.cmml" xref="S4.SS4.p2.2.m2.2.3.3.2"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝐗</ci><ci id="S4.SS4.p2.2.m2.2.2.cmml" xref="S4.SS4.p2.2.m2.2.2">𝐘</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.2c">\text{sim}(\mathbf{X},\mathbf{Y})</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.2d">sim ( bold_X , bold_Y )</annotation></semantics></math> is the similarity measure between positive pairs, and <math alttext="\mathbf{N}_{k}" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><msub id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">𝐍</mi><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">𝐍</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">\mathbf{N}_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">bold_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> represents negative samples.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">CLIP (Contrastive Language-Image Pre-training) is a typical model in contrastive learning, designed to concurrently learn representations of images and text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib227" title="">227</a>]</cite>. CLIP can be considered a form of instance contrast because it encourages the model to align representations of a given instance across different modalities. The model achieves this by minimizing the negative logarithmic probability of similarity between positive image-text pairs while contrasting against negative pairs. Mathematically, the CLIP loss function (<math alttext="\mathcal{L}_{\text{CLIP}}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3a.cmml">CLIP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">ℒ</ci><ci id="S4.SS4.p3.1.m1.1.1.3a.cmml" xref="S4.SS4.p3.1.m1.1.1.3"><mtext id="S4.SS4.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p3.1.m1.1.1.3">CLIP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\mathcal{L}_{\text{CLIP}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT CLIP end_POSTSUBSCRIPT</annotation></semantics></math>) is expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\small\mathcal{L}_{\text{CLIP}}(\mathbf{I},\mathbf{T})=-\log\left(\frac{\exp(%
\text{sim}(\mathbf{I},\mathbf{T}))}{\exp(\text{sim}(\mathbf{I},\mathbf{T}))+%
\sum_{k=1}^{K}\exp(\text{sim}(\mathbf{I},\mathbf{N}_{k}))}\right)." class="ltx_Math" display="block" id="S4.E12.m1.15"><semantics id="S4.E12.m1.15a"><mrow id="S4.E12.m1.15.15.1" xref="S4.E12.m1.15.15.1.1.cmml"><mrow id="S4.E12.m1.15.15.1.1" xref="S4.E12.m1.15.15.1.1.cmml"><mrow id="S4.E12.m1.15.15.1.1.2" xref="S4.E12.m1.15.15.1.1.2.cmml"><msub id="S4.E12.m1.15.15.1.1.2.2" xref="S4.E12.m1.15.15.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E12.m1.15.15.1.1.2.2.2" mathsize="90%" xref="S4.E12.m1.15.15.1.1.2.2.2.cmml">ℒ</mi><mtext id="S4.E12.m1.15.15.1.1.2.2.3" mathsize="90%" xref="S4.E12.m1.15.15.1.1.2.2.3a.cmml">CLIP</mtext></msub><mo id="S4.E12.m1.15.15.1.1.2.1" xref="S4.E12.m1.15.15.1.1.2.1.cmml">⁢</mo><mrow id="S4.E12.m1.15.15.1.1.2.3.2" xref="S4.E12.m1.15.15.1.1.2.3.1.cmml"><mo id="S4.E12.m1.15.15.1.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S4.E12.m1.15.15.1.1.2.3.1.cmml">(</mo><mi id="S4.E12.m1.12.12" mathsize="90%" xref="S4.E12.m1.12.12.cmml">𝐈</mi><mo id="S4.E12.m1.15.15.1.1.2.3.2.2" mathsize="90%" xref="S4.E12.m1.15.15.1.1.2.3.1.cmml">,</mo><mi id="S4.E12.m1.13.13" mathsize="90%" xref="S4.E12.m1.13.13.cmml">𝐓</mi><mo id="S4.E12.m1.15.15.1.1.2.3.2.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.15.15.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E12.m1.15.15.1.1.1" mathsize="90%" xref="S4.E12.m1.15.15.1.1.1.cmml">=</mo><mrow id="S4.E12.m1.15.15.1.1.3" xref="S4.E12.m1.15.15.1.1.3.cmml"><mo id="S4.E12.m1.15.15.1.1.3a" mathsize="90%" rspace="0.167em" xref="S4.E12.m1.15.15.1.1.3.cmml">−</mo><mrow id="S4.E12.m1.15.15.1.1.3.2.2" xref="S4.E12.m1.15.15.1.1.3.2.1.cmml"><mi id="S4.E12.m1.14.14" mathsize="90%" xref="S4.E12.m1.14.14.cmml">log</mi><mo id="S4.E12.m1.15.15.1.1.3.2.2a" xref="S4.E12.m1.15.15.1.1.3.2.1.cmml">⁡</mo><mrow id="S4.E12.m1.15.15.1.1.3.2.2.1" xref="S4.E12.m1.15.15.1.1.3.2.1.cmml"><mo id="S4.E12.m1.15.15.1.1.3.2.2.1.1" xref="S4.E12.m1.15.15.1.1.3.2.1.cmml">(</mo><mfrac id="S4.E12.m1.11.11" xref="S4.E12.m1.11.11.cmml"><mrow id="S4.E12.m1.4.4.4.4" xref="S4.E12.m1.4.4.4.5.cmml"><mi id="S4.E12.m1.3.3.3.3" mathsize="90%" xref="S4.E12.m1.3.3.3.3.cmml">exp</mi><mo id="S4.E12.m1.4.4.4.4a" xref="S4.E12.m1.4.4.4.5.cmml">⁡</mo><mrow id="S4.E12.m1.4.4.4.4.1" xref="S4.E12.m1.4.4.4.5.cmml"><mo id="S4.E12.m1.4.4.4.4.1.2" maxsize="90%" minsize="90%" xref="S4.E12.m1.4.4.4.5.cmml">(</mo><mrow id="S4.E12.m1.4.4.4.4.1.1" xref="S4.E12.m1.4.4.4.4.1.1.cmml"><mtext id="S4.E12.m1.4.4.4.4.1.1.2" mathsize="90%" xref="S4.E12.m1.4.4.4.4.1.1.2a.cmml">sim</mtext><mo id="S4.E12.m1.4.4.4.4.1.1.1" xref="S4.E12.m1.4.4.4.4.1.1.1.cmml">⁢</mo><mrow id="S4.E12.m1.4.4.4.4.1.1.3.2" xref="S4.E12.m1.4.4.4.4.1.1.3.1.cmml"><mo id="S4.E12.m1.4.4.4.4.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S4.E12.m1.4.4.4.4.1.1.3.1.cmml">(</mo><mi id="S4.E12.m1.1.1.1.1" mathsize="90%" xref="S4.E12.m1.1.1.1.1.cmml">𝐈</mi><mo id="S4.E12.m1.4.4.4.4.1.1.3.2.2" mathsize="90%" xref="S4.E12.m1.4.4.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E12.m1.2.2.2.2" mathsize="90%" xref="S4.E12.m1.2.2.2.2.cmml">𝐓</mi><mo id="S4.E12.m1.4.4.4.4.1.1.3.2.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.4.4.4.4.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E12.m1.4.4.4.4.1.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.4.4.4.5.cmml">)</mo></mrow></mrow><mrow id="S4.E12.m1.11.11.11" xref="S4.E12.m1.11.11.11.cmml"><mrow id="S4.E12.m1.10.10.10.6.1" xref="S4.E12.m1.10.10.10.6.2.cmml"><mi id="S4.E12.m1.7.7.7.3" mathsize="90%" xref="S4.E12.m1.7.7.7.3.cmml">exp</mi><mo id="S4.E12.m1.10.10.10.6.1a" xref="S4.E12.m1.10.10.10.6.2.cmml">⁡</mo><mrow id="S4.E12.m1.10.10.10.6.1.1" xref="S4.E12.m1.10.10.10.6.2.cmml"><mo id="S4.E12.m1.10.10.10.6.1.1.2" maxsize="90%" minsize="90%" xref="S4.E12.m1.10.10.10.6.2.cmml">(</mo><mrow id="S4.E12.m1.10.10.10.6.1.1.1" xref="S4.E12.m1.10.10.10.6.1.1.1.cmml"><mtext id="S4.E12.m1.10.10.10.6.1.1.1.2" mathsize="90%" xref="S4.E12.m1.10.10.10.6.1.1.1.2a.cmml">sim</mtext><mo id="S4.E12.m1.10.10.10.6.1.1.1.1" xref="S4.E12.m1.10.10.10.6.1.1.1.1.cmml">⁢</mo><mrow id="S4.E12.m1.10.10.10.6.1.1.1.3.2" xref="S4.E12.m1.10.10.10.6.1.1.1.3.1.cmml"><mo id="S4.E12.m1.10.10.10.6.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S4.E12.m1.10.10.10.6.1.1.1.3.1.cmml">(</mo><mi id="S4.E12.m1.5.5.5.1" mathsize="90%" xref="S4.E12.m1.5.5.5.1.cmml">𝐈</mi><mo id="S4.E12.m1.10.10.10.6.1.1.1.3.2.2" mathsize="90%" xref="S4.E12.m1.10.10.10.6.1.1.1.3.1.cmml">,</mo><mi id="S4.E12.m1.6.6.6.2" mathsize="90%" xref="S4.E12.m1.6.6.6.2.cmml">𝐓</mi><mo id="S4.E12.m1.10.10.10.6.1.1.1.3.2.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.10.10.10.6.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E12.m1.10.10.10.6.1.1.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.10.10.10.6.2.cmml">)</mo></mrow></mrow><mo id="S4.E12.m1.11.11.11.8" mathsize="90%" rspace="0.055em" xref="S4.E12.m1.11.11.11.8.cmml">+</mo><mrow id="S4.E12.m1.11.11.11.7" xref="S4.E12.m1.11.11.11.7.cmml"><msubsup id="S4.E12.m1.11.11.11.7.2" xref="S4.E12.m1.11.11.11.7.2.cmml"><mo id="S4.E12.m1.11.11.11.7.2.2.2" maxsize="90%" minsize="90%" stretchy="true" xref="S4.E12.m1.11.11.11.7.2.2.2.cmml">∑</mo><mrow id="S4.E12.m1.11.11.11.7.2.2.3" xref="S4.E12.m1.11.11.11.7.2.2.3.cmml"><mi id="S4.E12.m1.11.11.11.7.2.2.3.2" mathsize="90%" xref="S4.E12.m1.11.11.11.7.2.2.3.2.cmml">k</mi><mo id="S4.E12.m1.11.11.11.7.2.2.3.1" mathsize="90%" xref="S4.E12.m1.11.11.11.7.2.2.3.1.cmml">=</mo><mn id="S4.E12.m1.11.11.11.7.2.2.3.3" mathsize="90%" xref="S4.E12.m1.11.11.11.7.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E12.m1.11.11.11.7.2.3" mathsize="90%" xref="S4.E12.m1.11.11.11.7.2.3.cmml">K</mi></msubsup><mrow id="S4.E12.m1.11.11.11.7.1.1" xref="S4.E12.m1.11.11.11.7.1.2.cmml"><mi id="S4.E12.m1.9.9.9.5" mathsize="90%" xref="S4.E12.m1.9.9.9.5.cmml">exp</mi><mo id="S4.E12.m1.11.11.11.7.1.1a" xref="S4.E12.m1.11.11.11.7.1.2.cmml">⁡</mo><mrow id="S4.E12.m1.11.11.11.7.1.1.1" xref="S4.E12.m1.11.11.11.7.1.2.cmml"><mo id="S4.E12.m1.11.11.11.7.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E12.m1.11.11.11.7.1.2.cmml">(</mo><mrow id="S4.E12.m1.11.11.11.7.1.1.1.1" xref="S4.E12.m1.11.11.11.7.1.1.1.1.cmml"><mtext id="S4.E12.m1.11.11.11.7.1.1.1.1.3" mathsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.3a.cmml">sim</mtext><mo id="S4.E12.m1.11.11.11.7.1.1.1.1.2" xref="S4.E12.m1.11.11.11.7.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.2.cmml"><mo id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E12.m1.8.8.8.4" mathsize="90%" xref="S4.E12.m1.8.8.8.4.cmml">𝐈</mi><mo id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.3" mathsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.2.cmml">,</mo><msub id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.cmml"><mi id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.2" mathsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.2.cmml">𝐍</mi><mi id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.3" mathsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.4" maxsize="90%" minsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E12.m1.11.11.11.7.1.1.1.3" maxsize="90%" minsize="90%" xref="S4.E12.m1.11.11.11.7.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac><mo id="S4.E12.m1.15.15.1.1.3.2.2.1.2" xref="S4.E12.m1.15.15.1.1.3.2.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E12.m1.15.15.1.2" lspace="0em" mathsize="90%" xref="S4.E12.m1.15.15.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E12.m1.15b"><apply id="S4.E12.m1.15.15.1.1.cmml" xref="S4.E12.m1.15.15.1"><eq id="S4.E12.m1.15.15.1.1.1.cmml" xref="S4.E12.m1.15.15.1.1.1"></eq><apply id="S4.E12.m1.15.15.1.1.2.cmml" xref="S4.E12.m1.15.15.1.1.2"><times id="S4.E12.m1.15.15.1.1.2.1.cmml" xref="S4.E12.m1.15.15.1.1.2.1"></times><apply id="S4.E12.m1.15.15.1.1.2.2.cmml" xref="S4.E12.m1.15.15.1.1.2.2"><csymbol cd="ambiguous" id="S4.E12.m1.15.15.1.1.2.2.1.cmml" xref="S4.E12.m1.15.15.1.1.2.2">subscript</csymbol><ci id="S4.E12.m1.15.15.1.1.2.2.2.cmml" xref="S4.E12.m1.15.15.1.1.2.2.2">ℒ</ci><ci id="S4.E12.m1.15.15.1.1.2.2.3a.cmml" xref="S4.E12.m1.15.15.1.1.2.2.3"><mtext id="S4.E12.m1.15.15.1.1.2.2.3.cmml" mathsize="63%" xref="S4.E12.m1.15.15.1.1.2.2.3">CLIP</mtext></ci></apply><interval closure="open" id="S4.E12.m1.15.15.1.1.2.3.1.cmml" xref="S4.E12.m1.15.15.1.1.2.3.2"><ci id="S4.E12.m1.12.12.cmml" xref="S4.E12.m1.12.12">𝐈</ci><ci id="S4.E12.m1.13.13.cmml" xref="S4.E12.m1.13.13">𝐓</ci></interval></apply><apply id="S4.E12.m1.15.15.1.1.3.cmml" xref="S4.E12.m1.15.15.1.1.3"><minus id="S4.E12.m1.15.15.1.1.3.1.cmml" xref="S4.E12.m1.15.15.1.1.3"></minus><apply id="S4.E12.m1.15.15.1.1.3.2.1.cmml" xref="S4.E12.m1.15.15.1.1.3.2.2"><log id="S4.E12.m1.14.14.cmml" xref="S4.E12.m1.14.14"></log><apply id="S4.E12.m1.11.11.cmml" xref="S4.E12.m1.11.11"><divide id="S4.E12.m1.11.11.12.cmml" xref="S4.E12.m1.11.11"></divide><apply id="S4.E12.m1.4.4.4.5.cmml" xref="S4.E12.m1.4.4.4.4"><exp id="S4.E12.m1.3.3.3.3.cmml" xref="S4.E12.m1.3.3.3.3"></exp><apply id="S4.E12.m1.4.4.4.4.1.1.cmml" xref="S4.E12.m1.4.4.4.4.1.1"><times id="S4.E12.m1.4.4.4.4.1.1.1.cmml" xref="S4.E12.m1.4.4.4.4.1.1.1"></times><ci id="S4.E12.m1.4.4.4.4.1.1.2a.cmml" xref="S4.E12.m1.4.4.4.4.1.1.2"><mtext id="S4.E12.m1.4.4.4.4.1.1.2.cmml" mathsize="90%" xref="S4.E12.m1.4.4.4.4.1.1.2">sim</mtext></ci><interval closure="open" id="S4.E12.m1.4.4.4.4.1.1.3.1.cmml" xref="S4.E12.m1.4.4.4.4.1.1.3.2"><ci id="S4.E12.m1.1.1.1.1.cmml" xref="S4.E12.m1.1.1.1.1">𝐈</ci><ci id="S4.E12.m1.2.2.2.2.cmml" xref="S4.E12.m1.2.2.2.2">𝐓</ci></interval></apply></apply><apply id="S4.E12.m1.11.11.11.cmml" xref="S4.E12.m1.11.11.11"><plus id="S4.E12.m1.11.11.11.8.cmml" xref="S4.E12.m1.11.11.11.8"></plus><apply id="S4.E12.m1.10.10.10.6.2.cmml" xref="S4.E12.m1.10.10.10.6.1"><exp id="S4.E12.m1.7.7.7.3.cmml" xref="S4.E12.m1.7.7.7.3"></exp><apply id="S4.E12.m1.10.10.10.6.1.1.1.cmml" xref="S4.E12.m1.10.10.10.6.1.1.1"><times id="S4.E12.m1.10.10.10.6.1.1.1.1.cmml" xref="S4.E12.m1.10.10.10.6.1.1.1.1"></times><ci id="S4.E12.m1.10.10.10.6.1.1.1.2a.cmml" xref="S4.E12.m1.10.10.10.6.1.1.1.2"><mtext id="S4.E12.m1.10.10.10.6.1.1.1.2.cmml" mathsize="90%" xref="S4.E12.m1.10.10.10.6.1.1.1.2">sim</mtext></ci><interval closure="open" id="S4.E12.m1.10.10.10.6.1.1.1.3.1.cmml" xref="S4.E12.m1.10.10.10.6.1.1.1.3.2"><ci id="S4.E12.m1.5.5.5.1.cmml" xref="S4.E12.m1.5.5.5.1">𝐈</ci><ci id="S4.E12.m1.6.6.6.2.cmml" xref="S4.E12.m1.6.6.6.2">𝐓</ci></interval></apply></apply><apply id="S4.E12.m1.11.11.11.7.cmml" xref="S4.E12.m1.11.11.11.7"><apply id="S4.E12.m1.11.11.11.7.2.cmml" xref="S4.E12.m1.11.11.11.7.2"><csymbol cd="ambiguous" id="S4.E12.m1.11.11.11.7.2.1.cmml" xref="S4.E12.m1.11.11.11.7.2">superscript</csymbol><apply id="S4.E12.m1.11.11.11.7.2.2.cmml" xref="S4.E12.m1.11.11.11.7.2"><csymbol cd="ambiguous" id="S4.E12.m1.11.11.11.7.2.2.1.cmml" xref="S4.E12.m1.11.11.11.7.2">subscript</csymbol><sum id="S4.E12.m1.11.11.11.7.2.2.2.cmml" xref="S4.E12.m1.11.11.11.7.2.2.2"></sum><apply id="S4.E12.m1.11.11.11.7.2.2.3.cmml" xref="S4.E12.m1.11.11.11.7.2.2.3"><eq id="S4.E12.m1.11.11.11.7.2.2.3.1.cmml" xref="S4.E12.m1.11.11.11.7.2.2.3.1"></eq><ci id="S4.E12.m1.11.11.11.7.2.2.3.2.cmml" xref="S4.E12.m1.11.11.11.7.2.2.3.2">𝑘</ci><cn id="S4.E12.m1.11.11.11.7.2.2.3.3.cmml" type="integer" xref="S4.E12.m1.11.11.11.7.2.2.3.3">1</cn></apply></apply><ci id="S4.E12.m1.11.11.11.7.2.3.cmml" xref="S4.E12.m1.11.11.11.7.2.3">𝐾</ci></apply><apply id="S4.E12.m1.11.11.11.7.1.2.cmml" xref="S4.E12.m1.11.11.11.7.1.1"><exp id="S4.E12.m1.9.9.9.5.cmml" xref="S4.E12.m1.9.9.9.5"></exp><apply id="S4.E12.m1.11.11.11.7.1.1.1.1.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1"><times id="S4.E12.m1.11.11.11.7.1.1.1.1.2.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.2"></times><ci id="S4.E12.m1.11.11.11.7.1.1.1.1.3a.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.3"><mtext id="S4.E12.m1.11.11.11.7.1.1.1.1.3.cmml" mathsize="90%" xref="S4.E12.m1.11.11.11.7.1.1.1.1.3">sim</mtext></ci><interval closure="open" id="S4.E12.m1.11.11.11.7.1.1.1.1.1.2.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1"><ci id="S4.E12.m1.8.8.8.4.cmml" xref="S4.E12.m1.8.8.8.4">𝐈</ci><apply id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.2.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.2">𝐍</ci><ci id="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.3.cmml" xref="S4.E12.m1.11.11.11.7.1.1.1.1.1.1.1.3">𝑘</ci></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E12.m1.15c">\small\mathcal{L}_{\text{CLIP}}(\mathbf{I},\mathbf{T})=-\log\left(\frac{\exp(%
\text{sim}(\mathbf{I},\mathbf{T}))}{\exp(\text{sim}(\mathbf{I},\mathbf{T}))+%
\sum_{k=1}^{K}\exp(\text{sim}(\mathbf{I},\mathbf{N}_{k}))}\right).</annotation><annotation encoding="application/x-llamapun" id="S4.E12.m1.15d">caligraphic_L start_POSTSUBSCRIPT CLIP end_POSTSUBSCRIPT ( bold_I , bold_T ) = - roman_log ( divide start_ARG roman_exp ( sim ( bold_I , bold_T ) ) end_ARG start_ARG roman_exp ( sim ( bold_I , bold_T ) ) + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( sim ( bold_I , bold_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.p3.3">Here, <math alttext="\mathbf{I}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m1.1"><semantics id="S4.SS4.p3.2.m1.1a"><mi id="S4.SS4.p3.2.m1.1.1" xref="S4.SS4.p3.2.m1.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m1.1b"><ci id="S4.SS4.p3.2.m1.1.1.cmml" xref="S4.SS4.p3.2.m1.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m1.1c">\mathbf{I}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m1.1d">bold_I</annotation></semantics></math> and <math alttext="\mathbf{T}" class="ltx_Math" display="inline" id="S4.SS4.p3.3.m2.1"><semantics id="S4.SS4.p3.3.m2.1a"><mi id="S4.SS4.p3.3.m2.1.1" xref="S4.SS4.p3.3.m2.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m2.1b"><ci id="S4.SS4.p3.3.m2.1.1.cmml" xref="S4.SS4.p3.3.m2.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m2.1c">\mathbf{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.m2.1d">bold_T</annotation></semantics></math> represent image and text representations, respectively. In our survey, contrastive data fusion can be categorized into <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.3.1">instance contrast-based fusion</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS1" title="4.4.1 Instance Contrast-based Fusion ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>) and <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.3.2">batch contrast-based fusion</span> (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS4.SSS2" title="4.4.2 Batch Contrast-based Fusion ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>), as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F15" title="Figure 15 ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">15</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S4.F15.g1" src="extracted/5670403/Images_zxc/contrast.png" width="228"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>The general framework of contrast-based cross-domain data fusion in urban computing: (a) instant contrast; (b) batch contrast.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Instance Contrast-based Fusion</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">In instance contrast, the model aims to fuse the knowledge between different views of the same instance. It is effective for capturing intricate details within individual data points, promoting the model’s ability to recognize fine-grained patterns and features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib183" title="">183</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">Inspired by the computer vision domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib319" title="">319</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib407" title="">407</a>]</cite>, urban multi-modal research started to construct self-augmented data as contrastive pairs. For example, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite> explored self-similarity across urban images to construct contrastive samples via data augmentation methods including rotation, gray-scale, and flipping. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite> learned the physical properties of the geographies via intra-modal contrast among very high resolution (VHR) image augmentations. In addition to imagery augmentation, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite> designed three POI-level augmentations for intra-view contrastive fusion: random insertion, random deletion, and random replacement. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Mao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib196" title="">196</a>]</cite> introduced domain-specific augmentations for road-road contrast and trajectory-trajectory contrast separately, i.e., road segment with its contextual neighbors and trajectory with its detour replaced and dropped alternatives.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Batch Contrast-based Fusion</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Batch contrast involves contrasting samples within the same batch. It introduces a form of global context, where the model learns to distinguish features not just within instances but also in relation to the entire batch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib183" title="">183</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Geographical similarity-guided contrastive learning is pivotal in urban computing because urban images adhere to Tobler’s First Law of Geography <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib198" title="">198</a>]</cite>. For instance, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite> further enhanced the contrastive learning method by taking into account geographical similarity and minimizing the feature distance between two images that are geo-adjacent.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">Besides, the CLIP-based paradigm has been explored in urban computing in recent years. For instance, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>]</cite> proposed the KnowCL model for socioeconomic prediction, which is the first solution that introduces the regional knowledge graph-based semantics and its associated imagery representation as contrastive pairs. Except for self-augmented contrast, <cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite> also bridged the socio-economic semantic gap through inter-modal contrast between VHR images and POIs. The RecMVC model of <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite> included an inter-view contrastive learning module between POI and mobility, serving as a soft co-regularizer to transfer knowledge across multi-views. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib173" title="">173</a>]</cite> designed a trajectory contrastive learning paradigm, where hub and link representations in the same trajectory are enforced to have a higher correlation with one another. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Mao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib196" title="">196</a>]</cite> introduced road-trajectory cross-scale contrast to bridge the two scales by maximizing the total mutual information. This contrast is elaborately tailored via novel positive sampling and adaptive weighting strategies. Following the conventional CLIP setting, <cite class="ltx_cite ltx_citemacro_citet">Yan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite> leveraged satellite imagery and associated LLM-generated description as positive pairs, to learn a robust visual embedding for urban region profiling (depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F16" title="Figure 16 ‣ 4.4.2 Batch Contrast-based Fusion ‣ 4.4 Contrast-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">16</span></a>). Subsequent work, UrbanVLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib97" title="">97</a>]</cite>, has broadened the contrastive learning paradigm to encompass multi-granularity visual clues, including satellite and street-view images. Concurrently, it also proposes effective methods to guarantee the generated text quality.</p>
</div>
<figure class="ltx_figure" id="S4.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S4.F16.g1" src="x3.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>The framework of UrbanCLIP, the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling. It utilizes the CLIP rationale for language-image pertaining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Generation-based Data Fusion</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The primary objective of a generation-based model is to produce the desired data based on specified input conditions.
Both input and output data may manifest in diverse formats
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib323" title="">323</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib356" title="">356</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib358" title="">358</a>]</cite>,
rendering it a favorable choice for data fusion.
<span class="ltx_text" id="S4.SS5.p1.1.1" style="color:#000000;">By generating new content that correlates with input data, generative models are driven to discern intricate correspondences between multimodal information, thereby facilitating efficient information aggregation. However, these models are marked by significant computational complexity and considerable training challenges. Consequently, they are not suitable for tasks requiring high real-time performance. Instead, they excel with large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib131" title="">131</a>]</cite>, making them particularly well-suited for industries such as transportation, economics, and energy.</span>
Compared to the general Computer Vision (CV) or Natural Language Processing (NLP) area,
the field of urban computing exhibits a relatively limited volume of generative research contributions.
However, in recent years, there has been a growing number of generative works emerging, particularly influenced by the rise of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib180" title="">180</a>]</cite>. Based on the specific methods of generation, here we categorize the generation-based data fusion into four types, including autoregressive (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS1" title="4.5.1 Autoregressive Model ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>), mask modeling (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS2" title="4.5.2 Mask Modeling-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>), diffusion-based (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS3" title="4.5.3 Diffusion-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.3</span></a>), and LLM-enhanced models (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.SS5.SSS4" title="4.5.4 LLM-enhanced Data Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">4.5.4</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Autoregressive Model</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.1">The autoregressive model was first developed for the Language Modeling (LM) tasks in NLP, which predict future data based on historical data.
Given a text sequence, <math alttext="x_{1:T}=[x_{1},x_{2},...,x_{T}]" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p1.1.m1.4"><semantics id="S4.SS5.SSS1.p1.1.m1.4a"><mrow id="S4.SS5.SSS1.p1.1.m1.4.4" xref="S4.SS5.SSS1.p1.1.m1.4.4.cmml"><msub id="S4.SS5.SSS1.p1.1.m1.4.4.5" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.cmml"><mi id="S4.SS5.SSS1.p1.1.m1.4.4.5.2" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.2.cmml">x</mi><mrow id="S4.SS5.SSS1.p1.1.m1.4.4.5.3" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.cmml"><mn id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.2" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.2.cmml">1</mn><mo id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.1" lspace="0.278em" rspace="0.278em" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.1.cmml">:</mo><mi id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.3" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.3.cmml">T</mi></mrow></msub><mo id="S4.SS5.SSS1.p1.1.m1.4.4.4" xref="S4.SS5.SSS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS5.SSS1.p1.1.m1.4.4.3.3" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml"><mo id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml">[</mo><msub id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.5" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.2" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.3" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.6" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS5.SSS1.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS5.SSS1.p1.1.m1.1.1.cmml">…</mi><mo id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.7" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.2" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.3" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS1.p1.1.m1.4b"><apply id="S4.SS5.SSS1.p1.1.m1.4.4.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4"><eq id="S4.SS5.SSS1.p1.1.m1.4.4.4.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.4"></eq><apply id="S4.SS5.SSS1.p1.1.m1.4.4.5.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5"><csymbol cd="ambiguous" id="S4.SS5.SSS1.p1.1.m1.4.4.5.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5">subscript</csymbol><ci id="S4.SS5.SSS1.p1.1.m1.4.4.5.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.2">𝑥</ci><apply id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3"><ci id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.1">:</ci><cn id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.2.cmml" type="integer" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.2">1</cn><ci id="S4.SS5.SSS1.p1.1.m1.4.4.5.3.3.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.5.3.3">𝑇</ci></apply></apply><list id="S4.SS5.SSS1.p1.1.m1.4.4.3.4.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3"><apply id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS5.SSS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS5.SSS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS5.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.1.1">…</ci><apply id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS5.SSS1.p1.1.m1.4.4.3.3.3.3">𝑇</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS1.p1.1.m1.4c">x_{1:T}=[x_{1},x_{2},...,x_{T}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS1.p1.1.m1.4d">italic_x start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ]</annotation></semantics></math>, the learning objective of a language model is to maximize the probability of a sequence, which can be mathematically formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathcal{L}_{AR}={\rm max}_{\theta}\sum{}{\rm log}P_{\theta}(x_{t}|x_{t-k},...%
,x_{t-1})," class="ltx_Math" display="block" id="S4.E13.m1.2"><semantics id="S4.E13.m1.2a"><mrow id="S4.E13.m1.2.2.1" xref="S4.E13.m1.2.2.1.1.cmml"><mrow id="S4.E13.m1.2.2.1.1" xref="S4.E13.m1.2.2.1.1.cmml"><msub id="S4.E13.m1.2.2.1.1.3" xref="S4.E13.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E13.m1.2.2.1.1.3.2" xref="S4.E13.m1.2.2.1.1.3.2.cmml">ℒ</mi><mrow id="S4.E13.m1.2.2.1.1.3.3" xref="S4.E13.m1.2.2.1.1.3.3.cmml"><mi id="S4.E13.m1.2.2.1.1.3.3.2" xref="S4.E13.m1.2.2.1.1.3.3.2.cmml">A</mi><mo id="S4.E13.m1.2.2.1.1.3.3.1" xref="S4.E13.m1.2.2.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E13.m1.2.2.1.1.3.3.3" xref="S4.E13.m1.2.2.1.1.3.3.3.cmml">R</mi></mrow></msub><mo id="S4.E13.m1.2.2.1.1.2" xref="S4.E13.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E13.m1.2.2.1.1.1" xref="S4.E13.m1.2.2.1.1.1.cmml"><msub id="S4.E13.m1.2.2.1.1.1.3" xref="S4.E13.m1.2.2.1.1.1.3.cmml"><mi id="S4.E13.m1.2.2.1.1.1.3.2" xref="S4.E13.m1.2.2.1.1.1.3.2.cmml">max</mi><mi id="S4.E13.m1.2.2.1.1.1.3.3" xref="S4.E13.m1.2.2.1.1.1.3.3.cmml">θ</mi></msub><mo id="S4.E13.m1.2.2.1.1.1.2" xref="S4.E13.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E13.m1.2.2.1.1.1.1" xref="S4.E13.m1.2.2.1.1.1.1.cmml"><mo id="S4.E13.m1.2.2.1.1.1.1.2" movablelimits="false" xref="S4.E13.m1.2.2.1.1.1.1.2.cmml">∑</mo><mrow id="S4.E13.m1.2.2.1.1.1.1.1" xref="S4.E13.m1.2.2.1.1.1.1.1.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.3" xref="S4.E13.m1.2.2.1.1.1.1.1.3.cmml">log</mi><mo id="S4.E13.m1.2.2.1.1.1.1.1.2" xref="S4.E13.m1.2.2.1.1.1.1.1.2.cmml">⁢</mo><msub id="S4.E13.m1.2.2.1.1.1.1.1.4" xref="S4.E13.m1.2.2.1.1.1.1.1.4.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.4.2" xref="S4.E13.m1.2.2.1.1.1.1.1.4.2.cmml">P</mi><mi id="S4.E13.m1.2.2.1.1.1.1.1.4.3" xref="S4.E13.m1.2.2.1.1.1.1.1.4.3.cmml">θ</mi></msub><mo id="S4.E13.m1.2.2.1.1.1.1.1.2a" xref="S4.E13.m1.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E13.m1.2.2.1.1.1.1.1.1.1" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">x</mi><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">t</mi></msub><mo fence="false" id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></mrow></msub><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S4.E13.m1.1.1" mathvariant="normal" xref="S4.E13.m1.1.1.cmml">…</mi><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.4" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">t</mi><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S4.E13.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E13.m1.2.2.1.2" xref="S4.E13.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E13.m1.2b"><apply id="S4.E13.m1.2.2.1.1.cmml" xref="S4.E13.m1.2.2.1"><eq id="S4.E13.m1.2.2.1.1.2.cmml" xref="S4.E13.m1.2.2.1.1.2"></eq><apply id="S4.E13.m1.2.2.1.1.3.cmml" xref="S4.E13.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.3.1.cmml" xref="S4.E13.m1.2.2.1.1.3">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.3.2.cmml" xref="S4.E13.m1.2.2.1.1.3.2">ℒ</ci><apply id="S4.E13.m1.2.2.1.1.3.3.cmml" xref="S4.E13.m1.2.2.1.1.3.3"><times id="S4.E13.m1.2.2.1.1.3.3.1.cmml" xref="S4.E13.m1.2.2.1.1.3.3.1"></times><ci id="S4.E13.m1.2.2.1.1.3.3.2.cmml" xref="S4.E13.m1.2.2.1.1.3.3.2">𝐴</ci><ci id="S4.E13.m1.2.2.1.1.3.3.3.cmml" xref="S4.E13.m1.2.2.1.1.3.3.3">𝑅</ci></apply></apply><apply id="S4.E13.m1.2.2.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1"><times id="S4.E13.m1.2.2.1.1.1.2.cmml" xref="S4.E13.m1.2.2.1.1.1.2"></times><apply id="S4.E13.m1.2.2.1.1.1.3.cmml" xref="S4.E13.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.1.3.1.cmml" xref="S4.E13.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.1.3.2.cmml" xref="S4.E13.m1.2.2.1.1.1.3.2">max</ci><ci id="S4.E13.m1.2.2.1.1.1.3.3.cmml" xref="S4.E13.m1.2.2.1.1.1.3.3">𝜃</ci></apply><apply id="S4.E13.m1.2.2.1.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1"><sum id="S4.E13.m1.2.2.1.1.1.1.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.2"></sum><apply id="S4.E13.m1.2.2.1.1.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1"><times id="S4.E13.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.2"></times><ci id="S4.E13.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.3">log</ci><apply id="S4.E13.m1.2.2.1.1.1.1.1.4.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.1.1.1.4.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.1.1.1.4.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.4.2">𝑃</ci><ci id="S4.E13.m1.2.2.1.1.1.1.1.4.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.4.3">𝜃</ci></apply><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.2">𝑥</ci><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.4.3">𝑡</ci></apply><list id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply><ci id="S4.E13.m1.1.1.cmml" xref="S4.E13.m1.1.1">…</ci><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><apply id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><minus id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1"></minus><ci id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">𝑡</ci><cn id="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S4.E13.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E13.m1.2c">\mathcal{L}_{AR}={\rm max}_{\theta}\sum{}{\rm log}P_{\theta}(x_{t}|x_{t-k},...%
,x_{t-1}),</annotation><annotation encoding="application/x-llamapun" id="S4.E13.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_A italic_R end_POSTSUBSCRIPT = roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∑ roman_log italic_P start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t - italic_k end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS5.SSS1.p1.2">where <math alttext="k" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p1.2.m1.1"><semantics id="S4.SS5.SSS1.p1.2.m1.1a"><mi id="S4.SS5.SSS1.p1.2.m1.1.1" xref="S4.SS5.SSS1.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS1.p1.2.m1.1b"><ci id="S4.SS5.SSS1.p1.2.m1.1.1.cmml" xref="S4.SS5.SSS1.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS1.p1.2.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS1.p1.2.m1.1d">italic_k</annotation></semantics></math> is the size of the sliding window.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p2">
<p class="ltx_p" id="S4.SS5.SSS1.p2.1">Gradually, the definition of input data and output data for autoregressive model has expanded from text to encompass multi-modal data. Owing to the multi-modal nature of the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib268" title="">268</a>]</cite>, whereby queries, keys, and values can stem from different modalities, it has facilitated the exploration of multi-modal fusion.
VirTex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib57" title="">57</a>]</cite> proposed that compared with contrastive learning which uses classification labels as a learning signal, captions can provide a more semantically dense learning signal.
SimVLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib295" title="">295</a>]</cite> reformulated the typical encoder-decoder architecture for end-to-end training with a single prefix language modeling objective, achieving the efficient utilization of weakly aligned image-text pairs.
CoCa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib344" title="">344</a>]</cite> is a follow-up work to ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib152" title="">152</a>]</cite> and SimVLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib295" title="">295</a>]</cite>, which creatively integrated attentional pooling methods in a creative manner, combining contrastive and generative approaches to achieve exceptional performance.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p3">
<p class="ltx_p" id="S4.SS5.SSS1.p3.1">In the field of urban computing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>]</cite>, the generative decoder has played a significant role, even before the era of transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib268" title="">268</a>]</cite>.
GeoMAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib162" title="">162</a>]</cite> first introduced a multi-layer attention mechanism for spatio-temporal data prediction based on the encoder-decoder architecture. The generative decoder combined LSTM and Temporal Attention to predict the future performance of sensors. It achieved excellent performance in applications such as water quality prediction and air quality.
<cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib382" title="">382</a>]</cite> implemented a bottom-up and top-down framework to do street-view image classification, which utilized RNN units to make predictions based on visual elements and contextual information.
Besides, in urban computing area <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>]</cite>, the limited availability and diverse nature of domain data, as well as the challenges in its acquisition, have led to increased interest in multi-modal pretraining in recent years.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Mask Modeling-based Fusion</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.1">The concept of masking modeling task has its origins in the field of NLP, contributing to the success of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib60" title="">60</a>]</cite>.
MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib102" title="">102</a>]</cite> is the first work that introduces masking modeling into CV. Compared with masking modeling works in NLP, MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib102" title="">102</a>]</cite> has a larger mask ratio due to the fact that the images have less information density than natural language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib102" title="">102</a>]</cite>.
GraphMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib104" title="">104</a>]</cite> unleashes the power of mask modeling for graph structure. Different from most graph autoencoders’ efforts in structure reconstruction, it proposes to focus on feature reconstruction with both a masking strategy and scaled cosine error.
In the context of mask modeling structure for multi-modal data fusion, various modalities are typically provided as input, followed by the masking of a portion or all of the modal data at varying proportions. Subsequently, information from other modalities is integrated to reconstruct the masked data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib142" title="">142</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib61" title="">61</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS2.p2">
<p class="ltx_p" id="S4.SS5.SSS2.p2.1">MGeo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib61" title="">61</a>]</cite> combined text and geolocation to implement location embedding for query-POI matching, which utilized two unimodal masked language modeling (MLM) loss and one Multi-Modal MLM loss to enforce information interaction.
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F17" title="Figure 17 ‣ 4.5.2 Mask Modeling-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">17</span></a>, G2PTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib304" title="">304</a>]</cite> is a graph-based pre-trained model for text-based delivery address embedding.
It leveraged MLM as one of the pre-training tasks to simulate input noise in missing or incorrect address cases in real-world scenarios.
QUERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib314" title="">314</a>]</cite> adapted pre-trained language models to specific Travel Domain Search applications, which designed a Geography-aware masking strategy to force the pre-trained model to pay more attention to the geographical location phrases.
ERNIE-GeoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib110" title="">110</a>]</cite> was dedicated to integrating toponym knowledge and spatial knowledge. By utilizing a Masked Language Modeling training procedure, it can acquire four types of toponym knowledge, including relationships between POI name, address, and type.</p>
</div>
<figure class="ltx_figure" id="S4.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S4.F17.g1" src="extracted/5670403/Images_zxc/g2ptl.jpg" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>The framework of G2PTL, a Geography-Graph Pre-Trained model for delivery address in the Logistics field, which uses the MLM task to learn the semantic information in the address <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib304" title="">304</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.SSS2.p3">
<p class="ltx_p" id="S4.SS5.SSS2.p3.1">In the remote sensing area, despite the similarities between remote sensing images and RGB images, they also possess distinctions, with remote sensing images containing geographical information and temporal tags, as well as spectral multiple channels, and diverse resolutions and scales.
SatMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib49" title="">49</a>]</cite> discussed the domain-specific temporal and spectral characteristics of satellite imagery,
and device’s temporal encoding and spectral positional encoding respectively to adapt MAE structure to satellite imagery representation learning.
Scale-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib230" title="">230</a>]</cite> was designed to address another challenge in remote sensing satellite imagery: the variation in image scale due to data from multi-scale sensors.
The pretraining process aims to learn a better representation of multiscale tasks by reconstructing low and high
frequency features at different scales.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Diffusion-based Fusion</h4>
<div class="ltx_para" id="S4.SS5.SSS3.p1">
<p class="ltx_p" id="S4.SS5.SSS3.p1.1">In recent years, diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib325" title="">325</a>]</cite>, as an emerging and potent type of deep generative models, have demonstrated state-of-the-art performance across various modalities such as images, speech, and video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib30" title="">30</a>]</cite>.
Diffusion models can serve as an appropriate framework for data fusion, enabling the seamless incorporation of new modalities due to the existence of conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib136" title="">136</a>]</cite>. Simultaneously, the integration of multiple modalities can also enhance the quality of generation.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS3.p2">
<p class="ltx_p" id="S4.SS5.SSS3.p2.1">Urban imagery, as the primary provider of visual information, has borrowed numerous techniques from the general computer vision area and achieved successful applications.
DiffusionSat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib136" title="">136</a>]</cite>, inspired by Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib234" title="">234</a>]</cite> and ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib367" title="">367</a>]</cite>, provided the first large-scale generative foundation model for satellite imagery. It combined data from different modalities including geospatial metadata (latitude, longitude,ground-sampling distance), timestamp, and texts, achieving promising performance on a series of tasks such as super-resolution, temporal generation, and in-painting.
The generation of street-view images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib276" title="">276</a>]</cite> for spatio-temporal applications, however, remains an underexplored area, which represents a promising research direction in the future.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS3.p3">
<p class="ltx_p" id="S4.SS5.SSS3.p3.1">Aside from common modalities like image, text, and audio, the spatio-temporal modality data covers a broader range of data formats in urban computing, including traffic situations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib358" title="">358</a>]</cite>, urban flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib320" title="">320</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib299" title="">299</a>]</cite>, and trajectories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib356" title="">356</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib354" title="">354</a>]</cite>. In this context, the flexibility of diffusion models allows them to potentially exert a more considerable influence.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS3.p4">
<p class="ltx_p" id="S4.SS5.SSS3.p4.1">ChatTraffic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib358" title="">358</a>]</cite> creatively introduced fine-grained text in the Text-to-Traffic Generation (TTG) task to adapt to unusual events. Besides, when enhanced with GCN to incorporate the spatial information inherent in the road network, it achieved more accurate and realistic long-term prediction.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS3.p5">
<p class="ltx_p" id="S4.SS5.SSS3.p5.1">DiffSTG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib299" title="">299</a>]</cite> is the first work that generalizes the diffusion model DDPM to spatio-temporal graphs. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F18" title="Figure 18 ‣ 4.5.3 Diffusion-based Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">18</span></a>, it combined the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. The collection of fine-grained urban flow data is widely recognized as challenging due to the high costs associated with deployment and maintenance, as well as the presence of noise.
Diffusion models are suitable tools to generate fine-grained flow maps from the coarse-grained ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib320" title="">320</a>]</cite>.
DiffUFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib401" title="">401</a>]</cite> is the first generative approach for fine-grained urban flow inference. An ELFetcher module is proposed to utilize various external factors and land features as conditional guidance for the reverse denoising process.</p>
</div>
<figure class="ltx_figure" id="S4.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="S4.F18.g1" src="x4.png" width="705"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Illustration of DiffSTG and denoising network UGnet, which leverages an Unet-based architecture to capture multi-scale temporal dependencies and the Graph Neural Network (GNN) to model spatial correlations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib299" title="">299</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.SSS3.p6">
<p class="ltx_p" id="S4.SS5.SSS3.p6.1">Trajectory data represents another essential type of spatio-temporal data. However, it often raises privacy concerns due to the inclusion of personal geolocation information. One promising solution for this challenge is trajectory generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib356" title="">356</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib354" title="">354</a>]</cite>, which aims to generate high-fidelity, privacy-free trajectories.
The diffusion model, as a more reliable and robust method of generation than canonical methods, begins to be increasingly explored.
DiffTraj <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib356" title="">356</a>]</cite> is the first exploration of trajectory generation by the diffusion model, which proposed a Traj-UNet architecture to predict the noise of each diffusion time step. It proved that the step-by-step denoising process of the diffusion model is an appropriate choice due to the stochastic and uncertain characteristics of human activities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.4 </span>LLM-enhanced Data Fusion</h4>
<div class="ltx_para" id="S4.SS5.SSS4.p1">
<p class="ltx_p" id="S4.SS5.SSS4.p1.1">With the rise of GPTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib214" title="">214</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib24" title="">24</a>]</cite>, the remarkable capabilities of LLMs across a broad spectrum of fields have garnered significant attention, sparking a wave of research paradigm shift and shedding light on artificial general intelligence (AGI).
The academia usually terms “large language models (LLM)” for these large-sized PLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib388" title="">388</a>]</cite> due to the scaling laws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib131" title="">131</a>]</cite> of Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib268" title="">268</a>]</cite> architecture.
LLM-enhanced data fusion can be considered as a specific instance of encoder-based alignment, employing LLMs for feature encoding and information interaction. The extensive parameter size of LLMs endows it with potent alignment capabilities.
LLMs have multifaceted impacts on the field of urban computing, including works that utilize LLMs’ geospatial capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib232" title="">232</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib195" title="">195</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib291" title="">291</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib322" title="">322</a>]</cite>, applications in remote sensing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib141" title="">141</a>]</cite>, works in the time series domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib181" title="">181</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib402" title="">402</a>]</cite>, etc, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F19" title="Figure 19 ‣ 4.5.4 LLM-enhanced Data Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">19</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="323" id="S4.F19.g1" src="x5.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span><span class="ltx_text" id="S4.F19.2.1" style="color:#000000;">Categories of LLM-enhanced data fusion.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.SSS4.p2">
<p class="ltx_p" id="S4.SS5.SSS4.p2.1">LLMs are able to compress and store geospatial knowledge within the training data.
Exploring how to effectively utilize this compressed knowledge and design prompts that stimulate it are areas of current research interest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib232" title="">232</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib195" title="">195</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib291" title="">291</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib322" title="">322</a>]</cite>.
GPT4GEO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib232" title="">232</a>]</cite> provided a comprehensive investigation of the extent to which GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib24" title="">24</a>]</cite> has mastered factual geographic knowledge and its ability to use this knowledge for reasoning.
GeoLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib195" title="">195</a>]</cite> proposed that constructing the right prompt is key to extracting geospatial knowledge. Through providing geo-context information near a specific location, LLMs can be fine-tuned to achieve state-of-the-art performance on a variety of large-scale geospatial datasets for tasks such as predicting population density, house price, women’s education, etc.
LLM-Mob <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib291" title="">291</a>]</cite> demonstrated the first attempt to apply LLM to modeling human mobility, which reformulated mobility data through historical stays and context stays to introduce long-term and short-term dependencies for prediction and reasoning.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS4.p3">
<p class="ltx_p" id="S4.SS5.SSS4.p3.1">Large Multi-modal Models (LMMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib174" title="">174</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib332" title="">332</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib369" title="">369</a>]</cite> demonstrate substantial efficacy in information fusion and modal alignment. In the domain of urban computing, the utilization of remote sensing imagery is prevalent for integrating visual elements from a bird’s-eye view perspective.
Inspired by InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib53" title="">53</a>]</cite>, RSGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib105" title="">105</a>]</cite> utilized frozen Image Encoder and LLM, while training a lightweight Q-Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib151" title="">151</a>]</cite> to align the two, achieving state-of-the-art remote sensing image captioning and remote sensing visual question answering downstream tasks.
Developed based on LLaVA-v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib174" title="">174</a>]</cite>, GeoChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib141" title="">141</a>]</cite> introduced multi-modal instruction-tuning into the remote sensing domain and proposed the first versatile remote sensing Large Vision-Language Model with multitask conversational capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS4.p4">
<p class="ltx_p" id="S4.SS5.SSS4.p4.1">LLMs have the potential to revolutionize time series analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib127" title="">127</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib165" title="">165</a>]</cite>. Following the success of large foundation models in NLP and CV, there are desires in the time series forecasting area to utilize pre-trained LLMs as powerful representation learners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib181" title="">181</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib402" title="">402</a>]</cite>.
LLM4TS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib34" title="">34</a>]</cite> combined patching and channel-independence techniques with temporal encoding, which unlocked the flexibility of pre-trained LLMs without introducing large parameter overhead.
As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S4.F20" title="Figure 20 ‣ 4.5.4 LLM-enhanced Data Fusion ‣ 4.5 Generation-based Data Fusion ‣ 4 Methodology Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">20</span></a>, Time-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib126" title="">126</a>]</cite> solved the alignment of time series data and natural language modality to unleash the power of LLMs for time series forecasting through Prompt-as-Prefix or Patch-as-Prefix, which reprogrammed time series into text prototype representations.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS4.p5">
<p class="ltx_p" id="S4.SS5.SSS4.p5.1"><span class="ltx_text" id="S4.SS5.SSS4.p5.1.1" style="color:#000000;">LLMs approach real-world cross-domain data fusion in the form of agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib127" title="">127</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib310" title="">310</a>]</cite>. In human perception of the world, diverse modalities ultimately converge into language, serving as the medium for expression and communication <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib239" title="">239</a>]</cite>.
Based on essential language capabilities, LLMs can store knowledge and process cross-modal information, thereby continuously receiving feedback and interacting with the environment, demonstrating potential in spatiotemporal domains for comprehensive multimodal data integration.</span>
<cite class="ltx_cite ltx_citemacro_citet">Zhou et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib404" title="">404</a>]</cite> first explores urban planning through multi-agent collaboration framework, partially demonstrated performance surpassing that of human experts. LLMLight <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib144" title="">144</a>]</cite> transcends the previous paradigm of using LLMs as assistants to enhance decision-making, by directly employing LLMs as Traffic Signal Control (TSC) agents for decision-making.</p>
</div>
<figure class="ltx_figure" id="S4.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S4.F20.g1" src="x6.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Time-LLM begins by reprogramming the input time series with text prototypes before feeding it into frozen LLM to align the two modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib126" title="">126</a>]</cite>.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Application Perspective</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we categorize and summarize related works from the application perspective. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F22" title="Figure 22 ‣ 5.1 Urban Planning ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">22</span></a> illustrates applications from seven domains, including urban planning, transportation, economy, public safety and security, society, environment, and energy. Subsequently, we provide a comprehensive exposition of these applications, delving into intricate details, while also highlighting the pivotal role that deep learning-based data fusion methods have played in facilitating these tasks.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Urban Planning</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Sensing a city and making effective planning and governing is of great importance to its development. Every political planning and strategy necessarily needs to be formulated under strong computational support which covers lots of factors, such as road network structures, geographical limitations, transportation situation, human mobility, and society. In earlier years, to formulate a decent planning or strategy for a city, planners always need to sensor the city through various labor-intensive surveys. Besides, the processing of these surveys from different sources was also tough to conduct.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">With the development of city infrastructures, most urban data and factors can be directly collected from various sensors and platforms. However, processing and understanding such big data from multiple sources becomes a challenge due to the poor performance of traditional data analysis methods on big data. Deep learning models provide researchers with opportunities to handle big and multi-source data with better efficiency and deeper understanding. Therefore, for most urban planning tasks leveraging the advantages of multi-source data, deep learning-based data fusion methods have become a strong support from urban planners in both city-level planning and region-level planning (shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F21" title="Figure 21 ‣ 5.1 Urban Planning ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">21</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="120" id="S5.F21.g1" src="extracted/5670403/Images_zxc/urban_planning.png" width="252"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>A city exhibits a multi-level structure for urban planning: city-level planning and region-level planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S5.F22.g1" src="extracted/5670403/Images_zxc/application.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Taxonomy of application (category) and common downstream tasks (sub-category) for cross-domain data fusion in urban computing.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>City-level Planning</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Multi-sources urban data provides comprehensive information about the operation and variation of cities. Deep learning-based data fusion could utilize the data to sense the city-level variation and understand it as well. This enables urban planners and researchers to understand urban dynamics, such as sensing social events <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib384" title="">384</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib92" title="">92</a>]</cite>, region’s prosperity, city’s vibrancy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite> and any important change happened to the city <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>]</cite>. To sense city-level dynamics, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib177" title="">177</a>]</cite> conducted research for understanding and predicting urban vibrancy evolution based on three data sources: mobile check-in data, POI data, and geographical data. <cite class="ltx_cite ltx_citemacro_citet">Balsebre et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib13" title="">13</a>]</cite> proposed a framework named <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p1.1.1">Geo-ER</span> to match geo-spatial entities from various data sources.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib397" title="">397</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib395" title="">395</a>]</cite> introduced deep reinforcement learning to autonomously generate road layouts, aiming to connect various locations at the lowest construction expense.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p2.1.1">Urban event analysis</span> is crucial for urban planning as variations around a city usually come with urban events. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib385" title="">385</a>]</cite> proposed a multi-task learning framework to sense and predict urban events from the variation on social media. This framework could effectively train forecasting models for multiple locations simultaneously with shared information by restricting all positions to select a common set of features. <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib123" title="">123</a>]</cite> proposed a recurrent neural network (RNN) based online system named <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p2.1.2">DeepUrbanEvent</span> to understand the crowd dynamic variation for social events. By extracting the deep trend from the current momentary observations on the historical GPS data from citizens, this system could generate an effective prediction for the crowd dynamic trend during a short future time at a big event in a city.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">Focusing on the advantages of multi-source data, <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib384" title="">384</a>]</cite> summarized the previous research in event prediction and stressed four challenges for variation understanding and event prediction from multi-source data: 1) geographical hierarchies; 2) hierarchical missing; 3) feature sparsity; 4) difficulty in update with incomplete multiple data. Subsequently, they proposed a multi-source feature learning model based on an <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p3.1.1">N</span>th-order geo-hierarchy and fused-overlapping group Lasso to handle these challenges. In their methodology, models can be instantly updated from new data from every data source with affordable computational cost, so the urban planner can respond to variation faster with this system.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p4.1.1">Road network analysis</span> plays a crucial role in the realm of urban planning, demanding significant attention from professionals in the field. The development of cities leads to continuous variation in the road conditions around a city. It is noteworthy that traditional road maps are unable to cope with these variations. Additionally, conventional surveying methods and database management techniques are inadequate for capturing and managing the rapidly evolving road networks.
<cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib339" title="">339</a>]</cite> pioneered the utilization of multi-source information for the automatic derivation of road attributes.
They decided on a multi-task learning framework to extract low-level feature embedding from every data source and applied attention-based fusion to fuse the representations. Based on this work, they could combine the information from GPS trajectory and existing map data and achieve a significant improvement in classification on the OpenStreetMap data in Singapore. Based on the conception of data fusion, <cite class="ltx_cite ltx_citemacro_citet">Yang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib324" title="">324</a>]</cite> proposed a <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p4.1.2">DuARE</span> system for large-scale automatic extraction by leveraging the GPS trajectory and the satellite images. Driven by its promising performance, DuARE has been deployed in China and has been updating the national road network by 100,000 km every month.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Region-level Planning</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Cities serve as the foundation for societal functions in modern society. Based on the planning efforts of urban managers or the natural social operating principles, cities always develop into distinct functional regions, such as industrial zones, residential areas, and financial districts. These regions are sometimes referred to as Areas of Interest (AOIs). Simultaneously, within these areas, there are diverse locations that cater to the specific needs and demands of residents, commonly known as Points of Interest (POIs). These functional regions and points fulfill the daily requirements of citizens, enabling efficient urban operations. Accurately perceiving the status of AOIs and POIs in a city and understanding them is crucial for urban planners and administrators in making informed decisions.
This encompasses activities such as the selection of appropriate park locations, the planning of transportation routes, and the development of regional policies.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.1">Region detection</span> is the first step for us to understand region-level information. For POI or AOI detection, <cite class="ltx_cite ltx_citemacro_citet">Xiao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib313" title="">313</a>]</cite> proposed a <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.2">Contextual Master-Slave Framework (CMSF)</span> that unitizing graph neural network to fusion the POI information and satellite image to detect urban villages. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib112" title="">112</a>]</cite> fused satellite image, and street-level image with mobility data in their study, and the extra vision data were embedded through a <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.3">Vision-LSTM</span> network and were demonstrated crucial for region detection through comprehensive ablation study. Their work achieved an overall accuracy of 91.6% in identifying urban villages in Shenzhen, China. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib381" title="">381</a>]</cite> carried out a research on integrating information from social media such as Twitter as well as real-world locations. They designed a supervised Bayesian Model (sBM) to analyze the textual information, spatial features, POI information, and user behaviors. Based on this work, researchers are able to find user interests in special regions and understand the regions’ properties.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p3.1.1">Region embedding</span> based on multi-source data is the foundation of region-level urban computing. <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite> proposed a multi-view POI network by varying the representation of edges to fusion the geographical distance information and human modality. There are 2 kinds of POI-POI graph networks (shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F23" title="Figure 23 ‣ 5.1.2 Region-level Planning ‣ 5.1 Urban Planning ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">23</span></a>) in this specialized multi-view framework: 1) distance-based POI-POI networks: <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p3.1.2">the weight of an edge represents the distance between two associated POIs</span>; 2)mobility connectivity-based POI-POI networks:<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p3.1.3"> the weight of an edge represents the mobility connectivity between two associated POIs.</span> <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite> fused human mobility information with region attributes information with a multi-view joint learning module to infer the land usage of a city and <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib366" title="">366</a>]</cite> designed a multi-task learning framework called <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p3.1.4">ReMVC</span> to classify regions by land usage and <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib156" title="">156</a>]</cite> conducted a research on fusion publicly available building footprint information on <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p3.1.5">OpenStreetMap</span> and POI data for region representation learning. They carried out an experiment in Singapore and New York and demonstrated its efficiency.</p>
</div>
<figure class="ltx_figure" id="S5.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="108" id="S5.F23.g1" src="extracted/5670403/Images_zxc/regionembedding.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>An example of the multi-view POI graph networks proposed by <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib74" title="">74</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1"><cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib10" title="">10</a>]</cite> also carried out research on geographic representation learning. In their methodology, satellite images and POI information are fused to solve multiple geographic mapping tasks. Besides, <cite class="ltx_cite ltx_citemacro_citet">Bing et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib20" title="">20</a>]</cite> presented a POI embedding method named CatEM, which jointly considered the spatial information and mobility information of POIs and achieved an impressive performance on POI classification tasks. Through efficient region embedding, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib273" title="">273</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib275" title="">275</a>]</cite> proposed a generative framework to generate land-use configuration of a trageted region for urban planning.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p5">
<p class="ltx_p" id="S5.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p5.1.1">Region boundary sensing</span> plays a crucial role in region-level urban planning as it encompasses not only the political boundaries of the city but also functional or cultural regions within the city. Traditional administrative boundaries, often determined by policies and planning, may not accurately reflect the actual natural boundaries of a city.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p6">
<p class="ltx_p" id="S5.SS1.SSS2.p6.1">Only a few deep learning researches were proposed for region boundary sensing in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib294" title="">294</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib270" title="">270</a>]</cite>. The natural boundaries of a city are constantly evolving and adapt to the real needs and functions of the urban area. In fact, accurately sensing these boundaries is crucial for formulating regional policies and adjusting administrative planning to align with the true dynamics of the city. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib294" title="">294</a>]</cite> conducted a study on mapping the urban boundary of Zhengzhou City, China with its satellite images and POI data and found the result is in great agreement with the boundary ground truth.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p7">
<p class="ltx_p" id="S5.SS1.SSS2.p7.1"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib37" title="">37</a>]</cite> designed a cross-city federated transfer learning framework named <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p7.1.1">CcFTL</span> that can deal with multi-source data including POIs, road networks, and population density. In their methodology, this framework could not only deal with information from a single city but also transfer within various cities with great robustness. Besides, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib40" title="">40</a>]</cite> proposed a boundary sensing framework for urban villages that fuse the information from satellite images, mobility from bike-sharing drop-off data, and POIs. This platform was successfully deployed on the government data platform of Xiamen City, China to serve both urban planners and citizens. Some researchers focused on sensing the boundaries of POIs, <cite class="ltx_cite ltx_citemacro_citet">Vu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib270" title="">270</a>]</cite> proposed a boundary estimation frameworks by pairing the geo-tagged text information on tweets with the real name of POI and analyzing their spatial correlations. This work demonstrated that the spatial distribution of relevant tweets on the platform could reflect the social boundary of the targeted POI.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Transportation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Transportation serves as the arteries of a city, acting as bridges that connect different urban entities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib62" title="">62</a>]</cite>. As a result, the comprehensive utilization of multi-modal data focuses a significant proportion of attention on downstream transportation-related tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib199" title="">199</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib245" title="">245</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib148" title="">148</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Public Transportation Service</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">The advancement of a city is primarily demonstrated by enhancements in its transportation systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib52" title="">52</a>]</cite>.
Public transportation, in comparison to private transportation, is recognized for its superior environmental sustainability and energy efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib199" title="">199</a>]</cite>.
Guided by government supervision, public transportation embodies the features of public goods.
In addition to fundamental passenger transportation, there are also freight transportation services, along with emerging services such as food delivery and ride-hailing, all of which contribute to enhancing urban convenience.
Different from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib391" title="">391</a>]</cite>, where transportation systems were categorized based on the type of vehicles used, this classification of services is based on their specific application scenarios.
Public transportation applications can primarily be categorized into three main groups: Safety, Flow Control, and Efficiency.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p2.1.1">Safety</span> is the most fundamental and urgent requirement. Consequently, roads, being high-risk areas for public transportation safety, have prompted numerous research efforts focusing on road safety.
PANDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> predicted road risks after natural disasters by combining trajectories data and road event data.
To identify road obstacles such as fallen trees and ponding water, RADAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib39" title="">39</a>]</cite> utilized co-training and active learning to fuse the heterogeneous features from trajectory data and environmental sensing data.
<cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite> proposed a multi-modal fusion network for inferencing missing road attributes. In their methodology (shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F24" title="Figure 24 ‣ 5.2.1 Public Transportation Service ‣ 5.2 Transportation ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">24</span></a>), the robustness of the network is largely enhanced by the pixel-level fusion of GPS traces and satellite images.</p>
</div>
<figure class="ltx_figure" id="S5.F24"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="87" id="S5.F24.g1" src="extracted/5670403/Images_zxc/traffic2.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Network architecture of the proposed multi-modal fusion network by <cite class="ltx_cite ltx_citemacro_citet">Yin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib337" title="">337</a>]</cite> for robust road attribute detection.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">With the rapid urbanization of numerous countries, abnormal traffic incidents, have emerged as a substantial threat to public health and development. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib287" title="">287</a>]</cite> used a Multi-View Multi-Task Spatio-Temporal Network to capture the various context features such as weather, POIs, and road conditions, which result in the occurrence of traffic accidents. The multi-task learning framework is effective in modeling features of different granularity levels, thanks to the tie of spatial associations.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">Human factors play a critical role in transportation safety and can be considered a primary factor in accidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib205" title="">205</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib201" title="">201</a>]</cite>.
SAX-DF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib178" title="">178</a>]</cite> is a Symbolic Aggregate approximation Data Fusion model to comprehensively utilize multi-source and heterogeneous data, which can effectively improve the driver behavior detection performance with the help of a Positive Danger Mapping algorithm.
The rise of autonomous driving has stimulated the demand for real-time driver behavior detection. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib111" title="">111</a>]</cite> proposed a deformable inverted residual network to adaptively detect real-time driver behavior.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p5">
<p class="ltx_p" id="S5.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p5.1.1">Flow control</span> is an important downstream application that holds great significance for public policies such as resource scheduling and urban planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib86" title="">86</a>]</cite>.
For instance, utilizing urban flow data enables governments to implement flow control measures during events such as New Year’s Eve at Shanghai’s Bund or Times Square in New York.
By managing the crowd flow and directing subway passengers to nearby stations, the government can prevent potential dangers and promote efficient social functioning, thus yielding social benefits.
Private ride-hailing platforms like DiDi and Uber allocate more transportation resources to densely populated areas, which allows them to gain better economic benefits.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p6">
<p class="ltx_p" id="S5.SS2.SSS1.p6.1">Indeed, flow control can be broadly categorized into two main areas: i) Traffic Flow Control <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib17" title="">17</a>]</cite>, which involves regulating and optimizing the flow of vehicles on roads and highways.
ii) Mobility Prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib360" title="">360</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib15" title="">15</a>]</cite>, which endeavors to forecast and comprehend patterns of mobility behavior exhibited by humans or vehicles, is intended to enhance the planning and management of urban transportation and resources.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p7">
<p class="ltx_p" id="S5.SS2.SSS1.p7.1">DeepSTN+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib170" title="">170</a>]</cite> utilized a spatio-temporal network to predict inflow and outflow regarding POIs and historical data.
To infer urban flow, UrbanSTC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib226" title="">226</a>]</cite> implemented contrastive pretraining in both spatial and temporal to learn robust features in both two modalities with different pretasks.
Likewise, CSST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib133" title="">133</a>]</cite> made use of a pretrain-finetuning paradigm to align low-quality GPS reports and external factors with the crowd flow.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p8">
<p class="ltx_p" id="S5.SS2.SSS1.p8.1">For mobility prediction,
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib200" title="">200</a>]</cite> combined GPS traces and geo-tagged tweets to model human crowd flow.
GraphTUL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib79" title="">79</a>]</cite> solved the Trajectory user linking (TUL) problem by training an adversarial network in a semi-supervised way.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p9">
<p class="ltx_p" id="S5.SS2.SSS1.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p9.1.1">Efficiency</span> primarily involves the promotion of efficiency in the allocation of transportation resources. It mainly relates to taxi/ride-hailing demand prediction, delivery time prediction, and passenger demand prediction.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p10">
<p class="ltx_p" id="S5.SS2.SSS1.p10.1">DMVST-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib329" title="">329</a>]</cite>, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F25" title="Figure 25 ‣ 5.2.1 Public Transportation Service ‣ 5.2 Transportation ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">25</span></a>, combined multi-view information to implement taxi demand prediction. It splits the process into spatial, temporal, and semantic views, which are modeled by local CNN, LSTM, and semantic graph embedding, respectively.
To address the challenge of predicting travel demands in city regions for future time intervals demands, DeepTP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib349" title="">349</a>]</cite> proposed to encode and capture three key properties from traffic data: Region-Level Correlations, Temporal Periodicity, and Inter-Traffic Correlations.
To jointly predict demands for various ride-hailing service modes like solo and shared rides, <cite class="ltx_cite ltx_citemacro_citet">Ke et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib132" title="">132</a>]</cite> combined multi-graph convolutional networks with two multi-task learning structures, enhancing prediction accuracy for diverse ride-hailing services in urban areas.
For delivery tasks, MetaSTP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib235" title="">235</a>]</cite> introduced a meta-learning-based neural network model for predicting service time in last-mile delivery. Utilizing a Transformer-based layer and location prior knowledge, MetaSTP addresses complex delivery scenarios, showing significant improvements in prediction accuracy and practical deployment in JD Logistics.</p>
</div>
<figure class="ltx_figure" id="S5.F25"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S5.F25.g1" src="x7.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>The architecture of DMVST-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib329" title="">329</a>]</cite>, which is designed to incorporate spatial, temporal, and semantic components for taxi demand prediction.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS1.p11">
<p class="ltx_p" id="S5.SS2.SSS1.p11.1">The prediction of passenger demand is also of great significance, which can assist transportation departments and related enterprises in enhancing the planning and management of passenger traffic.
<cite class="ltx_cite ltx_citemacro_citet">Bai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib12" title="">12</a>]</cite> proposed a deep learning framework combining graph convolutional recurrent neural networks and LSTM networks. This framework aims to accurately predict citywide passenger demand in ride-sharing platforms by analyzing historical demand data and external factors like weather and time. STG2Seq <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib11" title="">11</a>]</cite> solved the challenges for multi-step passenger demand forecasting in cities. It combines a Graph Convolutional Network (GCN) with an innovative encoder and attention-based output module, effectively capturing spatio-temporal correlations in-demand data and outperforming traditional methods in real-world tests.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Transportation Recommendation</h4>
<figure class="ltx_figure" id="S5.F26"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="S5.F26.g1" src="extracted/5670403/Images_zxc/route_recommondation.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>An overview of unified route representation learning for multi-modal transportation recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib173" title="">173</a>]</cite>. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Different from public services, <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p1.1.1" style="color:#000000;">transportation recommendations<span class="ltx_text ltx_font_medium" id="S5.SS2.SSS2.p1.1.1.1"> are proposed for specific personalized needs, significantly facilitating human society’s daily life, thus receiving increasing attention in recent years.</span></span>
<cite class="ltx_cite ltx_citemacro_citet">CondorFerries [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib48" title="">48</a>]</cite> demonstrated that the prevalence of private travel has been on the rise since 2016, with 83 million Americans looking to take a solo trip in 2023.
With the progress of human society and the development of urbanization,
transportation recommendations are expected to create more economic and social benefits in the future.
Among the various applications of transportation recommendations, route Recommendations, multi-modal transportation recommendations, and trip recommendations have garnered the most research attention. In this section, we will primarily introduce them respectively.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p2.1.1" style="color:#000000;">Route recommendation<span class="ltx_text ltx_font_medium" id="S5.SS2.SSS2.p2.1.1.1"> plays a core role in many applications such as taxi services like RoD (ride-on-demand) and navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib52" title="">52</a>]</cite>.</span></span>
<cite class="ltx_cite ltx_citemacro_citet">Guo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib93" title="">93</a>]</cite> presented a novel force-directed algorithm for improving route recommendations in ride-on-demand services. This method, inspired by electrostatic principles, utilizes urban data to align vehicle distribution with passenger demand, thereby enhancing route efficiency.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Given the origin and destination, <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p3.1.1" style="color:#000000;">multi-modal transportation recommendations<span class="ltx_text ltx_font_medium" id="S5.SS2.SSS2.p3.1.1.1"> offer travel plans consisting of diverse transportation modes</span></span> (e.g., driving, cycling, and public transit), as well as instructions for making seamless transitions between different modes. Doing so improves individuals’ convenience and well-being, particularly for older adults and children.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib176" title="">176</a>]</cite> proposed Trans2Vec, a model that enhances transportation mode recommendations by integrating heterogeneous data to capture user and location preferences, thereby improving accuracy in suggesting multi-modal transportation options.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib173" title="">173</a>]</cite> developed a novel framework (shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S5.F26" title="Figure 26 ‣ 5.2.2 Transportation Recommendation ‣ 5.2 Transportation ‣ 5 Application Perspective ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">26</span></a> for multi-modal transportation recommendations. The approach leverages a hierarchical multi-task route representation learning technique that integrates spatio-temporal autocorrelation modeling and coherent-aware attentive route learning. Enhanced by spatio-temporal pre-training strategies, the framework effectively utilizes time-dependent multi-view transportation graphs, offering a more nuanced and accurate transportation recommendation system.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p4">
<p class="ltx_p" id="S5.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p4.1.1" style="color:#000000;">Trip Recommendation<span class="ltx_text ltx_font_medium" id="S5.SS2.SSS2.p4.1.1.1"> aims to propose a series of POIs for individuals with specific travel preferences, such as the number of attractions, starting and ending points, and so on.</span></span>
<cite class="ltx_cite ltx_citemacro_citet">He et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib99" title="">99</a>]</cite> introduced a novel context-aware POI embedding model that jointly learns the impact of POI popularity, user preferences, and co-occurring POIs.
Uniquely integrating visual features, Photo2Trip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib386" title="">386</a>]</cite> utilized geo-tagged photos with collaborative filtering models to enhance personalized tour recommendations.
GraphTrip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib77" title="">77</a>]</cite> addressed the sparsity of data and the need for more personalized trip recommendations by effectively integrating diverse knowledge domains. The paper introduces a dual-grained mobility learning approach that uses spatio-temporal graph representation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Economy</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The economy serves as a critical indicator of urban development, and data collected from cities inherently reflects the economic conditions of the regions. For instance, the density of POIs can indicate the popularity of an area, satellite images can provide insights into the urbanization level in a region, and human mobility data can contribute to the estimation of the economic vitality in regions.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Real estate price</span> is a vital indicator for a city which is affected by various factors. To manage citywide real estate information, <cite class="ltx_cite ltx_citemacro_citet">Du et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib63" title="">63</a>]</cite> designed a dual-level collective learning (<span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">DLCL</span>) framework to collectively learn spatial representation through intra-region and inter-region geographical structure knowledge which contain POI information as well as geographical and human mobility information for accurate prediction of the real estate price of Beijing. In their framework, intra-region POI and other side information are constructed through multi-view POI-POI networks. The constructed intra-region structure and inter-region autocorrelation are embedded through the Adversarial AutoEncoder.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Jenkins et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib116" title="">116</a>]</cite> proposed a model named <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1">RegionEncoder</span> for various data sources and conducted experiments on Chicago and New York City for the perdition of real estate prices as well. They added satellite image data to the input of the model as a supplementary and gained impressive results in the experiment.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">In addition to real estate price prediction, the study conducted by <cite class="ltx_cite ltx_citemacro_citet">Xi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib309" title="">309</a>]</cite> introduced an attention model that combines satellite imagery with POI information to <span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">estimate various economic indicators</span>. Their methodology addresses the limitations of previous studies on satellite image data by leveraging the complementary nature of POI data in capturing human activity factors. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib154" title="">154</a>]</cite> conducted the first research to organize multi-view urban images (including satellite images and street-view images) by leveraging city structural information. The effective combination of the street-view image data contributed to a significant improvement of 10% in urban economic indicators predicting compared to previous studies before 2022. Besides, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib186" title="">186</a>]</cite> also emphasized the necessity of integrating street-view images with satellite images in their research about urban economy.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Public Safety and Security</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In the process of daily urban operations, ensuring public safety and security is a critical responsibility of cities and a standing focus for city administrators and researchers. The activities of humans and the movement of vehicles often encounter various safety hazards. Additionally, natural events like landslides and air pollution can also pose threats to people’s lives. The various urban big data provide us with opportunities to gain insights into unsafe factors and predict unsafe events. By integrating and analyzing diverse data sources such as human mobility, social media data, sensor data, and emergency response records, we can identify patterns and trends that may indicate potential safety risks. For example, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib368" title="">368</a>]</cite> proposed a multi-view region embedding framework with human mobility, region attribution, and crime records of New York City to predict the number of crime events in each region. <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib124" title="">124</a>]</cite> designed a framework to utilize vehicle trajectories and road environment data to infer traffic violation-prone locations in Xiamen City.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">Traffic safety</span>, being a paramount concern for numerous countries, has emerged as a prominent focus in recent data fusion studies pertaining to urban security. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib272" title="">272</a>]</cite> proposed a Geographical and Semantic spatio-temporal Network <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.2">(GSNet)</span> model for predicting traffic accidents from taxi order, POI, and weather data. They conducted an experiment on New York City and Chicago based on real-world traffic accident datasets, the result demonstrated a satisfactory result for their model. Then <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib288" title="">288</a>]</cite> designed a cross-scale feature fusion mechanism to integrate features from different scale data as well as a feature fusion component to integrate multi-view features. Based on this work, a multi-task learning framework that can simultaneously predict fine and coarse-grained traffic accident risks was proposed.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Environment safety</span> and the mitigation of natural disasters have also been remarkable research focal points. These areas of study have garnered significant attention due to their profound implications for urban environments and the well-being of communities. Researchers have made notable strides in leveraging data fusion techniques to enhance environmental monitoring, early warning systems, and disaster response strategies. For instance, based on the existing study of trajectories, <cite class="ltx_cite ltx_citemacro_citet">Luo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib188" title="">188</a>]</cite> combined traffic trajectory information with road networks to efficiently identify traffic bottlenecks on the road. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib39" title="">39</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">You et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib342" title="">342</a>]</cite> focused on studying the road obstacle situation or road risks after disasters in a city from its vehicle trajectory, satellite image, and meteorology data. Besides, <cite class="ltx_cite ltx_citemacro_citet">Song et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib249" title="">249</a>]</cite> built an intelligent system named <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.2">DeepMob</span> in Japan based on users trajectories, earthquake records, text reports, and transportation network data to analyze and predict human behavior and mobility following natural disasters.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Social</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The advances in wireless communication and location acquisition technologies enable people to add a location dimension to traditional social networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib204" title="">204</a>]</cite>. With the advancement of data fusion advantage and the popularity of recommendation devices, services based on Location-based Social Networks (LBSN) have penetrated people’s lives. Users on social media generate a variety of content with geo-information every day. Such posts combine geographical coordinates (always in GPS location) and user-generated content (text, image, or audio) which might be associated with the semantic meaning of those places. It is strongly promising to fusion the geographical information and other information through the users’ posts to bridge the gap between users’ activities in digital and physical worlds and contribute to various downstream tasks such us POI or friend recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib219" title="">219</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib155" title="">155</a>]</cite> and community analysis and detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib256" title="">256</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib184" title="">184</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib316" title="">316</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p2.1.1">Recommendation on LBSN</span> based on deep learning is a complex task in real applications as the irregular spatial structure of geo-location data is non-Euclidean and traditional neural network-based deep learning is incapable of such non-Euclidean data. So graph neural network is widely used to understand the spatial structure information with geo-location data. <cite class="ltx_cite ltx_citemacro_citet">Fang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib69" title="">69</a>]</cite> proposed a multi-graph fusion approach for POI recommendation by constructing a user-POI interaction graph on LBSN. For LBSN friend recommendation, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib155" title="">155</a>]</cite> designed a learning framework by leveraging multi-graph to model raw LBSN data and defining various connections between nodes to represent spatio-temporal information. In their methodology, a contrastive learning model was proposed to integrate spatio-temporal features of human trajectories in cities for user node embedding learning.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Environment</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">Rapid urbanization may result in a potential threat to cities’ environment. For example, it is reported that ninety percent of air pollution is caused by urban transportation. Environment protection is an essential topic for urban computing. We have witnessed much research on the environment from different aspects of urban computing, such as air quality prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib138" title="">138</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib129" title="">129</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib279" title="">279</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib166" title="">166</a>]</cite>, noise controlling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib117" title="">117</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib185" title="">185</a>]</cite> and weather forecasting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib399" title="">399</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib341" title="">341</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib247" title="">247</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib87" title="">87</a>]</cite>. Various data sources cloud provide information from various aspects for us to sense the environment. Further, the fusion those information from multiple sources was proven to be effective in understanding and predicting the variation of the environment in a city.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zheng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib399" title="">399</a>]</cite> conducted research on cities’ air quality based on multi-source big data. They proposed a multi-view hybrid model to predict future 48-hour air quality of a single station point by converging historical air quality data of one station, current meteorological data in the area, weather forecasting data as well as air quality data from other stations. The various meteorological factors and spatial relations between different stations are both considered by effectively fusing these data which significantly improved the capability and accuracy of fine-grain air quality prediction. Further, <cite class="ltx_cite ltx_citemacro_citet">Ma et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib192" title="">192</a>]</cite> utilized a graph neural network to model spatio-temporal correlations between different meteorological variables and different stations as graph neural networks have demonstrated excellent performances for modeling spatio-temporal information. This kind of research makes city-level fine-grained weather forecasting possible with multi-source spatio-temporal data.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>Energy</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1">From the perspective of energy, cities can be seen as machines that consume a significant amount of energy. With the acceleration of urbanization and the growing global energy concerns, technologies that enable the perception of energy consumption and energy efficiency have become increasingly important. In urban computing, the fusion of diverse data sources allows us to have a more comprehensive understanding of urban energy consumption and make more efficient urban planning or transportation decisions to help reduce energy consumption.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.1 </span>Energy Efficient Urban Planning</h4>
<div class="ltx_para" id="S5.SS7.SSS1.p1">
<p class="ltx_p" id="S5.SS7.SSS1.p1.1">Electric vehicles are regarded as a promising solution for green transportation and sustainable cities. With the gradual expansion of the market share of electrical vehicles, the demand for car charging facilities is increasing. One important issue in urban planning is the selection of charging station locations. Proper selection of these locations not only optimizes city management but also meets the daily commuting needs of users, reducing unnecessary energy consumption during the search for charging stations. The selection of charging station locations should take into account various factors, such as the economic situation of the region, the number of electric vehicles in circulation, residents’ travel patterns, road conditions, and traffic conditions. This necessitates the integration and analysis of multiple data sources to estimate the regional charging demand and identify suitable locations for charging stations. <cite class="ltx_cite ltx_citemacro_citet">Tu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib267" title="">267</a>]</cite> proposed a spatio-temporal demand coverage location model based on the taxi GPS data in Shenzhen, China as well as data from the charging stations. This model provided a charging station sitting approach by considering the waiting time which could hugely reduce energy consumption for charging. <cite class="ltx_cite ltx_citemacro_citet">He et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib100" title="">100</a>]</cite> developed a location method with the consideration of cars’ driving range extracted from the GPS data. <cite class="ltx_cite ltx_citemacro_citet">Battaïa et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib16" title="">16</a>]</cite> proposed a framework for charging situation sitting considering the real-life problem of charging infrastructure and sustainability.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.2 </span>Energy Efficient Urban Transportation</h4>
<div class="ltx_para" id="S5.SS7.SSS2.p1">
<p class="ltx_p" id="S5.SS7.SSS2.p1.1">Transportation energy efficiency aims to recommend and navigate energy-efficient travel routes for every transportation participant. Nowadays, within the worldwide energy shortage, fuel prices have been continuously rising <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib387" title="">387</a>]</cite>. However, traffic congestion in 439 cities in the United States resulted in a total loss of 3.3 billion gallons of wasted fuel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib237" title="">237</a>]</cite> in only one year. Traditional route planning studies primarily emphasized faster and more reliable routes, often overlooking the differences in energy consumption among different travel routes and modes for the same travel demand. For instance, in certain situations, taking a route through a congested city center may result in faster arrival times compared to taking a detour on a highway. Despite the first route being shorter in distance, the driver may experience delays due to frequent stops and slow speeds caused by city congestion.</p>
</div>
<div class="ltx_para" id="S5.SS7.SSS2.p2">
<p class="ltx_p" id="S5.SS7.SSS2.p2.1">In application, recommending energy-efficient routes for users is a highly complex task. Unlike traditional shortest path problems, it requires considering various factors such as vehicle efficiency, driving habits, road conditions, POIs, and more. The key to addressing this task lies in effectively integrating information from multiple sources.
<cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib293" title="">293</a>]</cite> researched personalized fuel-efficient route recommendations based on history trajectories. The temporal information from the trajectories as well as the driving factors are input into a transformer model to compose fuel efficiency prediction on the target route. By combining with a genetic algorithm for the best recommendation, this research could effectively find the fuel-efficient routes on the real-world dataset. DeepFEC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib66" title="">66</a>]</cite> proposed a framework to predict road-level energy consumption through a fusion of contextual vehicle data such as type, weight and engine configuration, and spatio-temporal traffic data on roads. In this framework, a deep convolution residual model is designed to capture the spatial dynamic information from the data and a Bi-LSTM model is responsible for the temporal information. <cite class="ltx_cite ltx_citemacro_citet">Oh et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib209" title="">209</a>]</cite> is the first large dataset that combines trajectories of personal cars with users’ information, making it possible to mine users’ driving behaviors. Based on this multi-source dataset, <cite class="ltx_cite ltx_citemacro_citet">Lai et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib145" title="">145</a>]</cite> designed a meta-learning framework for predicting vehicle energy consumption in Ann Arbor, Michigan, USA that can be fine-tuned based on a user’s driving preference.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Challenges and Future Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">While urban multi-modal research has made significant advancements in recent years, several challenging issues persist, highlighting directions for potential future research.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#S6.F27" title="Figure 27 ‣ 6 Challenges and Future Directions ‣ Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook"><span class="ltx_text ltx_ref_tag">27</span></a>, we summarize these challenges and suggest potentially feasible research directions as follows:</p>
</div>
<figure class="ltx_figure" id="S6.F27"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="101" id="S6.F27.g1" src="extracted/5670403/Images_zxc/challenge.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span><span class="ltx_text" id="S6.F27.2.1" style="color:#000000;">Challenges and future directions of cross-domain data fusion in Urban Computing.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.p2">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">LLM-enhanced Application:</span> Since the advent of GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib211" title="">211</a>]</cite> and Sora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib22" title="">22</a>]</cite>, the academic community has embarked on extensive research on the roles of LLM as predictors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib106" title="">106</a>]</cite>, enhancers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib336" title="">336</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib221" title="">221</a>]</cite>, controllers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib242" title="">242</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib317" title="">317</a>]</cite>, and evaluators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib255" title="">255</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib169" title="">169</a>]</cite>. However, the field of cross-domain data fusion in urban computing is still in its initial stage in terms of exploring how to apply LLMs effectively. For example, there exists research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib232" title="">232</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib195" title="">195</a>]</cite> focusing on LLM as urban predictors but also as naive reasoners yet; whereas others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib141" title="">141</a>]</cite> attempt to leverage LLM as model backbone but such works are limited within remote sensing domain instead of general urban computing. <span class="ltx_text" id="S6.I1.i1.p1.1.2" style="color:#000000;">A critical limitation currently facing the application of LLMs in Urban Computing is their inability to perceive spatial relationships and dependencies inherent in natural spaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib408" title="">408</a>]</cite>, a trait typically associated with language models. For instance, mainstream LLMs struggle to comprehend intricate spatial orientations and positions of urban entities. Moreover, the majority of publicly accessible LLMs are predominantly text-based and exhibit limited performance in other modalities. Nevertheless, we are encouraged by the growing number of successful initiatives in the graph learning domain that aim to enhance LLMs’ comprehension of complex relations within graph data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib259" title="">259</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib160" title="">160</a>]</cite>. Furthermore, the latest multimodal LLMs, such as GPT-4o, demonstrate considerable promise in offering effective multimodal solutions.</span> Therefore, we eagerly anticipate that the research community will focus on various applications of LLMs in urban computing, exploring their impact similar to their roles in NLP and other domains.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Agent-based Simulation:</span> Agent-based simulation aims to model the behavior of individual components or agents to comprehend their interactions and how they collectively contribute to the functioning of the entire system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib310" title="">310</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib280" title="">280</a>]</cite>. It has been extensively used in various fields such as biology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib6" title="">6</a>]</cite>, ecology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib89" title="">89</a>]</cite>, sociology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib72" title="">72</a>]</cite>, etc., to model systems where individual entities influence collective behavior. However, the early attempts at agent-based simulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib389" title="">389</a>]</cite> are limited because they are not autonomous and need human-defined rules or goals, which may not simulate the complex dynamics of urban systems. Hence, LLM-driven agents such as Urban Generative Intelligence (UGI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib317" title="">317</a>]</cite>, can serve as up-to-date solutions for simulating urban dynamics based on cross-domain urban data. This paradigm not only propels forward the field of urban computing, but also paves the way for future cities that are more adaptive and responsive to the evolving needs of their inhabitants.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">Multi-modal Causal Learning:</span> Causal learning or inference aims to investigate causal relationships between variables, ensuring stable and robust learning and inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib206" title="">206</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib220" title="">220</a>]</cite>. Integrating deep learning techniques with causal inference has shown great success in recent years, especially in the fields of spatio-temporal graph forecasting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib403" title="">403</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib312" title="">312</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib149" title="">149</a>]</cite>, CV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib290" title="">290</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib359" title="">359</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib168" title="">168</a>]</cite>, NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib269" title="">269</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib371" title="">371</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib264" title="">264</a>]</cite>, and recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib393" title="">393</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib75" title="">75</a>]</cite>. However, regarding cross-domain data fusion in urban computing, the application of causal learning is still in its early stages. <span class="ltx_text" id="S6.I1.i3.p1.1.2" style="color:#000000;">One of the most severe challenges is to represent the complex cross-modal causality in urban scenarios. The deep learning community has yet to converge on a universally accepted approach for accurately yet efficiently representing cross-modal relations. However, as an increasing number of studies delve into the semantic essence of information across various modalities and advance the fields of graph learning, representation learning, and urban foundational models,</span> we eagerly anticipate further research on multi-modal causal learning of urban data, aiming to improve the interpretability of intricate and dynamic urban systems.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i4.p1.1.1">Multi-source Data Privacy:</span> Urban data can be highly sensitive, especially in multi-source scenarios in the field of economy and healthcare. When models are trained upon such shared data, there is a risk that they may memorize specific information from the training data, potentially compromising the privacy of individuals. <span class="ltx_text" id="S6.I1.i4.p1.1.2" style="color:#000000;">Particularly in the context of deploying data fusion models, the inclusion of data from diverse sources and modalities can indeed bolster the models’ performance. However, this also heightens the risk of inadvertently exposing sensitive privacy data. Given these privacy concerns, many institutions and city governments are reluctant to share urban data, thereby impeding the deployment of deep learning models. This poses a significant barrier to the advancement of the community. Consequently, there is a pressing need for research that explores the integration of privacy-preserving techniques, such as differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib326" title="">326</a>]</cite> and federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib357" title="">357</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib153" title="">153</a>]</cite>. The goal is to protect data privacy while still reaping the benefits of cross-domain data fusion in urban computing.</span></p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p" id="S6.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i5.p1.1.1">Open Benchmark:</span> The challenge of developing an open benchmark for cross-domain data fusion in urban computing lies in the complexity of integrating diverse data sources, such as sensor data, images, and even text, to understand urban environments comprehensively. This complexity arises due to the heterogeneity of data formats, modalities, and the need for effective fusion methods to extract meaningful insights. A potential solution involves collaborative efforts to standardize data formats, develop unified evaluation metrics, and establish shared benchmarks that facilitate the evaluation and comparison of cross-domain data fusion models.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i6.p1">
<p class="ltx_p" id="S6.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i6.p1.1.1">Downstream Task Diversity:</span> Existing urban research predominantly concentrates on specific task domains such as transportation and urban planning, overlooking the inherent diversity of challenges in real-life urban environments. This limitation exists due to the compartmentalized nature of current research efforts, hindering a holistic understanding of cross-domain data fusion in urban computing. Therefore, we anticipate a more extensive scope of urban research encompassing diverse applications in the realms of economy, society, and environment, providing a thorough comprehension of the intricate conditions prevailing in urban settings.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i7.p1">
<p class="ltx_p" id="S6.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i7.p1.1.1">Computation Efficiency:</span> Current urban research emphasizes the fulfillment of specific applications within cross-domain urban computing, but it overlooks the crucial aspect of computational efficiency. This oversight hampers the practical deployment of these computational models in real-life scenarios. Addressing this challenge requires our focus on optimizing computation efficiency, involving model compression (e.g., knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib90" title="">90</a>]</cite>, low-rank decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib331" title="">331</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib159" title="">159</a>]</cite>, and quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib59" title="">59</a>]</cite>), efficient training (e.g., prompt tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib180" title="">180</a>]</cite> and hardware-assisted attention acceleration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib54" title="">54</a>]</cite>), and efficient architecture (e.g., mixture of experts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19348v2#bib.bib241" title="">241</a>]</cite>), to enhance the feasibility and effectiveness of deploying cross-domain data fusion solutions in practical urban environments.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this survey, we present an extensive and up-to-date survey of cross-domain data fusion tailored for urban computing, aiming to offer a fresh perspective on this evolving field by introducing a novel taxonomy that categorizes the reviewed fusion methods. In particular, we initially explore the data perspective to understand the significance of each modality and data source, classify the methodology into four types (i.e., <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">feature-based</span>, <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">alignment-based</span>, <span class="ltx_text ltx_font_italic" id="S7.p1.1.3">contrast-based</span>, and <span class="ltx_text ltx_font_italic" id="S7.p1.1.4">generation-based</span>), and further categorize cross-domain urban applications into seven types. Besides, We succinctly outline the possible challenges while shedding light on promising directions for future research. The potential for urban computing-related investigations within this captivating cross-domain fusion field is limitless. We hope this can ignite more curiosity and cultivate a long-lasting enthusiasm for cross-domain urban studies, therefore achieving real urban intelligence.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Github Link</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Please refer to <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoshall/Awesome-Multimodal-Urban-Computing" title="">https://github.com/yoshall/Awesome-Multimodal-Urban-Computing</a> for comprehensive and up-to-date paper list.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbaspour et al. [2015]</span>
<span class="ltx_bibblock">
Abbaspour, M., Karimi, E., Nassiri, P., Monazzam, M.R., Taghavi, L., 2015.

</span>
<span class="ltx_bibblock">Hierarchal assessment of noise pollution in urban areas–a case study.

</span>
<span class="ltx_bibblock">Transportation Research Part D: Transport and Environment 34, 95–103.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afyouni et al. [2022]</span>
<span class="ltx_bibblock">
Afyouni, I., Al Aghbari, Z., Razack, R.A., 2022.

</span>
<span class="ltx_bibblock">Multi-feature, multi-modal, and multi-source social event detection: A comprehensive survey.

</span>
<span class="ltx_bibblock">Information Fusion 79, 279–308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aheto et al. [2023]</span>
<span class="ltx_bibblock">
Aheto, J.M.K., Olowe, I.D., Chan, H.M.T., Ekeh, A., Dieng, B., Fafunmi, B., Setayesh, H., Atuhaire, B., Crawford, J., Tatem, A.J., Utazi, C.E., 2023.

</span>
<span class="ltx_bibblock">Geospatial analyses of recent household surveys to assess changes in the distribution of zero-dose children and their associated factors before and during the covid-19 pandemic in nigeria.

</span>
<span class="ltx_bibblock">Vaccines 11.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/vaccines11121830" title="">https://doi.org/10.3390/vaccines11121830</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3390/vaccines11121830" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/vaccines11121830</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alfarrarjeh et al. [2021]</span>
<span class="ltx_bibblock">
Alfarrarjeh, A., Yang, X., Jabal, A.A., Kim, S.H., Shahabi, C., 2021.

</span>
<span class="ltx_bibblock">Exploring the spatial-visual locality of geo-tagged urban street images, in: 2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR), IEEE. pp. 104–110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amirian et al. [2019]</span>
<span class="ltx_bibblock">
Amirian, J., Van Toll, W., Hayet, J.B., Pettré, J., 2019.

</span>
<span class="ltx_bibblock">Data-driven crowd simulation with generative adversarial networks, in: Proceedings of the 32nd International Conference on Computer Animation and Social Agents, pp. 7–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">An [2021]</span>
<span class="ltx_bibblock">
An, G., 2021.

</span>
<span class="ltx_bibblock">Agent-based modeling in translational systems biology.

</span>
<span class="ltx_bibblock">Complex Systems and Computational Biology Approaches to Acute Inflammation: A Framework for Model-based Precision Medicine , 31–52.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anbalagan [2022]</span>
<span class="ltx_bibblock">
Anbalagan, B., 2022.

</span>
<span class="ltx_bibblock">Event location detection from online clustering algorithms using geo-tagged user data in social streams, in: Disruptive Technologies for Big Data and Cloud Applications: Proceedings of ICBDCC 2021. Springer, pp. 227–235.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderegg et al. [2020]</span>
<span class="ltx_bibblock">
Anderegg, W.R., Trugman, A.T., Badgley, G., Anderson, C.M., Bartuska, A., Ciais, P., Cullenward, D., Field, C.B., Freeman, J., Goetz, S.J., et al., 2020.

</span>
<span class="ltx_bibblock">Climate-driven risks to the climate mitigation potential of forests.

</span>
<span class="ltx_bibblock">Science 368, eaaz7005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arslan Ay et al. [2009]</span>
<span class="ltx_bibblock">
Arslan Ay, S., Zhang, L., Kim, S.H., He, M., Zimmermann, R., 2009.

</span>
<span class="ltx_bibblock">Grvs: a georeferenced video search engine, in: Proceedings of the 17th ACM International Conference on Multimedia, pp. 977–978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2023]</span>
<span class="ltx_bibblock">
Bai, L., Huang, W., Zhang, X., Du, S., Cong, G., Wang, H., Liu, B., 2023.

</span>
<span class="ltx_bibblock">Geographic mapping with unsupervised multi-modal representation learning from VHR images and POIs.

</span>
<span class="ltx_bibblock">ISPRS Journal of Photogrammetry and Remote Sensing 201, 193–208.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.isprsjprs.2023.05.006" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.isprsjprs.2023.05.006</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2019a]</span>
<span class="ltx_bibblock">
Bai, L., Yao, L., Kanhere, S., Wang, X., Sheng, Q., et al., 2019a.

</span>
<span class="ltx_bibblock">Stg2seq: Spatial-temporal graph to sequence model for multi-step passenger demand forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1905.10069 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2019b]</span>
<span class="ltx_bibblock">
Bai, L., Yao, L., Kanhere, S.S., Wang, X., Liu, W., Yang, Z., 2019b.

</span>
<span class="ltx_bibblock">Spatio-temporal graph convolutional and recurrent networks for citywide passenger demand prediction, in: Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 2293–2296.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balsebre et al. [2022]</span>
<span class="ltx_bibblock">
Balsebre, P., Yao, D., Cong, G., Hai, Z., 2022.

</span>
<span class="ltx_bibblock">Geospatial entity resolution, in: Proceedings of the ACM Web Conference 2022, Association for Computing Machinery, New York, NY, USA. pp. 3061–3070.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3485447.3512026" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3485447.3512026</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baltrušaitis et al. [2018]</span>
<span class="ltx_bibblock">
Baltrušaitis, T., Ahuja, C., Morency, L.P., 2018.

</span>
<span class="ltx_bibblock">Multimodal machine learning: A survey and taxonomy.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine intelligence 41, 423–443.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbosa et al. [2018]</span>
<span class="ltx_bibblock">
Barbosa, H., Barthelemy, M., Ghoshal, G., James, C.R., Lenormand, M., Louail, T., Menezes, R., Ramasco, J.J., Simini, F., Tomasini, M., 2018.

</span>
<span class="ltx_bibblock">Human mobility: Models and applications.

</span>
<span class="ltx_bibblock">Physics Reports 734, 1–74.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Battaïa et al. [2023]</span>
<span class="ltx_bibblock">
Battaïa, O., Dolgui, A., Guschinsky, N., Rozin, B., 2023.

</span>
<span class="ltx_bibblock">Milp model for fleet and charging infrastructure decisions for fast-charging city electric bus services.

</span>
<span class="ltx_bibblock">Computers &amp; Industrial Engineering , 109336.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellemans et al. [2002]</span>
<span class="ltx_bibblock">
Bellemans, T., De Schutter, B., De Moor, B., 2002.

</span>
<span class="ltx_bibblock">Models for traffic control.

</span>
<span class="ltx_bibblock">JOURNAL A 43, 13–22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biljecki and Ito [2021]</span>
<span class="ltx_bibblock">
Biljecki, F., Ito, K., 2021.

</span>
<span class="ltx_bibblock">Street view imagery in urban analytics and gis: A review.

</span>
<span class="ltx_bibblock">Landscape and Urban Planning 215, 104217.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bin et al. [2023]</span>
<span class="ltx_bibblock">
Bin, J., Gardiner, B., Liu, H., Li, E., Liu, Z., 2023.

</span>
<span class="ltx_bibblock">Rhpmf: A context-aware matrix factorization approach for understanding regional real estate market.

</span>
<span class="ltx_bibblock">Information Fusion 94, 229–242.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bing et al. [2023]</span>
<span class="ltx_bibblock">
Bing, J., Chen, M., Yang, M., Huang, W., Gong, Y., Nie, L., 2023.

</span>
<span class="ltx_bibblock">Pre-trained semantic embeddings for POI categories based on multiple contexts.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 35, 8893–8904.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TKDE.2022.3218851" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TKDE.2022.3218851</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Breunig et al. [2020]</span>
<span class="ltx_bibblock">
Breunig, M., Bradley, P.E., Jahn, M., Kuper, P., Mazroob, N., Rösch, N., Al-Doori, M., Stefanakis, E., Jadidi, M., 2020.

</span>
<span class="ltx_bibblock">Geospatial data management research: Progress and future directions.

</span>
<span class="ltx_bibblock">ISPRS International Journal of Geo-Information 9, 95.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al. [2024]</span>
<span class="ltx_bibblock">
Brooks, T., Peebles, B., Homes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A., 2024.

</span>
<span class="ltx_bibblock">Video generation models as world simulators URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/research/video-generation-models-as-world-simulators" title="">https://openai.com/research/video-generation-models-as-world-simulators</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 33, 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. [2023]</span>
<span class="ltx_bibblock">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al., 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.12712 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bui [2023]</span>
<span class="ltx_bibblock">
Bui, T.H., 2023.

</span>
<span class="ltx_bibblock">Automatic construction of POI address lists at city streets from geo-tagged photos and web data: a case study of San Jose City.

</span>
<span class="ltx_bibblock">Multimedia Tools and Applications , 1–22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke et al. [2021]</span>
<span class="ltx_bibblock">
Burke, M., Driscoll, A., Lobell, D.B., Ermon, S., 2021.

</span>
<span class="ltx_bibblock">Using satellite imagery to understand and promote sustainable development.

</span>
<span class="ltx_bibblock">Science 371, eabe8628.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bwire and Zengo [2020]</span>
<span class="ltx_bibblock">
Bwire, H., Zengo, E., 2020.

</span>
<span class="ltx_bibblock">Comparison of efficiency between public and private transport modes using excess commuting: An experience in dar es salaam.

</span>
<span class="ltx_bibblock">Journal of Transport Geography 82, 102616.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. [2023]</span>
<span class="ltx_bibblock">
Cai, T., Wan, H., Wu, F., Wen, H., Guo, S., Wu, L., Hu, H., Lin, Y., 2023.

</span>
<span class="ltx_bibblock">M 2 g4rtp: A multi-level and multi-task graph model for instant-logistics route and time joint prediction, in: 2023 IEEE 39th International Conference on Data Engineering (ICDE), IEEE. pp. 3296–3308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2023]</span>
<span class="ltx_bibblock">
Cao, D., Jia, F., Arik, S.O., Pfister, T., Zheng, Y., Ye, W., Liu, Y., 2023.

</span>
<span class="ltx_bibblock">Tempo: Prompt-based generative pre-trained transformer for time series forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.04948 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2022]</span>
<span class="ltx_bibblock">
Cao, H., Tan, C., Gao, Z., Xu, Y., Chen, G., Heng, P.A., Li, S.Z., 2022.

</span>
<span class="ltx_bibblock">A survey on generative diffusion model.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2209.02646 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2020]</span>
<span class="ltx_bibblock">
Cao, K., Guo, J., Meng, G., Liu, H., Liu, Y., Li, G., 2020.

</span>
<span class="ltx_bibblock">Points-of-interest recommendation algorithm based on LBSN in edge computing environment.

</span>
<span class="ltx_bibblock">IEEE Access 8, 47973–47983.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ACCESS.2020.2979922" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ACCESS.2020.2979922</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carozzi and Roth [2023]</span>
<span class="ltx_bibblock">
Carozzi, F., Roth, S., 2023.

</span>
<span class="ltx_bibblock">Dirty density: Air quality and the density of american cities.

</span>
<span class="ltx_bibblock">Journal of Environmental Economics and Management 118, 102767.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandra et al. [2021]</span>
<span class="ltx_bibblock">
Chandra, D.K., Leopold, J., Fu, Y., 2021.

</span>
<span class="ltx_bibblock">NodeSense2Vec: Spatiotemporal context-aware network embedding for heterogeneous urban mobility data, in: 2021 IEEE International Conference on Big Data (Big Data), pp. 2884–2893.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/BigData52589.2021.9672072" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/BigData52589.2021.9672072</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2023]</span>
<span class="ltx_bibblock">
Chang, C., Peng, W.C., Chen, T.F., 2023.

</span>
<span class="ltx_bibblock">Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.08469 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021a]</span>
<span class="ltx_bibblock">
Chen, B., Xu, B., Gong, P., 2021a.

</span>
<span class="ltx_bibblock">Mapping essential urban land use categories (euluc) using geospatial big data: Progress, challenges, and opportunities.

</span>
<span class="ltx_bibblock">Big Earth Data 5, 410–441.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022a]</span>
<span class="ltx_bibblock">
Chen, G., Liu, S., Jiang, F., 2022a.

</span>
<span class="ltx_bibblock">Daily weather forecasting based on deep learning model: A case study of shenzhen city, china.

</span>
<span class="ltx_bibblock">Atmosphere 13, 1208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022b]</span>
<span class="ltx_bibblock">
Chen, G., Su, Y., Zhang, X., Hu, A., Chen, G., Feng, S., Xiang, J., Zhang, J., Zheng, Y., 2022b.

</span>
<span class="ltx_bibblock">A cross-city federated transfer learning framework: A case study on urban region profiling.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.48550/arXiv.2206.00007" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2206.00007</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2206.00007" title="">arXiv:2206.00007</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Chen, J., Zhou, C., Li, F., 2020.

</span>
<span class="ltx_bibblock">Quantifying the green view indicator for assessing urban greening quality: An analysis based on internet-crawling street view data.

</span>
<span class="ltx_bibblock">Ecological Indicators 113, 106192.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018]</span>
<span class="ltx_bibblock">
Chen, L., Fan, X., Wang, L., Zhang, D., Yu, Z., Li, J., Thi Mai Trang, N., Pan, G., Wang, C., 2018.

</span>
<span class="ltx_bibblock">RADAR: Road obstacle identification for disaster response leveraging cross-domain urban data.

</span>
<span class="ltx_bibblock">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 1–23.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3161159" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3161159</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021b]</span>
<span class="ltx_bibblock">
Chen, L., Lu, C., Yuan, F., Jiang, Z., Wang, L., Zhang, D., Luo, R., Fan, X., Wang, C., 2021b.

</span>
<span class="ltx_bibblock">UVLens: Urban village boundary identification and population estimation leveraging open government data.

</span>
<span class="ltx_bibblock">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 57:1–57:26.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3463495" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3463495</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Chen, W., Wang, G., Zeng, J., 2023.

</span>
<span class="ltx_bibblock">Impact of urbanization on ecosystem health in chinese urban agglomerations.

</span>
<span class="ltx_bibblock">Environmental Impact Assessment Review 98, 106964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
Chen, X., Zeng, Y., Cong, G., Qin, S., Xiang, Y., Dai, Y., 2015.

</span>
<span class="ltx_bibblock">On information coverage for location category based point-of-interest recommendation.

</span>
<span class="ltx_bibblock">Proceedings of the AAAI Conference on Artificial Intelligence 29.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1609/aaai.v29i1.9191" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1609/aaai.v29i1.9191</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021c]</span>
<span class="ltx_bibblock">
Chen, Z., Chen, L., Cong, G., Jensen, C.S., 2021c.

</span>
<span class="ltx_bibblock">Location- and keyword-based querying of geo-textual data: a survey.

</span>
<span class="ltx_bibblock">The VLDB Journal 30, 603–640.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00778-021-00661-w" title="">https://doi.org/10.1007/s00778-021-00661-w</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/s00778-021-00661-w" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s00778-021-00661-w</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022c]</span>
<span class="ltx_bibblock">
Chen, Z., Deng, Y., Wu, Y., Gu, Q., Li, Y., 2022c.

</span>
<span class="ltx_bibblock">Towards understanding the mixture-of-experts layer in deep learning, in: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (Eds.), Advances in Neural Information Processing Systems, Curran Associates, Inc.. pp. 23049–23062.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2023]</span>
<span class="ltx_bibblock">
Cheng, Q., Jing, Q., Collender, P.A., Head, J.R., Li, Q., Yu, H., Li, Z., Ju, Y., Chen, T., Wang, P., Cleary, E., Lai, S., 2023.

</span>
<span class="ltx_bibblock">Prior water availability modifies the effect of heavy rainfall on dengue transmission: a time series analysis of passive surveillance data from southern china.

</span>
<span class="ltx_bibblock">Frontiers in Public Health URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3389/fpubh.2023.1287678" title="">https://doi.org/10.3389/fpubh.2023.1287678</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3389/fpubh.2023.1287678" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3389/fpubh.2023.1287678</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chengchuang et al. [2021]</span>
<span class="ltx_bibblock">
Chengchuang, L., Chun, S., Gansen, Z., et al., 2021.

</span>
<span class="ltx_bibblock">Review of image data augmentation in computer vision.

</span>
<span class="ltx_bibblock">Journal of Frontiers of Computer Science &amp; Technology 15, 583.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chua et al. [2009]</span>
<span class="ltx_bibblock">
Chua, T.S., Tang, J., Hong, R., Li, H., Luo, Z., Zheng, Y., 2009.

</span>
<span class="ltx_bibblock">Nus-wide: A real-world web image database from national university of singapore, in: ACM International Conference on Image and Video Retrieval, pp. 48:1–48:9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CondorFerries [2023]</span>
<span class="ltx_bibblock">
CondorFerries, 2023.

</span>
<span class="ltx_bibblock">Explore solo travel trends &amp; stats by demographics, destination, industry &amp; why solo travel continues to rise!

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.condorferries.co.uk/solo-travel-statistics/" title="">https://www.condorferries.co.uk/solo-travel-statistics/</a>,.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cong et al. [2022]</span>
<span class="ltx_bibblock">
Cong, Y., Khanna, S., Meng, C., Liu, P., Rozi, E., He, Y., Burke, M., Lobell, D., Ermon, S., 2022.

</span>
<span class="ltx_bibblock">Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 197–211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conger [2023]</span>
<span class="ltx_bibblock">
Conger, K., 2023.

</span>
<span class="ltx_bibblock">So what do we call twitter now anyway?

</span>
<span class="ltx_bibblock">The New York Times Archived from the original on October 12, 2023. Retrieved August 29, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. [2023]</span>
<span class="ltx_bibblock">
Cui, J., Li, Z., Yan, Y., Chen, B., Yuan, L., 2023.

</span>
<span class="ltx_bibblock">Chatlaw: Open-source legal large language model with integrated external knowledge bases.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.16092 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2015]</span>
<span class="ltx_bibblock">
Dai, J., Yang, B., Guo, C., Ding, Z., 2015.

</span>
<span class="ltx_bibblock">Personalized route recommendation using big trajectory data, in: 2015 IEEE 31st international conference on data engineering, IEEE. pp. 543–554.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S., .

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.06500 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao [2023]</span>
<span class="ltx_bibblock">
Dao, T., 2023.

</span>
<span class="ltx_bibblock">Flashattention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.08691 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. [2022]</span>
<span class="ltx_bibblock">
Dao, T., Fu, D., Ermon, S., Rudra, A., Ré, C., 2022.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 16344–16359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldari et al. [2022]</span>
<span class="ltx_bibblock">
Deldari, S., Xue, H., Saeed, A., He, J., Smith, D.V., Salim, F.D., 2022.

</span>
<span class="ltx_bibblock">Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2206.02353 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai and Johnson [2021]</span>
<span class="ltx_bibblock">
Desai, K., Johnson, J., 2021.

</span>
<span class="ltx_bibblock">Virtex: Learning visual representations from textual annotations, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162–11173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. [2022]</span>
<span class="ltx_bibblock">
Dettmers, T., Lewis, M., Belkada, Y., Zettlemoyer, L., 2022.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2208.07339 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. [2023]</span>
<span class="ltx_bibblock">
Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L., 2023.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14314 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2018]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1810.04805 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2023]</span>
<span class="ltx_bibblock">
Ding, R., Chen, B., Xie, P., Huang, F., Li, X., Zhang, Q., Xu, Y., 2023.

</span>
<span class="ltx_bibblock">Mgeo: Multi-modal geographic language model pre-training, in: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 185–194.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doi [2015]</span>
<span class="ltx_bibblock">
Doi, K., 2015.

</span>
<span class="ltx_bibblock">Cities and transportation.

</span>
<span class="ltx_bibblock">Traffic and Safety Sciences—Interdisciplinary Wisdom of IATSS, International Association of Traffic and Safety Sciences , 12–21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. [2019]</span>
<span class="ltx_bibblock">
Du, J., Zhang, Y., Wang, P., Leopold, J., Fu, Y., 2019.

</span>
<span class="ltx_bibblock">Beyond geo-first law: Learning spatial representations via integrated autocorrelations and complementarity, in: 2019 IEEE International Conference on Data Mining (ICDM), pp. 160–169.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ICDM.2019.00026" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICDM.2019.00026</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dutta et al. [2017]</span>
<span class="ltx_bibblock">
Dutta, J., Pramanick, P., Roy, S., 2017.

</span>
<span class="ltx_bibblock">Noisesense: Crowdsourced context aware sensing for real time noise pollution monitoring of the city, in: 2017 IEEE international conference on advanced networks and telecommunications systems (ANTS), IEEE. pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork [2008]</span>
<span class="ltx_bibblock">
Dwork, C., 2008.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results, in: International conference on theory and applications of models of computation, Springer. pp. 1–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elmi and Tan [2021]</span>
<span class="ltx_bibblock">
Elmi, S., Tan, K.L., 2021.

</span>
<span class="ltx_bibblock">Deepfec: energy consumption prediction under real-world driving conditions for smart cities, in: Proceedings of the Web Conference 2021, pp. 1880–1890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fadhel et al. [2024]</span>
<span class="ltx_bibblock">
Fadhel, M.A., Duhaim, A.M., Saihood, A., Sewify, A., Al-Hamadani, M.N., Albahri, A., Alzubaidi, L., Gupta, A., Mirjalili, S., Gu, Y., 2024.

</span>
<span class="ltx_bibblock">Comprehensive systematic review of information fusion methods in smart cities and urban environments.

</span>
<span class="ltx_bibblock">Information Fusion , 102317.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. [2023]</span>
<span class="ltx_bibblock">
Fan, Z., Zhang, F., Loo, B.P.Y., Ratti, C., 2023.

</span>
<span class="ltx_bibblock">Urban visual intelligence: Uncovering hidden city profiles with street view images.

</span>
<span class="ltx_bibblock">Proceedings of the National Academy of Sciences 120, e2220417120.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.2220417120" title="">https://www.pnas.org/doi/abs/10.1073/pnas.2220417120</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1073/pnas.2220417120" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1073/pnas.2220417120</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/https://www.pnas.org/doi/pdf/10.1073/pnas.2220417120" title="">arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2220417120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. [2023]</span>
<span class="ltx_bibblock">
Fang, J., Meng, X., Qi, X., 2023.

</span>
<span class="ltx_bibblock">A top-k POI recommendation approach based on LBSN and multi-graph fusion.

</span>
<span class="ltx_bibblock">Neurocomputing 518, 219–230.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.neucom.2022.10.048" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.neucom.2022.10.048</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farmer and Foley [2009]</span>
<span class="ltx_bibblock">
Farmer, J.D., Foley, D., 2009.

</span>
<span class="ltx_bibblock">The economy needs agent-based modelling.

</span>
<span class="ltx_bibblock">Nature 460, 685–686.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fisch et al. [2013]</span>
<span class="ltx_bibblock">
Fisch, D., Kalkowski, E., Sick, B., 2013.

</span>
<span class="ltx_bibblock">Knowledge fusion for probabilistic generative classifiers with data mining applications.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 26, 652–666.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flache et al. [2022]</span>
<span class="ltx_bibblock">
Flache, A., Mäs, M., Keijzer, M.A., 2022.

</span>
<span class="ltx_bibblock">Computational approaches in rigorous sociology: agent-based computational modeling and computational social science.

</span>
<span class="ltx_bibblock">Handbook of Sociological Science , 57–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Foosherian et al. [2023]</span>
<span class="ltx_bibblock">
Foosherian, M., Purwins, H., Rathnayake, P., Alam, T., Teimao, R., Thoben, K.D., 2023.

</span>
<span class="ltx_bibblock">Enhancing pipeline-based conversational agents with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2309.03748 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2019]</span>
<span class="ltx_bibblock">
Fu, Y., Wang, P., Du, J., Wu, L., Li, X., 2019.

</span>
<span class="ltx_bibblock">Efficient region embedding with multi-view spatial networks: A perspective of locality-constrained spatial autocorrelations, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 906–913.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2022a]</span>
<span class="ltx_bibblock">
Gao, C., Zheng, Y., Wang, W., Feng, F., He, X., Li, Y., 2022a.

</span>
<span class="ltx_bibblock">Causal inference in recommender systems: A survey and future directions.

</span>
<span class="ltx_bibblock">ACM Transactions on Information Systems .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2022b]</span>
<span class="ltx_bibblock">
Gao, N., Xue, H., Shao, W., Zhao, S., Qin, K.K., Prabowo, A., Rahaman, M.S., Salim, F.D., 2022b.

</span>
<span class="ltx_bibblock">Generative adversarial networks for spatio-temporal data: A survey.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology (TIST) 13, 1–25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023a]</span>
<span class="ltx_bibblock">
Gao, Q., Wang, W., Huang, L., Yang, X., Li, T., Fujita, H., 2023a.

</span>
<span class="ltx_bibblock">Dual-grained human mobility learning for location-aware trip recommendation with spatial–temporal graph knowledge fusion.

</span>
<span class="ltx_bibblock">Information Fusion 92, 46–63.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023b]</span>
<span class="ltx_bibblock">
Gao, Q., Wang, W., Huang, L., Yang, X., Li, T., Fujita, H., 2023b.

</span>
<span class="ltx_bibblock">Dual-grained human mobility learning for location-aware trip recommendation with spatial–temporal graph knowledge fusion.

</span>
<span class="ltx_bibblock">Information Fusion 92, 46–63.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.inffus.2022.11.018" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.inffus.2022.11.018</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2022c]</span>
<span class="ltx_bibblock">
Gao, Q., Zhou, F., Zhong, T., Trajcevski, G., Yang, X., Li, T., 2022c.

</span>
<span class="ltx_bibblock">Contextual spatio-temporal graph representation learning for reinforced human mobility mining.

</span>
<span class="ltx_bibblock">Information Sciences 606, 230–249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023c]</span>
<span class="ltx_bibblock">
Gao, R., Chen, K., Xie, E., Hong, L., Li, Z., Yeung, D.Y., Xu, Q., 2023c.

</span>
<span class="ltx_bibblock">Magicdrive: Street view generation with diverse 3d geometry control.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.02601 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geanakoplos [2010]</span>
<span class="ltx_bibblock">
Geanakoplos, J., 2010.

</span>
<span class="ltx_bibblock">The leverage cycle.

</span>
<span class="ltx_bibblock">NBER macroeconomics annual 24, 1–66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. [2019a]</span>
<span class="ltx_bibblock">
Geng, X., Li, Y., Wang, L., Zhang, L., Yang, Q., Ye, J., Liu, Y., 2019a.

</span>
<span class="ltx_bibblock">Spatiotemporal multi-graph convolution network for ride-hailing demand forecasting, in: Proceedings of the AAAI conference on artificial intelligence, pp. 3656–3663.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. [2022]</span>
<span class="ltx_bibblock">
Geng, X., Liu, H., Lee, L., Schuurmans, D., Levine, S., Abbeel, P., 2022.

</span>
<span class="ltx_bibblock">Multimodal masked autoencoders learn transferable representations.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2205.14204 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. [2019b]</span>
<span class="ltx_bibblock">
Geng, X., Wu, X., Zhang, L., Yang, Q., Liu, Y., Ye, J., 2019b.

</span>
<span class="ltx_bibblock">Multi-modal graph interaction for multi-graph convolution network in urban spatiotemporal forecasting.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.48550/arXiv.1905.11395" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.1905.11395</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1905.11395" title="">arXiv:1905.11395</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
GeoVid Project, .

</span>
<span class="ltx_bibblock">GeoVid Project.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://geovid.org/" title="">http://geovid.org/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerla and Kleinrock [1980]</span>
<span class="ltx_bibblock">
Gerla, M., Kleinrock, L., 1980.

</span>
<span class="ltx_bibblock">Flow control: A comparative survey.

</span>
<span class="ltx_bibblock">IEEE Transactions on Communications 28, 553–574.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghoneim et al. [2017]</span>
<span class="ltx_bibblock">
Ghoneim, O.A., Manjunatha, B., et al., 2017.

</span>
<span class="ltx_bibblock">Forecasting of ozone concentration in smart city using deep learning, in: 2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI), IEEE. pp. 1320–1326.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2019]</span>
<span class="ltx_bibblock">
Gong, Z., Ma, Q., Kan, C., Qi, Q., 2019.

</span>
<span class="ltx_bibblock">Classifying street spaces with street view images for a spatial indicator of urban functions.

</span>
<span class="ltx_bibblock">Sustainability 11, 6424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">González-Crespo et al. [2023]</span>
<span class="ltx_bibblock">
González-Crespo, C., Martínez-López, B., Conejero, C., Castillo-Contreras, R., Serrano, E., López-Martín, J.M., Lavín, S., López-Olvera, J.R., 2023.

</span>
<span class="ltx_bibblock">Predicting human-wildlife interaction in urban environments through agent-based models.

</span>
<span class="ltx_bibblock">Landscape and Urban Planning 240, 104878.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et al. [2021]</span>
<span class="ltx_bibblock">
Gou, J., Yu, B., Maybank, S.J., Tao, D., 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation: A survey.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision 129, 1789–1819.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2020]</span>
<span class="ltx_bibblock">
Guo, C., Yang, B., Hu, J., Jensen, C.S., Chen, L., 2020.

</span>
<span class="ltx_bibblock">Context-aware, preference-based vehicle routing.

</span>
<span class="ltx_bibblock">The VLDB Journal 29, 1149–1170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Gong [2016]</span>
<span class="ltx_bibblock">
Guo, J., Gong, Z., 2016.

</span>
<span class="ltx_bibblock">A nonparametric model for event discovery in the geospatial-temporal space, in: Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, Association for Computing Machinery, New York, NY, USA. pp. 499–508.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/2983323.2983790" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/2983323.2983790</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2022]</span>
<span class="ltx_bibblock">
Guo, S., Chen, C., Wang, J., Ding, Y., Liu, Y., Xu, K., Yu, Z., Zhang, D., 2022.

</span>
<span class="ltx_bibblock">A force-directed approach to seeking route recommendation in ride-on-demand service using multi-source urban data.

</span>
<span class="ltx_bibblock">IEEE Transactions on Mobile Computing 21, 1909–1926.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TMC.2020.3033274" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TMC.2020.3033274</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2019]</span>
<span class="ltx_bibblock">
Guo, S., Chen, C., Wang, J., Liu, Y., Xu, K., Yu, Z., Zhang, D., Chiu, D.M., 2019.

</span>
<span class="ltx_bibblock">Rod-revenue: Seeking strategies analysis and revenue prediction in ride-on-demand service using multi-source urban data.

</span>
<span class="ltx_bibblock">IEEE Transactions on Mobile Computing 19, 2202–2220.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajela et al. [2021]</span>
<span class="ltx_bibblock">
Hajela, G., Chawla, M., Rasool, A., 2021.

</span>
<span class="ltx_bibblock">A multi-dimensional crime spatial pattern analysis and prediction model based on classification.

</span>
<span class="ltx_bibblock">ETRI Journal 43, 272–287.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2021]</span>
<span class="ltx_bibblock">
Han, P., Wang, J., Yao, D., Shang, S., Zhang, X., 2021.

</span>
<span class="ltx_bibblock">A graph-based approach for trajectory similarity computation in spatial networks, in: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, pp. 556–564.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. [2024]</span>
<span class="ltx_bibblock">
Hao, X., Chen, W., Yan, Y., Zhong, S., Wang, K., Wen, Q., Liang, Y., 2024.

</span>
<span class="ltx_bibblock">Urbanvlp: A multi-granularity vision-language pre-trained foundation model for urban indicator prediction.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2403.16831 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hashem et al. [2023]</span>
<span class="ltx_bibblock">
Hashem, I.A.T., Usmani, R.S.A., Almutairi, M.S., Ibrahim, A.O., Zakari, A., Alotaibi, F., Alhashmi, S.M., Chiroma, H., 2023.

</span>
<span class="ltx_bibblock">Urban computing for sustainable smart cities: Recent advances, taxonomy, and open research challenges.

</span>
<span class="ltx_bibblock">Sustainability 15, 3916.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2019]</span>
<span class="ltx_bibblock">
He, J., Qi, J., Ramamohanarao, K., 2019.

</span>
<span class="ltx_bibblock">A joint context-aware embedding for trip recommendations, in: 2019 IEEE 35th International Conference on Data Engineering (ICDE), IEEE. pp. 292–303.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2018]</span>
<span class="ltx_bibblock">
He, J., Yang, H., Tang, T.Q., Huang, H.J., 2018.

</span>
<span class="ltx_bibblock">An optimal charging station location model with the consideration of electric vehicle’s driving range.

</span>
<span class="ltx_bibblock">Transportation Research Part C: Emerging Technologies 86, 641–654.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
He, J., Yang, K., Tang, W., Lu, H., Qin, J., Chen, Y., Li, X., 2020.

</span>
<span class="ltx_bibblock">The first high-resolution meteorological forcing dataset for land process studies over China.

</span>
<span class="ltx_bibblock">Scientific Data 7, 25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2022]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R., 2022.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000–16009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermans and Schrauwen [2013]</span>
<span class="ltx_bibblock">
Hermans, M., Schrauwen, B., 2013.

</span>
<span class="ltx_bibblock">Training and analysing deep recurrent neural networks.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al. [2022]</span>
<span class="ltx_bibblock">
Hou, Z., Liu, X., Cen, Y., Dong, Y., Yang, H., Wang, C., Tang, J., 2022.

</span>
<span class="ltx_bibblock">Graphmae: Self-supervised masked graph autoencoders, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 594–604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2023a]</span>
<span class="ltx_bibblock">
Hu, Y., Yuan, J., Wen, C., Lu, X., Li, X., 2023a.

</span>
<span class="ltx_bibblock">Rsgpt: A remote sensing vision language model and benchmark.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.15266 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2023b]</span>
<span class="ltx_bibblock">
Hu, Z., Feng, Y., Luu, A.T., Hooi, B., Lipani, A., 2023b.

</span>
<span class="ltx_bibblock">Unlocking the potential of user feedback: Leveraging large language model as user simulator to enhance dialogue system.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.09821 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Wang [2016]</span>
<span class="ltx_bibblock">
Huang, C., Wang, D., 2016.

</span>
<span class="ltx_bibblock">Exploiting spatial-temporal-social constraints for localness inference using online social media, in: 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pp. 287–294.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ASONAM.2016.7752247" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ASONAM.2016.7752247</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2018]</span>
<span class="ltx_bibblock">
Huang, C., Zhang, J., Zheng, Y., Chawla, N.V., 2018.

</span>
<span class="ltx_bibblock">Deepcrime: Attentive hierarchical recurrent networks for crime prediction, in: Proceedings of the 27th ACM international conference on information and knowledge management, pp. 1423–1432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2020]</span>
<span class="ltx_bibblock">
Huang, J., Chai, J., Cho, S., 2020.

</span>
<span class="ltx_bibblock">Deep learning in finance and banking: A literature review and classification.

</span>
<span class="ltx_bibblock">Frontiers of Business Research in China 14, 1–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2022a]</span>
<span class="ltx_bibblock">
Huang, J., Wang, H., Sun, Y., Shi, Y., Huang, Z., Zhuo, A., Feng, S., 2022a.

</span>
<span class="ltx_bibblock">ERNIE-GeoL: A geography-and-language pre-trained model and its applications in baidu maps, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3029–3039.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3534678.3539021" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3534678.3539021</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2203.09127" title="">arXiv:2203.09127</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2022b]</span>
<span class="ltx_bibblock">
Huang, T., Fu, R., Chen, Y., Sun, Q., 2022b.

</span>
<span class="ltx_bibblock">Real-time driver behavior detection based on deep deformable inverted residual network with an attention mechanism for human-vehicle co-driving system.

</span>
<span class="ltx_bibblock">IEEE Transactions on Vehicular Technology 71, 12475–12488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023a]</span>
<span class="ltx_bibblock">
Huang, Y., Zhang, F., Gao, Y., Tu, W., Duarte, F., Ratti, C., Guo, D., Liu, Y., 2023a.

</span>
<span class="ltx_bibblock">Comprehensive urban space representation with varying numbers of street-level images.

</span>
<span class="ltx_bibblock">Computers, Environment and Urban Systems 106, 102043.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0198971523001060" title="">https://www.sciencedirect.com/science/article/pii/S0198971523001060</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/https://doi.org/10.1016/j.compenvurbsys.2023.102043" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.compenvurbsys.2023.102043</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023b]</span>
<span class="ltx_bibblock">
Huang, Y., Zhang, F., Gao, Y., Tu, W., Duarte, F., Ratti, C., Guo, D., Liu, Y., 2023b.

</span>
<span class="ltx_bibblock">Comprehensive urban space representation with varying numbers of street-level images.

</span>
<span class="ltx_bibblock">Computers, Environment and Urban Systems 106, 102043.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.compenvurbsys.2023.102043" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.compenvurbsys.2023.102043</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iskandaryan et al. [2020]</span>
<span class="ltx_bibblock">
Iskandaryan, D., Ramos, F., Trilles, S., 2020.

</span>
<span class="ltx_bibblock">Air quality prediction in smart cities using machine learning technologies based on sensor data: a review.

</span>
<span class="ltx_bibblock">Applied Sciences 10, 2401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. [2016]</span>
<span class="ltx_bibblock">
Jain, A., Zamir, A.R., Savarese, S., Saxena, A., 2016.

</span>
<span class="ltx_bibblock">Structural-rnn: Deep learning on spatio-temporal graphs, in: Proceedings of the ieee conference on computer vision and pattern recognition, pp. 5308–5317.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jenkins et al. [2019]</span>
<span class="ltx_bibblock">
Jenkins, P., Farag, A., Wang, S., Li, Z., 2019.

</span>
<span class="ltx_bibblock">Unsupervised representation learning of spatial data via multimodal embedding, in: Proceedings of the 28th ACM International Conference on Information and Knowledge Management, Association for Computing Machinery, New York, NY, USA. pp. 1993–2002.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3357384.3358001" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3357384.3358001</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">JEZDOVIÄ et al. [2018]</span>
<span class="ltx_bibblock">
JEZDOVIÄ, I., NEDELJKOVIÄ, N., Å<math alttext="1/2" class="ltx_Math" display="inline" id="bib.bib117.2.m1.1"><semantics id="bib.bib117.2.m1.1a"><mrow id="bib.bib117.2.m1.1.1" xref="bib.bib117.2.m1.1.1.cmml"><mn id="bib.bib117.2.m1.1.1.2" xref="bib.bib117.2.m1.1.1.2.cmml">1</mn><mo id="bib.bib117.2.m1.1.1.1" xref="bib.bib117.2.m1.1.1.1.cmml">/</mo><mn id="bib.bib117.2.m1.1.1.3" xref="bib.bib117.2.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="bib.bib117.2.m1.1b"><apply id="bib.bib117.2.m1.1.1.cmml" xref="bib.bib117.2.m1.1.1"><divide id="bib.bib117.2.m1.1.1.1.cmml" xref="bib.bib117.2.m1.1.1.1"></divide><cn id="bib.bib117.2.m1.1.1.2.cmml" type="integer" xref="bib.bib117.2.m1.1.1.2">1</cn><cn id="bib.bib117.2.m1.1.1.3.cmml" type="integer" xref="bib.bib117.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib117.2.m1.1c">1/2</annotation><annotation encoding="application/x-llamapun" id="bib.bib117.2.m1.1d">1 / 2</annotation></semantics></math>IVOJINOVIÄ, L., RADENKOVIÄ, B., Labus, A., 2018.

</span>
<span class="ltx_bibblock">Smart city: A system for measuring noise pollution.

</span>
<span class="ltx_bibblock">Smart Cities and Regional Development (SCRD) Journal 2, 79–85.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jezdović et al. [2021]</span>
<span class="ltx_bibblock">
Jezdović, I., Popović, S., Radenković, M., Labus, A., Bogdanović, Z., 2021.

</span>
<span class="ltx_bibblock">A crowdsensing platform for real-time monitoring and analysis of noise pollution in smart cities.

</span>
<span class="ltx_bibblock">Sustainable Computing: Informatics and Systems 31, 100588.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2023a]</span>
<span class="ltx_bibblock">
Ji, J., Wang, J., Huang, C., Wu, J., Xu, B., Wu, Z., Zhang, J., Zheng, Y., 2023a.

</span>
<span class="ltx_bibblock">Spatio-temporal self-supervised learning for traffic flow prediction, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4356–4364.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2023b]</span>
<span class="ltx_bibblock">
Ji, J., Yu, F., Lei, M., 2023b.

</span>
<span class="ltx_bibblock">Self-supervised spatiotemporal graph neural networks with self-distillation for traffic prediction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Intelligent Transportation Systems 24, 1580–1593.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TITS.2022.3219626" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TITS.2022.3219626</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2016]</span>
<span class="ltx_bibblock">
Ji, S., Zheng, Y., Li, T., 2016.

</span>
<span class="ltx_bibblock">Urban sensing based on human mobility, in: Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pp. 1040–1051.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2020]</span>
<span class="ltx_bibblock">
Ji, Z., Wang, H., Han, J., Pang, Y., 2020.

</span>
<span class="ltx_bibblock">Sman: Stacked multimodal attention network for cross-modal image–text retrieval.

</span>
<span class="ltx_bibblock">IEEE transactions on cybernetics 52, 1086–1097.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2019]</span>
<span class="ltx_bibblock">
Jiang, R., Song, X., Huang, D., Song, X., Xia, T., Cai, Z., Wang, Z., Kim, K.S., Shibasaki, R., 2019.

</span>
<span class="ltx_bibblock">DeepUrbanEvent: A system for predicting citywide crowd dynamics at big events, in: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Association for Computing Machinery, New York, NY, USA. pp. 2114–2122.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3292500.3330654" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3292500.3330654</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2021]</span>
<span class="ltx_bibblock">
Jiang, Z., Chen, L., Zhou, B., Huang, J., Xie, T., Fan, X., Wang, C., 2021.

</span>
<span class="ltx_bibblock">ITV: Inferring traffic violation-prone locations with vehicle trajectories and road environment data.

</span>
<span class="ltx_bibblock">IEEE Systems Journal 15, 3913–3924.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/JSYST.2020.3012743" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/JSYST.2020.3012743</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2023a]</span>
<span class="ltx_bibblock">
Jin, G., Liang, Y., Fang, Y., Shao, Z., Huang, J., Zhang, J., Zheng, Y., 2023a.

</span>
<span class="ltx_bibblock">Spatio-temporal graph neural networks for predictive learning in urban computing: A survey.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering , 1–20doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TKDE.2023.3333824" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TKDE.2023.3333824</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2023b]</span>
<span class="ltx_bibblock">
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.Y., Shi, X., Chen, P.Y., Liang, Y., Li, Y.F., Pan, S., et al., 2023b.

</span>
<span class="ltx_bibblock">Time-llm: Time series forecasting by reprogramming large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.01728 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2024]</span>
<span class="ltx_bibblock">
Jin, M., Zhang, Y., Chen, W., Zhang, K., Liang, Y., Yang, B., Wang, J., Pan, S., Wen, Q., 2024.

</span>
<span class="ltx_bibblock">Position paper: What can large language models tell us about time series analysis, in: International Conference on Machine Learning (ICML 2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johari et al. [2020]</span>
<span class="ltx_bibblock">
Johari, F., Peronato, G., Sadeghian, P., Zhao, X., Widén, J., 2020.

</span>
<span class="ltx_bibblock">Urban building energy modeling: State of the art and future prospects.

</span>
<span class="ltx_bibblock">Renewable and Sustainable Energy Reviews 128, 109902.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. [2018]</span>
<span class="ltx_bibblock">
Kang, G.K., Gao, J.Z., Chiao, S., Lu, S., Xie, G., 2018.

</span>
<span class="ltx_bibblock">Air quality prediction: Big data and machine learning approaches.

</span>
<span class="ltx_bibblock">Int. J. Environ. Sci. Dev 9, 8–16.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. [2020]</span>
<span class="ltx_bibblock">
Kang, Y., Zhang, F., Gao, S., Lin, H., Liu, Y., 2020.

</span>
<span class="ltx_bibblock">A review of urban physical environment sensing using street view imagery in public health studies.

</span>
<span class="ltx_bibblock">Annals of GIS 26, 261–275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. [2020]</span>
<span class="ltx_bibblock">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D., 2020.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2001.08361 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2021]</span>
<span class="ltx_bibblock">
Ke, J., Feng, S., Zhu, Z., Yang, H., Ye, J., 2021.

</span>
<span class="ltx_bibblock">Joint predictions of multi-modal ride-hailing demands: A deep multi-task multi-graph learning-based approach.

</span>
<span class="ltx_bibblock">Transportation Research Part C: Emerging Technologies 127, 103063.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2023]</span>
<span class="ltx_bibblock">
Ke, S., Li, T., Song, L., Sun, Y., Sun, Q., Zhang, J., Zheng, Y., 2023.

</span>
<span class="ltx_bibblock">Spatio-temporal contrastive self-supervised learning for poi-level crowd flow inference.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2309.03239 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keerthi Chandra et al. [2020]</span>
<span class="ltx_bibblock">
Keerthi Chandra, D., Wang, P., Leopold, J., Fu, Y., 2020.

</span>
<span class="ltx_bibblock">Collective embedding with feature importance: A unified approach for spatiotemporal network embedding, in: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, Association for Computing Machinery, New York, NY, USA. pp. 615–624.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3340531.3412030" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3340531.3412030</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2022]</span>
<span class="ltx_bibblock">
Khan, I., Hou, F., Zakari, A., Tawiah, V., Ali, S.A., 2022.

</span>
<span class="ltx_bibblock">Energy use and urbanization as determinants of china’s environmental quality: prospects of the paris climate agreement.

</span>
<span class="ltx_bibblock">Journal of Environmental Planning and Management 65, 2363–2386.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanna et al. [2023]</span>
<span class="ltx_bibblock">
Khanna, S., Liu, P., Zhou, L., Meng, C., Rombach, R., Burke, M., Lobell, D., Ermon, S., 2023.

</span>
<span class="ltx_bibblock">Diffusionsat: A generative foundation model for satellite imagery.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.03606 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2014]</span>
<span class="ltx_bibblock">
Kim, S.H., Lu, Y., Constantinou, G., Shahabi, C., Wang, G., Zimmermann, R., 2014.

</span>
<span class="ltx_bibblock">Mediaq: mobile multimedia management system, in: Proceedings of the 5th ACM Multimedia Systems Conference, pp. 224–235.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kök et al. [2017]</span>
<span class="ltx_bibblock">
Kök, İ., Şimşek, M.U., Özdemir, S., 2017.

</span>
<span class="ltx_bibblock">A deep learning model for air quality prediction in smart cities, in: 2017 IEEE international conference on big data (big data), IEEE. pp. 1983–1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2012]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kruszyna et al. [2021]</span>
<span class="ltx_bibblock">
Kruszyna, M., Śleszyński, P., Rychlewski, J., 2021.

</span>
<span class="ltx_bibblock">Dependencies between demographic urbanization and the agglomeration road traffic volumes: Evidence from poland.

</span>
<span class="ltx_bibblock">Land 10, 47.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuckreja et al. [2023]</span>
<span class="ltx_bibblock">
Kuckreja, K., Danish, M.S., Naseer, M., Das, A., Khan, S., Khan, F.S., 2023.

</span>
<span class="ltx_bibblock">Geochat: Grounded large vision-language model for remote sensing.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.15826 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. [2022]</span>
<span class="ltx_bibblock">
Kwon, G., Cai, Z., Ravichandran, A., Bas, E., Bhotika, R., Soatto, S., 2022.

</span>
<span class="ltx_bibblock">Masked vision and language modeling for multi-modal representation learning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2208.02131 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lablack and Shen [2023]</span>
<span class="ltx_bibblock">
Lablack, M., Shen, Y., 2023.

</span>
<span class="ltx_bibblock">Spatio-temporal graph mixformer for traffic forecasting.

</span>
<span class="ltx_bibblock">Expert Systems with Applications 228, 120281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2023a]</span>
<span class="ltx_bibblock">
Lai, S., Xu, Z., Zhang, W., Liu, H., Xiong, H., 2023a.

</span>
<span class="ltx_bibblock">Large language models as traffic signal control agents: Capacity and opportunity.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.16044 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2023b]</span>
<span class="ltx_bibblock">
Lai, S., Zhang, W., Liu, H., 2023b.

</span>
<span class="ltx_bibblock">A preference-aware meta-optimization framework for personalized vehicle energy consumption estimation, in: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. pp. 4346–4356.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3580305.3599767" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3580305.3599767</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laud et al. [2023]</span>
<span class="ltx_bibblock">
Laud, T., Spokoyny, D., Corringham, T., Berg-Kirkpatrick, T., 2023.

</span>
<span class="ltx_bibblock">Climabench: A benchmark dataset for climate change text understanding in english.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2301.04253 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le-Khac et al. [2020]</span>
<span class="ltx_bibblock">
Le-Khac, P.H., Healy, G., Smeaton, A.F., 2020.

</span>
<span class="ltx_bibblock">Contrastive representation learning: A framework and review.

</span>
<span class="ltx_bibblock">Ieee Access 8, 193907–193934.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
Li, F., Feng, J., Yan, H., Jin, G., Yang, F., Sun, F., Jin, D., Li, Y., 2023a.

</span>
<span class="ltx_bibblock">Dynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution.

</span>
<span class="ltx_bibblock">ACM Trans. Knowl. Discov. Data 17.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3532611" title="">https://doi.org/10.1145/3532611</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3532611" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3532611</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022a]</span>
<span class="ltx_bibblock">
Li, H., Wang, X., Zhang, Z., Zhu, W., 2022a.

</span>
<span class="ltx_bibblock">Ood-gnn: Out-of-distribution generalized graph neural network.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Li, J., Cheong, T.S., Shen, J., Fu, D., 2019.

</span>
<span class="ltx_bibblock">Urbanization and rural–urban consumption disparity: Evidence from china.

</span>
<span class="ltx_bibblock">The Singapore Economic Review 64, 983–996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., Hoi, S., 2023b.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2301.12597 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021a]</span>
<span class="ltx_bibblock">
Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H., 2021a.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with momentum distillation.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 34, 9694–9705.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021b]</span>
<span class="ltx_bibblock">
Li, Q., Wen, Z., Wu, Z., Hu, S., Wang, N., Li, Y., Liu, X., He, B., 2021b.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: Vision, hype and reality for data privacy and protection.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022b]</span>
<span class="ltx_bibblock">
Li, T., Xin, S., Xi, Y., Tarkoma, S., Hui, P., Li, Y., 2022b.

</span>
<span class="ltx_bibblock">Predicting multi-level socioeconomic indicators from structural urban imagery, in: Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, pp. 3282–3291.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023c]</span>
<span class="ltx_bibblock">
Li, Y., Fan, Z., Yin, D., Jiang, R., Deng, J., Song, X., 2023c.

</span>
<span class="ltx_bibblock">HMGCL: Heterogeneous multigraph contrastive learning for LBSN friend recommendation.

</span>
<span class="ltx_bibblock">World Wide Web 26, 1625–1648.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/s11280-022-01092-5" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s11280-022-01092-5</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023d]</span>
<span class="ltx_bibblock">
Li, Y., Huang, W., Cong, G., Wang, H., Wang, Z., 2023d.

</span>
<span class="ltx_bibblock">Urban region representation learning with OpenStreetMap building footprints, in: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. pp. 1363–1373.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3580305.3599538" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3580305.3599538</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2017a]</span>
<span class="ltx_bibblock">
Li, Y., Su, H., Demiryurek, U., Zheng, B., He, T., Shahabi, C., 2017a.

</span>
<span class="ltx_bibblock">Pare: A system for personalized route guidance, in: Proceedings of the 26th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE. p. 637–646.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3038912.3052717" title="">https://doi.org/10.1145/3038912.3052717</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3038912.3052717" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3038912.3052717</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2017b]</span>
<span class="ltx_bibblock">
Li, Y., Yu, R., Shahabi, C., Liu, Y., 2017b.

</span>
<span class="ltx_bibblock">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1707.01926 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023e]</span>
<span class="ltx_bibblock">
Li, Y., Yu, Y., Zhang, Q., Liang, C., He, P., Chen, W., Zhao, T., 2023e.

</span>
<span class="ltx_bibblock">Losparse: Structured compression of large language models based on low-rank and sparse approximation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.11222 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
Li, Z., Xia, L., Tang, J., Xu, Y., Shi, L., Xia, L., Yin, D., Huang, C., 2024.

</span>
<span class="ltx_bibblock">Urbangpt: Spatio-temporal large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2403.00813 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2019a]</span>
<span class="ltx_bibblock">
Liang, L., Wang, Z., Li, J., 2019a.

</span>
<span class="ltx_bibblock">The effect of urbanization on environmental pollution in rapidly developing urban agglomerations.

</span>
<span class="ltx_bibblock">Journal of cleaner production 237, 117649.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2018]</span>
<span class="ltx_bibblock">
Liang, Y., Ke, S., Zhang, J., Yi, X., Zheng, Y., 2018.

</span>
<span class="ltx_bibblock">Geoman: Multi-level attention networks for geo-sensory time series prediction., in: IJCAI, pp. 3428–3434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2019b]</span>
<span class="ltx_bibblock">
Liang, Y., Ouyang, K., Jing, L., Ruan, S., Liu, Y., Zhang, J., Rosenblum, D.S., Zheng, Y., 2019b.

</span>
<span class="ltx_bibblock">Urbanfm: Inferring fine-grained urban flows, in: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp. 3132–3142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2021]</span>
<span class="ltx_bibblock">
Liang, Y., Ouyang, K., Sun, J., Wang, Y., Zhang, J., Zheng, Y., Rosenblum, D., Zimmermann, R., 2021.

</span>
<span class="ltx_bibblock">Fine-grained urban flow prediction, in: Proceedings of the Web Conference 2021, Association for Computing Machinery, New York, NY, USA. pp. 1833–1845.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3442381.3449792" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3442381.3449792</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2024a]</span>
<span class="ltx_bibblock">
Liang, Y., Wen, H., Nie, Y., Jiang, Y., Jin, M., Song, D., Pan, S., Wen, Q., 2024a.

</span>
<span class="ltx_bibblock">Foundation models for time series analysis: A tutorial and survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2403.14735 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2023]</span>
<span class="ltx_bibblock">
Liang, Y., Xia, Y., Ke, S., Wang, Y., Wen, Q., Zhang, J., Zheng, Y., Zimmermann, R., 2023.

</span>
<span class="ltx_bibblock">Airformer: Predicting nationwide air quality in china with transformers, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 14329–14337.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2024b]</span>
<span class="ltx_bibblock">
Liang, Y., Zhao, Z., Ding, F., Tang, Y., He, Z., 2024b.

</span>
<span class="ltx_bibblock">Time-dependent trip generation for bike sharing planning: A multi-task memory-augmented graph neural network.

</span>
<span class="ltx_bibblock">Information Fusion , 102294.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
Lin, X., Chen, Y., Li, G., Yu, Y., 2022.

</span>
<span class="ltx_bibblock">A causal inference look at unsupervised video anomaly detection, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1620–1629.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen [2023]</span>
<span class="ltx_bibblock">
Lin, Y.T., Chen, Y.N., 2023.

</span>
<span class="ltx_bibblock">Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.13711 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2019]</span>
<span class="ltx_bibblock">
Lin, Z., Feng, J., Lu, Z., Li, Y., Jin, D., 2019.

</span>
<span class="ltx_bibblock">Deepstn+: Context-aware spatial-temporal neural network for crowd flow prediction in metropolis, in: Proceedings of the AAAI conference on artificial intelligence, pp. 1020–1027.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023a]</span>
<span class="ltx_bibblock">
Liu, H., Dong, Z., Jiang, R., Deng, J., Deng, J., Chen, Q., Song, X., 2023a.

</span>
<span class="ltx_bibblock">Spatio-temporal adaptive embedding makes vanilla transformer sota for traffic forecasting, in: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 4125–4129.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023b]</span>
<span class="ltx_bibblock">
Liu, H., Guo, Q., Zhu, H., Fu, Y., Zhuang, F., Ma, X., Xiong, H., 2023b.

</span>
<span class="ltx_bibblock">Characterizing and forecasting urban vibrancy evolution: A multi-view graph mining perspective.

</span>
<span class="ltx_bibblock">ACM Transactions on Knowledge Discovery from Data 17, 68:1–68:24.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3568683" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3568683</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022a]</span>
<span class="ltx_bibblock">
Liu, H., Han, J., Fu, Y., Li, Y., Chen, K., Xiong, H., 2022a.

</span>
<span class="ltx_bibblock">Unified route representation learning for multi-modal transportation recommendation with spatiotemporal pre-training.

</span>
<span class="ltx_bibblock">The VLDB Journal — The International Journal on Very Large Data Bases 32, 325–342.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/s00778-022-00748-y" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s00778-022-00748-y</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023c]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Li, Y., Lee, Y.J., 2023c.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.03744 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019a]</span>
<span class="ltx_bibblock">
Liu, H., Li, T., Hu, R., Fu, Y., Gu, J., Xiong, H., 2019a.

</span>
<span class="ltx_bibblock">Joint representation learning for multi-modal transportation recommendation.

</span>
<span class="ltx_bibblock">Proceedings of the AAAI Conference on Artificial Intelligence 33, 1036–1043.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1609/aaai.v33i01.33011036" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1609/aaai.v33i01.33011036</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019b]</span>
<span class="ltx_bibblock">
Liu, H., Li, T., Hu, R., Fu, Y., Gu, J., Xiong, H., 2019b.

</span>
<span class="ltx_bibblock">Joint representation learning for multi-modal transportation recommendation, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1036–1043.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2020a]</span>
<span class="ltx_bibblock">
Liu, J., Li, T., Xie, P., Du, S., Teng, F., Yang, X., 2020a.

</span>
<span class="ltx_bibblock">Urban big data fusion based on deep learning: An overview.

</span>
<span class="ltx_bibblock">Information Fusion 53, 123–133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022b]</span>
<span class="ltx_bibblock">
Liu, J., Li, T., Yuan, Z., Huang, W., Xie, P., Huang, Q., 2022b.

</span>
<span class="ltx_bibblock">Symbolic aggregate approximation based data fusion model for dangerous driving behavior detection.

</span>
<span class="ltx_bibblock">Information Sciences 609, 626–643.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu [2023]</span>
<span class="ltx_bibblock">
Liu, P., 2023.

</span>
<span class="ltx_bibblock">A review on remote sensing data fusion with generative adversarial networks (gan).

</span>
<span class="ltx_bibblock">Authorea Preprints .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023d]</span>
<span class="ltx_bibblock">
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G., 2023d.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock">ACM Computing Surveys 55, 1–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023e]</span>
<span class="ltx_bibblock">
Liu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., Zimmermann, R., 2023e.

</span>
<span class="ltx_bibblock">Unitime: A language-empowered unified model for cross-domain time series forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.09751 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023f]</span>
<span class="ltx_bibblock">
Liu, X., Xia, Y., Liang, Y., Hu, J., Wang, Y., Bai, L., Huang, C., Liu, Z., Hooi, B., Zimmermann, R., 2023f.

</span>
<span class="ltx_bibblock">Largest: A benchmark dataset for large-scale traffic forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.08259 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J., Tang, J., 2021.

</span>
<span class="ltx_bibblock">Self-supervised learning: Generative or contrastive.

</span>
<span class="ltx_bibblock">IEEE transactions on knowledge and data engineering 35, 857–876.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022c]</span>
<span class="ltx_bibblock">
Liu, Y., Ao, X., Dong, L., Zhang, C., Wang, J., He, Q., 2022c.

</span>
<span class="ltx_bibblock">Spatiotemporal activity modeling via hierarchical cross-modal embedding.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 34, 462–474.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TKDE.2020.2983892" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TKDE.2020.2983892</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2020b]</span>
<span class="ltx_bibblock">
Liu, Y., Ma, X., Shu, L., Yang, Q., Zhang, Y., Huo, Z., Zhou, Z., 2020b.

</span>
<span class="ltx_bibblock">Internet of things for noise mapping in smart cities: state of the art and future directions.

</span>
<span class="ltx_bibblock">IEEE Network 34, 112–118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023g]</span>
<span class="ltx_bibblock">
Liu, Y., Zhang, X., Ding, J., Xi, Y., Li, Y., 2023g.

</span>
<span class="ltx_bibblock">Knowledge-infused contrastive learning for urban imagery-based socioeconomic prediction, in: Proceedings of the ACM Web Conference 2023, pp. 4150–4160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2016]</span>
<span class="ltx_bibblock">
Lu, Y., To, H., Alfarrarjeh, A., Kim, S.H., Yin, Y., Zimmermann, R., Shahabi, C., 2016.

</span>
<span class="ltx_bibblock">Geougv: User-generated mobile video dataset with fine granularity spatial metadata, in: Proceedings of the 7th international conference on multimedia systems, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2021a]</span>
<span class="ltx_bibblock">
Luo, H., Bao, Z., Cong, G., Culpepper, J.S., Khoa, N.L.D., 2021a.

</span>
<span class="ltx_bibblock">Let trajectories speak out the traffic bottlenecks.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.48550/arXiv.2107.12948" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2107.12948</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2107.12948" title="">arXiv:2107.12948</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2023]</span>
<span class="ltx_bibblock">
Luo, W., Liu, Q., Zhou, Y., Ran, Y., Liu, Z., Hou, W., Pei, S., Lai, S., 2023.

</span>
<span class="ltx_bibblock">Spatiotemporal variations of “triple-demic” outbreaks of respiratory infections in the united states in the post-covid-19 era.

</span>
<span class="ltx_bibblock">BMC Public Health 23.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s12889-023-17406-9" title="">https://doi.org/10.1186/s12889-023-17406-9</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1186/s12889-023-17406-9" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1186/s12889-023-17406-9</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2021b]</span>
<span class="ltx_bibblock">
Luo, Y., Ji, J., Sun, X., Cao, L., Wu, Y., Huang, F., Lin, C.W., Ji, R., 2021b.

</span>
<span class="ltx_bibblock">Dual-level collaborative transformer for image captioning, in: Proceedings of the AAAI conference on artificial intelligence, pp. 2286–2293.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. [2018]</span>
<span class="ltx_bibblock">
Lv, Z., Xu, J., Zheng, K., Yin, H., Zhao, P., Zhou, X., 2018.

</span>
<span class="ltx_bibblock">Lc-rnn: A deep learning model for traffic speed prediction., in: IJCAI, p. 27th.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023a]</span>
<span class="ltx_bibblock">
Ma, M., Xie, P., Teng, F., Wang, B., Ji, S., Zhang, J., Li, T., 2023a.

</span>
<span class="ltx_bibblock">HiSTGNN: Hierarchical spatio-temporal graph neural network for weather forecasting.

</span>
<span class="ltx_bibblock">Information Sciences 648, 119580.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023b]</span>
<span class="ltx_bibblock">
Ma, Z., Meng, C., Ren, H., Ruan, S., Bao, J., Wang, X., Li, T., Zheng, Y., 2023b.

</span>
<span class="ltx_bibblock">Sainf: Stay area inference of vehicles using surveillance camera records .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmud et al. [2021]</span>
<span class="ltx_bibblock">
Mahmud, M., Kaiser, M.S., McGinnity, T.M., Hussain, A., 2021.

</span>
<span class="ltx_bibblock">Deep learning in mining biological data.

</span>
<span class="ltx_bibblock">Cognitive computation 13, 1–33.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manvi et al. [2023]</span>
<span class="ltx_bibblock">
Manvi, R., Khanna, S., Mai, G., Burke, M., Lobell, D., Ermon, S., 2023.

</span>
<span class="ltx_bibblock">Geollm: Extracting geospatial knowledge from large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.06213 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. [2022]</span>
<span class="ltx_bibblock">
Mao, Z., Li, Z., Li, D., Bai, L., Zhao, R., 2022.

</span>
<span class="ltx_bibblock">Jointly contrastive representation learning on road network and trajectory, in: Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, pp. 1501–1510.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
MediaQ Project, .

</span>
<span class="ltx_bibblock">MediaQ Project.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://mediaq1.cloudapp.net/home/" title="">http://mediaq1.cloudapp.net/home/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller [2004]</span>
<span class="ltx_bibblock">
Miller, H.J., 2004.

</span>
<span class="ltx_bibblock">Tobler’s first law and spatial analysis.

</span>
<span class="ltx_bibblock">Annals of the association of American geographers 94, 284–289.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al. [2016]</span>
<span class="ltx_bibblock">
Miller, P., de Barros, A.G., Kattan, L., Wirasinghe, S., 2016.

</span>
<span class="ltx_bibblock">Public transportation and sustainability: A review.

</span>
<span class="ltx_bibblock">KSCE Journal of Civil Engineering 20, 1076–1083.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miyazawa et al. [2019]</span>
<span class="ltx_bibblock">
Miyazawa, S., Song, X., Xia, T., Shibasaki, R., Kaneda, H., 2019.

</span>
<span class="ltx_bibblock">Integrating gps trajectory and topics from twitter stream for human mobility estimation.

</span>
<span class="ltx_bibblock">Frontiers of Computer Science 13, 460–470.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mozaffari et al. [2020]</span>
<span class="ltx_bibblock">
Mozaffari, S., Al-Jarrah, O.Y., Dianati, M., Jennings, P., Mouzakitis, A., 2020.

</span>
<span class="ltx_bibblock">Deep learning-based vehicle behavior prediction for autonomous driving applications: A review.

</span>
<span class="ltx_bibblock">IEEE Transactions on Intelligent Transportation Systems 23, 33–47.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakada et al. [2023]</span>
<span class="ltx_bibblock">
Nakada, R., Gulluk, H.I., Deng, Z., Ji, W., Zou, J., Zhang, L., 2023.

</span>
<span class="ltx_bibblock">Understanding multimodal contrastive learning and incorporating unpaired data, in: International Conference on Artificial Intelligence and Statistics, PMLR. pp. 4348–4380.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam and Yang [2022]</span>
<span class="ltx_bibblock">
Nam, K.W., Yang, K., 2022.

</span>
<span class="ltx_bibblock">Realroi: Discovering real regions of interest from geotagged photos.

</span>
<span class="ltx_bibblock">IEEE Access 10, 83489–83497.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan and Cherukuri [2016]</span>
<span class="ltx_bibblock">
Narayanan, M., Cherukuri, A.K., 2016.

</span>
<span class="ltx_bibblock">A study and analysis of recommendation systems for location-based social network (LBSN) with big data.

</span>
<span class="ltx_bibblock">IIMB Management Review 28, 25–30.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.iimb.2016.01.001" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.iimb.2016.01.001</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Negash and Yang [2023]</span>
<span class="ltx_bibblock">
Negash, N.M., Yang, J., 2023.

</span>
<span class="ltx_bibblock">Driver behavior modeling towards autonomous vehicles: Comprehensive review.

</span>
<span class="ltx_bibblock">IEEE Access .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neuberg [2003]</span>
<span class="ltx_bibblock">
Neuberg, L.G., 2003.

</span>
<span class="ltx_bibblock">Causality: models, reasoning, and inference, by judea pearl, cambridge university press, 2000.

</span>
<span class="ltx_bibblock">Econometric Theory 19, 675–685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neupane et al. [2021]</span>
<span class="ltx_bibblock">
Neupane, B., Horanont, T., Aryal, J., 2021.

</span>
<span class="ltx_bibblock">Deep learning-based semantic segmentation of urban features in satellite images: A review and meta-analysis.

</span>
<span class="ltx_bibblock">Remote Sensing 13, 808.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nourmohammadi et al. [2021]</span>
<span class="ltx_bibblock">
Nourmohammadi, Z., Lilasathapornkit, T., Ashfaq, M., Gu, Z., Saberi, M., 2021.

</span>
<span class="ltx_bibblock">Mapping urban environmental performance with emerging data sources: A case of urban greenery and traffic noise in sydney, australia.

</span>
<span class="ltx_bibblock">Sustainability 13, 605.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oh et al. [2019]</span>
<span class="ltx_bibblock">
Oh, G., Leblanc, D., Peng, H., 2019.

</span>
<span class="ltx_bibblock">Vehicle Energy Dataset (VED), a large-scale dataset for vehicle energy consumption research.

</span>
<span class="ltx_bibblock">IEEE Transactions on Intelligent Transportation Systems 23, 3302–3312.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:146120975" title="">https://api.semanticscholar.org/CorpusID:146120975</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord et al. [2018]</span>
<span class="ltx_bibblock">
Oord, A.v.d., Li, Y., Vinyals, O., 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1807.03748 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI, 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" title="">arXiv:2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouallane et al. [2022]</span>
<span class="ltx_bibblock">
Ouallane, A.A., Bakali, A., Bahnasse, A., Broumi, S., Talea, M., 2022.

</span>
<span class="ltx_bibblock">Fusion of engineering insights and emerging trends: Intelligent urban traffic management system.

</span>
<span class="ltx_bibblock">Information Fusion 88, 218–248.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2020]</span>
<span class="ltx_bibblock">
Ouyang, K., Liang, Y., Liu, Y., Tong, Z., Ruan, S., Zheng, Y., Rosenblum, D.S., 2020.

</span>
<span class="ltx_bibblock">Fine-grained urban flow inference.

</span>
<span class="ltx_bibblock">IEEE transactions on knowledge and data engineering 34, 2755–2770.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozbayoglu et al. [2020]</span>
<span class="ltx_bibblock">
Ozbayoglu, A.M., Gudelek, M.U., Sezer, O.B., 2020.

</span>
<span class="ltx_bibblock">Deep learning for financial applications: A survey.

</span>
<span class="ltx_bibblock">Applied Soft Computing 93, 106384.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2023]</span>
<span class="ltx_bibblock">
Pan, L., Ren, Q., Li, J., 2023.

</span>
<span class="ltx_bibblock">Spatial-temporal graph contrastive learning for urban traffic flow forecasting.

</span>
<span class="ltx_bibblock">Authorea Preprints .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2019]</span>
<span class="ltx_bibblock">
Pan, Z., Liang, Y., Wang, W., Yu, Y., Zheng, Y., Zhang, J., 2019.

</span>
<span class="ltx_bibblock">Urban traffic prediction from spatio-temporal data using deep meta learning, in: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp. 1720–1730.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2020]</span>
<span class="ltx_bibblock">
Pan, Z., Zhang, W., Liang, Y., Zhang, W., Yu, Y., Zhang, J., Zheng, Y., 2020.

</span>
<span class="ltx_bibblock">Spatio-temporal meta learning for urban traffic prediction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 34, 1462–1476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park and Han [2018]</span>
<span class="ltx_bibblock">
Park, K.G., Han, S., 2018.

</span>
<span class="ltx_bibblock">How use of location-based social network (LBSN) services contributes to accumulation of social capital.

</span>
<span class="ltx_bibblock">Social Indicators Research 136, 379–396.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/s11205-016-1525-9" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s11205-016-1525-9</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pearl et al. [2016]</span>
<span class="ltx_bibblock">
Pearl, J., Glymour, M., Jewell, N.P., 2016.

</span>
<span class="ltx_bibblock">Causal inference in statistics: A primer. 2016.

</span>
<span class="ltx_bibblock">Internet resource .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2023]</span>
<span class="ltx_bibblock">
Peng, L., Zhang, Y., Shang, J., 2023.

</span>
<span class="ltx_bibblock">Generating efficient training data via llm-based attribute manipulation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.07099 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perera et al. [2020]</span>
<span class="ltx_bibblock">
Perera, A., Nik, V.M., Chen, D., Scartezzini, J.L., Hong, T., 2020.

</span>
<span class="ltx_bibblock">Quantifying the impacts of climate change and extreme climate events on energy systems.

</span>
<span class="ltx_bibblock">Nature Energy 5, 150–159.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piccialli et al. [2024]</span>
<span class="ltx_bibblock">
Piccialli, F., Canzaniello, M., Chiaro, D., Izzo, S., Qi, P., 2024.

</span>
<span class="ltx_bibblock">Graphite—generative reasoning and analysis for predictive handling in traffic efficiency.

</span>
<span class="ltx_bibblock">Information Fusion 106, 102265.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Psyllidis et al. [2022]</span>
<span class="ltx_bibblock">
Psyllidis, A., Gao, S., Hu, Y., Kim, E.K., McKenzie, G., Purves, R., Yuan, M., Andris, C., 2022.

</span>
<span class="ltx_bibblock">Points of interest (poi): a commentary on the state of the art, challenges, and prospects for the future.

</span>
<span class="ltx_bibblock">Computational Urban Science 2, 20.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiang et al. [2023]</span>
<span class="ltx_bibblock">
Qiang, Y., Wen, H., Wu, L., Mao, X., Wu, F., Wan, H., Hu, H., 2023.

</span>
<span class="ltx_bibblock">Modeling intra-and inter-community information for route and time prediction in last-mile delivery, in: 2023 IEEE 39th International Conference on Data Engineering (ICDE), IEEE. pp. 3106–3112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. [2022]</span>
<span class="ltx_bibblock">
Qu, H., Gong, Y., Chen, M., Zhang, J., Zheng, Y., Yin, Y., 2022.

</span>
<span class="ltx_bibblock">Forecasting fine-grained urban flows via spatio-temporal contrastive self-supervision.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision, in: International conference on machine learning, PMLR. pp. 8748–8763.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajeh et al. [2023]</span>
<span class="ltx_bibblock">
Rajeh, T.M., Li, T., Li, C., Javed, M.H., Luo, Z., Alhaek, F., 2023.

</span>
<span class="ltx_bibblock">Modeling multi-regional temporal correlation with gated recurrent unit and multiple linear regression for urban traffic flow prediction.

</span>
<span class="ltx_bibblock">Knowledge-Based Systems 262.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.knosys.2022.110237" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.knosys.2022.110237</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasenberg et al. [2020]</span>
<span class="ltx_bibblock">
Rasenberg, M., Özyürek, A., Dingemanse, M., 2020.

</span>
<span class="ltx_bibblock">Alignment in multimodal interaction: An integrative framework.

</span>
<span class="ltx_bibblock">Cognitive science 44, e12911.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed et al. [2023]</span>
<span class="ltx_bibblock">
Reed, C.J., Gupta, R., Li, S., Brockman, S., Funk, C., Clipp, B., Keutzer, K., Candido, S., Uyttendaele, M., Darrell, T., 2023.

</span>
<span class="ltx_bibblock">Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4088–4099.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Risal et al. [2020]</span>
<span class="ltx_bibblock">
Risal, A., Parajuli, P.B., Dash, P., Ouyang, Y., Linhoss, A., 2020.

</span>
<span class="ltx_bibblock">Sensitivity of hydrology and water quality to variation in land use and land cover data.

</span>
<span class="ltx_bibblock">Agricultural Water Management 241, 106366.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. [2023]</span>
<span class="ltx_bibblock">
Roberts, J., Lüddecke, T., Das, S., Han, K., Albanie, S., 2023.

</span>
<span class="ltx_bibblock">Gpt4geo: How a language model sees the world’s geography.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.00020 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogers et al. [2023]</span>
<span class="ltx_bibblock">
Rogers, G., Koper, P., Ruktanonchai, C., , Ruktanonchai, N., Utazi, E., Woods, D., Cunningham, A., Tatem, A.J., Steele, J., Lai, S., Sorichetta, A., 2023.

</span>
<span class="ltx_bibblock">Exploring the relationship between temporal fluctuations in satellite nightlight imagery and human mobility across africa.

</span>
<span class="ltx_bibblock">Remote Sensing 15.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/rs15174252" title="">https://doi.org/10.3390/rs15174252</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3390/rs15174252" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/rs15174252</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. [2022]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B., 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684–10695.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al. [2022]</span>
<span class="ltx_bibblock">
Ruan, S., Long, C., Ma, Z., Bao, J., He, T., Li, R., Chen, Y., Wu, S., Zheng, Y., 2022.

</span>
<span class="ltx_bibblock">Service time prediction for delivery tasks via spatial meta-learning, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3829–3837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schimanski et al. [2023]</span>
<span class="ltx_bibblock">
Schimanski, T., Bingler, J., Hyslop, C., Kraus, M., Leippold, M., 2023.

</span>
<span class="ltx_bibblock">Climatebert-netzero: Detecting and assessing net zero and reduction targets.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.08096 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrank et al. [2019]</span>
<span class="ltx_bibblock">
Schrank, D., Eisele, B., Lomax, T., et al., 2019.

</span>
<span class="ltx_bibblock">Urban mobility report 2019 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahid et al. [2019]</span>
<span class="ltx_bibblock">
Shahid, N., Rappon, T., Berta, W., 2019.

</span>
<span class="ltx_bibblock">Applications of artificial neural networks in health care organizational decision-making: A scoping review.

</span>
<span class="ltx_bibblock">PloS one 14, e0212356.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2023]</span>
<span class="ltx_bibblock">
Shao, R., Yang, C., Li, Q., Zhu, Q., Zhang, Y., Li, Y., Liu, Y., Tang, Y., Liu, D., Yang, S., et al., 2023.

</span>
<span class="ltx_bibblock">Allspark: a multimodal spatiotemporal general model.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2401.00546 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2018]</span>
<span class="ltx_bibblock">
Shen, D., Zhang, L., Cao, J., Wang, S., 2018.

</span>
<span class="ltx_bibblock">Forecasting citywide traffic congestion based on social media.

</span>
<span class="ltx_bibblock">Wireless Personal Communications 103, 1037–1057.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2023a]</span>
<span class="ltx_bibblock">
Shen, S., Yao, Z., Li, C., Darrell, T., Keutzer, K., He, Y., 2023a.

</span>
<span class="ltx_bibblock">Scaling vision-language models with sparse mixture of experts.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.07226 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2023b]</span>
<span class="ltx_bibblock">
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y., 2023b.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.17580 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2015]</span>
<span class="ltx_bibblock">
Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c., 2015.

</span>
<span class="ltx_bibblock">Convolutional lstm network: A machine learning approach for precipitation nowcasting.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2023]</span>
<span class="ltx_bibblock">
Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al., 2023.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock">Nature 620, 172–180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha et al. [2019]</span>
<span class="ltx_bibblock">
Sinha, R., Olsson, L.E., Frostell, B., 2019.

</span>
<span class="ltx_bibblock">Sustainable personal transport modes in a life cycle perspective—public or private?

</span>
<span class="ltx_bibblock">Sustainability 11, 7092.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2019a]</span>
<span class="ltx_bibblock">
Song, J., Tong, X., Wang, L., Zhao, C., Prishchepov, A.V., 2019a.

</span>
<span class="ltx_bibblock">Monitoring finer-scale population density in urban functional zones: A remote sensing data fusion approach.

</span>
<span class="ltx_bibblock">Landscape and urban planning 190, 103580.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2019b]</span>
<span class="ltx_bibblock">
Song, K., Yang, G., Wang, Q., Xu, C., Liu, J., Liu, W., Shi, C., Wang, Y., Zhang, G., Yu, X., et al., 2019b.

</span>
<span class="ltx_bibblock">Deep learning prediction of incoming rainfalls: An operational service for the city of beijing china, in: 2019 International Conference on Data Mining Workshops (ICDMW), IEEE. pp. 180–185.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2016]</span>
<span class="ltx_bibblock">
Song, X., Kanasugi, H., Shibasaki, R., 2016.

</span>
<span class="ltx_bibblock">Deeptransport: Prediction and simulation of human mobility and transportation mode at a citywide level, in: Proceedings of the twenty-fifth international joint conference on artificial intelligence, pp. 2618–2624.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2017]</span>
<span class="ltx_bibblock">
Song, X., Shibasaki, R., Yuan, N.J., Xie, X., Li, T., Adachi, R., 2017.

</span>
<span class="ltx_bibblock">DeepMob: Learning deep knowledge of human emergency behavior and mobility from big and heterogeneous data.

</span>
<span class="ltx_bibblock">ACM Transactions on Information Systems 35, 41:1–41:19.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3057280" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3057280</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steurer and Bayr [2020]</span>
<span class="ltx_bibblock">
Steurer, M., Bayr, C., 2020.

</span>
<span class="ltx_bibblock">Measuring urban sprawl using land use data.

</span>
<span class="ltx_bibblock">Land Use Policy 97, 104799.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiglic et al. [2020]</span>
<span class="ltx_bibblock">
Stiglic, G., Kocbek, P., Fijacko, N., Zitnik, M., Verbert, K., Cilar, L., 2020.

</span>
<span class="ltx_bibblock">Interpretability of machine learning-based prediction models in healthcare.

</span>
<span class="ltx_bibblock">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10, e1379.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stratton [2021]</span>
<span class="ltx_bibblock">
Stratton, S.J., 2021.

</span>
<span class="ltx_bibblock">Population research: convenience sampling strategies.

</span>
<span class="ltx_bibblock">Prehospital and disaster Medicine 36, 373–374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2020]</span>
<span class="ltx_bibblock">
Sun, J., Zhang, J., Li, Q., Yi, X., Liang, Y., Zheng, Y., 2020.

</span>
<span class="ltx_bibblock">Predicting citywide crowd flows in irregular regions using multi-view graph convolutional networks.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 34, 2348–2359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023]</span>
<span class="ltx_bibblock">
Sun, Y., Li, Y., Borozan, S., Wang, G., Qiu, J., Strbac, G., 2023.

</span>
<span class="ltx_bibblock">Battery swapping dispatch for self-sustained highway energy system based on spatiotemporal deep-learning traffic flow prediction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Industry Applications .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Svikhnushina and Pu [2023]</span>
<span class="ltx_bibblock">
Svikhnushina, E., Pu, P., 2023.

</span>
<span class="ltx_bibblock">Approximating online human evaluation of social chatbots with prompting, in: Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pp. 268–281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talpur and Zhang [2018]</span>
<span class="ltx_bibblock">
Talpur, A., Zhang, Y., 2018.

</span>
<span class="ltx_bibblock">A study of tourist sequential activity pattern through location based social network (LBSN), in: 2018 International Conference on Orange Technologies (ICOT), pp. 1–8.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ICOT.2018.8705895" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICOT.2018.8705895</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2019]</span>
<span class="ltx_bibblock">
Tang, B., Pan, Z., Yin, K., Khateeb, A., 2019.

</span>
<span class="ltx_bibblock">Recent advances of deep learning in bioinformatics and computational biology.

</span>
<span class="ltx_bibblock">Frontiers in genetics 10, 214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023a]</span>
<span class="ltx_bibblock">
Tang, J., Xia, L., Hu, J., Huang, C., 2023a.

</span>
<span class="ltx_bibblock">Spatio-temporal meta contrastive learning, in: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 2412–2421.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023b]</span>
<span class="ltx_bibblock">
Tang, J., Yang, Y., Wei, W., Shi, L., Su, L., Cheng, S., Yin, D., Huang, C., 2023b.

</span>
<span class="ltx_bibblock">Graphgpt: Graph instruction tuning for large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.13023 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2022]</span>
<span class="ltx_bibblock">
Tang, R., Zhao, J., Liu, Y., Huang, X., Zhang, Y., Zhou, D., Ding, A., Nielsen, C.P., Wang, H., 2022.

</span>
<span class="ltx_bibblock">Air quality and health co-benefits of china’s carbon dioxide emissions peaking before 2030.

</span>
<span class="ltx_bibblock">Nature communications 13, 1008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tedjopurnomo et al. [2021]</span>
<span class="ltx_bibblock">
Tedjopurnomo, D.A., Li, X., Bao, Z., Cong, G., Choudhury, F., Qin, A.K., 2021.

</span>
<span class="ltx_bibblock">Similar trajectory search with spatio-temporal deep representation learning.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology (TIST) 12, 1–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirunavukarasu et al. [2023]</span>
<span class="ltx_bibblock">
Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W., 2023.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock">Nature medicine 29, 1930–1940.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomee et al. [2015]</span>
<span class="ltx_bibblock">
Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K.H., Poland, D., Borth, D., Li, L.J., 2015.

</span>
<span class="ltx_bibblock">The new data and new challenges in multimedia research.

</span>
<span class="ltx_bibblock">arXiv 1.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1503.01817" title="">arXiv:1503.01817</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. [2022]</span>
<span class="ltx_bibblock">
Tian, B., Cao, Y., Zhang, Y., Xing, C., 2022.

</span>
<span class="ltx_bibblock">Debiasing nlu models via causal intervention and counterfactual reasoning, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 11376–11384.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2021]</span>
<span class="ltx_bibblock">
Tran, K., Sakla, W., Krim, H., 2021.

</span>
<span class="ltx_bibblock">Generative information fusion, in: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 3990–3994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trisos et al. [2020]</span>
<span class="ltx_bibblock">
Trisos, C.H., Merow, C., Pigot, A.L., 2020.

</span>
<span class="ltx_bibblock">The projected timing of abrupt ecological disruption from climate change.

</span>
<span class="ltx_bibblock">Nature 580, 496–501.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. [2016]</span>
<span class="ltx_bibblock">
Tu, W., Li, Q., Fang, Z., Shaw, S.l., Zhou, B., Chang, X., 2016.

</span>
<span class="ltx_bibblock">Optimizing the locations of electric taxi charging stations: A spatial–temporal demand coverage approach.

</span>
<span class="ltx_bibblock">Transportation Research Part C: Emerging Technologies 65, 172–189.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veitch et al. [2020]</span>
<span class="ltx_bibblock">
Veitch, V., Sridhar, D., Blei, D., 2020.

</span>
<span class="ltx_bibblock">Adapting text embeddings for causal inference, in: Conference on Uncertainty in Artificial Intelligence, PMLR. pp. 919–928.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. [2016]</span>
<span class="ltx_bibblock">
Vu, D.D., To, H., Shin, W.Y., Shahabi, C., 2016.

</span>
<span class="ltx_bibblock">GeoSocialBound: An efficient framework for estimating social POI boundaries using spatio–textual information, in: Proceedings of the Third International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data, ACM, San Francisco California. pp. 1–6.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/2948649.2948652" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/2948649.2948652</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallin et al. [2019]</span>
<span class="ltx_bibblock">
Wallin, M.T., Culpepper, W.J., Campbell, J.D., Nelson, L.M., Langer-Gould, A., Marrie, R.A., Cutter, G.R., Kaye, W.E., Wagner, L., Tremlett, H., et al., 2019.

</span>
<span class="ltx_bibblock">The prevalence of ms in the united states: a population-based estimate using health claims data.

</span>
<span class="ltx_bibblock">Neurology 92, e1029–e1040.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021a]</span>
<span class="ltx_bibblock">
Wang, B., Lin, Y., Guo, S., Wan, H., 2021a.

</span>
<span class="ltx_bibblock">GSNet: Learning spatial-temporal correlations from geographical and semantic aspects for traffic accident risk forecasting.

</span>
<span class="ltx_bibblock">Proceedings of the AAAI Conference on Artificial Intelligence 35, 4402–4409.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1609/aaai.v35i5.16566" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1609/aaai.v35i5.16566</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021b]</span>
<span class="ltx_bibblock">
Wang, D., Liu, K., Johnson, P., Sun, L., Du, B., Fu, Y., 2021b.

</span>
<span class="ltx_bibblock">Deep human-guided conditional variational generative modeling for automated urban planning, in: 2021 IEEE international conference on data mining (ICDM), IEEE. pp. 679–688.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024a]</span>
<span class="ltx_bibblock">
Wang, D., Peng, J., Tao, X., Duan, Y., 2024a.

</span>
<span class="ltx_bibblock">Boosting urban prediction tasks with domain-sharing knowledge via meta-learning.

</span>
<span class="ltx_bibblock">Information Fusion , 102324.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023a]</span>
<span class="ltx_bibblock">
Wang, D., Wu, L., Zhang, D., Zhou, J., Sun, L., Fu, Y., 2023a.

</span>
<span class="ltx_bibblock">Human-instructed deep hierarchical generative learning for automated urban planning, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4660–4667.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024b]</span>
<span class="ltx_bibblock">
Wang, H., Xiang, X., Fan, Y., Xue, J.H., 2024b.

</span>
<span class="ltx_bibblock">Customizing 360-degree panoramas through text-to-image diffusion models, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 4933–4943.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021c]</span>
<span class="ltx_bibblock">
Wang, H., Yu, Q., Liu, Y., Jin, D., Li, Y., 2021c.

</span>
<span class="ltx_bibblock">Spatio-temporal urban knowledge graph enabled mobility prediction.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.48550/arXiv.2111.03465" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2111.03465</span></a>, <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2111.03465" title="">arXiv:2111.03465</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021d]</span>
<span class="ltx_bibblock">
Wang, H., Yu, Q., Liu, Y., Jin, D., Li, Y., 2021d.

</span>
<span class="ltx_bibblock">Spatio-temporal urban knowledge graph enabled mobility prediction.

</span>
<span class="ltx_bibblock">Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies 5, 1–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2017a]</span>
<span class="ltx_bibblock">
Wang, J., Zhang, X., Guo, Z., Lu, H., 2017a.

</span>
<span class="ltx_bibblock">Developing an early-warning system for air quality prediction and assessment of cities in china.

</span>
<span class="ltx_bibblock">Expert systems with applications 84, 102–116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023b]</span>
<span class="ltx_bibblock">
Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al., 2023b.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.11432 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020a]</span>
<span class="ltx_bibblock">
Wang, Q., Lin, J., Zhou, K., Fan, J., Kwan, M.P., 2020a.

</span>
<span class="ltx_bibblock">Does urbanization lead to less residential energy consumption? a comparative study of 136 countries.

</span>
<span class="ltx_bibblock">Energy 202, 117765.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020b]</span>
<span class="ltx_bibblock">
Wang, S., Cao, J., Philip, S.Y., 2020b.

</span>
<span class="ltx_bibblock">Deep learning for spatio-temporal data mining: A survey.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 34, 3681–3700.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2016a]</span>
<span class="ltx_bibblock">
Wang, S., He, L., Stenneth, L., Philip, S.Y., Li, Z., Huang, Z., 2016a.

</span>
<span class="ltx_bibblock">Estimating urban traffic congestions with multi-sourced data, in: 2016 17th IEEE International conference on mobile data management (MDM), IEEE. pp. 82–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2015]</span>
<span class="ltx_bibblock">
Wang, S., He, L., Stenneth, L., Yu, P.S., Li, Z., 2015.

</span>
<span class="ltx_bibblock">Citywide traffic congestion estimation with social media, in: Proceedings of the 23rd SIGSPATIAL international conference on advances in geographic information systems, pp. 1–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2016b]</span>
<span class="ltx_bibblock">
Wang, S., Li, F., Stenneth, L., Yu, P.S., 2016b.

</span>
<span class="ltx_bibblock">Enhancing traffic congestion estimation with social media by coupled hidden markov model, in: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part II 16, Springer. pp. 247–264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020c]</span>
<span class="ltx_bibblock">
Wang, S., Li, Y., Zhang, J., Meng, Q., Meng, L., Gao, F., 2020c.

</span>
<span class="ltx_bibblock">Pm2. 5-gnn: A domain knowledge enhanced graph neural network for pm2. 5 forecasting, in: Proceedings of the 28th international conference on advances in geographic information systems, pp. 163–166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021e]</span>
<span class="ltx_bibblock">
Wang, S., Zhang, J., Li, J., Miao, H., Cao, J., 2021e.

</span>
<span class="ltx_bibblock">Traffic accident risk prediction via multi-view multi-task spatio-temporal networks.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023c]</span>
<span class="ltx_bibblock">
Wang, S., Zhang, J., Li, J., Miao, H., Cao, J., 2023c.

</span>
<span class="ltx_bibblock">Traffic accident risk prediction via multi-view multi-task spatio-temporal networks.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 35, 12323–12336.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TKDE.2021.3135621" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TKDE.2021.3135621</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2017b]</span>
<span class="ltx_bibblock">
Wang, S., Zhang, X., Cao, J., He, L., Stenneth, L., Yu, P.S., Li, Z., Huang, Z., 2017b.

</span>
<span class="ltx_bibblock">Computing urban traffic congestions by incorporating sparse gps probe data and social media data.

</span>
<span class="ltx_bibblock">ACM Transactions on Information Systems (TOIS) 35, 1–30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020d]</span>
<span class="ltx_bibblock">
Wang, T., Huang, J., Zhang, H., Sun, Q., 2020d.

</span>
<span class="ltx_bibblock">Visual commonsense r-cnn, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10760–10770.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023d]</span>
<span class="ltx_bibblock">
Wang, X., Fang, M., Zeng, Z., Cheng, T., 2023d.

</span>
<span class="ltx_bibblock">Where would i go next? large language models as human mobility predictors.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.15197 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Zhu [2024]</span>
<span class="ltx_bibblock">
Wang, Y., Zhu, D., 2024.

</span>
<span class="ltx_bibblock">A hypergraph-based hybrid graph convolutional network for intracity human activity intensity prediction and geographic relationship interpretation.

</span>
<span class="ltx_bibblock">Information Fusion 104, 102149.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Wang, Z., Peng, Z., Wang, S., Song, Q., 2022.

</span>
<span class="ltx_bibblock">Personalized long-distance fuel-efficient route recommendation through historical trajectories mining, in: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, ACM, Virtual Event AZ USA. pp. 1072–1080.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3488560.3498512" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3488560.3498512</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020e]</span>
<span class="ltx_bibblock">
Wang, Z., Wang, H., Qin, F., Han, Z., Miao, C., 2020e.

</span>
<span class="ltx_bibblock">Mapping an urban boundary based on multi-temporal sentinel-2 and POI data: A case study of zhengzhou city.

</span>
<span class="ltx_bibblock">Remote Sensing 12, 4103.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3390/rs12244103" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/rs12244103</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021f]</span>
<span class="ltx_bibblock">
Wang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., Cao, Y., 2021f.

</span>
<span class="ltx_bibblock">Simvlm: Simple visual language model pretraining with weak supervision.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2108.10904 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2020]</span>
<span class="ltx_bibblock">
Wei, X., Zhang, T., Li, Y., Zhang, Y., Wu, F., 2020.

</span>
<span class="ltx_bibblock">Multi-modality cross attention network for image and sentence matching, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10941–10950.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2023a]</span>
<span class="ltx_bibblock">
Wen, H., Lin, Y., Hu, Y., Wu, F., Xia, M., Zhang, X., Wu, L., Hu, H., Wan, H., 2023a.

</span>
<span class="ltx_bibblock">Modeling spatial–temporal constraints and spatial-transfer patterns for couriers’ package pick-up route prediction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Intelligent Transportation Systems .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2023b]</span>
<span class="ltx_bibblock">
Wen, H., Lin, Y., Wu, F., Wan, H., Sun, Z., Cai, T., Liu, H., Guo, S., Zheng, J., Song, C., et al., 2023b.

</span>
<span class="ltx_bibblock">Enough waiting for the couriers: Learning to estimate package pick-up arrival time from couriers’ spatial-temporal behaviors.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology 14, 1–22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2023c]</span>
<span class="ltx_bibblock">
Wen, H., Lin, Y., Xia, Y., Wan, H., Wen, Q., Zimmermann, R., Liang, Y., 2023c.

</span>
<span class="ltx_bibblock">Diffstg: Probabilistic spatio-temporal graph forecasting with denoising diffusion models, in: Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems, pp. 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2014]</span>
<span class="ltx_bibblock">
Wen, Y., Bein, D., Phoha, S., 2014.

</span>
<span class="ltx_bibblock">Dynamic clustering of multi-modal sensor networks in urban scenarios.

</span>
<span class="ltx_bibblock">Information Fusion 15, 130–140.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2015]</span>
<span class="ltx_bibblock">
Wu, F., Li, Z., Lee, W.C., Wang, H., Huang, Z., 2015.

</span>
<span class="ltx_bibblock">Semantic annotation of mobility data using social media, in: Proceedings of the 24th international conference on world wide web, pp. 1253–1263.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2017]</span>
<span class="ltx_bibblock">
Wu, G., Ding, Y., Li, Y., Bao, J., Zheng, Y., Luo, J., 2017.

</span>
<span class="ltx_bibblock">Mining spatio-temporal reachable regions over massive trajectory data, in: 2017 IEEE 33rd International Conference on Data Engineering (ICDE), pp. 1283–1294.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ICDE.2017.171" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICDE.2017.171</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2019a]</span>
<span class="ltx_bibblock">
Wu, H., Hao, Y., Weng, J.H., 2019a.

</span>
<span class="ltx_bibblock">How does energy consumption affect china’s urbanization? new evidence from dynamic threshold panel models.

</span>
<span class="ltx_bibblock">Energy policy 127, 24–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023a]</span>
<span class="ltx_bibblock">
Wu, L., Liu, J., Lou, J., Hu, H., Zheng, J., Wen, H., Song, C., He, S., 2023a.

</span>
<span class="ltx_bibblock">G2ptl: A pre-trained model for delivery address and its applications in logistics system.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.01559 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023b]</span>
<span class="ltx_bibblock">
Wu, L., Wen, H., Hu, H., Mao, X., Xia, Y., Shan, E., Zhen, J., Lou, J., Liang, Y., Yang, L., et al., 2023b.

</span>
<span class="ltx_bibblock">Lade: The first comprehensive last-mile delivery dataset from industry.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.10675 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023c]</span>
<span class="ltx_bibblock">
Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., Mann, G., 2023c.

</span>
<span class="ltx_bibblock">Bloomberggpt: A large language model for finance.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.17564 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2016]</span>
<span class="ltx_bibblock">
Wu, T., Xiang, L., Gong, J., 2016.

</span>
<span class="ltx_bibblock">Updating road networks by local renewal from gps trajectories.

</span>
<span class="ltx_bibblock">ISPRS International Journal of Geo-Information 5, 163.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2019b]</span>
<span class="ltx_bibblock">
Wu, Z., Pan, S., Long, G., Jiang, J., Zhang, C., 2019b.

</span>
<span class="ltx_bibblock">Graph wavenet for deep spatial-temporal graph modeling.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1906.00121 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al. [2022]</span>
<span class="ltx_bibblock">
Xi, Y., Li, T., Wang, H., Li, Y., Tarkoma, S., Hui, P., 2022.

</span>
<span class="ltx_bibblock">Beyond the first law of geography: Learning representations of satellite imagery by leveraging point-of-interests, in: Proceedings of the ACM Web Conference 2022, ACM, Virtual Event, Lyon France. pp. 3308–3316.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3485447.3512149" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3485447.3512149</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al. [2023]</span>
<span class="ltx_bibblock">
Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al., 2023.

</span>
<span class="ltx_bibblock">The rise and potential of large language model based agents: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2309.07864 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. [2021]</span>
<span class="ltx_bibblock">
Xia, L., Huang, C., Xu, Y., Dai, P., Bo, L., Zhang, X., Chen, T., 2021.

</span>
<span class="ltx_bibblock">Spatial-temporal sequential hypergraph network for crime prediction with dynamic multiplex relation learning, in: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, International Joint Conferences on Artificial Intelligence Organization.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. [2024]</span>
<span class="ltx_bibblock">
Xia, Y., Liang, Y., Wen, H., Liu, X., Wang, K., Zhou, Z., Zimmermann, R., 2024.

</span>
<span class="ltx_bibblock">Deciphering spatio-temporal graph forecasting: A causal lens and treatment.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2023]</span>
<span class="ltx_bibblock">
Xiao, C., Zhou, J., Huang, J., Zhu, H., Xu, T., Dou, D., Xiong, H., 2023.

</span>
<span class="ltx_bibblock">A contextual master-slave framework on urban region graph for urban village detection, in: 2023 IEEE 39th International Conference on Data Engineering (ICDE), IEEE Computer Society. pp. 736–748.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/ICDE55515.2023.00062" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICDE55515.2023.00062</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2023]</span>
<span class="ltx_bibblock">
Xie, J., Liang, Y., Liu, J., Xiao, Y., Wu, B., Ni, S., 2023.

</span>
<span class="ltx_bibblock">Quert: Continual pre-training of language model for query understanding in travel domain search.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.06707 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2020]</span>
<span class="ltx_bibblock">
Xie, P., Li, T., Liu, J., Du, S., Yang, X., Zhang, J., 2020.

</span>
<span class="ltx_bibblock">Urban flow prediction from spatiotemporal data using machine learning: A survey.

</span>
<span class="ltx_bibblock">Information Fusion 59, 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023a]</span>
<span class="ltx_bibblock">
Xu, D., Chen, Y., Cui, N., Li, J., 2023a.

</span>
<span class="ltx_bibblock">Towards multi-dimensional knowledge-aware approach for effective community detection in LBSN.

</span>
<span class="ltx_bibblock">World Wide Web 26, 1435–1458.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/s11280-022-01101-7" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s11280-022-01101-7</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023b]</span>
<span class="ltx_bibblock">
Xu, F., Zhang, J., Gao, C., Feng, J., Li, Y., 2023b.

</span>
<span class="ltx_bibblock">Urban generative intelligence (ugi): A foundational platform for agents in embodied city environment.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.11813 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023c]</span>
<span class="ltx_bibblock">
Xu, J., Wang, S., Ying, N., Xiao, X., Zhang, J., Jin, Z., Cheng, Y., Zhang, G., 2023c.

</span>
<span class="ltx_bibblock">Dynamic graph neural network with adaptive edge attributes for air quality prediction: A case study in china.

</span>
<span class="ltx_bibblock">Heliyon 9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023d]</span>
<span class="ltx_bibblock">
Xu, M., Yoon, S., Fuentes, A., Park, D.S., 2023d.

</span>
<span class="ltx_bibblock">A comprehensive survey of image augmentation techniques for deep learning.

</span>
<span class="ltx_bibblock">Pattern Recognition , 109347.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023e]</span>
<span class="ltx_bibblock">
Xu, X., Wei, Y., Wang, P., Luo, X., Zhou, F., Trajcevski, G., 2023e.

</span>
<span class="ltx_bibblock">Diffusion probabilistic modeling for fine-grained urban traffic flow inference with relaxed structural constraint, in: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023f]</span>
<span class="ltx_bibblock">
Xu, Z., Peng, J., Liu, Y., Qiu, S., Zhang, H., Dong, J., 2023f.

</span>
<span class="ltx_bibblock">Exploring the combined impact of ecosystem services and urbanization on sdgs realization.

</span>
<span class="ltx_bibblock">Applied Geography 153, 102907.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. [2022]</span>
<span class="ltx_bibblock">
Xue, H., Voutharoja, B.P., Salim, F.D., 2022.

</span>
<span class="ltx_bibblock">Leveraging language foundation models for human mobility forecasting, in: Proceedings of the 30th International Conference on Advances in Geographic Information Systems, pp. 1–9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2023]</span>
<span class="ltx_bibblock">
Yan, Y., Wen, H., Zhong, S., Chen, W., Chen, H., Wen, Q., Zimmermann, R., Liang, Y., 2023.

</span>
<span class="ltx_bibblock">When urban region profiling meets large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.18340 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2022]</span>
<span class="ltx_bibblock">
Yang, J., Ye, X., Wu, B., Gu, Y., Wang, Z., Xia, D., Huang, J., 2022.

</span>
<span class="ltx_bibblock">DuARE: Automatic road extraction with aerial images and trajectory data at baidu maps, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. pp. 4321–4331.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3534678.3539029" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3534678.3539029</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023a]</span>
<span class="ltx_bibblock">
Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., Yang, M.H., 2023a.

</span>
<span class="ltx_bibblock">Diffusion models: A comprehensive survey of methods and applications.

</span>
<span class="ltx_bibblock">ACM Computing Surveys 56, 1–39.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023b]</span>
<span class="ltx_bibblock">
Yang, M., Guo, T., Zhu, T., Tjuawinata, I., Zhao, J., Lam, K.Y., 2023b.

</span>
<span class="ltx_bibblock">Local differential privacy and its applications: A comprehensive survey.

</span>
<span class="ltx_bibblock">Computer Standards &amp; Interfaces , 103827.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023c]</span>
<span class="ltx_bibblock">
Yang, W., Ueda, A., Sugiura, K., 2023c.

</span>
<span class="ltx_bibblock">Multimodal encoder with gated cross-attention for text-vqa tasks, in: 29th Annual Conference of the Language Processing Society, pp. 1580–1585.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2022]</span>
<span class="ltx_bibblock">
Yao, D., Hu, H., Du, L., Cong, G., Han, S., Bi, J., 2022.

</span>
<span class="ltx_bibblock">Trajgat: A graph-based long-term dependency modeling approach for trajectory similarity computation, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2275–2285.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2018]</span>
<span class="ltx_bibblock">
Yao, H., Wu, F., Ke, J., Tang, X., Jia, Y., Lu, S., Gong, P., Ye, J., Li, Z., 2018.

</span>
<span class="ltx_bibblock">Deep multi-view spatial-temporal network for taxi demand prediction, in: Proceedings of the AAAI conference on artificial intelligence.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Jiang [2021]</span>
<span class="ltx_bibblock">
Yao, Y., Jiang, L., 2021.

</span>
<span class="ltx_bibblock">Urbanization forces driving rural urban income disparity: Evidence from metropolitan areas in china.

</span>
<span class="ltx_bibblock">Journal of Cleaner Production 312, 127748.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2023]</span>
<span class="ltx_bibblock">
Yao, Z., Wu, X., Li, C., Youn, S., He, Y., 2023.

</span>
<span class="ltx_bibblock">Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.08302 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023]</span>
<span class="ltx_bibblock">
Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al., 2023.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.14178 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2019]</span>
<span class="ltx_bibblock">
Ye, Y., Xie, H., Fang, J., Jiang, H., Wang, D., 2019.

</span>
<span class="ltx_bibblock">Daily accessed street greenery and housing price: Measuring economic performance of human-scale streetscapes via new urban data.

</span>
<span class="ltx_bibblock">Sustainability 11, 1741.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. [2020]</span>
<span class="ltx_bibblock">
Yi, X., Duan, Z., Li, R., Zhang, J., Li, T., Zheng, Y., 2020.

</span>
<span class="ltx_bibblock">Predicting fine-grained air quality based on deep neural networks.

</span>
<span class="ltx_bibblock">IEEE Transactions on Big Data 8, 1326–1339.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. [2018]</span>
<span class="ltx_bibblock">
Yi, X., Zhang, J., Wang, Z., Li, T., Zheng, Y., 2018.

</span>
<span class="ltx_bibblock">Deep distributed fusion network for air quality prediction, in: Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp. 965–973.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2023a]</span>
<span class="ltx_bibblock">
Yin, B., Xie, J., Qin, Y., Ding, Z., Feng, Z., Li, X., Lin, W., 2023a.

</span>
<span class="ltx_bibblock">Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm, in: Proceedings of the 17th ACM Conference on Recommender Systems, pp. 599–601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2023b]</span>
<span class="ltx_bibblock">
Yin, Y., Hu, W., Tran, A., Zhang, Y., Wang, G., Kruppa, H., Zimmermann, R., Ng, S.K., 2023b.

</span>
<span class="ltx_bibblock">Multimodal deep learning for robust road attribute detection.

</span>
<span class="ltx_bibblock">ACM Transactions on Spatial Algorithms and Systems .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2019]</span>
<span class="ltx_bibblock">
Yin, Y., Liu, Z., Zhang, Y., Wang, S., Shah, R.R., Zimmermann, R., 2019.

</span>
<span class="ltx_bibblock">Gps2vec: Towards generating worldwide gps embeddings, in: Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 416–419.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib339">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2020]</span>
<span class="ltx_bibblock">
Yin, Y., Varadarajan, J., Wang, G., Wang, X., Sahrawat, D., Zimmermann, R., Ng, S.K., 2020.

</span>
<span class="ltx_bibblock">A multi-task learning framework for road attribute updating via joint analysis of map data and GPS traces, in: Proceedings of The Web Conference 2020, Association for Computing Machinery, New York, NY, USA. pp. 2662–2668.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3366423.3380021" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3366423.3380021</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib340">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2021]</span>
<span class="ltx_bibblock">
Yin, Y., Zhang, Y., Liu, Z., Liang, Y., Wang, S., Shah, R.R., Zimmermann, R., 2021.

</span>
<span class="ltx_bibblock">Learning multi-context aware location representations from large-scale geotagged images, in: Proceedings of the 29th ACM International Conference on Multimedia, pp. 899–907.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib341">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yonekura et al. [2018]</span>
<span class="ltx_bibblock">
Yonekura, K., Hattori, H., Suzuki, T., 2018.

</span>
<span class="ltx_bibblock">Short-term local weather forecast using dense weather station by deep neural network, in: 2018 IEEE international conference on big data (big data), IEEE. pp. 1683–1690.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib342">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al. [2022]</span>
<span class="ltx_bibblock">
You, J., Muhammad, A.S., He, X., Xie, T., Wang, Z., Fan, X., Yu, Z., Chen, L., Wang, C., 2022.

</span>
<span class="ltx_bibblock">Panda: predicting road risks after natural disasters leveraging heterogeneous urban data.

</span>
<span class="ltx_bibblock">CCF Transactions on Pervasive Computing and Interaction 4, 393–407.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib343">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2017]</span>
<span class="ltx_bibblock">
Yu, B., Yin, H., Zhu, Z., 2017.

</span>
<span class="ltx_bibblock">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1709.04875 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib344">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022]</span>
<span class="ltx_bibblock">
Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y., 2022.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2205.01917 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib345">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2023]</span>
<span class="ltx_bibblock">
Yu, X., Chen, Z., Ling, Y., Dong, S., Liu, Z., Lu, Y., 2023.

</span>
<span class="ltx_bibblock">Temporal data meets llm–explainable financial time series forecasting.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.11025 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib346">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2021]</span>
<span class="ltx_bibblock">
Yu, X., Shi, S., Xu, L., 2021.

</span>
<span class="ltx_bibblock">A spatial–temporal graph attention network approach for air temperature forecasting.

</span>
<span class="ltx_bibblock">Applied Soft Computing 113, 107888.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib347">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2015]</span>
<span class="ltx_bibblock">
Yu, Z., Xu, H., Yang, Z., Guo, B., 2015.

</span>
<span class="ltx_bibblock">Personalized travel package with multi-point-of-interest recommendation based on crowdsourced user footprints.

</span>
<span class="ltx_bibblock">IEEE Transactions on Human-Machine Systems 46, 151–158.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib348">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan and Li [2021]</span>
<span class="ltx_bibblock">
Yuan, H., Li, G., 2021.

</span>
<span class="ltx_bibblock">A survey of traffic prediction: from spatio-temporal data to intelligent transportation.

</span>
<span class="ltx_bibblock">Data Science and Engineering 6, 63–85.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib349">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2021a]</span>
<span class="ltx_bibblock">
Yuan, H., Li, G., Bao, Z., Feng, L., 2021a.

</span>
<span class="ltx_bibblock">An effective joint prediction model for travel demands and traffic flows, in: 2021 IEEE 37th International Conference on Data Engineering (ICDE), IEEE. pp. 348–359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib350">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2011]</span>
<span class="ltx_bibblock">
Yuan, J., Zheng, Y., Xie, X., Sun, G., 2011.

</span>
<span class="ltx_bibblock">Driving with knowledge from the physical world, in: Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 316–324.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib351">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2010]</span>
<span class="ltx_bibblock">
Yuan, J., Zheng, Y., Zhang, C., Xie, W., Xie, X., Sun, G., Huang, Y., 2010.

</span>
<span class="ltx_bibblock">T-drive: driving directions based on taxi trajectories, in: Proceedings of the 18th SIGSPATIAL International conference on advances in geographic information systems, pp. 99–108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib352">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2017]</span>
<span class="ltx_bibblock">
Yuan, Q., Zhang, W., Zhang, C., Geng, X., Cong, G., Han, J., 2017.

</span>
<span class="ltx_bibblock">Pred: Periodic region detection for mobility modeling of social media users, in: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pp. 263–272.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib353">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2021b]</span>
<span class="ltx_bibblock">
Yuan, X., Shi, J., Gu, L., 2021b.

</span>
<span class="ltx_bibblock">A review of deep learning methods for semantic segmentation of remote sensing imagery.

</span>
<span class="ltx_bibblock">Expert Systems with Applications 169, 114417.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib354">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2022]</span>
<span class="ltx_bibblock">
Yuan, Y., Ding, J., Wang, H., Jin, D., Li, Y., 2022.

</span>
<span class="ltx_bibblock">Activity trajectory generation via modeling spatiotemporal dynamics, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4752–4762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib355">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2023]</span>
<span class="ltx_bibblock">
Yuan, Y., Wang, H., Ding, J., Jin, D., Li, Y., 2023.

</span>
<span class="ltx_bibblock">Learning to simulate daily activities via modeling dynamic human needs, in: Proceedings of the ACM Web Conference 2023, pp. 906–916.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib356">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuanshao et al. [2023]</span>
<span class="ltx_bibblock">
Yuanshao, Z., Ye, Y., Zhang, S., Zhao, X., James, J.Y., 2023.

</span>
<span class="ltx_bibblock">Difftraj: Generating gps trajectory with diffusion probabilistic model, in: Proceedings of the 37th Annual Conference on Neural Information Processing Systems.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib357">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021a]</span>
<span class="ltx_bibblock">
Zhang, C., Xie, Y., Bai, H., Yu, B., Li, W., Gao, Y., 2021a.

</span>
<span class="ltx_bibblock">A survey on federated learning.

</span>
<span class="ltx_bibblock">Knowledge-Based Systems 216, 106775.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib358">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Zhang, C., Zhang, Y., Shao, Q., Li, B., Lv, Y., Piao, X., Yin, B., 2023a.

</span>
<span class="ltx_bibblock">Chattraffic: Text-to-traffic generation via diffusion model.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.16203 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib359">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020a]</span>
<span class="ltx_bibblock">
Zhang, D., Zhang, H., Tang, J., Hua, X.S., Sun, Q., 2020a.

</span>
<span class="ltx_bibblock">Causal intervention for weakly-supervised semantic segmentation.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 33, 655–666.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib360">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Dai [2018]</span>
<span class="ltx_bibblock">
Zhang, H., Dai, L., 2018.

</span>
<span class="ltx_bibblock">Mobility prediction: A survey on state-of-the-art schemes and future applications.

</span>
<span class="ltx_bibblock">IEEE access 7, 802–822.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib361">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
Zhang, J., Xie, Y., Ding, W., Wang, Z., 2023b.

</span>
<span class="ltx_bibblock">Cross on cross attention: Deep fusion transformer for image captioning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Circuits and Systems for Video Technology .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib362">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2017]</span>
<span class="ltx_bibblock">
Zhang, J., Zheng, Y., Qi, D., 2017.

</span>
<span class="ltx_bibblock">Deep spatio-temporal residual networks for citywide crowd flows prediction, in: Proceedings of the AAAI conference on artificial intelligence.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib363">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2016]</span>
<span class="ltx_bibblock">
Zhang, J., Zheng, Y., Qi, D., Li, R., Yi, X., 2016.

</span>
<span class="ltx_bibblock">Dnn-based prediction model for spatio-temporal data, in: Proceedings of the 24th ACM SIGSPATIAL international conference on advances in geographic information systems, pp. 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib364">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
Zhang, J., Zheng, Y., Qi, D., Li, R., Yi, X., Li, T., 2018.

</span>
<span class="ltx_bibblock">Predicting citywide crowd flows using deep spatio-temporal residual networks.

</span>
<span class="ltx_bibblock">Artificial Intelligence 259, 147–166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib365">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022a]</span>
<span class="ltx_bibblock">
Zhang, L., Geng, X., Qin, Z., Wang, H., Wang, X., Zhang, Y., Liang, J., Wu, G., Song, X., Wang, Y., 2022a.

</span>
<span class="ltx_bibblock">Multi-modal graph interaction for multi-graph convolution network in urban spatiotemporal forecasting.

</span>
<span class="ltx_bibblock">Sustainability 14, 12397.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib366">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023c]</span>
<span class="ltx_bibblock">
Zhang, L., Long, C., Cong, G., 2023c.

</span>
<span class="ltx_bibblock">Region embedding with intra and inter-view contrastive learning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 35, 9031–9036.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TKDE.2022.3220874" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TKDE.2022.3220874</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib367">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023d]</span>
<span class="ltx_bibblock">
Zhang, L., Rao, A., Agrawala, M., 2023d.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836–3847.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib368">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021b]</span>
<span class="ltx_bibblock">
Zhang, M., Li, T., Li, Y., Hui, P., 2021b.

</span>
<span class="ltx_bibblock">Multi-view joint graph representation learning for urban region embedding, in: Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Yokohama, Japan. pp. 4431–4437.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib369">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023e]</span>
<span class="ltx_bibblock">
Zhang, P., Dong, X., Wang, B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Zhang, W., Yan, H., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., Wang, J., 2023e.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2309.15112" title="">arXiv:2309.15112</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib370">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022b]</span>
<span class="ltx_bibblock">
Zhang, Q., Han, Y., Li, V.O., Lam, J.C., 2022b.

</span>
<span class="ltx_bibblock">Deep-air: A hybrid cnn-lstm framework for fine-grained air pollution estimation and forecast in metropolitan cities.

</span>
<span class="ltx_bibblock">IEEE Access 10, 55818–55841.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib371">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023f]</span>
<span class="ltx_bibblock">
Zhang, R., Kennard, N.N., Smith, D., McFarland, D., McCallum, A., Keith, K., 2023f.

</span>
<span class="ltx_bibblock">Causal matching with text embeddings: A case study in estimating the causal effects of peer review policies, in: Findings of the Association for Computational Linguistics: ACL 2023, pp. 1284–1297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib372">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022c]</span>
<span class="ltx_bibblock">
Zhang, S., Fatih, D., Abdulqadir, F., Schwarz, T., Ma, X., 2022c.

</span>
<span class="ltx_bibblock">Extended vehicle energy dataset (eved): an enhanced large-scale dataset for deep learning on vehicle trip energy consumption.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2203.08630 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib373">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023g]</span>
<span class="ltx_bibblock">
Zhang, X., Gong, Y., Zhang, C., Wu, X., Guo, Y., Lu, W., Zhao, L., Dong, X., 2023g.

</span>
<span class="ltx_bibblock">Spatio-temporal fusion and contrastive learning for urban flow prediction.

</span>
<span class="ltx_bibblock">Knowledge-Based Systems 282, 111104.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib374">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022d]</span>
<span class="ltx_bibblock">
Zhang, X., Han, L., Wei, H., Tan, X., Zhou, W., Li, W., Qian, Y., 2022d.

</span>
<span class="ltx_bibblock">Linking urbanization and air quality together: A review and a perspective on the future sustainable urban development.

</span>
<span class="ltx_bibblock">Journal of Cleaner Production 346, 130988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib375">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021c]</span>
<span class="ltx_bibblock">
Zhang, X., Huang, C., Xu, Y., Xia, L., Dai, P., Bo, L., Zhang, J., Zheng, Y., 2021c.

</span>
<span class="ltx_bibblock">Traffic flow forecasting with spatial-temporal graph diffusion network, in: Proceedings of the AAAI conference on artificial intelligence, pp. 15008–15015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib376">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022e]</span>
<span class="ltx_bibblock">
Zhang, X., Liu, L., Lan, M., Song, G., Xiao, L., Chen, J., 2022e.

</span>
<span class="ltx_bibblock">Interpretable machine learning models for crime prediction.

</span>
<span class="ltx_bibblock">Computers, Environment and Urban Systems 94, 101789.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib377">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2019]</span>
<span class="ltx_bibblock">
Zhang, Y., Li, Q., Tu, W., Mai, K., Yao, Y., Chen, Y., 2019.

</span>
<span class="ltx_bibblock">Functional urban land use recognition integrating multi-source geospatial data and cross-correlations.

</span>
<span class="ltx_bibblock">Computers, Environment and Urban Systems 78, 101374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib378">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020b]</span>
<span class="ltx_bibblock">
Zhang, Y., Yin, Y., Zimmermann, R., Wang, G., Varadarajan, J., Ng, S.K., 2020b.

</span>
<span class="ltx_bibblock">An enhanced gan model for automatic satellite-to-map image conversion.

</span>
<span class="ltx_bibblock">IEEE Access 8, 176704–176716.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib379">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2021a]</span>
<span class="ltx_bibblock">
Zhao, B., Zhang, S., Xu, C., Sun, Y., Deng, C., 2021a.

</span>
<span class="ltx_bibblock">Deep fake geography? when geospatial data encounter artificial intelligence.

</span>
<span class="ltx_bibblock">Cartography and Geographic Information Science 48, 338–352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib380">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020]</span>
<span class="ltx_bibblock">
Zhao, K., Cong, G., Li, X., 2020.

</span>
<span class="ltx_bibblock">Pgeotopic: A distributed solution for mining geographical topic models.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 34, 881–893.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib381">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2016a]</span>
<span class="ltx_bibblock">
Zhao, K., Cong, G., Sun, A., 2016a.

</span>
<span class="ltx_bibblock">Annotating points of interest with geo-tagged tweets, in: Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, Association for Computing Machinery, New York, NY, USA. pp. 417–426.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/2983323.2983850" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/2983323.2983850</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib382">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2021b]</span>
<span class="ltx_bibblock">
Zhao, K., Liu, Y., Hao, S., Lu, S., Liu, H., Zhou, L., 2021b.

</span>
<span class="ltx_bibblock">Bounding boxes are all we need: street view image classification via context encoding of detected buildings.

</span>
<span class="ltx_bibblock">IEEE Transactions on Geoscience and Remote Sensing 60, 1–17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib383">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2016b]</span>
<span class="ltx_bibblock">
Zhao, K., Liu, Y., Yuan, Q., Chen, L., Chen, Z., Cong, G., 2016b.

</span>
<span class="ltx_bibblock">Towards personalized maps: Mining user preferences from geo-textual data.

</span>
<span class="ltx_bibblock">Proceedings of the VLDB Endowment 9, 1545–1548.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib384">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2022]</span>
<span class="ltx_bibblock">
Zhao, L., Gao, Y., Ye, J., Chen, F., Ye, Y., Lu, C.T., Ramakrishnan, N., 2022.

</span>
<span class="ltx_bibblock">Spatio-temporal event forecasting using incremental multi-source feature learning.

</span>
<span class="ltx_bibblock">ACM Transactions on Knowledge Discovery from Data 16, 1–28.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3464976" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3464976</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib385">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2015]</span>
<span class="ltx_bibblock">
Zhao, L., Sun, Q., Ye, J., Chen, F., Lu, C.T., Ramakrishnan, N., 2015.

</span>
<span class="ltx_bibblock">Multi-task learning for spatio-temporal event forecasting, in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. p. 1503–1512.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2783258.2783377" title="">https://doi.org/10.1145/2783258.2783377</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/2783258.2783377" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/2783258.2783377</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib386">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2017]</span>
<span class="ltx_bibblock">
Zhao, P., Xu, X., Liu, Y., Sheng, V.S., Zheng, K., Xiong, H., 2017.

</span>
<span class="ltx_bibblock">Photo2trip: Exploiting visual contents in geo-tagged photos for personalized tour recommendation, in: Proceedings of the 25th ACM international conference on Multimedia, pp. 916–924.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib387">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Yu [2017]</span>
<span class="ltx_bibblock">
Zhao, S., Yu, Y., 2017.

</span>
<span class="ltx_bibblock">Effect of short-term regional traffic restriction on urban submicron particulate pollution.

</span>
<span class="ltx_bibblock">Journal of Environmental Sciences 55, 86–99.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib388">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023]</span>
<span class="ltx_bibblock">
Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al., 2023.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.18223 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib389">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2022]</span>
<span class="ltx_bibblock">
Zheng, S., Trott, A., Srinivasa, S., Parkes, D.C., Socher, R., 2022.

</span>
<span class="ltx_bibblock">The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning.

</span>
<span class="ltx_bibblock">Science advances 8, eabk2607.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib390">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng [2015]</span>
<span class="ltx_bibblock">
Zheng, Y., 2015.

</span>
<span class="ltx_bibblock">Methodologies for cross-domain data fusion: An overview.

</span>
<span class="ltx_bibblock">IEEE transactions on big data 1, 16–34.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib391">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2014a]</span>
<span class="ltx_bibblock">
Zheng, Y., Capra, L., Wolfson, O., Yang, H., 2014a.

</span>
<span class="ltx_bibblock">Urban computing: concepts, methodologies, and applications.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology (TIST) 5, 1–55.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib392">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2014b]</span>
<span class="ltx_bibblock">
Zheng, Y., Chen, X., Jin, Q., Chen, Y., Qu, X., Liu, X., Chang, E., Ma, W.Y., Rui, Y., Sun, W., 2014b.

</span>
<span class="ltx_bibblock">A cloud-based knowledge discovery system for monitoring fine-grained air quality.

</span>
<span class="ltx_bibblock">MSR-TR-2014–40, Tech. Rep. .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib393">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2021]</span>
<span class="ltx_bibblock">
Zheng, Y., Gao, C., Li, X., He, X., Li, Y., Jin, D., 2021.

</span>
<span class="ltx_bibblock">Disentangling user interest and conformity for recommendation with causal embedding, in: Proceedings of the Web Conference 2021, pp. 2980–2991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib394">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2008]</span>
<span class="ltx_bibblock">
Zheng, Y., Li, Q., Chen, Y., Xie, X., Ma, W.Y., 2008.

</span>
<span class="ltx_bibblock">Understanding mobility based on gps data, in: Proceedings of the 10th international conference on Ubiquitous computing, pp. 312–321.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib395">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023a]</span>
<span class="ltx_bibblock">
Zheng, Y., Lin, Y., Zhao, L., Wu, T., Jin, D., Li, Y., 2023a.

</span>
<span class="ltx_bibblock">Spatial planning of urban communities via deep reinforcement learning.

</span>
<span class="ltx_bibblock">Nature Computational Science 3, 748–762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib396">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2013]</span>
<span class="ltx_bibblock">
Zheng, Y., Liu, F., Hsieh, H.P., 2013.

</span>
<span class="ltx_bibblock">U-air: When urban air quality inference meets big data, in: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1436–1444.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib397">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023b]</span>
<span class="ltx_bibblock">
Zheng, Y., Su, H., Ding, J., Jin, D., Li, Y., 2023b.

</span>
<span class="ltx_bibblock">Road planning for slums via deep reinforcement learning, in: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, p. 5695–5706.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib398">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2010]</span>
<span class="ltx_bibblock">
Zheng, Y., Xie, X., Ma, W.Y., et al., 2010.

</span>
<span class="ltx_bibblock">Geolife: A collaborative social networking service among user, location and trajectory. .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib399">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2015]</span>
<span class="ltx_bibblock">
Zheng, Y., Yi, X., Li, M., Li, R., Shan, Z., Chang, E., Li, T., 2015.

</span>
<span class="ltx_bibblock">Forecasting fine-grained air quality based on big data, in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. pp. 2267–2276.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/2783258.2788573" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/2783258.2788573</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib400">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2009]</span>
<span class="ltx_bibblock">
Zheng, Y., Zhang, L., Xie, X., Ma, W.Y., 2009.

</span>
<span class="ltx_bibblock">Mining interesting locations and travel sequences from gps trajectories, in: Proceedings of the 18th international conference on World wide web, pp. 791–800.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib401">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023c]</span>
<span class="ltx_bibblock">
Zheng, Y., Zhong, L., Wang, S., Yang, Y., Gu, W., Zhang, J., Wang, J., 2023c.

</span>
<span class="ltx_bibblock">Diffuflow: Robust fine-grained urban flow inference with denoising diffusion model, in: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 3505–3513.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib402">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023a]</span>
<span class="ltx_bibblock">
Zhou, T., Niu, P., Wang, X., Sun, L., Jin, R., 2023a.

</span>
<span class="ltx_bibblock">One fits all: Power general time series analysis by pretrained lm.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2302.11939 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib403">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023b]</span>
<span class="ltx_bibblock">
Zhou, Z., Huang, Q., Yang, K., Wang, K., Wang, X., Zhang, Y., Liang, Y., Wang, Y., 2023b.

</span>
<span class="ltx_bibblock">Maintaining the status quo: Capturing invariant relations for ood spatiotemporal learning .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib404">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024]</span>
<span class="ltx_bibblock">
Zhou, Z., Lin, Y., Jin, D., Li, Y., 2024.

</span>
<span class="ltx_bibblock">Large language model for participatory urban planning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.17161 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib405">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2020]</span>
<span class="ltx_bibblock">
Zhu, S., Wang, D., Liu, L., Wang, Y., Guo, D., 2020.

</span>
<span class="ltx_bibblock">Inferring region significance by using multi-source spatial data.

</span>
<span class="ltx_bibblock">Neural Computing and Applications 32, 6523–6531.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib406">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zohourianshahzadi and Kalita [2022]</span>
<span class="ltx_bibblock">
Zohourianshahzadi, Z., Kalita, J.K., 2022.

</span>
<span class="ltx_bibblock">Neural attention for image captioning: review of outstanding methods.

</span>
<span class="ltx_bibblock">Artificial Intelligence Review 55, 3833–3862.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib407">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zoph et al. [2020]</span>
<span class="ltx_bibblock">
Zoph, B., Cubuk, E.D., Ghiasi, G., Lin, T.Y., Shlens, J., Le, Q.V., 2020.

</span>
<span class="ltx_bibblock">Learning data augmentation strategies for object detection, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16, Springer. pp. 566–583.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib408">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. [2024]</span>
<span class="ltx_bibblock">
Zou, X., Huang, J., Hao, X., Yang, Y., Wen, H., Yan, Y., Huang, C., Liang, Y., 2024.

</span>
<span class="ltx_bibblock">Learning geospatial region embedding with heterogeneous graph.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2405.14135 .

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jun 16 10:13:07 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
