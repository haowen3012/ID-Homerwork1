<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN</title>
<!--Generated on Sun Oct  6 04:01:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Hand Gesture Recognition,  Real-Time,  Data-Level Fusion,  Multi-Stream Network,  Skeleton,  Human-Computer Interaction.
" lang="en" name="keywords"/>
<base href="/html/2406.15003v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S1" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S2" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S2.SS1" title="In II Related Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Skeleton Input Modality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S2.SS2" title="In II Related Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Multi-Stream Networks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S2.SS3" title="In II Related Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Data-Level Fusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S2.SS4" title="In II Related Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Real-Time Applications</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S3" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Dynamic Hand Gesture Recognition Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S3.SS1" title="In III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Temporal Information Condensation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S3.SS2" title="In III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Multi-Stream CNN Architecture</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S4" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Framework Implementation &amp; Training</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S4.SS1" title="In IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Benchmark Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S4.SS2" title="In IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Data-Level Fusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S4.SS3" title="In IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">e2eET Multi-Stream Network</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S4.SS4" title="In IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Model Training</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation &amp; Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS1" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Evaluation: CNR Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS2" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation: LMDHG Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS3" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Evaluation: FPHA Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS4" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Evaluation: SHREC2017 Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS5" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Evaluation: DHG1428 Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S5.SS6" title="In V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">Ablation Study: SBUKID Dataset</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S6" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Real-Time Application</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S7" title="In Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusions &amp; Future Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S7.SS1" title="In VII Conclusions &amp; Future Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Conclusions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#S7.SS2" title="In VII Conclusions &amp; Future Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Future Work</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Real-Time Hand Gesture Recognition:
<br class="ltx_break"/>Integrating Skeleton-Based Data Fusion and Multi-Stream CNN
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oluwaleke Yusuf, Maki Habib, Mohamed Moustafa
</span><span class="ltx_author_notes">Oluwaleke Yusuf was with The American University in Cairo (AUC), Egypt. He is now with The Norwegian University of Science and Technology (NTNU), Norway.Maki Habib is with The American University in Cairo (AUC), Egypt.Mohamed Moustafa was with The American University in Cairo (AUC), Egypt. He is now with Amazon, Seattle, USA.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Hand Gesture Recognition (HGR) enables intuitive human-computer interactions in various real-world contexts. However, existing frameworks often struggle to meet the real-time requirements essential for practical HGR applications. This study introduces a robust, skeleton-based framework for dynamic HGR that simplifies the recognition of dynamic hand gestures into a static image classification task, effectively reducing both hardware and computational demands.
Our framework utilizes a data-level fusion technique to encode 3D skeleton data from dynamic gestures into static RGB spatiotemporal images. It incorporates a specialized end-to-end Ensemble Tuner (e2eET) Multi-Stream CNN architecture that optimizes the semantic connections between data representations while minimizing computational needs.
Tested across five benchmark datasets—SHREC’17, DHG-14/28, FPHA, LMDHG, and CNR—the framework showed competitive performance with the state-of-the-art. Its capability to support real-time HGR applications was also demonstrated through deployment on standard consumer PC hardware, showcasing low latency and minimal resource usage in real-world settings.
The successful deployment of this framework underscores its potential to enhance real-time applications in fields such as virtual/augmented reality, ambient intelligence, and assistive technologies, providing a scalable and efficient solution for dynamic gesture recognition.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Hand Gesture Recognition, Real-Time, Data-Level Fusion, Multi-Stream Network, Skeleton, Human-Computer Interaction.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Hand Gesture Recognition (HGR) plays a vital role in perceptual computing by enabling computational devices to capture and understand human hand gestures using mathematical algorithms. HGR has the potential to facilitate advanced applications in domains involving human-machine interactions, human behaviour analysis, active and assisted living, virtual/augmented/mixed reality, as well as ambient intelligence. However, gesture recognition faces unique challenges due to the complex morphology of the human hand, which can adopt numerous poses and varies in physical characteristics such as size and colour among individuals. Furthermore, HGR applications often operate in challenging real-world environments characterized by occlusions, changing backgrounds, noisy inputs, and the need for real-time processing.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">An HGR framework must successfully navigate these morphological and environmental challenges to satisfy the requirements of developers and end-users in real-world applications. Such requirements encompass ease of use, computational demand, hardware needs, response time, and accuracy. Hand gestures are inherently dynamic, with poses and positions changing over time, introducing a temporal dimension for accurate recognition. Thus, a sequence of hand poses must be interpreted to understand the contextual meaning of a gesture. To resolve these challenges and meet performance requirements, various frameworks have been developed for dynamic gesture recognition, each utilizing different combinations of input modalities and network architectures.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Some HGR frameworks utilize “multi-stream networks” by combining multiple sub-networks with distinct input channels and fusing their outputs (features extracted or class scores) for the overall network’s gesture recognition output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>]</cite>.
On the other hand, “multimodal frameworks” combine multiple input modalities—including RGB, depth, skeleton, optical flow, and segmentation—to provide the network with more semantic information about the gestures. Said input modalities can be processed separately in the sub-networks of a multi-stream network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib12" title="">12</a>]</cite> or combined as a unified input to a “single-stream network” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Whether multi-stream or single-stream, HGR frameworks have employed various (combinations of) data-driven neural network architectures, such as Graph Convolutional Network (GCN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib6" title="">6</a>]</cite>, Attention Network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>]</cite>, and 1D/2D/3D Convolutional Neural Network (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib13" title="">13</a>]</cite>.
Also, the efficacy of CNNs at handling spatial information is often combined with that of Recurrent Neural Networks (RNN) at handling temporal information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib12" title="">12</a>]</cite> to deal with the spatiotemporal information contained in dynamic gestures. Some frameworks have also employed exotic approaches such as Hyperbolic Manifold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib22" title="">22</a>]</cite>, Neural Architecture Search (NAS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib11" title="">11</a>]</cite> and Spatial-Temporal Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">However, the ultimate aim of research in the HGR domain is the development of practical, real-world HGR applications for end-users. Towards that end, most developed frameworks often prioritize maximizing performance—in terms of gesture recognition accuracy—by utilizing multimodal and multi-stream approaches which require additional, specialized hardware and increased computational complexity, respectively. These frameworks also require significant training data and data augmentation for maximal performance. Said requirements result in impractical HGR applications with higher costs, reduced user-friendliness, and longer inference times. Furthermore, aside from a few frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib24" title="">24</a>]</cite>, most developed frameworks are not being integrated into applications to demonstrate their practical utility.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The optimal HGR framework and its application would aim to minimize computational costs, eliminate the need for additional hardware, and operate in real-time, all while maintaining gesture recognition accuracy comparable to state-of-the-art (SOTA) frameworks. In recent years, the exclusive use of the skeleton modality has become prevalent to reduce computational costs. In addition, the image classification domain has developed a suite of frameworks that achieve real-time performance on resource-constrained devices. Thus, a skeleton-based HGR framework that successfully transforms the dynamic hand gesture recognition task into an ordinary image classification task would come close to being optimal.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To this end, this paper presents a skeleton-based framework for dynamic hand gesture recognition that combines data-level fusion techniques with a specialized CNN architecture. This framework effectively encodes the 3D skeleton data from dynamic hand gestures into RGB images and employs an end-to-end Ensemble Tuner (e2eET) Multi-Stream CNN architecture for the subsequent image classification. Our framework underpins a robust, lightweight, and real-time HGR application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib25" title="">25</a>]</cite>, distinguishing our work in addressing the challenges of dynamic gesture recognition. The contributions of this paper include the following:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We enhance the transformation of 3D skeleton data into spatiotemporal 2D through a data-level fusion process that incorporates temporal information condensation, denoising, and sequence fitting.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop a streamlined, fully trainable multi-stream CNN architecture that strengthens semantic interpretation by integrating multiple data representations during image classification.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our framework significantly reduces hardware and computational demands for dynamic gesture recognition, while matching or exceeding state-of-the-art performance metrics across benchmark datasets.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We demonstrate the practical viability of our HGR framework by deploying a real-time application that operates efficiently on standard consumer PC hardware, ensuring accessibility and ease of use in real-world settings.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">This work builds upon previous efforts in developing a lightweight real-time HGR application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib25" title="">25</a>]</cite>. This paper specifically focuses on the underlying framework driving the application, situating it within related works in the HGR domain and providing detailed insights into its implementation and training.
Our framework was also extensively evaluated against SOTA frameworks using five publicly available benchmark datasets: 3D Hand Gesture Recognition Using a Depth and Skeleton Dataset (SHREC2017) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib26" title="">26</a>]</cite>, Dynamic Hand Gesture 14/28 Dataset (DHG1428) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib27" title="">27</a>]</cite>, First-Person Hand Action Benchmark (FPHA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib28" title="">28</a>]</cite>, Leap Motion Dynamic Hand Gesture Benchmark (LMDHG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib29" title="">29</a>]</cite>, and Consiglio Nazionale delle Ricerche Hand Gestures Dataset (CNR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Our framework achieved performance closely aligned with, and in some instances surpassing, the state-of-the-art on the aforementioned datasets, with classification accuracies ranging from -4.10% to +5.16% compared to the highest benchmarks. Furthermore, the low latency of the HGR application successfully demonstrated the viability of data-level fusion for practical, real-time HGR applications.
We conducted an exploratory study with the SBU Kinect Interaction Dataset (SBUKID) to assess the adaptability of our framework to Human Action Recognition (HAR). The preliminary findings from this study indicate promising potential for extending our framework to broader applications.</p>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">The rest of this paper is structured as follows:
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S2" title="II Related Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section II</span></a> reviews the relevant HGR literature on skeleton modality, multi-stream networks, data-level fusion, and real-time applications;
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3" title="III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section III</span></a> details the proposed HGR framework and its underlying components;
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S4" title="IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section IV</span></a> outlines the configurations in the implementation and training of the framework presented in this work:
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5" title="V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section V</span></a> discusses the framework’s evaluation on benchmark datasets and its performance relative to the SOTA;
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S6" title="VI Real-Time Application ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section VI</span></a> gives an overview of the real-time application; and
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S7" title="VII Conclusions &amp; Future Work ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section VII</span></a> concludes the paper with some future research directions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section provides an overview of relevant research in the gesture recognition domain, crucial for understanding the principles which drove our design choices during framework development. This section covers frameworks that utilize skeleton modality for gesture recognition and those that employ multi-stream network architectures. Furthermore, we explore frameworks that apply data-level fusion of spatiotemporal information and those that go further to develop real-time applications.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Skeleton Input Modality</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Historically, HGR frameworks primarily relied on RGB videos or depth maps. However, with the evolution of technology and the emergence of a deeper understanding of gesture complexities, recent frameworks have shifted towards using skeleton poses as an input modality. This modality, usually extracted from RGB or depth sources, provides a robust representation of hand positions and is effective at addressing issues like occlusions, complex backgrounds, and differences in individual hand morphologies. However, skeleton-based methods require offline or online preprocessing to extract skeleton data (pose estimation) from the primary sources and are vulnerable to inaccuracies during this estimation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The utility of the skeleton modality in reducing computational complexity without compromising accuracy has led to its widespread adoption by high-performing frameworks in recent years. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib16" title="">16</a>]</cite> showcased a Hierarchical Self-Attention Network using pure self-attention mechanisms to capture spatiotemporal features while achieving competitive results with reduced computational complexity. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib30" title="">30</a>]</cite> presented ResGCNeXt, a lightweight GCN that combines enhanced preprocessing, BottleGroup structure, and SENet-PartAttention to achieve high accuracy with fewer network parameters.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Some frameworks have attempted to further optimize the extraction of semantic information from skeleton modality. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib15" title="">15</a>]</cite> proposed a GCN method that learns graph topology and uses an orthogonal connectivity basis for node aggregation, enhancing skeleton-based HGR by mitigating overfitting with structured regularization. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib14" title="">14</a>]</cite> introduced an angle-based GCN that improves gesture recognition accuracy by adding novel edges and designing features based on angles and distances between joints. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib22" title="">22</a>]</cite> explored a hyperbolic manifold aware network that uses hyperbolic space to capture hierarchical features and Euclidean filters for spatiotemporal features and enhance performance without relying on dynamic graphs.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Regarding performance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib31" title="">31</a>]</cite> established a skeleton-based hand motion representation model utilizing pose features, a Temporal Convolutional Network, and a summarization module to achieve SOTA results in intra-domain scenarios and strong performance in cross-domain scenarios. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib21" title="">21</a>]</cite> introduced a BiLSTM-based model with a soft attention mechanism, which significantly mitigates the challenges of intra-class and inter-class variability in gesture classes. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>]</cite> also explored a Deep Convolutional LSTM model that efficiently captures spatiotemporal features from skeleton data for real-time HGR with high accuracy and fast inference.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Our proposed framework follows this trend, capitalizing on the robust encoding capabilities of the skeleton modality towards achieving SOTA performance with reduced computational complexity. Furthermore, it is important to note that skeleton-based HGR applications inherently mitigate ethical concerns about surveillance and privacy infringement due to the minimal user-identifiable information present in skeleton data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Multi-Stream Networks</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Many high-performing HGR frameworks adopt multi-stream network architectures integrating decoupled spatiotemporal streams, multiple temporal scales, or varied input modalities. This approach captures extensive contextual details from dynamic gesture sequences and provides the model with alternate ‘views’ that enhance its ability to distinguish between (similar) gestures.
Multi-stream networks combine information from different sub-networks at two critical stages: feature-level fusion and decision-level fusion. Feature-level fusion combines raw features extracted by the sub-networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib1" title="">1</a>]</cite>, while decision-level fusion merges the classification probabilities (scores) predicted by each sub-network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>]</cite>. These fusion techniques enable the overall network to outperform any individual sub-network, leveraging the strengths of each to achieve superior performance.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Working with multiple modalities, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>]</cite> developed a Multimodal Fusion Hierarchical Self-Attention Network that fuses features from multiple modalities (skeleton, RGB, depth, and optical flow), leading to improved accuracy with reduced computational complexity. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib9" title="">9</a>]</cite> established a single-stage continuous gesture recognition framework which includes unimodal and multimodal feature mapping models for effective feature mapping between various input modalities using Temporal Multi-Modal Fusion. While <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib11" title="">11</a>]</cite> proposed an NAS-based method for gesture recognition with RGB and depth modalities which uses 3D Central Difference Convolution and optimized multi-sampling-rate backbones with lateral connections between the various modalities.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">For a single (skeleton) modality, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib1" title="">1</a>]</cite> presented the Multi-Features and Multi-Stream Network for real-time gesture recognition using joint distance, angle, and position features extracted with a 1D CNN to reduce model parameters and ensure comprehensive feature extraction. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>]</cite> introduced a Multi-View Hierarchical Aggregation Network, using a novel 2D non-uniform spatial sampling strategy for optimal viewpoints and hierarchical attention architecture to fuse multi-view features. While <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib4" title="">4</a>]</cite> proposed a Multi-Branch Attention-Based Graph and General Deep Learning Model, combining graph-based spatial-temporal and temporal-spatial features with general deep learning features from three branches.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Data-Level Fusion</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The challenges associated with recognizing dynamic hand gestures arise from their temporal attributes. Since hand gestures are composed of a sequence of hand poses changing over time, a framework must be capable of understanding the semantic connections between these successive poses for accurate gesture recognition. This issue is exacerbated with multimodal and multi-stream networks which further require precise pixel-level correspondence across various modalities and semantic alignment between the sub-networks, respectively. Thus, the effectiveness of the framework is heavily dependent on the fusion of information from all modalities and sub-networks at the data, feature or decision (score) levels.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">HGR frameworks commonly utilize online fusion at the feature and decision levels which require complex network architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>]</cite>, substantial training data (and augmentation), and specialized loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib9" title="">9</a>]</cite> to achieve desired performance gains. Data-level fusion is an offline preprocessing technique where raw data from multiple sources or modalities are merged into a single representation before being processed by the network. While less commonly used, data-level fusion eliminates the aforementioned issues with other fusion methods. Temporal Information Condensation, a form of data-level fusion for dynamic gestures, involves summarizing and aggregating spatiotemporal data from gesture sequences into static images as a compact unified input representation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">For RGB modality (videos), <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib13" title="">13</a>]</cite> introduced Motion Fused Frames, a data-level fusion strategy that integrates temporal information into static images by appending optical flow frames as extra channels, leading to better representation of the spatiotemporal states of dynamic gestures. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib18" title="">18</a>]</cite> also presented the “star RGB” representation, a method for encoding temporal information by condensing dynamic gestures into single RGB images. As a consequence, after encoding temporal information, these frameworks can perform gesture classification using ImageNet-pretrained CNNs with minimal modification. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib18" title="">18</a>]</cite> further enhanced this approach by using an ensemble of two ResNet CNNs with feature-level fusion using a soft-attention mechanism.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> developed a skeleton-based framework whereby dynamic gestures are converted into RGB images, such that the variation of hand joint positions during a gesture is projected onto a plane and temporal information is represented with the colour intensity of the projected points. The resulting images serve as input to an unmodified, ImageNet-pretrained CNN for gesture classification. This framework, by utilizing the skeleton modality and redefining gesture recognition as image classification, was able to minimize computational costs and serve as the foundation for real-time applications. Overall, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> demonstrate the effectiveness of temporal information condensation in reducing the computational complexity of HGR frameworks and enhancing the real-time performance of HGR applications.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">While promising, the framework developed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> has some drawbacks. The temporal encoding results in noisy and visually similar images, while the effectiveness of the view orientations during image generation depends heavily on the initial capture of skeleton data. Furthermore, using an unmodified CNN architecture makes it difficult for the model to extract discriminative semantic information about gestures, leading to reduced performance when tested against challenging datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib28" title="">28</a>]</cite> and real-world scenarios. This framework can be improved by refining temporal information condensation, optimizing the network architecture to better handle the spatiotemporal images generated through data-level fusion, and eliminating the need for a Leap Motion sensor to capture skeleton data.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Real-Time Applications</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">As previously noted, few HGR frameworks have developed real-time applications to demonstrate their practical effectiveness in real-world scenarios. Beyond maximizing classification accuracy on benchmark datasets, gesture recognition frameworks must also minimize hardware requirements, computational costs, and inference latency. These performance considerations inevitably influence design decisions during framework development.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">To this end, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> demonstrated a real-time acquisition, visualization, and classification pipeline for gesture recognition. Their initial analysis of inference time validates their combined choice of data-level fusion, skeleton modality extracted from a Leap Motion sensor, and a regular ResNet-50 model for their framework. Conversely, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib24" title="">24</a>]</cite> used the lightweight MediaPipe pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib35" title="">35</a>]</cite> to extract hand skeleton data from RGB videos captured by a webcam, eliminating the need for an additional specialized sensor. Furthermore, switching from RGB to skeleton modality allowed these frameworks to leverage multi-stream networks while reducing network parameters and inference time.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">The real-time application developed from our proposed HGR framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib25" title="">25</a>]</cite> combines the strengths of the aforementioned frameworks and their applications. Our pipeline: <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.1">(1)</span> extracts skeleton data from RGB webcam videos using MediaPipe, <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.2">(2)</span> performs temporal information condensation to transform the gesture recognition into image classification, generating images from multiple view orientations, <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.3">(3)</span> utilizes an end-to-end trainable multi-stream CNN model gesture recognition, and <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.4">(4)</span> achieves real-time performance running on the CPU of a consumer PC.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Dynamic Hand Gesture Recognition Framework</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section elaborates on the essential elements of the proposed skeleton-based framework for dynamic HGR as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F1" title="Figure 1 ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> as follows:
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.SS1" title="III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">III-A</span></span></a> explains the method adopted for generating spatiotemporal RGB images from dynamic gesture sequences, and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.SS2" title="III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">III-B</span></span></a> details the specialized network architecture designed to maximize semantic information extracted from said images. Finally, both elements are combined into a robust skeleton-based HGR framework demonstrably capable of supporting real-time applications.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S3.F1.g1" src="extracted/5904591/proposed-framework-block-diagram-v6.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagrammatic Overview of the Proposed HGR Framework, Highlighting Key Components for Recognizing and Classifying Dynamic Hand Gestures.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Temporal Information Condensation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Temporal information condensation, a form of data-level fusion, serves as a preprocessing stage which can be conducted offline for framework evaluation or online for real-time applications. In either case, 3d skeleton data from the decoupled spatial and temporal channels of a dynamic gesture sequence are encoded into a static 2D spatiotemporal image with a square aspect ratio. This process involves condensing the temporal information present in the original gesture sequence while retaining pertinent semantic information. As in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite>, the problem of transforming a dynamic gesture sequence into a static spatiotemporal image can be defined as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">If <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denotes a dynamic gesture and <math alttext="S={C_{h}}_{h\text{=}1}^{N}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">S</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">=</mo><mmultiscripts id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2.2.2" xref="S3.SS1.p2.2.m2.1.1.3.2.2.2.cmml">C</mi><mi id="S3.SS1.p2.2.m2.1.1.3.2.2.3" xref="S3.SS1.p2.2.m2.1.1.3.2.2.3.cmml">h</mi><mrow id="S3.SS1.p2.2.m2.1.1.3a" xref="S3.SS1.p2.2.m2.1.1.3.cmml"></mrow><mrow id="S3.SS1.p2.2.m2.1.1.3.2.3" xref="S3.SS1.p2.2.m2.1.1.3.2.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.3.2.cmml">h</mi><mo id="S3.SS1.p2.2.m2.1.1.3.2.3.1" xref="S3.SS1.p2.2.m2.1.1.3.2.3.1.cmml">⁢</mo><mtext id="S3.SS1.p2.2.m2.1.1.3.2.3.3" xref="S3.SS1.p2.2.m2.1.1.3.2.3.3a.cmml">=</mtext><mo id="S3.SS1.p2.2.m2.1.1.3.2.3.1a" xref="S3.SS1.p2.2.m2.1.1.3.2.3.1.cmml">⁢</mo><mn id="S3.SS1.p2.2.m2.1.1.3.2.3.4" xref="S3.SS1.p2.2.m2.1.1.3.2.3.4.cmml">1</mn></mrow><mi id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">N</mi></mmultiscripts></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><eq id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></eq><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑆</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">subscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.2.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.2.2">𝐶</ci><ci id="S3.SS1.p2.2.m2.1.1.3.2.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.2.3">ℎ</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.3"><times id="S3.SS1.p2.2.m2.1.1.3.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.3.1"></times><ci id="S3.SS1.p2.2.m2.1.1.3.2.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.3.2">ℎ</ci><ci id="S3.SS1.p2.2.m2.1.1.3.2.3.3a.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2.3.3"><mtext id="S3.SS1.p2.2.m2.1.1.3.2.3.3.cmml" mathsize="70%" xref="S3.SS1.p2.2.m2.1.1.3.2.3.3">=</mtext></ci><cn id="S3.SS1.p2.2.m2.1.1.3.2.3.4.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3.2.3.4">1</cn></apply></apply><ci id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">S={C_{h}}_{h\text{=}1}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_S = italic_C start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> represents the set of gesture sequences across <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_N</annotation></semantics></math> classes, the temporal variation of <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">g</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑔</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is defined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.E1" title="1 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Equation 1</span></a> as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G_{i}=\{G_{i}^{\tau}\}_{\tau\text{=}1}^{T_{i}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">G</mi><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><msubsup id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">τ</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">τ</mi><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⁢</mo><mtext id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3a.cmml">=</mtext><mo id="S3.E1.m1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⁢</mo><mn id="S3.E1.m1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.3.4.cmml">1</mn></mrow><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">T</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">i</mi></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝐺</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1">subscript</csymbol><set id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">𝐺</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝜏</ci></apply></set><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝜏</ci><ci id="S3.E1.m1.1.1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.3">=</mtext></ci><cn id="S3.E1.m1.1.1.1.1.3.4.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.4">1</cn></apply></apply><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝑇</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">G_{i}=\{G_{i}^{\tau}\}_{\tau\text{=}1}^{T_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_τ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_τ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.8">For a temporal window of size <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\tau\in[1,T_{i}]" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.2"><semantics id="S3.SS1.p4.2.m2.2a"><mrow id="S3.SS1.p4.2.m2.2.2" xref="S3.SS1.p4.2.m2.2.2.cmml"><mi id="S3.SS1.p4.2.m2.2.2.3" xref="S3.SS1.p4.2.m2.2.2.3.cmml">τ</mi><mo id="S3.SS1.p4.2.m2.2.2.2" xref="S3.SS1.p4.2.m2.2.2.2.cmml">∈</mo><mrow id="S3.SS1.p4.2.m2.2.2.1.1" xref="S3.SS1.p4.2.m2.2.2.1.2.cmml"><mo id="S3.SS1.p4.2.m2.2.2.1.1.2" stretchy="false" xref="S3.SS1.p4.2.m2.2.2.1.2.cmml">[</mo><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.p4.2.m2.2.2.1.1.3" xref="S3.SS1.p4.2.m2.2.2.1.2.cmml">,</mo><msub id="S3.SS1.p4.2.m2.2.2.1.1.1" xref="S3.SS1.p4.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.2" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.cmml">T</mi><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.3" xref="S3.SS1.p4.2.m2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p4.2.m2.2.2.1.1.4" stretchy="false" xref="S3.SS1.p4.2.m2.2.2.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.2b"><apply id="S3.SS1.p4.2.m2.2.2.cmml" xref="S3.SS1.p4.2.m2.2.2"><in id="S3.SS1.p4.2.m2.2.2.2.cmml" xref="S3.SS1.p4.2.m2.2.2.2"></in><ci id="S3.SS1.p4.2.m2.2.2.3.cmml" xref="S3.SS1.p4.2.m2.2.2.3">𝜏</ci><interval closure="closed" id="S3.SS1.p4.2.m2.2.2.1.2.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1"><cn id="S3.SS1.p4.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p4.2.m2.1.1">1</cn><apply id="S3.SS1.p4.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2">𝑇</ci><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.2c">\tau\in[1,T_{i}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.2d">italic_τ ∈ [ 1 , italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math> specifies a specific timestep <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_τ</annotation></semantics></math>, and <math alttext="G_{i}^{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.1"><semantics id="S3.SS1.p4.4.m4.1a"><msubsup id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2.2" xref="S3.SS1.p4.4.m4.1.1.2.2.cmml">G</mi><mi id="S3.SS1.p4.4.m4.1.1.2.3" xref="S3.SS1.p4.4.m4.1.1.2.3.cmml">i</mi><mi id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">τ</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">superscript</csymbol><apply id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.2.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2.2">𝐺</ci><ci id="S3.SS1.p4.4.m4.1.1.2.3.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">G_{i}^{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_τ end_POSTSUPERSCRIPT</annotation></semantics></math> indicates the frame of <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5.1"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">g</mi><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝑔</ci><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m5.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> at that timestep <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6.1"><semantics id="S3.SS1.p4.6.m6.1a"><mi id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><ci id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.6.m6.1d">italic_τ</annotation></semantics></math>. Thus, the classification of dynamic hand gestures involves assigning the correct class <math alttext="C_{h}" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7.1"><semantics id="S3.SS1.p4.7.m7.1a"><msub id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml"><mi id="S3.SS1.p4.7.m7.1.1.2" xref="S3.SS1.p4.7.m7.1.1.2.cmml">C</mi><mi id="S3.SS1.p4.7.m7.1.1.3" xref="S3.SS1.p4.7.m7.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><apply id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.2.cmml" xref="S3.SS1.p4.7.m7.1.1.2">𝐶</ci><ci id="S3.SS1.p4.7.m7.1.1.3.cmml" xref="S3.SS1.p4.7.m7.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">C_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.7.m7.1d">italic_C start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> to the sequence <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m8.1"><semantics id="S3.SS1.p4.8.m8.1a"><msub id="S3.SS1.p4.8.m8.1.1" xref="S3.SS1.p4.8.m8.1.1.cmml"><mi id="S3.SS1.p4.8.m8.1.1.2" xref="S3.SS1.p4.8.m8.1.1.2.cmml">g</mi><mi id="S3.SS1.p4.8.m8.1.1.3" xref="S3.SS1.p4.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.1b"><apply id="S3.SS1.p4.8.m8.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.1.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p4.8.m8.1.1.2.cmml" xref="S3.SS1.p4.8.m8.1.1.2">𝑔</ci><ci id="S3.SS1.p4.8.m8.1.1.3.cmml" xref="S3.SS1.p4.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.8.m8.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.10">The gesture sequences in the set <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">italic_S</annotation></semantics></math> have varying temporal windows depending on the gesture’s class and its execution by individuals. Each gesture sequence <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><msub id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">𝑔</ci><ci id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_S</annotation></semantics></math> is resampled to a uniform temporal window <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4.1"><semantics id="S3.SS1.p5.4.m4.1a"><mi id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><ci id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.4.m4.1d">italic_T</annotation></semantics></math>, set to be larger than any existing window <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS1.p5.5.m5.1"><semantics id="S3.SS1.p5.5.m5.1a"><msub id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml"><mi id="S3.SS1.p5.5.m5.1.1.2" xref="S3.SS1.p5.5.m5.1.1.2.cmml">T</mi><mi id="S3.SS1.p5.5.m5.1.1.3" xref="S3.SS1.p5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><apply id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.5.m5.1.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p5.5.m5.1.1.2.cmml" xref="S3.SS1.p5.5.m5.1.1.2">𝑇</ci><ci id="S3.SS1.p5.5.m5.1.1.3.cmml" xref="S3.SS1.p5.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.5.m5.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p5.6.m6.1"><semantics id="S3.SS1.p5.6.m6.1a"><mi id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><ci id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.6.m6.1d">italic_S</annotation></semantics></math>. This resampling has a denoising effect which smoothens out inaccuracies in individual frames <math alttext="G_{i}^{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p5.7.m7.1"><semantics id="S3.SS1.p5.7.m7.1a"><msubsup id="S3.SS1.p5.7.m7.1.1" xref="S3.SS1.p5.7.m7.1.1.cmml"><mi id="S3.SS1.p5.7.m7.1.1.2.2" xref="S3.SS1.p5.7.m7.1.1.2.2.cmml">G</mi><mi id="S3.SS1.p5.7.m7.1.1.2.3" xref="S3.SS1.p5.7.m7.1.1.2.3.cmml">i</mi><mi id="S3.SS1.p5.7.m7.1.1.3" xref="S3.SS1.p5.7.m7.1.1.3.cmml">τ</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.7.m7.1b"><apply id="S3.SS1.p5.7.m7.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.7.m7.1.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1">superscript</csymbol><apply id="S3.SS1.p5.7.m7.1.1.2.cmml" xref="S3.SS1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.7.m7.1.1.2.1.cmml" xref="S3.SS1.p5.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p5.7.m7.1.1.2.2.cmml" xref="S3.SS1.p5.7.m7.1.1.2.2">𝐺</ci><ci id="S3.SS1.p5.7.m7.1.1.2.3.cmml" xref="S3.SS1.p5.7.m7.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.p5.7.m7.1.1.3.cmml" xref="S3.SS1.p5.7.m7.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.7.m7.1c">G_{i}^{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.7.m7.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_τ end_POSTSUPERSCRIPT</annotation></semantics></math> resulting from the pose estimation and minimizes minor variances in motion paths and sequence durations during gesture execution by individuals. Overall, denoising highlights the similarities between gesture sequences <math alttext="g_{i}\in C_{h}" class="ltx_Math" display="inline" id="S3.SS1.p5.8.m8.1"><semantics id="S3.SS1.p5.8.m8.1a"><mrow id="S3.SS1.p5.8.m8.1.1" xref="S3.SS1.p5.8.m8.1.1.cmml"><msub id="S3.SS1.p5.8.m8.1.1.2" xref="S3.SS1.p5.8.m8.1.1.2.cmml"><mi id="S3.SS1.p5.8.m8.1.1.2.2" xref="S3.SS1.p5.8.m8.1.1.2.2.cmml">g</mi><mi id="S3.SS1.p5.8.m8.1.1.2.3" xref="S3.SS1.p5.8.m8.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p5.8.m8.1.1.1" xref="S3.SS1.p5.8.m8.1.1.1.cmml">∈</mo><msub id="S3.SS1.p5.8.m8.1.1.3" xref="S3.SS1.p5.8.m8.1.1.3.cmml"><mi id="S3.SS1.p5.8.m8.1.1.3.2" xref="S3.SS1.p5.8.m8.1.1.3.2.cmml">C</mi><mi id="S3.SS1.p5.8.m8.1.1.3.3" xref="S3.SS1.p5.8.m8.1.1.3.3.cmml">h</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.8.m8.1b"><apply id="S3.SS1.p5.8.m8.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1"><in id="S3.SS1.p5.8.m8.1.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1.1"></in><apply id="S3.SS1.p5.8.m8.1.1.2.cmml" xref="S3.SS1.p5.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.1.1.2.1.cmml" xref="S3.SS1.p5.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p5.8.m8.1.1.2.2.cmml" xref="S3.SS1.p5.8.m8.1.1.2.2">𝑔</ci><ci id="S3.SS1.p5.8.m8.1.1.2.3.cmml" xref="S3.SS1.p5.8.m8.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p5.8.m8.1.1.3.cmml" xref="S3.SS1.p5.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.1.1.3.1.cmml" xref="S3.SS1.p5.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.p5.8.m8.1.1.3.2.cmml" xref="S3.SS1.p5.8.m8.1.1.3.2">𝐶</ci><ci id="S3.SS1.p5.8.m8.1.1.3.3.cmml" xref="S3.SS1.p5.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.8.m8.1c">g_{i}\in C_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.8.m8.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_C start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p5.9.m9.1"><semantics id="S3.SS1.p5.9.m9.1a"><mi id="S3.SS1.p5.9.m9.1.1" xref="S3.SS1.p5.9.m9.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.9.m9.1b"><ci id="S3.SS1.p5.9.m9.1.1.cmml" xref="S3.SS1.p5.9.m9.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.9.m9.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.9.m9.1d">italic_S</annotation></semantics></math>, thereby improving the downstream gesture classification accuracy. Each resampled gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p5.10.m10.1"><semantics id="S3.SS1.p5.10.m10.1a"><msub id="S3.SS1.p5.10.m10.1.1" xref="S3.SS1.p5.10.m10.1.1.cmml"><mi id="S3.SS1.p5.10.m10.1.1.2" xref="S3.SS1.p5.10.m10.1.1.2.cmml">g</mi><mi id="S3.SS1.p5.10.m10.1.1.3" xref="S3.SS1.p5.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.10.m10.1b"><apply id="S3.SS1.p5.10.m10.1.1.cmml" xref="S3.SS1.p5.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.10.m10.1.1.1.cmml" xref="S3.SS1.p5.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p5.10.m10.1.1.2.cmml" xref="S3.SS1.p5.10.m10.1.1.2">𝑔</ci><ci id="S3.SS1.p5.10.m10.1.1.3.cmml" xref="S3.SS1.p5.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.10.m10.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.10.m10.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is subsequently decoupled into its spatial and temporal channel as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The <span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">spatial</span> channel encodes changes in hand pose across each frame <math alttext="G_{i}^{\tau}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><msubsup id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.1.2.2.cmml">G</mi><mi id="S3.I1.i1.p1.1.m1.1.1.2.3" xref="S3.I1.i1.p1.1.m1.1.1.2.3.cmml">i</mi><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">τ</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">superscript</csymbol><apply id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.2">𝐺</ci><ci id="S3.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">G_{i}^{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_τ end_POSTSUPERSCRIPT</annotation></semantics></math> throughout the gesture sequence as a 3D visualization of the hand’s skeleton. A distinctive palette of colours is used during visualization to create a strong contrast between the fingers and the background.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.4">The <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.4.1">temporal</span> channel encodes hand movement as a 3D visualization of “temporal trails” of the five fingertips, from the start <math alttext="G_{i}^{1}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><msubsup id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2.2" xref="S3.I1.i2.p1.1.m1.1.1.2.2.cmml">G</mi><mi id="S3.I1.i2.p1.1.m1.1.1.2.3" xref="S3.I1.i2.p1.1.m1.1.1.2.3.cmml">i</mi><mn id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">superscript</csymbol><apply id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.2">𝐺</ci><ci id="S3.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.3">𝑖</ci></apply><cn id="S3.I1.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">G_{i}^{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> to end <math alttext="G_{i}^{T}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><msubsup id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.1.1.2.2" xref="S3.I1.i2.p1.2.m2.1.1.2.2.cmml">G</mi><mi id="S3.I1.i2.p1.2.m2.1.1.2.3" xref="S3.I1.i2.p1.2.m2.1.1.2.3.cmml">i</mi><mi id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">superscript</csymbol><apply id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2.2">𝐺</ci><ci id="S3.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S3.I1.i2.p1.2.m2.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">G_{i}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> of the gesture sequence. These trails consist of a time series of markers with distinct colours corresponding to each finger. Temporal dynamics are captured by altering the transparency of these markers over time, with markers at the start of the gesture sequence (<math alttext="\tau\approx 1" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><mrow id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">τ</mi><mo id="S3.I1.i2.p1.3.m3.1.1.1" xref="S3.I1.i2.p1.3.m3.1.1.1.cmml">≈</mo><mn id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><approx id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1.1"></approx><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">𝜏</ci><cn id="S3.I1.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\tau\approx 1</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">italic_τ ≈ 1</annotation></semantics></math>) more transparent and those near the end (<math alttext="\tau\approx T" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.1"><semantics id="S3.I1.i2.p1.4.m4.1a"><mrow id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml"><mi id="S3.I1.i2.p1.4.m4.1.1.2" xref="S3.I1.i2.p1.4.m4.1.1.2.cmml">τ</mi><mo id="S3.I1.i2.p1.4.m4.1.1.1" xref="S3.I1.i2.p1.4.m4.1.1.1.cmml">≈</mo><mi id="S3.I1.i2.p1.4.m4.1.1.3" xref="S3.I1.i2.p1.4.m4.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><apply id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1"><approx id="S3.I1.i2.p1.4.m4.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1.1"></approx><ci id="S3.I1.i2.p1.4.m4.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.1.1.2">𝜏</ci><ci id="S3.I1.i2.p1.4.m4.1.1.3.cmml" xref="S3.I1.i2.p1.4.m4.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">\tau\approx T</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.1d">italic_τ ≈ italic_T</annotation></semantics></math>) more opaque.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.3">Temporal information condensation creates, for each resampled gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><msub id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, a 3D visualization of the dynamic gesture, as illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F2" title="Figure 2 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. This visualization contains a compact spatiotemporal encoding of the hand pose at the final frame <math alttext="G_{i}^{T}" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><msubsup id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml"><mi id="S3.SS1.p7.2.m2.1.1.2.2" xref="S3.SS1.p7.2.m2.1.1.2.2.cmml">G</mi><mi id="S3.SS1.p7.2.m2.1.1.2.3" xref="S3.SS1.p7.2.m2.1.1.2.3.cmml">i</mi><mi id="S3.SS1.p7.2.m2.1.1.3" xref="S3.SS1.p7.2.m2.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><apply id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.2.m2.1.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">superscript</csymbol><apply id="S3.SS1.p7.2.m2.1.1.2.cmml" xref="S3.SS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.2.m2.1.1.2.1.cmml" xref="S3.SS1.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p7.2.m2.1.1.2.2.cmml" xref="S3.SS1.p7.2.m2.1.1.2.2">𝐺</ci><ci id="S3.SS1.p7.2.m2.1.1.2.3.cmml" xref="S3.SS1.p7.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.p7.2.m2.1.1.3.cmml" xref="S3.SS1.p7.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">G_{i}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.2.m2.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> with the temporal trail across <math alttext="{G_{i}^{\tau}}_{\tau\text{=}1}^{T\text{-}1}" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><mmultiscripts id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml"><mi id="S3.SS1.p7.3.m3.1.1.2.2.2.2" xref="S3.SS1.p7.3.m3.1.1.2.2.2.2.cmml">G</mi><mi id="S3.SS1.p7.3.m3.1.1.2.2.2.3" xref="S3.SS1.p7.3.m3.1.1.2.2.2.3.cmml">i</mi><mi id="S3.SS1.p7.3.m3.1.1.2.2.3" xref="S3.SS1.p7.3.m3.1.1.2.2.3.cmml">τ</mi><mrow id="S3.SS1.p7.3.m3.1.1.2.3" xref="S3.SS1.p7.3.m3.1.1.2.3.cmml"><mi id="S3.SS1.p7.3.m3.1.1.2.3.2" xref="S3.SS1.p7.3.m3.1.1.2.3.2.cmml">τ</mi><mo id="S3.SS1.p7.3.m3.1.1.2.3.1" xref="S3.SS1.p7.3.m3.1.1.2.3.1.cmml">⁢</mo><mtext id="S3.SS1.p7.3.m3.1.1.2.3.3" xref="S3.SS1.p7.3.m3.1.1.2.3.3a.cmml">=</mtext><mo id="S3.SS1.p7.3.m3.1.1.2.3.1a" xref="S3.SS1.p7.3.m3.1.1.2.3.1.cmml">⁢</mo><mn id="S3.SS1.p7.3.m3.1.1.2.3.4" xref="S3.SS1.p7.3.m3.1.1.2.3.4.cmml">1</mn></mrow><mrow id="S3.SS1.p7.3.m3.1.1.3" xref="S3.SS1.p7.3.m3.1.1.3.cmml"><mi id="S3.SS1.p7.3.m3.1.1.3.2" xref="S3.SS1.p7.3.m3.1.1.3.2.cmml">T</mi><mo id="S3.SS1.p7.3.m3.1.1.3.1" xref="S3.SS1.p7.3.m3.1.1.3.1.cmml">⁢</mo><mtext id="S3.SS1.p7.3.m3.1.1.3.3" xref="S3.SS1.p7.3.m3.1.1.3.3a.cmml">-</mtext><mo id="S3.SS1.p7.3.m3.1.1.3.1a" xref="S3.SS1.p7.3.m3.1.1.3.1.cmml">⁢</mo><mn id="S3.SS1.p7.3.m3.1.1.3.4" xref="S3.SS1.p7.3.m3.1.1.3.4.cmml">1</mn></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><apply id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m3.1.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.p7.3.m3.1.1.2.cmml" xref="S3.SS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m3.1.1.2.1.cmml" xref="S3.SS1.p7.3.m3.1.1">subscript</csymbol><apply id="S3.SS1.p7.3.m3.1.1.2.2.cmml" xref="S3.SS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m3.1.1.2.2.1.cmml" xref="S3.SS1.p7.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.p7.3.m3.1.1.2.2.2.cmml" xref="S3.SS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS1.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p7.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS1.p7.3.m3.1.1.2.2.2.2">𝐺</ci><ci id="S3.SS1.p7.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS1.p7.3.m3.1.1.2.2.2.3">𝑖</ci></apply><ci id="S3.SS1.p7.3.m3.1.1.2.2.3.cmml" xref="S3.SS1.p7.3.m3.1.1.2.2.3">𝜏</ci></apply><apply id="S3.SS1.p7.3.m3.1.1.2.3.cmml" xref="S3.SS1.p7.3.m3.1.1.2.3"><times id="S3.SS1.p7.3.m3.1.1.2.3.1.cmml" xref="S3.SS1.p7.3.m3.1.1.2.3.1"></times><ci id="S3.SS1.p7.3.m3.1.1.2.3.2.cmml" xref="S3.SS1.p7.3.m3.1.1.2.3.2">𝜏</ci><ci id="S3.SS1.p7.3.m3.1.1.2.3.3a.cmml" xref="S3.SS1.p7.3.m3.1.1.2.3.3"><mtext id="S3.SS1.p7.3.m3.1.1.2.3.3.cmml" mathsize="70%" xref="S3.SS1.p7.3.m3.1.1.2.3.3">=</mtext></ci><cn id="S3.SS1.p7.3.m3.1.1.2.3.4.cmml" type="integer" xref="S3.SS1.p7.3.m3.1.1.2.3.4">1</cn></apply></apply><apply id="S3.SS1.p7.3.m3.1.1.3.cmml" xref="S3.SS1.p7.3.m3.1.1.3"><times id="S3.SS1.p7.3.m3.1.1.3.1.cmml" xref="S3.SS1.p7.3.m3.1.1.3.1"></times><ci id="S3.SS1.p7.3.m3.1.1.3.2.cmml" xref="S3.SS1.p7.3.m3.1.1.3.2">𝑇</ci><ci id="S3.SS1.p7.3.m3.1.1.3.3a.cmml" xref="S3.SS1.p7.3.m3.1.1.3.3"><mtext id="S3.SS1.p7.3.m3.1.1.3.3.cmml" mathsize="70%" xref="S3.SS1.p7.3.m3.1.1.3.3">-</mtext></ci><cn id="S3.SS1.p7.3.m3.1.1.3.4.cmml" type="integer" xref="S3.SS1.p7.3.m3.1.1.3.4">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">{G_{i}^{\tau}}_{\tau\text{=}1}^{T\text{-}1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.3.m3.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_τ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_τ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> which can be projected onto any arbitrary plane (view orientation).</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="421" id="S3.F2.g1" src="extracted/5904591/spatiotemporal-RGB-representation-2.png" width="533"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Temporal Information Condensation Workflow: Illustrating the Transformation of 3D Skeleton Data into 2D Spatiotemporal RGB Representations for Image Classification.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.3">We limit our framework to six (6) view orientations (VOs)—axonometric, front-away, side-left, top-down, front-to, and side-right—shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F3" title="Figure 3 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. These VOs are defined by the elevation and azimuth angles of the virtual camera in the 3D visualization space, which are dataset-specific due to the varying methods used to extract the skeleton data from the raw RGB-D sources.
Thus, temporal information condensation produces a single 2D image from the 3D spatiotemporal representation of any gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.1"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from any of the six view orientations <math alttext="VO_{j}" class="ltx_Math" display="inline" id="S3.SS1.p8.2.m2.1"><semantics id="S3.SS1.p8.2.m2.1a"><mrow id="S3.SS1.p8.2.m2.1.1" xref="S3.SS1.p8.2.m2.1.1.cmml"><mi id="S3.SS1.p8.2.m2.1.1.2" xref="S3.SS1.p8.2.m2.1.1.2.cmml">V</mi><mo id="S3.SS1.p8.2.m2.1.1.1" xref="S3.SS1.p8.2.m2.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p8.2.m2.1.1.3" xref="S3.SS1.p8.2.m2.1.1.3.cmml"><mi id="S3.SS1.p8.2.m2.1.1.3.2" xref="S3.SS1.p8.2.m2.1.1.3.2.cmml">O</mi><mi id="S3.SS1.p8.2.m2.1.1.3.3" xref="S3.SS1.p8.2.m2.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.2.m2.1b"><apply id="S3.SS1.p8.2.m2.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1"><times id="S3.SS1.p8.2.m2.1.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1.1"></times><ci id="S3.SS1.p8.2.m2.1.1.2.cmml" xref="S3.SS1.p8.2.m2.1.1.2">𝑉</ci><apply id="S3.SS1.p8.2.m2.1.1.3.cmml" xref="S3.SS1.p8.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p8.2.m2.1.1.3.1.cmml" xref="S3.SS1.p8.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p8.2.m2.1.1.3.2.cmml" xref="S3.SS1.p8.2.m2.1.1.3.2">𝑂</ci><ci id="S3.SS1.p8.2.m2.1.1.3.3.cmml" xref="S3.SS1.p8.2.m2.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.2.m2.1c">VO_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.2.m2.1d">italic_V italic_O start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext="j\in[1,6]" class="ltx_Math" display="inline" id="S3.SS1.p8.3.m3.2"><semantics id="S3.SS1.p8.3.m3.2a"><mrow id="S3.SS1.p8.3.m3.2.3" xref="S3.SS1.p8.3.m3.2.3.cmml"><mi id="S3.SS1.p8.3.m3.2.3.2" xref="S3.SS1.p8.3.m3.2.3.2.cmml">j</mi><mo id="S3.SS1.p8.3.m3.2.3.1" xref="S3.SS1.p8.3.m3.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p8.3.m3.2.3.3.2" xref="S3.SS1.p8.3.m3.2.3.3.1.cmml"><mo id="S3.SS1.p8.3.m3.2.3.3.2.1" stretchy="false" xref="S3.SS1.p8.3.m3.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p8.3.m3.1.1" xref="S3.SS1.p8.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p8.3.m3.2.3.3.2.2" xref="S3.SS1.p8.3.m3.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p8.3.m3.2.2" xref="S3.SS1.p8.3.m3.2.2.cmml">6</mn><mo id="S3.SS1.p8.3.m3.2.3.3.2.3" stretchy="false" xref="S3.SS1.p8.3.m3.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.3.m3.2b"><apply id="S3.SS1.p8.3.m3.2.3.cmml" xref="S3.SS1.p8.3.m3.2.3"><in id="S3.SS1.p8.3.m3.2.3.1.cmml" xref="S3.SS1.p8.3.m3.2.3.1"></in><ci id="S3.SS1.p8.3.m3.2.3.2.cmml" xref="S3.SS1.p8.3.m3.2.3.2">𝑗</ci><interval closure="closed" id="S3.SS1.p8.3.m3.2.3.3.1.cmml" xref="S3.SS1.p8.3.m3.2.3.3.2"><cn id="S3.SS1.p8.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p8.3.m3.1.1">1</cn><cn id="S3.SS1.p8.3.m3.2.2.cmml" type="integer" xref="S3.SS1.p8.3.m3.2.2">6</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.3.m3.2c">j\in[1,6]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.3.m3.2d">italic_j ∈ [ 1 , 6 ]</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S3.F3.g1" src="extracted/5904591/train-tap-f2s18e5-3d-all-vos-4.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of View Orientations Used for Spatiotemporal Gesture Representations. Left-to-Right: Top-Down, Front-To, Front-Away, Side-Right, Side-Left, and Axonometric.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.3">However, the zoom level and position of the virtual camera need to be adjusted beforehand to ensure that the spatiotemporal representation is fully enclosed within the image, preventing any cropping or truncation. These adjustments cannot be manually configured as they differ for each gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p9.1.m1.1"><semantics id="S3.SS1.p9.1.m1.1a"><msub id="S3.SS1.p9.1.m1.1.1" xref="S3.SS1.p9.1.m1.1.1.cmml"><mi id="S3.SS1.p9.1.m1.1.1.2" xref="S3.SS1.p9.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p9.1.m1.1.1.3" xref="S3.SS1.p9.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.1.m1.1b"><apply id="S3.SS1.p9.1.m1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p9.1.m1.1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p9.1.m1.1.1.2.cmml" xref="S3.SS1.p9.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p9.1.m1.1.1.3.cmml" xref="S3.SS1.p9.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.1.m1.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and set <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p9.2.m2.1"><semantics id="S3.SS1.p9.2.m2.1a"><mi id="S3.SS1.p9.2.m2.1.1" xref="S3.SS1.p9.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.2.m2.1b"><ci id="S3.SS1.p9.2.m2.1.1.cmml" xref="S3.SS1.p9.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.2.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.2.m2.1d">italic_S</annotation></semantics></math>. Instead, the sequence fitting for each gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p9.3.m3.1"><semantics id="S3.SS1.p9.3.m3.1a"><msub id="S3.SS1.p9.3.m3.1.1" xref="S3.SS1.p9.3.m3.1.1.cmml"><mi id="S3.SS1.p9.3.m3.1.1.2" xref="S3.SS1.p9.3.m3.1.1.2.cmml">g</mi><mi id="S3.SS1.p9.3.m3.1.1.3" xref="S3.SS1.p9.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.3.m3.1b"><apply id="S3.SS1.p9.3.m3.1.1.cmml" xref="S3.SS1.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p9.3.m3.1.1.1.cmml" xref="S3.SS1.p9.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p9.3.m3.1.1.2.cmml" xref="S3.SS1.p9.3.m3.1.1.2">𝑔</ci><ci id="S3.SS1.p9.3.m3.1.1.3.cmml" xref="S3.SS1.p9.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.3.m3.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.3.m3.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is dynamically estimated as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="g_{i}=g_{i}-mean(g_{i})-(L/2)" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml"><mi id="S3.E2.m1.2.2.4.2" xref="S3.E2.m1.2.2.4.2.cmml">g</mi><mi id="S3.E2.m1.2.2.4.3" xref="S3.E2.m1.2.2.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">=</mo><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><msub id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml"><mi id="S3.E2.m1.2.2.2.4.2" xref="S3.E2.m1.2.2.2.4.2.cmml">g</mi><mi id="S3.E2.m1.2.2.2.4.3" xref="S3.E2.m1.2.2.2.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">−</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">m</mi><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">e</mi><mo id="S3.E2.m1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">a</mi><mo id="S3.E2.m1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.6.cmml">n</mi><mo id="S3.E2.m1.1.1.1.1.2c" xref="S3.E2.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">g</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2.3a" xref="S3.E2.m1.2.2.2.3.cmml">−</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml"><mo id="S3.E2.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.2.cmml">L</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">/</mo><mn id="S3.E2.m1.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.3.cmml">2</mn></mrow><mo id="S3.E2.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"></eq><apply id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.4.1.cmml" xref="S3.E2.m1.2.2.4">subscript</csymbol><ci id="S3.E2.m1.2.2.4.2.cmml" xref="S3.E2.m1.2.2.4.2">𝑔</ci><ci id="S3.E2.m1.2.2.4.3.cmml" xref="S3.E2.m1.2.2.4.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><minus id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></minus><apply id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.4.1.cmml" xref="S3.E2.m1.2.2.2.4">subscript</csymbol><ci id="S3.E2.m1.2.2.2.4.2.cmml" xref="S3.E2.m1.2.2.2.4.2">𝑔</ci><ci id="S3.E2.m1.2.2.2.4.3.cmml" xref="S3.E2.m1.2.2.2.4.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3">𝑚</ci><ci id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4">𝑒</ci><ci id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5">𝑎</ci><ci id="S3.E2.m1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.6">𝑛</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">𝑔</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><divide id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1"></divide><ci id="S3.E2.m1.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.2">𝐿</ci><cn id="S3.E2.m1.2.2.2.2.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.2.2.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">g_{i}=g_{i}-mean(g_{i})-(L/2)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_m italic_e italic_a italic_n ( italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - ( italic_L / 2 )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p11">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P=(L/2)" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">P</mi><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">L</mi><mo id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">/</mo><mn id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="S3.E3.m1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><divide id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"></divide><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">𝐿</ci><cn id="S3.E3.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">P=(L/2)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_P = ( italic_L / 2 )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p12">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z_{i}=max(g_{i})-min(g_{i})+\gamma" class="ltx_Math" display="block" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><msub id="S3.E4.m1.2.2.4" xref="S3.E4.m1.2.2.4.cmml"><mi id="S3.E4.m1.2.2.4.2" xref="S3.E4.m1.2.2.4.2.cmml">Z</mi><mi id="S3.E4.m1.2.2.4.3" xref="S3.E4.m1.2.2.4.3.cmml">i</mi></msub><mo id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml">=</mo><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml"><mrow id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml"><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml">m</mi><mo id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.4.cmml">a</mi><mo id="S3.E4.m1.1.1.1.1.1.2a" xref="S3.E4.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.5" xref="S3.E4.m1.1.1.1.1.1.5.cmml">x</mi><mo id="S3.E4.m1.1.1.1.1.1.2b" xref="S3.E4.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">g</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.3.cmml">−</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.3.cmml">m</mi><mo id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml">⁢</mo><mi id="S3.E4.m1.2.2.2.2.2.4" xref="S3.E4.m1.2.2.2.2.2.4.cmml">i</mi><mo id="S3.E4.m1.2.2.2.2.2.2a" xref="S3.E4.m1.2.2.2.2.2.2.cmml">⁢</mo><mi id="S3.E4.m1.2.2.2.2.2.5" xref="S3.E4.m1.2.2.2.2.2.5.cmml">n</mi><mo id="S3.E4.m1.2.2.2.2.2.2b" xref="S3.E4.m1.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml"><mo id="S3.E4.m1.2.2.2.2.2.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E4.m1.2.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E4.m1.2.2.2.2.2.1.1.1.2" xref="S3.E4.m1.2.2.2.2.2.1.1.1.2.cmml">g</mi><mi id="S3.E4.m1.2.2.2.2.2.1.1.1.3" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.2.2.2.2.2.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.2.3.cmml">+</mo><mi id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.4.cmml">γ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3"></eq><apply id="S3.E4.m1.2.2.4.cmml" xref="S3.E4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.1.cmml" xref="S3.E4.m1.2.2.4">subscript</csymbol><ci id="S3.E4.m1.2.2.4.2.cmml" xref="S3.E4.m1.2.2.4.2">𝑍</ci><ci id="S3.E4.m1.2.2.4.3.cmml" xref="S3.E4.m1.2.2.4.3">𝑖</ci></apply><apply id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"><plus id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.3"></plus><apply id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2"><minus id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.3"></minus><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3">𝑚</ci><ci id="S3.E4.m1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.4">𝑎</ci><ci id="S3.E4.m1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.1.1.5">𝑥</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">𝑔</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2"><times id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2"></times><ci id="S3.E4.m1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.3">𝑚</ci><ci id="S3.E4.m1.2.2.2.2.2.4.cmml" xref="S3.E4.m1.2.2.2.2.2.4">𝑖</ci><ci id="S3.E4.m1.2.2.2.2.2.5.cmml" xref="S3.E4.m1.2.2.2.2.2.5">𝑛</ci><apply id="S3.E4.m1.2.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.2">𝑔</ci><ci id="S3.E4.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3">𝑖</ci></apply></apply></apply><ci id="S3.E4.m1.2.2.2.4.cmml" xref="S3.E4.m1.2.2.2.4">𝛾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">Z_{i}=max(g_{i})-min(g_{i})+\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_m italic_a italic_x ( italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_m italic_i italic_n ( italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_γ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p13">
<p class="ltx_p" id="S3.SS1.p13.8"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.E2" title="2 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Equation 2</span></a> adjusts each gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p13.1.m1.1"><semantics id="S3.SS1.p13.1.m1.1a"><msub id="S3.SS1.p13.1.m1.1.1" xref="S3.SS1.p13.1.m1.1.1.cmml"><mi id="S3.SS1.p13.1.m1.1.1.2" xref="S3.SS1.p13.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p13.1.m1.1.1.3" xref="S3.SS1.p13.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.1.m1.1b"><apply id="S3.SS1.p13.1.m1.1.1.cmml" xref="S3.SS1.p13.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p13.1.m1.1.1.1.cmml" xref="S3.SS1.p13.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p13.1.m1.1.1.2.cmml" xref="S3.SS1.p13.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p13.1.m1.1.1.3.cmml" xref="S3.SS1.p13.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.1.m1.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (a sequence of 3D skeleton coordinates) by subtracting its mean and half the target length <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p13.2.m2.1"><semantics id="S3.SS1.p13.2.m2.1a"><mi id="S3.SS1.p13.2.m2.1.1" xref="S3.SS1.p13.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.2.m2.1b"><ci id="S3.SS1.p13.2.m2.1.1.cmml" xref="S3.SS1.p13.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.2.m2.1d">italic_L</annotation></semantics></math> of the static image. This adjustment shifts the gesture to the centre of the image and ensures that the virtual camera is consistently fixed at the image centre <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p13.3.m3.1"><semantics id="S3.SS1.p13.3.m3.1a"><mi id="S3.SS1.p13.3.m3.1.1" xref="S3.SS1.p13.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.3.m3.1b"><ci id="S3.SS1.p13.3.m3.1.1.cmml" xref="S3.SS1.p13.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.3.m3.1d">italic_P</annotation></semantics></math> for all <math alttext="g_{i}\in S" class="ltx_Math" display="inline" id="S3.SS1.p13.4.m4.1"><semantics id="S3.SS1.p13.4.m4.1a"><mrow id="S3.SS1.p13.4.m4.1.1" xref="S3.SS1.p13.4.m4.1.1.cmml"><msub id="S3.SS1.p13.4.m4.1.1.2" xref="S3.SS1.p13.4.m4.1.1.2.cmml"><mi id="S3.SS1.p13.4.m4.1.1.2.2" xref="S3.SS1.p13.4.m4.1.1.2.2.cmml">g</mi><mi id="S3.SS1.p13.4.m4.1.1.2.3" xref="S3.SS1.p13.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p13.4.m4.1.1.1" xref="S3.SS1.p13.4.m4.1.1.1.cmml">∈</mo><mi id="S3.SS1.p13.4.m4.1.1.3" xref="S3.SS1.p13.4.m4.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.4.m4.1b"><apply id="S3.SS1.p13.4.m4.1.1.cmml" xref="S3.SS1.p13.4.m4.1.1"><in id="S3.SS1.p13.4.m4.1.1.1.cmml" xref="S3.SS1.p13.4.m4.1.1.1"></in><apply id="S3.SS1.p13.4.m4.1.1.2.cmml" xref="S3.SS1.p13.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p13.4.m4.1.1.2.1.cmml" xref="S3.SS1.p13.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p13.4.m4.1.1.2.2.cmml" xref="S3.SS1.p13.4.m4.1.1.2.2">𝑔</ci><ci id="S3.SS1.p13.4.m4.1.1.2.3.cmml" xref="S3.SS1.p13.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.p13.4.m4.1.1.3.cmml" xref="S3.SS1.p13.4.m4.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.4.m4.1c">g_{i}\in S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.4.m4.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S</annotation></semantics></math>, as outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.E3" title="3 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Equation 3</span></a>.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.E4" title="4 ‣ III-A Temporal Information Condensation ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Equation 4</span></a> dynamically estimates the zoom level <math alttext="Z_{i}" class="ltx_Math" display="inline" id="S3.SS1.p13.5.m5.1"><semantics id="S3.SS1.p13.5.m5.1a"><msub id="S3.SS1.p13.5.m5.1.1" xref="S3.SS1.p13.5.m5.1.1.cmml"><mi id="S3.SS1.p13.5.m5.1.1.2" xref="S3.SS1.p13.5.m5.1.1.2.cmml">Z</mi><mi id="S3.SS1.p13.5.m5.1.1.3" xref="S3.SS1.p13.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.5.m5.1b"><apply id="S3.SS1.p13.5.m5.1.1.cmml" xref="S3.SS1.p13.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p13.5.m5.1.1.1.cmml" xref="S3.SS1.p13.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p13.5.m5.1.1.2.cmml" xref="S3.SS1.p13.5.m5.1.1.2">𝑍</ci><ci id="S3.SS1.p13.5.m5.1.1.3.cmml" xref="S3.SS1.p13.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.5.m5.1c">Z_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.5.m5.1d">italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> of the virtual camera for each gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p13.6.m6.1"><semantics id="S3.SS1.p13.6.m6.1a"><msub id="S3.SS1.p13.6.m6.1.1" xref="S3.SS1.p13.6.m6.1.1.cmml"><mi id="S3.SS1.p13.6.m6.1.1.2" xref="S3.SS1.p13.6.m6.1.1.2.cmml">g</mi><mi id="S3.SS1.p13.6.m6.1.1.3" xref="S3.SS1.p13.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.6.m6.1b"><apply id="S3.SS1.p13.6.m6.1.1.cmml" xref="S3.SS1.p13.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p13.6.m6.1.1.1.cmml" xref="S3.SS1.p13.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p13.6.m6.1.1.2.cmml" xref="S3.SS1.p13.6.m6.1.1.2">𝑔</ci><ci id="S3.SS1.p13.6.m6.1.1.3.cmml" xref="S3.SS1.p13.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.6.m6.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.6.m6.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> based on the minimum and maximum bounds of the gesture’s 3D visualization. An optional padding value <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS1.p13.7.m7.1"><semantics id="S3.SS1.p13.7.m7.1a"><mi id="S3.SS1.p13.7.m7.1.1" xref="S3.SS1.p13.7.m7.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.7.m7.1b"><ci id="S3.SS1.p13.7.m7.1.1.cmml" xref="S3.SS1.p13.7.m7.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.7.m7.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.7.m7.1d">italic_γ</annotation></semantics></math> can be added to all gestures <math alttext="g_{i}\in S" class="ltx_Math" display="inline" id="S3.SS1.p13.8.m8.1"><semantics id="S3.SS1.p13.8.m8.1a"><mrow id="S3.SS1.p13.8.m8.1.1" xref="S3.SS1.p13.8.m8.1.1.cmml"><msub id="S3.SS1.p13.8.m8.1.1.2" xref="S3.SS1.p13.8.m8.1.1.2.cmml"><mi id="S3.SS1.p13.8.m8.1.1.2.2" xref="S3.SS1.p13.8.m8.1.1.2.2.cmml">g</mi><mi id="S3.SS1.p13.8.m8.1.1.2.3" xref="S3.SS1.p13.8.m8.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p13.8.m8.1.1.1" xref="S3.SS1.p13.8.m8.1.1.1.cmml">∈</mo><mi id="S3.SS1.p13.8.m8.1.1.3" xref="S3.SS1.p13.8.m8.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.8.m8.1b"><apply id="S3.SS1.p13.8.m8.1.1.cmml" xref="S3.SS1.p13.8.m8.1.1"><in id="S3.SS1.p13.8.m8.1.1.1.cmml" xref="S3.SS1.p13.8.m8.1.1.1"></in><apply id="S3.SS1.p13.8.m8.1.1.2.cmml" xref="S3.SS1.p13.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p13.8.m8.1.1.2.1.cmml" xref="S3.SS1.p13.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p13.8.m8.1.1.2.2.cmml" xref="S3.SS1.p13.8.m8.1.1.2.2">𝑔</ci><ci id="S3.SS1.p13.8.m8.1.1.2.3.cmml" xref="S3.SS1.p13.8.m8.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.p13.8.m8.1.1.3.cmml" xref="S3.SS1.p13.8.m8.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.8.m8.1c">g_{i}\in S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p13.8.m8.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S</annotation></semantics></math> or set to zero.</p>
</div>
<div class="ltx_para" id="S3.SS1.p14">
<p class="ltx_p" id="S3.SS1.p14.6">In conclusion, temporal information condensation utilizes data-level fusion to transform dynamic hand gesture recognition into static image classification. Thus, there exists a mapping function <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS1.p14.1.m1.1"><semantics id="S3.SS1.p14.1.m1.1a"><mi id="S3.SS1.p14.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p14.1.m1.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.1.m1.1b"><ci id="S3.SS1.p14.1.m1.1.1.cmml" xref="S3.SS1.p14.1.m1.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.1.m1.1c">\Phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.1.m1.1d">roman_Φ</annotation></semantics></math> that transforms the 3D skeleton data of a dynamic gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS1.p14.2.m2.1"><semantics id="S3.SS1.p14.2.m2.1a"><msub id="S3.SS1.p14.2.m2.1.1" xref="S3.SS1.p14.2.m2.1.1.cmml"><mi id="S3.SS1.p14.2.m2.1.1.2" xref="S3.SS1.p14.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS1.p14.2.m2.1.1.3" xref="S3.SS1.p14.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.2.m2.1b"><apply id="S3.SS1.p14.2.m2.1.1.cmml" xref="S3.SS1.p14.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p14.2.m2.1.1.1.cmml" xref="S3.SS1.p14.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p14.2.m2.1.1.2.cmml" xref="S3.SS1.p14.2.m2.1.1.2">𝑔</ci><ci id="S3.SS1.p14.2.m2.1.1.3.cmml" xref="S3.SS1.p14.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.2.m2.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from any view orientation <math alttext="VO_{j}" class="ltx_Math" display="inline" id="S3.SS1.p14.3.m3.1"><semantics id="S3.SS1.p14.3.m3.1a"><mrow id="S3.SS1.p14.3.m3.1.1" xref="S3.SS1.p14.3.m3.1.1.cmml"><mi id="S3.SS1.p14.3.m3.1.1.2" xref="S3.SS1.p14.3.m3.1.1.2.cmml">V</mi><mo id="S3.SS1.p14.3.m3.1.1.1" xref="S3.SS1.p14.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p14.3.m3.1.1.3" xref="S3.SS1.p14.3.m3.1.1.3.cmml"><mi id="S3.SS1.p14.3.m3.1.1.3.2" xref="S3.SS1.p14.3.m3.1.1.3.2.cmml">O</mi><mi id="S3.SS1.p14.3.m3.1.1.3.3" xref="S3.SS1.p14.3.m3.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.3.m3.1b"><apply id="S3.SS1.p14.3.m3.1.1.cmml" xref="S3.SS1.p14.3.m3.1.1"><times id="S3.SS1.p14.3.m3.1.1.1.cmml" xref="S3.SS1.p14.3.m3.1.1.1"></times><ci id="S3.SS1.p14.3.m3.1.1.2.cmml" xref="S3.SS1.p14.3.m3.1.1.2">𝑉</ci><apply id="S3.SS1.p14.3.m3.1.1.3.cmml" xref="S3.SS1.p14.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p14.3.m3.1.1.3.1.cmml" xref="S3.SS1.p14.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p14.3.m3.1.1.3.2.cmml" xref="S3.SS1.p14.3.m3.1.1.3.2">𝑂</ci><ci id="S3.SS1.p14.3.m3.1.1.3.3.cmml" xref="S3.SS1.p14.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.3.m3.1c">VO_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.3.m3.1d">italic_V italic_O start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> a 2D spatiotemporal image <math alttext="I_{ij}=\Phi(G_{i})" class="ltx_Math" display="inline" id="S3.SS1.p14.4.m4.1"><semantics id="S3.SS1.p14.4.m4.1a"><mrow id="S3.SS1.p14.4.m4.1.1" xref="S3.SS1.p14.4.m4.1.1.cmml"><msub id="S3.SS1.p14.4.m4.1.1.3" xref="S3.SS1.p14.4.m4.1.1.3.cmml"><mi id="S3.SS1.p14.4.m4.1.1.3.2" xref="S3.SS1.p14.4.m4.1.1.3.2.cmml">I</mi><mrow id="S3.SS1.p14.4.m4.1.1.3.3" xref="S3.SS1.p14.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p14.4.m4.1.1.3.3.2" xref="S3.SS1.p14.4.m4.1.1.3.3.2.cmml">i</mi><mo id="S3.SS1.p14.4.m4.1.1.3.3.1" xref="S3.SS1.p14.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p14.4.m4.1.1.3.3.3" xref="S3.SS1.p14.4.m4.1.1.3.3.3.cmml">j</mi></mrow></msub><mo id="S3.SS1.p14.4.m4.1.1.2" xref="S3.SS1.p14.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS1.p14.4.m4.1.1.1" xref="S3.SS1.p14.4.m4.1.1.1.cmml"><mi id="S3.SS1.p14.4.m4.1.1.1.3" mathvariant="normal" xref="S3.SS1.p14.4.m4.1.1.1.3.cmml">Φ</mi><mo id="S3.SS1.p14.4.m4.1.1.1.2" xref="S3.SS1.p14.4.m4.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p14.4.m4.1.1.1.1.1" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p14.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p14.4.m4.1.1.1.1.1.1" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p14.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.2.cmml">G</mi><mi id="S3.SS1.p14.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p14.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.4.m4.1b"><apply id="S3.SS1.p14.4.m4.1.1.cmml" xref="S3.SS1.p14.4.m4.1.1"><eq id="S3.SS1.p14.4.m4.1.1.2.cmml" xref="S3.SS1.p14.4.m4.1.1.2"></eq><apply id="S3.SS1.p14.4.m4.1.1.3.cmml" xref="S3.SS1.p14.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p14.4.m4.1.1.3.1.cmml" xref="S3.SS1.p14.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p14.4.m4.1.1.3.2.cmml" xref="S3.SS1.p14.4.m4.1.1.3.2">𝐼</ci><apply id="S3.SS1.p14.4.m4.1.1.3.3.cmml" xref="S3.SS1.p14.4.m4.1.1.3.3"><times id="S3.SS1.p14.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p14.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p14.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p14.4.m4.1.1.3.3.2">𝑖</ci><ci id="S3.SS1.p14.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p14.4.m4.1.1.3.3.3">𝑗</ci></apply></apply><apply id="S3.SS1.p14.4.m4.1.1.1.cmml" xref="S3.SS1.p14.4.m4.1.1.1"><times id="S3.SS1.p14.4.m4.1.1.1.2.cmml" xref="S3.SS1.p14.4.m4.1.1.1.2"></times><ci id="S3.SS1.p14.4.m4.1.1.1.3.cmml" xref="S3.SS1.p14.4.m4.1.1.1.3">Φ</ci><apply id="S3.SS1.p14.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p14.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p14.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p14.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p14.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.2">𝐺</ci><ci id="S3.SS1.p14.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p14.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.4.m4.1c">I_{ij}=\Phi(G_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.4.m4.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = roman_Φ ( italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>. This image serves as a compact input representation for the classification task, where the goal is to determine the correct class <math alttext="C_{h}" class="ltx_Math" display="inline" id="S3.SS1.p14.5.m5.1"><semantics id="S3.SS1.p14.5.m5.1a"><msub id="S3.SS1.p14.5.m5.1.1" xref="S3.SS1.p14.5.m5.1.1.cmml"><mi id="S3.SS1.p14.5.m5.1.1.2" xref="S3.SS1.p14.5.m5.1.1.2.cmml">C</mi><mi id="S3.SS1.p14.5.m5.1.1.3" xref="S3.SS1.p14.5.m5.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.5.m5.1b"><apply id="S3.SS1.p14.5.m5.1.1.cmml" xref="S3.SS1.p14.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p14.5.m5.1.1.1.cmml" xref="S3.SS1.p14.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p14.5.m5.1.1.2.cmml" xref="S3.SS1.p14.5.m5.1.1.2">𝐶</ci><ci id="S3.SS1.p14.5.m5.1.1.3.cmml" xref="S3.SS1.p14.5.m5.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.5.m5.1c">C_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.5.m5.1d">italic_C start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="I_{ij}" class="ltx_Math" display="inline" id="S3.SS1.p14.6.m6.1"><semantics id="S3.SS1.p14.6.m6.1a"><msub id="S3.SS1.p14.6.m6.1.1" xref="S3.SS1.p14.6.m6.1.1.cmml"><mi id="S3.SS1.p14.6.m6.1.1.2" xref="S3.SS1.p14.6.m6.1.1.2.cmml">I</mi><mrow id="S3.SS1.p14.6.m6.1.1.3" xref="S3.SS1.p14.6.m6.1.1.3.cmml"><mi id="S3.SS1.p14.6.m6.1.1.3.2" xref="S3.SS1.p14.6.m6.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p14.6.m6.1.1.3.1" xref="S3.SS1.p14.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p14.6.m6.1.1.3.3" xref="S3.SS1.p14.6.m6.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p14.6.m6.1b"><apply id="S3.SS1.p14.6.m6.1.1.cmml" xref="S3.SS1.p14.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p14.6.m6.1.1.1.cmml" xref="S3.SS1.p14.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p14.6.m6.1.1.2.cmml" xref="S3.SS1.p14.6.m6.1.1.2">𝐼</ci><apply id="S3.SS1.p14.6.m6.1.1.3.cmml" xref="S3.SS1.p14.6.m6.1.1.3"><times id="S3.SS1.p14.6.m6.1.1.3.1.cmml" xref="S3.SS1.p14.6.m6.1.1.3.1"></times><ci id="S3.SS1.p14.6.m6.1.1.3.2.cmml" xref="S3.SS1.p14.6.m6.1.1.3.2">𝑖</ci><ci id="S3.SS1.p14.6.m6.1.1.3.3.cmml" xref="S3.SS1.p14.6.m6.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p14.6.m6.1c">I_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p14.6.m6.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Multi-Stream CNN Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">With the set <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_S</annotation></semantics></math> of dynamic hand gestures <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑔</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> converted into an equivalent set <math alttext="S^{*}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msup id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">S</mi><mo id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑆</ci><times id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">S^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> of static spatiotemporal images <math alttext="I_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.4.m4.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝐼</ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><times id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝑖</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">I_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, traditional CNN architectures can be effectively utilized for image classification and, consequently, gesture recognition. Furthermore, transfer learning can be leveraged to enable swift convergence during model training, expedite framework development, and reduce the amount of training data and augmentation required.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For transfer learning, a base architecture is selected, and the fully connected (FC) layers of a pretrained model are customized for the new task. We evaluated ImageNet-pretrained models from various families of CNN architectures known for their SOTA performance on various benchmarks. Our selection criteria for the base architecture focused on minimizing computational footprint while maximizing classification accuracy. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S4.T2" title="TABLE II ‣ IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table II</span></a> for details.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3">We replaced the original FC layers of the chosen architecture with a tailored classifier while retaining the primary convolutional layers and their pretrained weights to function as an encoder for feature extraction. Our classifier is trained from scratch and includes additional pooling, batch normalization, dropout, linear, and non-linear layers, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F4" title="Figure 4 ‣ III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.
This classifier effectively repurposes the features extracted by the encoder for gesture recognition and produces a set of probabilities, predicting the likelihood that the input image <math alttext="I_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">I</mi><mrow id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.1.m1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐼</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><times id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">I_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> belongs to any class <math alttext="C_{h},h\in[1,N]" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.4"><semantics id="S3.SS2.p3.2.m2.4a"><mrow id="S3.SS2.p3.2.m2.4.4" xref="S3.SS2.p3.2.m2.4.4.cmml"><mrow id="S3.SS2.p3.2.m2.4.4.1.1" xref="S3.SS2.p3.2.m2.4.4.1.2.cmml"><msub id="S3.SS2.p3.2.m2.4.4.1.1.1" xref="S3.SS2.p3.2.m2.4.4.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.4.4.1.1.1.2" xref="S3.SS2.p3.2.m2.4.4.1.1.1.2.cmml">C</mi><mi id="S3.SS2.p3.2.m2.4.4.1.1.1.3" xref="S3.SS2.p3.2.m2.4.4.1.1.1.3.cmml">h</mi></msub><mo id="S3.SS2.p3.2.m2.4.4.1.1.2" xref="S3.SS2.p3.2.m2.4.4.1.2.cmml">,</mo><mi id="S3.SS2.p3.2.m2.3.3" xref="S3.SS2.p3.2.m2.3.3.cmml">h</mi></mrow><mo id="S3.SS2.p3.2.m2.4.4.2" xref="S3.SS2.p3.2.m2.4.4.2.cmml">∈</mo><mrow id="S3.SS2.p3.2.m2.4.4.3.2" xref="S3.SS2.p3.2.m2.4.4.3.1.cmml"><mo id="S3.SS2.p3.2.m2.4.4.3.2.1" stretchy="false" xref="S3.SS2.p3.2.m2.4.4.3.1.cmml">[</mo><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">1</mn><mo id="S3.SS2.p3.2.m2.4.4.3.2.2" xref="S3.SS2.p3.2.m2.4.4.3.1.cmml">,</mo><mi id="S3.SS2.p3.2.m2.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml">N</mi><mo id="S3.SS2.p3.2.m2.4.4.3.2.3" stretchy="false" xref="S3.SS2.p3.2.m2.4.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.4b"><apply id="S3.SS2.p3.2.m2.4.4.cmml" xref="S3.SS2.p3.2.m2.4.4"><in id="S3.SS2.p3.2.m2.4.4.2.cmml" xref="S3.SS2.p3.2.m2.4.4.2"></in><list id="S3.SS2.p3.2.m2.4.4.1.2.cmml" xref="S3.SS2.p3.2.m2.4.4.1.1"><apply id="S3.SS2.p3.2.m2.4.4.1.1.1.cmml" xref="S3.SS2.p3.2.m2.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.4.4.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.4.4.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.4.4.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.4.4.1.1.1.2">𝐶</ci><ci id="S3.SS2.p3.2.m2.4.4.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.4.4.1.1.1.3">ℎ</ci></apply><ci id="S3.SS2.p3.2.m2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3">ℎ</ci></list><interval closure="closed" id="S3.SS2.p3.2.m2.4.4.3.1.cmml" xref="S3.SS2.p3.2.m2.4.4.3.2"><cn id="S3.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1">1</cn><ci id="S3.SS2.p3.2.m2.2.2.cmml" xref="S3.SS2.p3.2.m2.2.2">𝑁</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.4c">C_{h},h\in[1,N]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.4d">italic_C start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_h ∈ [ 1 , italic_N ]</annotation></semantics></math> within the set <math alttext="S^{*}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><msup id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">S</mi><mo id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑆</ci><times id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">S^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="383" id="S3.F4.g1" src="extracted/5904591/original-custom-classifiers.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of Classifiers: Original ImageNet Classifier (left) versus Tailored HGR Classifier (right).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">To leverage the various view orientations available for generating spatiotemporal images during temporal information condensation, we further engineered the end-to-end Ensemble Tuner Multi-Stream CNN Architecture illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F5" title="Figure 5 ‣ III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>. This architecture utilizes transfer learning and ensemble training to ensure both performance and practicality.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.5">For any gesture <math alttext="g_{i}\in S" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><msub id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2.2" xref="S3.SS2.p5.1.m1.1.1.2.2.cmml">g</mi><mi id="S3.SS2.p5.1.m1.1.1.2.3" xref="S3.SS2.p5.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">∈</mo><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><in id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></in><apply id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2.2">𝑔</ci><ci id="S3.SS2.p5.1.m1.1.1.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">g_{i}\in S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S</annotation></semantics></math>, data-level fusion produces <math alttext="{I_{ij}}_{j\text{=}1}^{6}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mmultiscripts id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2.2.2" xref="S3.SS2.p5.2.m2.1.1.2.2.2.cmml">I</mi><mrow id="S3.SS2.p5.2.m2.1.1.2.2.3" xref="S3.SS2.p5.2.m2.1.1.2.2.3.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2.2.3.2" xref="S3.SS2.p5.2.m2.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS2.p5.2.m2.1.1.2.2.3.1" xref="S3.SS2.p5.2.m2.1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.2.2.3.3" xref="S3.SS2.p5.2.m2.1.1.2.2.3.3.cmml">j</mi></mrow><mrow id="S3.SS2.p5.2.m2.1.1a" xref="S3.SS2.p5.2.m2.1.1.cmml"></mrow><mrow id="S3.SS2.p5.2.m2.1.1.2.3" xref="S3.SS2.p5.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2.3.2" xref="S3.SS2.p5.2.m2.1.1.2.3.2.cmml">j</mi><mo id="S3.SS2.p5.2.m2.1.1.2.3.1" xref="S3.SS2.p5.2.m2.1.1.2.3.1.cmml">⁢</mo><mtext id="S3.SS2.p5.2.m2.1.1.2.3.3" xref="S3.SS2.p5.2.m2.1.1.2.3.3a.cmml">=</mtext><mo id="S3.SS2.p5.2.m2.1.1.2.3.1a" xref="S3.SS2.p5.2.m2.1.1.2.3.1.cmml">⁢</mo><mn id="S3.SS2.p5.2.m2.1.1.2.3.4" xref="S3.SS2.p5.2.m2.1.1.2.3.4.cmml">1</mn></mrow><mn id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">6</mn></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.2.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p5.2.m2.1.1.2.2.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.2.2.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.2.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2.2.2">𝐼</ci><apply id="S3.SS2.p5.2.m2.1.1.2.2.3.cmml" xref="S3.SS2.p5.2.m2.1.1.2.2.3"><times id="S3.SS2.p5.2.m2.1.1.2.2.3.1.cmml" xref="S3.SS2.p5.2.m2.1.1.2.2.3.1"></times><ci id="S3.SS2.p5.2.m2.1.1.2.2.3.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2.2.3.2">𝑖</ci><ci id="S3.SS2.p5.2.m2.1.1.2.2.3.3.cmml" xref="S3.SS2.p5.2.m2.1.1.2.2.3.3">𝑗</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1.2.3.cmml" xref="S3.SS2.p5.2.m2.1.1.2.3"><times id="S3.SS2.p5.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p5.2.m2.1.1.2.3.1"></times><ci id="S3.SS2.p5.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2.3.2">𝑗</ci><ci id="S3.SS2.p5.2.m2.1.1.2.3.3a.cmml" xref="S3.SS2.p5.2.m2.1.1.2.3.3"><mtext id="S3.SS2.p5.2.m2.1.1.2.3.3.cmml" mathsize="70%" xref="S3.SS2.p5.2.m2.1.1.2.3.3">=</mtext></ci><cn id="S3.SS2.p5.2.m2.1.1.2.3.4.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.2.3.4">1</cn></apply></apply><cn id="S3.SS2.p5.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">{I_{ij}}_{j\text{=}1}^{6}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT</annotation></semantics></math> spatiotemporal images from any of the <math alttext="VO_{j},j\in[1,6]" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.4"><semantics id="S3.SS2.p5.3.m3.4a"><mrow id="S3.SS2.p5.3.m3.4.4" xref="S3.SS2.p5.3.m3.4.4.cmml"><mrow id="S3.SS2.p5.3.m3.4.4.1.1" xref="S3.SS2.p5.3.m3.4.4.1.2.cmml"><mrow id="S3.SS2.p5.3.m3.4.4.1.1.1" xref="S3.SS2.p5.3.m3.4.4.1.1.1.cmml"><mi id="S3.SS2.p5.3.m3.4.4.1.1.1.2" xref="S3.SS2.p5.3.m3.4.4.1.1.1.2.cmml">V</mi><mo id="S3.SS2.p5.3.m3.4.4.1.1.1.1" xref="S3.SS2.p5.3.m3.4.4.1.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p5.3.m3.4.4.1.1.1.3" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3.cmml"><mi id="S3.SS2.p5.3.m3.4.4.1.1.1.3.2" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3.2.cmml">O</mi><mi id="S3.SS2.p5.3.m3.4.4.1.1.1.3.3" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.SS2.p5.3.m3.4.4.1.1.2" xref="S3.SS2.p5.3.m3.4.4.1.2.cmml">,</mo><mi id="S3.SS2.p5.3.m3.3.3" xref="S3.SS2.p5.3.m3.3.3.cmml">j</mi></mrow><mo id="S3.SS2.p5.3.m3.4.4.2" xref="S3.SS2.p5.3.m3.4.4.2.cmml">∈</mo><mrow id="S3.SS2.p5.3.m3.4.4.3.2" xref="S3.SS2.p5.3.m3.4.4.3.1.cmml"><mo id="S3.SS2.p5.3.m3.4.4.3.2.1" stretchy="false" xref="S3.SS2.p5.3.m3.4.4.3.1.cmml">[</mo><mn id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">1</mn><mo id="S3.SS2.p5.3.m3.4.4.3.2.2" xref="S3.SS2.p5.3.m3.4.4.3.1.cmml">,</mo><mn id="S3.SS2.p5.3.m3.2.2" xref="S3.SS2.p5.3.m3.2.2.cmml">6</mn><mo id="S3.SS2.p5.3.m3.4.4.3.2.3" stretchy="false" xref="S3.SS2.p5.3.m3.4.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.4b"><apply id="S3.SS2.p5.3.m3.4.4.cmml" xref="S3.SS2.p5.3.m3.4.4"><in id="S3.SS2.p5.3.m3.4.4.2.cmml" xref="S3.SS2.p5.3.m3.4.4.2"></in><list id="S3.SS2.p5.3.m3.4.4.1.2.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1"><apply id="S3.SS2.p5.3.m3.4.4.1.1.1.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1"><times id="S3.SS2.p5.3.m3.4.4.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.1"></times><ci id="S3.SS2.p5.3.m3.4.4.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.2">𝑉</ci><apply id="S3.SS2.p5.3.m3.4.4.1.1.1.3.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.4.4.1.1.1.3.1.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.3.m3.4.4.1.1.1.3.2.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3.2">𝑂</ci><ci id="S3.SS2.p5.3.m3.4.4.1.1.1.3.3.cmml" xref="S3.SS2.p5.3.m3.4.4.1.1.1.3.3">𝑗</ci></apply></apply><ci id="S3.SS2.p5.3.m3.3.3.cmml" xref="S3.SS2.p5.3.m3.3.3">𝑗</ci></list><interval closure="closed" id="S3.SS2.p5.3.m3.4.4.3.1.cmml" xref="S3.SS2.p5.3.m3.4.4.3.2"><cn id="S3.SS2.p5.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p5.3.m3.1.1">1</cn><cn id="S3.SS2.p5.3.m3.2.2.cmml" type="integer" xref="S3.SS2.p5.3.m3.2.2">6</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.4c">VO_{j},j\in[1,6]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.4d">italic_V italic_O start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_j ∈ [ 1 , 6 ]</annotation></semantics></math> view orientations available. Each image <math alttext="I_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><msub id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">I</mi><mrow id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml"><mi id="S3.SS2.p5.4.m4.1.1.3.2" xref="S3.SS2.p5.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p5.4.m4.1.1.3.1" xref="S3.SS2.p5.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.4.m4.1.1.3.3" xref="S3.SS2.p5.4.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2">𝐼</ci><apply id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3"><times id="S3.SS2.p5.4.m4.1.1.3.1.cmml" xref="S3.SS2.p5.4.m4.1.1.3.1"></times><ci id="S3.SS2.p5.4.m4.1.1.3.2.cmml" xref="S3.SS2.p5.4.m4.1.1.3.2">𝑖</ci><ci id="S3.SS2.p5.4.m4.1.1.3.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">I_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is sequentially processed by the multi-stream encoder for feature extraction. The features are in turn processed by the multi-stream classifier in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.F4" title="Figure 4 ‣ III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> (right), producing a set of class probabilities <math alttext="{\{CP\}_{ij}^{h}}_{(h\text{=}1)}^{N}" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m5.2"><semantics id="S3.SS2.p5.5.m5.2a"><mmultiscripts id="S3.SS2.p5.5.m5.2.2" xref="S3.SS2.p5.5.m5.2.2.cmml"><mrow id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.2.cmml"><mo id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.3.cmml">P</mi></mrow><mo id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p5.5.m5.2.2.1.1.1.3" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p5.5.m5.2.2.1.1.1.3.2" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p5.5.m5.2.2.1.1.1.3.1" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.5.m5.2.2.1.1.1.3.3" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.3.cmml">j</mi></mrow><mi id="S3.SS2.p5.5.m5.2.2.1.1.3" xref="S3.SS2.p5.5.m5.2.2.1.1.3.cmml">h</mi><mrow id="S3.SS2.p5.5.m5.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.1.cmml"><mo id="S3.SS2.p5.5.m5.1.1.1.1.2" stretchy="false" xref="S3.SS2.p5.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.5.m5.1.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.1.1.1.2" xref="S3.SS2.p5.5.m5.1.1.1.1.1.2.cmml">h</mi><mo id="S3.SS2.p5.5.m5.1.1.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml">⁢</mo><mtext id="S3.SS2.p5.5.m5.1.1.1.1.1.3" xref="S3.SS2.p5.5.m5.1.1.1.1.1.3a.cmml">=</mtext><mo id="S3.SS2.p5.5.m5.1.1.1.1.1.1a" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml">⁢</mo><mn id="S3.SS2.p5.5.m5.1.1.1.1.1.4" xref="S3.SS2.p5.5.m5.1.1.1.1.1.4.cmml">1</mn></mrow><mo id="S3.SS2.p5.5.m5.1.1.1.1.3" stretchy="false" xref="S3.SS2.p5.5.m5.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS2.p5.5.m5.2.2.3" xref="S3.SS2.p5.5.m5.2.2.3.cmml">N</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.2b"><apply id="S3.SS2.p5.5.m5.2.2.cmml" xref="S3.SS2.p5.5.m5.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.2.2.2.cmml" xref="S3.SS2.p5.5.m5.2.2">superscript</csymbol><apply id="S3.SS2.p5.5.m5.2.2.1.cmml" xref="S3.SS2.p5.5.m5.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.2.2.1.2.cmml" xref="S3.SS2.p5.5.m5.2.2">subscript</csymbol><apply id="S3.SS2.p5.5.m5.2.2.1.1.cmml" xref="S3.SS2.p5.5.m5.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.2.2.1.1.2.cmml" xref="S3.SS2.p5.5.m5.2.2">superscript</csymbol><apply id="S3.SS2.p5.5.m5.2.2.1.1.1.cmml" xref="S3.SS2.p5.5.m5.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.2.2">subscript</csymbol><set id="S3.SS2.p5.5.m5.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1"><apply id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1"><times id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.1"></times><ci id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.2">𝐶</ci><ci id="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.1.1.1.3">𝑃</ci></apply></set><apply id="S3.SS2.p5.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3"><times id="S3.SS2.p5.5.m5.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.1"></times><ci id="S3.SS2.p5.5.m5.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p5.5.m5.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.1.3.3">𝑗</ci></apply></apply><ci id="S3.SS2.p5.5.m5.2.2.1.1.3.cmml" xref="S3.SS2.p5.5.m5.2.2.1.1.3">ℎ</ci></apply><apply id="S3.SS2.p5.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1"><times id="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1"></times><ci id="S3.SS2.p5.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1.2">ℎ</ci><ci id="S3.SS2.p5.5.m5.1.1.1.1.1.3a.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1.3"><mtext id="S3.SS2.p5.5.m5.1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p5.5.m5.1.1.1.1.1.3">=</mtext></ci><cn id="S3.SS2.p5.5.m5.1.1.1.1.1.4.cmml" type="integer" xref="S3.SS2.p5.5.m5.1.1.1.1.1.4">1</cn></apply></apply><ci id="S3.SS2.p5.5.m5.2.2.3.cmml" xref="S3.SS2.p5.5.m5.2.2.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.2c">{\{CP\}_{ij}^{h}}_{(h\text{=}1)}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m5.2d">{ italic_C italic_P } start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_h = 1 ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> for each image.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">The multi-stream encoder and classifier are shared across all inputs to reduce the network’s computational footprint. The multi-stream network (encoder and classifier) jointly learns the best weights for merging the view orientations guided by loss calculated for each image. Since each image yields a unique loss value, the chosen sequence (order and combination) of the input spatiotemporal images affects the performance of the multi-stream sub-network. This situation leads to an expanded search space for the optimal sequence.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">The class probabilities <math alttext="{\{CP\}_{ij}^{h}}_{(h\text{=}1)}^{N}" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.2"><semantics id="S3.SS2.p7.1.m1.2a"><mmultiscripts id="S3.SS2.p7.1.m1.2.2" xref="S3.SS2.p7.1.m1.2.2.cmml"><mrow id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.3.cmml">P</mi></mrow><mo id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p7.1.m1.2.2.1.1.1.3" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p7.1.m1.2.2.1.1.1.3.2" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p7.1.m1.2.2.1.1.1.3.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.2.2.1.1.1.3.3" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.3.cmml">j</mi></mrow><mi id="S3.SS2.p7.1.m1.2.2.1.1.3" xref="S3.SS2.p7.1.m1.2.2.1.1.3.cmml">h</mi><mrow id="S3.SS2.p7.1.m1.1.1.1.1" xref="S3.SS2.p7.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS2.p7.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p7.1.m1.1.1.1.1.1" xref="S3.SS2.p7.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p7.1.m1.1.1.1.1.1.2" xref="S3.SS2.p7.1.m1.1.1.1.1.1.2.cmml">h</mi><mo id="S3.SS2.p7.1.m1.1.1.1.1.1.1" xref="S3.SS2.p7.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mtext id="S3.SS2.p7.1.m1.1.1.1.1.1.3" xref="S3.SS2.p7.1.m1.1.1.1.1.1.3a.cmml">=</mtext><mo id="S3.SS2.p7.1.m1.1.1.1.1.1.1a" xref="S3.SS2.p7.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mn id="S3.SS2.p7.1.m1.1.1.1.1.1.4" xref="S3.SS2.p7.1.m1.1.1.1.1.1.4.cmml">1</mn></mrow><mo id="S3.SS2.p7.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS2.p7.1.m1.2.2.3" xref="S3.SS2.p7.1.m1.2.2.3.cmml">N</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.2b"><apply id="S3.SS2.p7.1.m1.2.2.cmml" xref="S3.SS2.p7.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.2.2.2.cmml" xref="S3.SS2.p7.1.m1.2.2">superscript</csymbol><apply id="S3.SS2.p7.1.m1.2.2.1.cmml" xref="S3.SS2.p7.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.2.2.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2">subscript</csymbol><apply id="S3.SS2.p7.1.m1.2.2.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2">superscript</csymbol><apply id="S3.SS2.p7.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2">subscript</csymbol><set id="S3.SS2.p7.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1"><apply id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1"><times id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.2">𝐶</ci><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.1.1.1.3">𝑃</ci></apply></set><apply id="S3.SS2.p7.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3"><times id="S3.SS2.p7.1.m1.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.1"></times><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.3">𝑗</ci></apply></apply><ci id="S3.SS2.p7.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.3">ℎ</ci></apply><apply id="S3.SS2.p7.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1.1.1"><times id="S3.SS2.p7.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1.1.1.1.1"></times><ci id="S3.SS2.p7.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.1.1.1.1.2">ℎ</ci><ci id="S3.SS2.p7.1.m1.1.1.1.1.1.3a.cmml" xref="S3.SS2.p7.1.m1.1.1.1.1.1.3"><mtext id="S3.SS2.p7.1.m1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p7.1.m1.1.1.1.1.1.3">=</mtext></ci><cn id="S3.SS2.p7.1.m1.1.1.1.1.1.4.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.1.1.1.4">1</cn></apply></apply><ci id="S3.SS2.p7.1.m1.2.2.3.cmml" xref="S3.SS2.p7.1.m1.2.2.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.2c">{\{CP\}_{ij}^{h}}_{(h\text{=}1)}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.2d">{ italic_C italic_P } start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_h = 1 ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> obtained from the multi-stream classifier are transformed into a single RGB pseudo-image through online decision-level fusion. This pseudo-image is processed by the ensemble tuner sub-network which produces the final set of class probabilities. The tuner sub-network employs a lightweight version of the base multi-stream architecture’s encoder while retaining a similar custom classifier.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.3">The ensemble tuner multi-stream CNN architecture undergoes end-to-end training, yielding <math alttext="(j+1)" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.1"><semantics id="S3.SS2.p8.1.m1.1a"><mrow id="S3.SS2.p8.1.m1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.cmml"><mo id="S3.SS2.p8.1.m1.1.1.1.2" stretchy="false" xref="S3.SS2.p8.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p8.1.m1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.1.1.2.cmml">j</mi><mo id="S3.SS2.p8.1.m1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p8.1.m1.1.1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p8.1.m1.1.1.1.3" stretchy="false" xref="S3.SS2.p8.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1"><plus id="S3.SS2.p8.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1"></plus><ci id="S3.SS2.p8.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.2">𝑗</ci><cn id="S3.SS2.p8.1.m1.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p8.1.m1.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">(j+1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.1d">( italic_j + 1 )</annotation></semantics></math> sets of losses and class probabilities for each gesture <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m2.1"><semantics id="S3.SS2.p8.2.m2.1a"><msub id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mi id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2">𝑔</ci><ci id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. To address the homoscedastic uncertainties resulting from the encoded images and pseudo-image processed by the multi-stream and ensemble tuner sub-networks respectively, we implemented a specialized loss function inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib36" title="">36</a>]</cite> that appropriately weighs the <math alttext="(j+1)" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m3.1"><semantics id="S3.SS2.p8.3.m3.1a"><mrow id="S3.SS2.p8.3.m3.1.1.1" xref="S3.SS2.p8.3.m3.1.1.1.1.cmml"><mo id="S3.SS2.p8.3.m3.1.1.1.2" stretchy="false" xref="S3.SS2.p8.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p8.3.m3.1.1.1.1" xref="S3.SS2.p8.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p8.3.m3.1.1.1.1.2" xref="S3.SS2.p8.3.m3.1.1.1.1.2.cmml">j</mi><mo id="S3.SS2.p8.3.m3.1.1.1.1.1" xref="S3.SS2.p8.3.m3.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p8.3.m3.1.1.1.1.3" xref="S3.SS2.p8.3.m3.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p8.3.m3.1.1.1.3" stretchy="false" xref="S3.SS2.p8.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.1b"><apply id="S3.SS2.p8.3.m3.1.1.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1.1"><plus id="S3.SS2.p8.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1.1.1.1"></plus><ci id="S3.SS2.p8.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p8.3.m3.1.1.1.1.2">𝑗</ci><cn id="S3.SS2.p8.3.m3.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p8.3.m3.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">(j+1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m3.1d">( italic_j + 1 )</annotation></semantics></math> cross-entropy losses. However, only the final class probabilities obtained from the ensemble tuner sub-network are considered for gesture classification.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">The multi-stream sub-network, by incorporating multiple inputs that are compact representations of the same dynamic gesture from different view orientations, can more accurately distinguish between gesture classes that would otherwise appear similar when visualized from a single view orientation.
In addition, using RGB pseudo-images for decision-level fusion in the ensemble tuner sub-network facilitates transfer learning and helps preserve semantic alignment among the outputs (and thus inputs) of the multi-stream sub-network.
Finally, integrating the multi-stream and ensemble tuner sub-networks into a single architecture eliminates the need to train multiple models separately, addressing a common limitation of conventional ensemble training approaches.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="593" id="S3.F5.g1" src="extracted/5904591/generalized-e2eet-network-architecture-3.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration Showing the Processing of Static Spatiotemporal Images by the Ensemble Tuner Multi-Stream CNN Architecture.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Framework Implementation &amp; Training</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Virtual Camera (elevation, azimuth) Angles (in degrees) For Each View Orientation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">View Orientation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">DHG1428</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">SHREC2017</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">FPHA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">LMDHG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">top-down</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">(0.0, 0.0)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">(0.0, 0.0)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">(90.0, 0.0)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">(0.0, 0.0)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">front-to</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">(90.0, 180.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">(90.0, 180.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">(0.0, 180.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">(-90.0, -180.0)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">front-away</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">(-90.0, 0.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">(-90.0, 0.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">(0.0, 0.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5">(90.0, 0.0)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">side-right</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">(0.0, -90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">(0.0, -90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4">(0.0, 90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.5">(0.0, 90.0)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">side-left</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2">(0.0, 90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">(0.0, 90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.4">(0.0, -90.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.5">(0.0, -90.0)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.7.6.1">custom</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.7.6.2">(30.0, -132.5)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.7.6.3">(30.0, -132.5)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.7.6.4">(25.0, 115.0)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.7.6.5">(-15.0, -135.0)</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparative Analysis of Various Pre-trained CNN Architectures.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">CNN Architecture</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Classification Accuracies (%)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.2.1">Family</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.2.2">Variants</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.3">TS1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.4">TS2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.5">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.3.1" rowspan="5"><span class="ltx_text" id="S4.T2.1.3.3.1.1">ResNet</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.3.2">ResNet18</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.3">80.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.4">77.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.5" rowspan="5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.3.5.1">81.49</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.4.1">ResNet34</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.2">81.90</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.3">79.52</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1">ResNet50</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.5.2.1">84.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.3">80.12</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1">ResNet101</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.2">82.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.6.3.1">82.86</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.7.1">ResNet152</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.2">83.10</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.3">82.50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.8.8.1" rowspan="4"><span class="ltx_text" id="S4.T2.1.8.8.1.1">Inception</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.8.8.2">Inception-v3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.8.8.3">81.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.8.8.4">77.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.8.8.5" rowspan="4"><span class="ltx_text" id="S4.T2.1.8.8.5.1">81.24</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.9.9.1">Inception-v4</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.2">83.10</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.3">79.64</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.10.1">Inception-ResNet-v1</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.2">81.79</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.3">82.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.11.11.1">Inception-ResNet-v2</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.11.2">81.67</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.11.3">81.90</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.12.12.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.12.12.1.1">EfficientNet</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.12.12.2">EfficientNet-B0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.12.12.3">81.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.12.12.4">79.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.12.12.5" rowspan="2"><span class="ltx_text" id="S4.T2.1.12.12.5.1">80.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.13.13.1">EfficientNet-B3</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.13.2">81.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.13.3">81.07</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.14.14.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.14.14.1.1">ResNeXt</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.14.14.2">ResNeXt26</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.14.3">80.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.14.4">77.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.14.5" rowspan="3"><span class="ltx_text" id="S4.T2.1.14.14.5.1">80.42</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.15.15.1">ResNeXt50</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.15.2">82.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.15.3">79.52</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.16.16.1">ResNeXt101</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.16.2">82.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.16.3">80.00</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.17.17.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.17.17.1.1">SE-ResNeXt</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.17.17.2">SE-ResNeXt50</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.17.17.3">81.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.17.17.4">75.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.17.17.5" rowspan="2"><span class="ltx_text" id="S4.T2.1.17.17.5.1">80.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.18.18.1">SE-ResNeXt101</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.18.2">84.05</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.18.3">79.88</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.19.19.1" rowspan="5"><span class="ltx_text" id="S4.T2.1.19.19.1.1">SE-ResNet</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.19.19.2">SE-ResNet18</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.19.19.3">78.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.19.19.4">77.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.19.19.5" rowspan="5"><span class="ltx_text" id="S4.T2.1.19.19.5.1">80.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.20.20.1">SE-ResNet26</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.20.2">80.48</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.20.3">77.26</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.21.21.1">SE-ResNet50</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.21.2">81.79</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.21.3">80.24</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.22.22.1">SE-ResNet101</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.22.2">83.10</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.22.3">81.31</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.23.23.1">SE-ResNet152</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.23.2">80.60</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.23.3">81.90</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.24.24.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.24.24.1.1">xResNet</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.24.24.2">xResNet50</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.24.24.3">74.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.24.24.4">73.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.24.24.5" rowspan="3"><span class="ltx_text" id="S4.T2.1.24.24.5.1">73.79</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.25.25.1">xResNet50-Deep</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.25.2">72.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.25.3">73.57</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.26.26.1">xResNet50-Deeper</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.26.26.2">74.05</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.26.26.3">75.12</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Benchmark Datasets</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our proposed framework was evaluated against SOTA frameworks on the following benchmark datasets, each presenting unique challenges for the development and assessment of gesture recognition frameworks and applications.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">CNR</span>: The CNR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> includes spatiotemporal images captured from a top-down view, covering 1925 gesture sequences across 16 classes. This dataset is split into a training set with 1348 images and a validation set with 577 images. The absence of raw skeleton data presents a limitation as our framework cannot utilize the full capabilities of its multi-stream CNN architecture.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">LMDHG</span>: Comprising 608 gesture sequences over 13 classes, the LMDHG dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib29" title="">29</a>]</cite> is divided into training (414 gestures) and validation (194 gestures) sets. The dataset exhibits minimal overlap in subjects across both subsets, featuring comprehensive 3D skeleton data comprising a total of 46 hand joints for each hand.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">FPHA</span>: This dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib28" title="">28</a>]</cite>, featuring a wide range of styles, viewpoints, and scenarios, includes 1175 gesture sequences across 45 classes. Its primary challenges stem from the similarity in motion patterns, the diverse range of objects involved, and a low ratio of gesture sequences to classes. The dataset is divided into 600 gestures for training and 575 for validation, providing 3D skeleton data for 21 hand joints per subject.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">SHREC2017</span>: With 2800 sequences performed by 28 subjects, the SHREC2017 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib26" title="">26</a>]</cite> is designed for both coarse and fine-grained gesture classification, divided into 14-gesture (14G) and 28-gesture (28G) benchmarks. The dataset provides 3D skeleton data for 22 hand joints and employs a 70:30 random-split protocol for the training (1960 gestures) and validation (840 gestures) subsets.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">DHG1428</span>: Following a structure similar to SHREC2017, the DHG1428 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib27" title="">27</a>]</cite> comprises 2800 sequences performance by 20 subjects for the 14G and 28G benchmarks. It provides equivalent skeleton data and follows the same 70:30 split for training and validation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">SBUKID</span>: This smaller HAR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib37" title="">37</a>]</cite> contains 282 action sequences across eight classes involving two-person interaction. SBUKID provides skeleton data for 15 joints per subject and uses a five-fold cross-validation evaluation protocol, with average accuracies reported across all folds.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Data-Level Fusion</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our proposed HGR framework, in its generalized form, incorporates data-level fusion to create static 2D spatiotemporal images combined with the specialized end-to-end Ensemble Tuner (e2eET) CNN architecture for classifying said images. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S4.T1" title="TABLE I ‣ IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table I</span></a> shows that the elevation and azimuth angles of the virtual camera required for the view orientations during data-level fusion are dataset-specific. The same padding value <math alttext="\gamma=0.125" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">γ</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">0.125</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝛾</ci><cn id="S4.SS2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1.3">0.125</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\gamma=0.125</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_γ = 0.125</annotation></semantics></math> was used for all datasets during sequence fitting. Note that the aforementioned settings do not apply to the CNR dataset as it only provides encoded images.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">e2eET Multi-Stream Network</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For the choice of base architecture, the performance of 26 CNN architectures from the ResNet, Inception, EfficientNet, ResNeXt, SE-ResNeXt, SE-ResNet, and xResNet families was evaluated as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S4.T2" title="TABLE II ‣ IV Framework Implementation &amp; Training ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table II</span></a>.
ImageNet-pretrained models for said architectures were modified as described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.SS2" title="III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">III-B</span></span></a> and evaluated in single-stream mode on spatiotemporal images generated from the DHG1428 dataset with the ‘front-to’ view orientation.
From the empirical results for two distinct training setups (TS1 and TS2), the ResNet variants ResNet-50 and ResNet-18 were chosen for the multi-stream and ensemble tuner sub-networks, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">As also discussed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S3.SS2" title="III-B Multi-Stream CNN Architecture ‣ III Dynamic Hand Gesture Recognition Framework ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">III-B</span></span></a>, the optimal sequence of spatiotemporal inputs to the multi-stream sub-network is dataset-specific due to the specifics of how the raw gestures were collected and the skeleton data extracted.
Our experiments showed that combinations of three unique VOs are adequate for reaching SOTA performance for gesture recognition. Thus, we employed the following iterative approach to determine the optimal sequence of VOs for each dataset:</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Each VO was trained separately in a single-stream network.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Paired combinations of the top-performing VOs were trained in a two-stream network.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Triple combinations of the top-performing VO pairs were trained in a three-stream network.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">The top-performing VO triplet was chosen as the optimal sequence for the dataset.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Model Training</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The developed framework was developed using Python 3.8.5 and tested on a Linux Ubuntu 18.04 server with four NVIDIA GeForce GTX TITAN X graphics cards.
For data-level fusion, the <a class="ltx_ref ltx_href" href="https://vispy.org/" title="">Vispy</a> visualization library was utilized, whereas <a class="ltx_ref ltx_href" href="https://opencv.org/" title="">OpenCV</a> was employed to generate pseudo-images for decision-level fusion.
Furthermore, the entire machine learning workflow, including the design of network architectures and the training of models, was carried out using <a class="ltx_ref ltx_href" href="https://pytorch.org/" title="">PyTorch</a> and <a class="ltx_ref ltx_href" href="https://www.fast.ai/" title="">FastAI</a>.
The code for this paper can be found at this GitHub repository: <a class="ltx_ref ltx_href" href="https://github.com/Outsiders17711/e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion" title="">https://github.com/Outsiders17711/e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion</a>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">During data-level fusion, each gesture sequence was resampled to the same temporal window <math alttext="T=250" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">T</mi><mo id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><eq id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1"></eq><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">𝑇</ci><cn id="S4.SS4.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.p2.1.m1.1.1.3">250</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">T=250</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_T = 250</annotation></semantics></math>. The encoded images and pseudo-images were generated with a 1:1 aspect ratio at 960px and 224px, respectively.
During the training phase, we used a batch size of 16, the Adam optimizer, and a custom homoscedastic cross-entropy loss function. The training was conducted in multiple stages, with the dimensions of the static input images progressively increasing from 224px to 276px, 328px, and 380px.
The cosine annealing schedule was applied to adjust the learning rate, with the initial rate for each stage automatically determined using FastAI’s <a class="ltx_ref ltx_href" href="https://docs.fast.ai/callback.schedule.html#learner.lr_find" title="">learner.lr_find</a> method.
Furthermore, various data augmentation techniques were applied during the training process, including random horizontal flips, affine transformations, perspective warping, random zooms, random rotations, and adjustments in colour space.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation &amp; Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section presents comparative evaluations of our proposed framework against other frameworks on the benchmark datasets. We also present the results of our ablation study on the efficacy of our framework in the human action recognition domain.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Evaluation: CNR Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For the CNR dataset (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T3" title="TABLE III ‣ V-A Evaluation: CNR Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table III</span></a>), our framework had a slightly lower performance than the SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite> by -1.73%. This performance drop was due to the absence of the raw skeleton data for the CNR dataset, which prevented our framework from leveraging the enhancements in temporal information condensation and our specialized multi-stream network.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of Validation Accuracy with SOTA on the CNR Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.2">Classification Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.2.1.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">97.05</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.3.2.1">Lupinetti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.2.1">98.78</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Evaluation: LMDHG Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">For the LMDHG dataset (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T4" title="TABLE IV ‣ V-B Evaluation: LMDHG Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table IV</span></a>), our framework surpassed the SOTA results presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>]</cite> by +5.16%, attributed to the utilization of our improved data-level fusion approach for generating spatiotemporal images. These evaluation results highlight the effectiveness of our data-level fusion enhancements and the subsequent shift from dynamic hand gesture recognition to static image classification. In addition, our transformation process effectively preserved crucial semantic information by utilizing an optimal sequence of VOs—[custom, front-away, top-down].</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of Validation Accuracy with SOTA on the LMDHG Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.2">Classification Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.1">Boulahia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib29" title="">29</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.2">84.78</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.2.1">Lupinetti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.2">92.11</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.4.3.1">Mohammed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2">93.81</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S5.T4.1.5.4.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.5.4.2.1">98.97</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Evaluation: FPHA Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The FPHA dataset is particularly challenging for HGR evaluation due to the low ratio of gesture sequences (1175) to gesture classes (45). This difficulty is compounded by the 1:1 evaluation protocol, which results in closely balanced training and validation proportions. As shown in (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T5" title="TABLE V ‣ V-C Evaluation: FPHA Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table V</span></a>), our framework fell short of the SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib31" title="">31</a>]</cite> by -4.10%, even with the optimal sequence of VOs—[front-away, custom, top-down].</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Comparison of Validation Accuracy with SOTA on the FPHA Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.2">Classification Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.1.1">Sahbi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib15" title="">15</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.2">86.78</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.3.2.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.2.2">87.32</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.4.3.1">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.2">89.04</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.5.4.1">Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.2">89.04</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.6.5.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5.2">90.26</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.7.6.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.6.2">91.83</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.8.7.1"><span class="ltx_text ltx_font_italic" id="S5.T5.1.8.7.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.8.7.2">91.83</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.9.8.1">Narayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.8.2">92.48</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.10.9.1">Rehan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib5" title="">5</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.9.2">93.91</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.11.10.1">Sabater et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib31" title="">31</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.11.10.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.11.10.2.1">95.93</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Evaluation: SHREC2017 Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">For the SHREC2017 dataset, the optimal sequence of VOs—[front-away, custom, front-to]—resulted in 14G and 28G validation accuracies of 97.86% and 95.36%, respectively. The results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T6" title="TABLE VI ‣ V-D Evaluation: SHREC2017 Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table VI</span></a> show our framework tied with the SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib23" title="">23</a>]</cite> with differences of +0.24% and -0.47% respectively.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Comparison of Validation Accuracy with SOTA on the SHREC2017 Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T6.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T6.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T6.1.1.1.2">Classification Accuracy (%)</th>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.2.1">14G</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.2.2">28G</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.2.3">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.3.1.1">Aiman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib14" title="">14</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.2">94.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.3">89.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.4">91.72</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.4.2.1">Mahmud et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.2">93.81</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.3">90.24</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.4">92.03</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.5.3.1">Sabater et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib31" title="">31</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.2">93.57</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.3">91.43</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.4">92.50</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.6.4.1">Balaji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.2">94.17</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.3">93.21</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.4">93.69</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.7.5.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.7.5.2">94.84</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.7.5.3">92.56</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.7.5.4">93.70</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.8.6.1">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.8.6.2">95.00</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.8.6.3">92.86</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.8.6.4">93.93</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.9.7.1">Rehan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib5" title="">5</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.9.7.2">95.60</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.9.7.3">92.74</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.9.7.4">94.17</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.10.8.1">Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.10.8.2">95.36</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.10.8.3">93.10</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.10.8.4">94.23</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.11.9.1">Mohammed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.11.9.2">95.60</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.11.9.3">93.10</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.11.9.4">94.35</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.12.10.1">Narayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.12.10.2">97.00</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.12.10.3">92.36</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.12.10.4">94.68</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.13.11.1">Deng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib1" title="">1</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.13.11.2">96.40</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.13.11.3">93.30</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.13.11.4">94.85</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.14.12.1">Miah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib4" title="">4</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.14.12.2">97.01</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.14.12.3">92.78</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.14.12.4">94.90</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.15.13.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.15.13.2">96.90</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.15.13.3">94.17</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.15.13.4">95.53</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.16.14.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.16.14.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.16.14.2">97.86</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.16.14.3">95.36</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.16.14.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.16.14.4.1">96.61</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.17.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.1.17.15.1">Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib23" title="">23</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.17.15.2">97.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.17.15.3">95.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.17.15.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.17.15.4.1">96.72</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Comparison of Validation Accuracy with SOTA on the DHG1428 Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T7.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T7.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T7.1.1.1.2">Classification Accuracy (%)</th>
</tr>
<tr class="ltx_tr" id="S5.T7.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.1.2.2.1">14G</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.1.2.2.2">28G</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.1.2.2.3">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.3.1.1">Aiman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib14" title="">14</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.1.2">90.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.1.3">88.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.1.4">89.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.4.2.1">Mahmud et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib10" title="">10</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.2.2">90.82</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.2.3">89.21</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.2.4">90.01</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.5.3.1">Miah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib4" title="">4</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.3.2">92.00</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.3.3">88.78</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.3.4">90.39</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.6.4.1">Mohammed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib19" title="">19</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.4.2">91.64</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.4.3">89.46</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.4.4">90.55</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.7.5.1">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.7.5.2">92.71</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.7.5.3">89.15</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.7.5.4">90.93</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.8.6.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.6.2">92.36</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.6.3">89.56</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.6.4">90.96</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.9.7.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib7" title="">7</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.7.2">94.21</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.7.3">92.11</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.7.4">93.16</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.10.8.1">Narayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib20" title="">20</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.8.2">94.64</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.8.3">91.79</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.8.4">93.22</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.11.9.1">Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib23" title="">23</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.11.9.2">94.82</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.11.9.3">93.18</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.11.9.4">94.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.12.10.1">Balaji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib8" title="">8</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.12.10.2">94.11</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.12.10.3">93.88</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.12.10.4">94.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.13.11.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.13.11.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.13.11.2">95.83</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.13.11.3">92.38</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.13.11.4">94.11</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.14.12.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib2" title="">2</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.14.12.2">96.31</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.14.12.3">94.05</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.14.12.4">95.18</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T7.1.15.13.1">Tripathi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.15.13.2">98.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.15.13.3">94.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.15.13.4"><span class="ltx_text ltx_font_bold" id="S5.T7.1.15.13.4.1">96.15</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">Evaluation: DHG1428 Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">For the DHG1428 dataset, the optimal sequence of view orientations—[custom, top-down, front-away]—produced 14G and 28G validation accuracies of 95.83% and 92.38%, respectively. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T7" title="TABLE VII ‣ V-D Evaluation: SHREC2017 Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table VII</span></a>, our framework exhibited a marginally lower performance than the SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib12" title="">12</a>]</cite> by -2.27% and -1.82%, respectively.
While the SHREC2017 and DHG1428 datasets share similarities, DHG1428 offers an equal distribution of subjects across all classes. Thus, classification accuracies for DHG1428 14G and 28G are consistently lower in the literature compared to SHREC2017, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T6" title="TABLE VI ‣ V-D Evaluation: SHREC2017 Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table VI</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T7" title="TABLE VII ‣ V-D Evaluation: SHREC2017 Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table VII</span></a>. Our framework follows this trend, with DHG1428 14G and 28G results being -2.03% and -2.98% lower than their SHREC2017 equivalents.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.F6" title="Figure 6 ‣ V-E Evaluation: DHG1428 Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> presents the confusion matrix of our framework’s performance on the DHG1428 28G dataset. The robust validation accuracy of 92.38% indicates a significant alignment between the actual gesture classes and the framework’s predictions. In the figure, class labels are augmented with numerical prefixes to differentiate between the DHG1428 performance modes, specifically denoting whether the gesture was executed with one finger (01-14) or the entire hand (15-28).
Consistent with the observations in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib22" title="">22</a>]</cite>, our analysis underscores a significant challenge in differentiating between the “Grab” and “Pinch” gesture classes across both 14G and 28G performance modes. This difficulty becomes evident when a visual inspection of spatiotemporal images from these two gesture classes reveals a visual similarity substantial enough to confuse the human eye.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="593" id="S5.F6.g1" src="extracted/5904591/confusion-matrix-dhg1428-28g-3.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Confusion Matrix for the Proposed Framework on the DHG1428 28G Dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.5.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.6.2">Ablation Study: SBUKID Dataset</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">Due to the design of the data-level fusion process, we believe our proposed framework is domain-agnostic and is similarly applicable to other domains involving the classification of dynamic temporal data in the form of 2D/3D coordinates. To test this hypothesis, we adapted our framework to the skeleton-based human action recognition domain, making slight alterations to the data-level fusion process only. Similar to HGR, HAR enables computers to recognize and interpret dynamic human actions by using the entire human body as input.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">We generated spatiotemporal datasets from the SBUKID dataset using all six VOs and used them to train single-stream versions of the multi-stream sub-network from our specialized e2eET CNN architecture. The evaluation results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S5.T8" title="TABLE VIII ‣ V-F Ablation Study: SBUKID Dataset ‣ V Evaluation &amp; Discussion ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Table VIII</span></a> show that our framework compares favourably with frameworks designed specifically for action recognition. The best classification accuracy was obtained from the [front-away] VO, which was only -4.34% lower than the SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">This ablation study provides empirical evidence supporting the effectiveness of our data-level fusion design in transforming temporal dynamic data from various domains into spatiotemporal images for accurate classification with lower computational requirements.</p>
</div>
<figure class="ltx_table" id="S5.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Comparison of Validation Accuracy with SOTA on the SBUKID Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T8.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T8.1.1.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T8.1.1.1.2">Average Cross-Validation</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T8.1.2.2.1">Classification Accuracy (%)</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T8.1.3.3.1">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib39" title="">39</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.3.3.2">93.50</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T8.1.4.4.1">Kacem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib40" title="">40</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.4.4.2">93.70</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T8.1.5.5.1"><span class="ltx_text ltx_font_italic" id="S5.T8.1.5.5.1.1">e2eET (Ours)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.5.5.2">93.96</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T8.1.6.6.1">Maghoumi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.6.6.2">95.70</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T8.1.7.7.1">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib38" title="">38</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.7.7.2.1">98.30</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Real-Time Application</span>
</h2>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S6.F7.g1" src="extracted/5904591/live-stream-demo-swipe+-screenshot-4.png" width="533"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Demonstration of the Real-Time HGR Application, Based on Our Proposed Dynamic HGR Framework.</figcaption>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To demonstrate the practical utility of our proposed framework in reducing hardware and computational demands for HGR-based applications, we developed a real-time HGR application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib25" title="">25</a>]</cite> based on a generalized version of our framework. The gesture recognition in the application was powered by an e2eET multi-stream model trained on the subset of “Swipe” gestures from the DHG1428 dataset: “Up”, “Down”, “Right”, “Left”, “+”, “V”, and “X”. The real-time operational pipeline of the prototype application is depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.15003v2#S6.F7" title="Figure 7 ‣ VI Real-Time Application ‣ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN"><span class="ltx_text ltx_ref_tag">Figure 7</span></a> and can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S6.p2">
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">The raw dynamic gesture data is captured as RGB videos using the inbuilt PC webcam.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">The skeleton data is extracted from the video frames using the MediaPipe Hands pipeline.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">Using data-level fusion (temporal information condensation), three spatiotemporal images are generated from [custom, top-down, front-away] VOs only.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1">These spatiotemporal images are then processed by the trained model, which produces four class predictions: three from the multi-stream sub-network and one from the ensemble tuner sub-network.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p" id="S6.I1.i5.p1.1">The application output—class predictions, input gesture statistics, application latency—are displayed in the graphical user interface.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The prototype real-time application demonstrates that a standard PC webcam is sufficient for capturing raw dynamic gesture data in real-world scenarios, eliminating the need for specialized and costly sensors.
Tested on a PC with an Intel Core i7-9750H CPU and 16GB of RAM, the application captured raw gestures at 15 frames per second and maintained a latency of  2 seconds from the user’s execution of the gesture to the display of its output.
Compared to other applications running on the PC, the application’s CPU and RAM consumption was acceptable, having a negligible impact on the PC’s functionality and allowing other applications to run smoothly simultaneously <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.15003v2#bib.bib25" title="">25</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusions &amp; Future Work</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Conclusions</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">This paper explores the practical utility of existing hand gesture recognition (HGR) frameworks for real-time applications in real-world scenarios. To address the limitations posed by the considerable hardware and computational demands of these frameworks, we introduce a robust skeleton-based framework that efficiently converts the recognition of dynamic hand gestures into static image classification, while preserving crucial semantic details.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">The framework incorporates an improved data-level fusion technique to generate static RGB spatiotemporal images from skeleton data of dynamic gestures and leverages a specialized end-to-end Ensemble Tuner (e2eET) Multi-Stream CNN architecture for classification. This architecture incorporates semantic information from multiple view orientations for accurate image classification while maintaining computational efficiency.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">The effectiveness and generalization of the proposed framework were extensively evaluated on five benchmark datasets: SHREC’17, DHG-14/28, FPHA, LMDHG, and CNR. The results demonstrated its competitive performance, with accuracies ranging from -4.10% to +5.16% compared to current state-of-the-art benchmarks. Furthermore, exploratory ablation studies in the human action recognition domain demonstrated the framework’s robust capability in processing temporal dynamic data across varied applications.</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">The practical utility of the framework was underscored by the development of a real-time HGR application, which operates with standard PC hardware using a built-in webcam and maintains low resource (CPU &amp; RAM) consumption. This successful implementation showcases the potential of data-level fusion to substantially reduce hardware and computational demands without sacrificing performance, making it a viable solution for real-time dynamic gesture recognition across multiple domains.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">Future Work</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Expanding our skeleton-based approach to human action recognition presents a logical progression, leveraging similarities between HGR and HAR to categorize human actions using skeletal data.
Enhancing the multi-stream network architecture by integrating attention mechanisms will eliminate the need for dataset-specific optimal sequences of view orientations and could also improve overall performance.
Further efforts should also concentrate on optimizing computational efficiency through tailored ML/DL optimization methods, enhancing both the performance and effectiveness of our framework.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Testing the framework and its applications in real-world scenarios, especially in healthcare or virtual reality, is crucial for gathering insights into its practicality and identifying areas for refinement.
A comprehensive evaluation of user experience is equally important, as it will provide essential feedback on usability that will shape future iterations of the framework. These steps will not only advance dynamic hand gesture recognition but also deepen our understanding of its applications and improve user-centric development.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Z. Deng, Q. Gao, Z. Ju, and X. Yu, “Skeleton-Based Multifeatures and Multistream Network for Real-Time Action Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE Sensors Journal</em>, vol. 23, no. 7, pp. 7397–7409, Apr. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Li, S. Li, Y. Gao, X. Zhang, and W. Li, “A Two-stream Neural Network for Pose-based Hand Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv:2101.08926 [cs]</em>, Jan. 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2101.08926" title="">http://arxiv.org/abs/2101.08926</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Li, Z. Liu, G. Duan, and J. Tan, “MVHANet: Multi-view hierarchical aggregation network for skeleton-based hand gesture recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Signal, Image and Video Processing</em>, vol. 17, no. 5, pp. 2521–2529, Jul. 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11760-022-02469-9" title="">https://doi.org/10.1007/s11760-022-02469-9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. S. M. Miah, M. A. M. Hasan, and J. Shin, “Dynamic Hand Gesture Recognition Using Multi-Branch Attention Based Graph and General Deep Learning Model,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Access</em>, vol. 11, pp. 4703–4716, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Rehan, H. Wannous, J. Alkheir, and K. Aboukassem, “Learning Co-occurrence Features Across Spatial and Temporal Domains for Hand Gesture Recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 19th International Conference on Content-based Multimedia Indexing</em>, ser. CBMI ’22.   New York, NY, USA: Association for Computing Machinery, Oct. 2022, pp. 36–42. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3549555.3549591" title="">https://dl.acm.org/doi/10.1145/3549555.3549591</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.-H. Song, K. Kong, and S.-J. Kang, “Dynamic Hand Gesture Recognition Using Improved Spatio-Temporal Graph Convolutional Network,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, vol. 32, no. 9, pp. 6227–6239, Sep. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Li, G. Wei, C. Desrosiers, and Y. Zhou, “Decoupled and boosted learning for skeleton-based dynamic hand gesture recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Pattern Recognition</em>, vol. 153, p. 110536, Sep. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0031320324002875" title="">https://www.sciencedirect.com/science/article/pii/S0031320324002875</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Balaji and M. Ranjan Prusty, “Multimodal fusion hierarchical self-attention network for dynamic hand gesture recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Journal of Visual Communication and Image Representation</em>, vol. 98, p. 104019, Feb. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1047320323002699" title="">https://www.sciencedirect.com/science/article/pii/S1047320323002699</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “TMMF: Temporal Multi-Modal Fusion for Single-Stage Continuous Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Image Processing</em>, vol. 30, pp. 7689–7701, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H. Mahmud, M. M. Morshed, and M. K. Hasan, “Quantized depth image and skeleton-based multimodal dynamic hand gesture recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">The Visual Computer</em>, vol. 40, no. 1, pp. 11–25, Jan. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00371-022-02762-1" title="">https://doi.org/10.1007/s00371-022-02762-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Yu, B. Zhou, J. Wan, P. Wang, H. Chen, X. Liu, S. Z. Li, and G. Zhao, “Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE Transactions on Image Processing</em>, vol. 30, pp. 5626–5640, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. Tripathi and B. Verma, “Motion feature estimation using bi-directional GRU for skeleton-based dynamic hand gesture recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Signal, Image and Video Processing</em>, vol. 18, no. 1, pp. 299–308, Aug. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11760-024-03153-w" title="">https://doi.org/10.1007/s11760-024-03153-w</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
O. Köpüklü, N. Köse, and G. Rigoll, “Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, Jun. 2018, pp. 2184–21 848. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8575454" title="">https://ieeexplore.ieee.org/document/8575454</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
U. Aiman and T. Ahmad, “Angle based hand gesture recognition using graph convolutional network,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Computer Animation and Virtual Worlds</em>, vol. 35, no. 1, p. e2207, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.2207" title="">https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.2207</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Sahbi, “Skeleton-based Hand-Gesture Recognition with Lightweight Graph Convolutional Networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv:2104.04255 [cs]</em>, Apr. 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2104.04255" title="">http://arxiv.org/abs/2104.04255</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Liu, Y. Wang, S. Xiang, and C. Pan, “HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv:2106.13391 [cs]</em>, Jun. 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.13391" title="">http://arxiv.org/abs/2106.13391</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Lupinetti, A. Ranieri, F. Giannini, and M. Monti, “3D Dynamic Hand Gestures Recognition Using the Leap Motion Sensor and Convolutional Neural Networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Augmented Reality, Virtual Reality, and Computer Graphics</em>, L. T. De Paolis and P. Bourdot, Eds.   Cham: Springer International Publishing, 2020, pp. 420–439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. C. dos Santos, J. L. A. Samatelo, and R. F. Vassallo, “Dynamic Gesture Recognition by Using CNNs and Star RGB: A Temporal Information Condensation,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Neurocomputing</em>, vol. 400, pp. 238–254, Aug. 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S092523122030391X" title="">https://www.sciencedirect.com/science/article/pii/S092523122030391X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Mohammed, Y. Gao, Z. Ji, J. Lv, S. Islam, and Y. Sang, “Automatic 3D Skeleton-based Dynamic Hand Gesture Recognition Using Multi-Layer Convolutional LSTM,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 7th International Conference on Robotics and Artificial Intelligence</em>, ser. ICRAI ’21.   New York, NY, USA: Association for Computing Machinery, Apr. 2022, pp. 8–14. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3505688.3505690" title="">https://dl.acm.org/doi/10.1145/3505688.3505690</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Narayan, A. P. Mazumdar, and S. K. Vipparthi, “SBI-DHGR: Skeleton-based intelligent dynamic hand gestures recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Expert Systems with Applications</em>, vol. 232, p. 120735, Dec. 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S095741742301237X" title="">https://www.sciencedirect.com/science/article/pii/S095741742301237X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. P. Singh and L. D. Singh, “Dyhand: Dynamic hand gesture recognition using BiLSTM and soft attention methods,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">The Visual Computer</em>, Mar. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00371-024-03307-4" title="">https://doi.org/10.1007/s00371-024-03307-4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Chen, C. Zhao, Q. Wang, and H. Meng, “HMANet: Hyperbolic Manifold Aware Network for Skeleton-Based Action Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Transactions on Cognitive and Developmental Systems</em>, vol. 15, no. 2, pp. 602–614, Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Zhao, H. Li, and S. Yan, “Spatial–Temporal Synchronous Transformer for Skeleton-Based Hand Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, vol. 34, no. 3, pp. 1403–1412, Mar. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10182358" title="">https://ieeexplore.ieee.org/document/10182358</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Floris, S. Porcu, and L. Atzori, “Controlling Media Player with Hands: A Transformer Approach and a Quality of Experience Assessment,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ACM Trans. Multimedia Comput. Commun. Appl.</em>, vol. 20, no. 5, pp. 132:1–132:22, Jan. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3638560" title="">https://doi.org/10.1145/3638560</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
O. Yusuf and M. Habib, “Development of a Lightweight Real-Time Application for Dynamic Hand Gesture Recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2023 IEEE International Conference on Mechatronics and Automation (ICMA)</em>, Aug. 2023, pp. 543–548.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Q. De Smedt, H. Wannous, J.-P. Vandeborre, J. Guerry, B. Le Saux, and D. Filliat, “SHREC’17 Track: 3D Hand Gesture Recognition Using a Depth and Skeletal Dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">3DOR - 10th Eurographics Workshop on 3D Object Retrieval</em>, I. Pratikakis, F. Dupont, and M. Ovsjanikov, Eds., Lyon, France, Apr. 2017, pp. 1–6. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hal.archives-ouvertes.fr/hal-01563505" title="">https://hal.archives-ouvertes.fr/hal-01563505</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Q. De Smedt, H. Wannous, and J.-P. Vandeborre, “Skeleton-Based Dynamic Hand Gesture Recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>.   Las Vegas, NV, USA: IEEE, Jun. 2016, pp. 1206–1214. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/7789643/" title="">http://ieeexplore.ieee.org/document/7789643/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim, “First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv:1704.02463 [cs]</em>, Apr. 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1704.02463" title="">http://arxiv.org/abs/1704.02463</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Y. Boulahia, E. Anquetil, F. Multon, and R. Kulpa, “Dynamic hand gesture recognition based on 3D pattern assembled trajectories,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)</em>, Nov. 2017, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S.-H. Peng and P.-H. Tsai, “An Efficient Graph Convolution Network for Skeleton-Based Dynamic Hand Gesture Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Transactions on Cognitive and Developmental Systems</em>, vol. 15, no. 4, pp. 2179–2189, Dec. 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10039714" title="">https://ieeexplore.ieee.org/document/10039714</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Sabater, I. Alonso, L. Montesano, and A. C. Murillo, “Domain and View-point Agnostic Hand Action Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv:2103.02303 [cs]</em>, Oct. 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2103.02303" title="">http://arxiv.org/abs/2103.02303</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. R. Morris, “AI and Accessibility,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Communications of the ACM</em>, vol. 63, no. 6, pp. 35–37, May 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3356727" title="">https://dl.acm.org/doi/10.1145/3356727</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
W. Mucha and M. Kampel, “Beyond Privacy of Depth Sensors in Active and Assisted Living Devices,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">The15th International Conference on PErvasive Technologies Related to Assistive Environments</em>.   Corfu Greece: ACM, Jun. 2022, pp. 425–429. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3529190.3534764" title="">https://dl.acm.org/doi/10.1145/3529190.3534764</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “Spatio-Temporal Attention-Based LSTM Networks for 3D Action Recognition and Detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE Transactions on Image Processing</em>, vol. 27, no. 7, pp. 3459–3471, Jul. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
F. Zhang, V. Bazarevsky, A. Vakunov, A. Tkachenka, G. Sung, C.-L. Chang, and M. Grundmann, “MediaPipe Hands: On-device Real-time Hand Tracking,” Jun. 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2006.10214" title="">http://arxiv.org/abs/2006.10214</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Kendall, Y. Gal, and R. Cipolla, “Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics,” Apr. 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1705.07115" title="">http://arxiv.org/abs/1705.07115</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras, “Two-person Interaction Detection Using Body-Pose Features and Multiple Instance Learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</em>, Jun. 2012, pp. 28–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng, “View Adaptive Neural Networks for High Performance Skeleton-Based Human Action Recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 41, no. 8, pp. 1963–1978, Aug. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
X. Liu, H. Shi, X. Hong, H. Chen, D. Tao, and G. Zhao, “3D Skeletal Gesture Recognition via Hidden States Exploration,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE Transactions on Image Processing</em>, vol. 29, pp. 4583–4597, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Kacem, M. Daoudi, B. B. Amor, S. Berretti, and J. C. Alvarez-Paiva, “A Novel Geometric Framework on Gram Matrix Trajectories for Human Behavior Understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 42, no. 1, pp. 1–14, Jan. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Maghoumi and J. J. LaViola Jr, “DeepGRU: Deep Gesture Recognition Utility,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv:1810.12514 [cs]</em>, Oct. 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1810.12514" title="">http://arxiv.org/abs/1810.12514</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct  6 04:01:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
