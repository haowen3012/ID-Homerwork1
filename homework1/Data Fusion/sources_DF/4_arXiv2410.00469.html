<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data</title>
<!--Generated on Tue Oct  1 07:48:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.00469v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S1" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S2" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S3" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S4" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Design and Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S5" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S6" title="In Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ivica Dimitrovski 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">ivica.dimitrovski@finki.ukim.mk</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Vlatko Spasev 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">vlatko.spasev@finki.ukim.mk</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id4.4.id4">\And</span>Ivan Kitanovski 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">ivan.kitanovski@finki.ukim.mk</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Accurate semantic segmentation of remote sensing imagery is critical for various Earth observation applications, such as land cover mapping, urban planning, and environmental monitoring. However, individual data sources often present limitations for this task. Very High Resolution (VHR) aerial imagery provides rich spatial details but cannot capture temporal information about land cover changes. Conversely, Satellite Image Time Series (SITS) capture temporal dynamics, such as seasonal variations in vegetation, but with limited spatial resolution, making it difficult to distinguish fine-scale objects. This paper proposes a late fusion deep learning model (LF-DLM) for semantic segmentation that leverages the complementary strengths of both VHR aerial imagery and SITS. The proposed model consists of two independent deep learning branches. One branch integrates detailed textures from aerial imagery captured by UNetFormer with a Multi-Axis Vision Transformer (MaxViT) backbone. The other branch captures complex spatio-temporal dynamics from the Sentinel-2 satellite image time series using a U-Net with Temporal Attention Encoder (U-TAE). This approach leads to state-of-the-art results on the FLAIR dataset, a large-scale benchmark for land cover segmentation using multi-source optical imagery. The findings highlight the importance of multi-modality fusion in improving the accuracy and robustness of semantic segmentation in remote sensing applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">K</em><span class="ltx_text ltx_font_bold" id="p1.1.2">eywords</span> Earth observation, semantic segmentation, remote sensing, multi-modality fusion, deep learning</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Remote sensing data is captured from a distance by sensors or instruments mounted on various platforms such as satellites, aircraft, drones, and other vehicles. This data collects information about the Earth’s surface, atmosphere, and other objects or phenomena without requiring direct physical contact <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib1" title="">1</a>]</cite>. There are two main techniques of remote sensing data acquisition: aerial and satellite. Satellite data is collected by satellites orbiting the Earth, capturing information over large areas at regular intervals. This provides a broad view of the entire planet. In contrast, aerial data is captured from airplanes or drones flying closer to the ground. This data covers smaller areas but with much finer detail, making it ideal for studying specific locations. Sensors are essential to remote sensing systems, as they collect data used to create images and other forms of information. Various types of sensors employed in remote sensing include optical sensors, radar sensors, lidar sensors, and electromagnetic sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Remote sensing data has four key properties: spectral, spatial, radiometric, and temporal resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib2" title="">2</a>]</cite>. Spectral resolution refers to the range of wavelengths a satellite sensor can detect. The more wavelengths a sensor can capture, the richer the information content of the imagery and the greater the detail it reveals about land use and cover. These captured wavelengths span a vast spectrum, including ultraviolet, visible light, near-infrared, infrared, and microwave. Some sensors capture just a few broad bands (multi-spectral), like Sentinel-2 with its 12 bands. Others, like Hyperion, are hyper-spectral, gathering thousands of narrow bands for a highly detailed spectral view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib2" title="">2</a>]</cite>. Spatial resolution refers to the size of each pixel in the image. Higher spatial resolution means smaller pixels, capturing finer details on the ground. Radiometric resolution describes how well the sensor can detect variations in radiated energy from the earth’s surface. Higher resolution allows for better detection of subtle changes. Landsat 7 captures 8-bit images, distinguishing 256 distinct gray values of reflected energy, whereas Sentinel-2 features a 12-bit radiometric resolution, allowing it to discern 4095 gray values. Temporal resolution refers to how often a specific location is imaged. For example, polar-orbiting satellites exhibit varying temporal resolutions, ranging from 1 to 16 days (e.g., ten days for Sentinel-2). This is important for monitoring changes over time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Machine learning is revolutionizing the way we analyze and understand remote sensing data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib3" title="">3</a>]</cite>. A particularly exciting area of research is the semantic segmentation of remote sensing data. The goal is to partition the image into meaningful regions, enabling detailed analysis and understanding of the Earth’s surface. Accurate semantic segmentation of remote sensing imagery is essential for a wide range of Earth observation (EO) applications, including land cover mapping, urban planning, and environmental monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib2" title="">2</a>]</cite>. The emergence of deep learning, particularly Convolutional Neural Networks (CNNs) and Fully Convolutional Networks (FCNs), ignited a revolution in semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib4" title="">4</a>]</cite>. These models automated the learning of complex, hierarchical representations from data, paving the way for significant advancements. FCNs, often paired with encoder-decoder architectures, became the dominant approach. Early methods relied on successive convolutions and spatial pooling to generate dense predictions. Subsequent innovations like U-Net and SegNet introduced upsampling techniques to combine high-level features with lower-level ones during decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib4" title="">4</a>]</cite>. This fusion aimed to capture both global context and precise object boundaries. To address the limited receptive field of standard convolutions in earlier layers of deep learning models, techniques like dilated (or atrous) convolutions were introduced by DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib5" title="">5</a>]</cite>. These convolutions allow capturing a larger context while maintaining the resolution of the feature maps. Subsequent advancements incorporated spatial pyramid pooling (SPP) to capture multi-scale contextual information in higher layers, as seen in models like PSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib4" title="">4</a>]</cite> and UperNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib6" title="">6</a>]</cite>. DeepLabV3+ built upon these advancements by combining atrous spatial pyramid pooling with a straightforward and efficient encoder-decoder architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib7" title="">7</a>]</cite>. However, recent developments like PSANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib8" title="">8</a>]</cite> and DRANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib9" title="">9</a>]</cite> have moved beyond traditional pooling, instead using attention mechanisms on top of encoder feature maps to capture long-range dependencies more effectively. Most recently, the adoption of transformer architectures, which utilize self-attention mechanisms and capture long-range dependencies, has marked additional advancement in semantic segmentation. Transformer encoder-decoder architectures like Segmenter, SegFormer, and MaskFormer harness transformers to enhance performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The abundance of diverse remote sensing modalities, like LiDARs, RGB-D cameras, and thermal cameras, has fostered the development of deep multimodal fusion techniques. These complementary sensors offer a richer picture of the scene, especially in complex environments. Deep learning excels at leveraging this data to reduce uncertainties and create a more comprehensive understanding. The core objective of deep multimodal fusion in segmentation is to learn an optimal joint representation by combining the strengths of individual modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib11" title="">11</a>]</cite>. This joint representation captures the rich and complementary features of the same scene, leading to more accurate segmentation results. For example, Very High Resolution (VHR) aerial imagery excels at providing rich spatial details, making it ideal for identifying fine-scale features such as individual buildings, roads, and small vegetation patches. This high level of detail is crucial for tasks that require precise mapping and analysis of specific locations. However, VHR aerial imagery typically lacks temporal information, which is essential for capturing changes over time to monitor dynamic processes such as seasonal variations in vegetation, urban growth, or the progression of environmental degradation. On the other hand, Satellite Image Time Series (SITS) data offers valuable temporal insights by capturing images of the same area at regular intervals. This capability is particularly useful for observing and analyzing temporal dynamics, such as the phenological cycles of crops, changes in land cover due to deforestation or reforestation, and the impact of natural disasters over time. However, SITS data generally has lower spatial resolution compared to VHR aerial imagery, which can make it difficult to distinguish fine-scale objects and detailed features on the ground.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Deep multimodal fusion methods can be broadly categorized based on the stage at which information from different modalities is combined <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib11" title="">11</a>]</cite>. Early fusion occurs at the raw data level (e.g., concatenating RGB and LiDAR data) or feature level (combining extracted features from each modality). This approach allows the model to learn a joint representation from the very beginning. Late fusion strategy involves processing each modality separately through individual deep learning branches. Then, the resulting feature maps are combined at a later stage (e.g., before the final prediction layer) using operations like concatenation, addition, or weighted voting. This approach offers greater flexibility in designing individual models for specific modalities. Hybrid fusion combines elements of both early and late fusion. It might involve initial feature-level fusion followed by late fusion of higher-level features. This allows for a more adaptive learning process based on the specific data and task. By effectively leveraging the complementary information from multiple modalities, deep multimodal fusion techniques are pushing the boundaries of semantic segmentation accuracy and robustness, particularly in complex remote sensing scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This paper tackles the challenge of accurate semantic segmentation in remote sensing by proposing a late fusion deep learning model (LF-DLM) that leverages the complementary strengths of VHR aerial imagery and SITS data. This approach aims to overcome the limitations inherent in single-source data, ultimately leading to more robust and informative land cover segmentation. The proposed LF-DLM architecture employs a dual-branch strategy, capitalizing on the specific advantages of each data source. Our research can be summarized by the following primary contributions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Introduction of a late fusion deep learning model that leverages the complementary strengths of VHR aerial imagery and SITS data, tailored to enhance semantic segmentation of remote sensing imagery.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Through comprehensive experimental evaluation, we demonstrate that the LF-DLM model effectively combines spatial and temporal information, leading to improved segmentation accuracy across various land cover types while maintaining efficient inference times.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our LF-DLM model achieves state-of-the-art results on the FLAIR dataset, surpassing previous benchmarks, thus establishing a new standard for semantic segmentation in multi-source optical imagery.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The subsequent sections of this paper are structured as follows: Section 2 provides an overview of the dataset utilized in the research. Section 3 details the key features of the proposed late fusion deep learning model. Section 4 comprehensively outlines the experimental design and setup, including data preprocessing, training protocols, model parameters, and evaluation metrics. Section 5 presents the experimental results alongside relevant discussions. Finally, Section 6 concludes the paper, summarizing the findings and contributions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The FLAIR dataset<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/IGNF/FLAIR-2" title="">https://github.com/IGNF/FLAIR-2</a></span></span></span> includes diverse sources of acquisition, each with unique characteristics and varying spatial, spectral, and temporal resolutions. This dataset provides detailed VHR aerial images, elevation models, and satellite image time series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib12" title="">12</a>]</cite>. Each aerial image measures 512 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mo id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><times id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">×</annotation></semantics></math> 512 pixels, with a spatial resolution of 20 cm per pixel, and includes four spectral bands: red, blue, green, and near-infrared. The dataset comprises 77762 patches. To ensure high-quality images, the aerial data is captured only during favorable weather conditions, specifically between April and November from 2018 to 2021. Each aerial image includes an elevation value. This value is derived from combining a digital elevation model and a digital surface model, obtained through photogrammetry on the aerial images, ensuring temporal consistency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.2">Each aerial image patch in FLAIR is accompanied by a corresponding time series of satellite images from the Sentinel-2 constellation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib13" title="">13</a>]</cite>. These satellite images offer a broader view with a spatial resolution of 10 meters per pixel and come in a size of 40<math alttext="\times" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mo id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><times id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">×</annotation></semantics></math>40 pixels, centered on the corresponding aerial image, and only 10<math alttext="\times" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mo id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><times id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">×</annotation></semantics></math>10 center pixels correspond to the aerial image patch. Each pixel provides information across 10 spectral bands, capturing data from the visible to the medium infrared spectrum. The time series for each patch spans the entire year during which the aerial image was acquired. The number of images within a series can vary between 20 and 110, depending on satellite availability and orbital characteristics. The dataset includes acquisitions with cloud cover and provides cloud and snow probability masks, obtained with Sen2cor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib14" title="">14</a>]</cite>, along with information about the satellite and its orbit. Example patches from the FLAIR dataset are given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S2.F1" title="Figure 1 ‣ 2 Dataset ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S2.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example patches from the FLAIR dataset. Each patch contains an aerial image with red, green, blue (RGB), and near-infrared (NIR) values; a pixel-precise digital surface model providing an elevation for each pixel; segmentation map with labels for each pixel; and an optical time series from several months, centered on the aerial image. The red frame marks the area that corresponds to the aerial image.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The VHR images are annotated with segmentation masks containing 18 different labels/classes, along with an ’other’ class for unknown land cover. Due to significant under-representation (less than 1% of the complete dataset), five of these classes are combined into the ’other’ class. This results in a nomenclature of 12 classes plus the ’other’ class. The classes are: ’building’, ’pervious surface’, ’impervious surface’, ’bare soil’, ’water’, ’coniferous’, ’deciduous’, ’brushwood’, ’vineyard’, ’herbaceous vegetation’, ’agricultural land’, and ’plowed land’. Annotations are not provided for the satellite images. Instead, these images are intended to support the aerial images by providing spatial context. The distribution of pixels within the labels across the train, validation, and test sets of the FLAIR dataset is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S2.F2" title="Figure 2 ‣ 2 Dataset ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="472" id="S2.F2.g1" src="x2.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The distribution of pixels within the labels across the train, validation, and test sets of the FLAIR dataset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The dataset comprises 50 spatial domains, each representing various landscapes and climates of metropolitan France. The training set includes 32 spatial domains, the validation set contains 8, and the remaining 10 domains are allocated to the test set. The dataset is part of the FLAIR #2 challenge where a key requirement is to leverage both aerial and satellite imagery to achieve optimal semantic segmentation results. The FLAIR #2 challenge introduces a second requirement: computational efficiency to ensure a balance between accuracy and practicality, considering the vast amount of data involved. The proposed approach’s inference time needs to be within 2.5 times the execution speed of the baseline model offered by the dataset creators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib12" title="">12</a>]</cite>. Within this paper, we have carefully designed our solution to effectively utilize both data sources while staying within the allowed inference time for the FLAIR #2 challenge.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This paper introduces a late fusion deep learning model (LF-DLM) for semantic segmentation, designed to exploit the complementary strengths of both Very High Resolution (VHR) aerial imagery and Satellite Image Time Series (SITS). The proposed model features two independent deep learning branches. The first branch integrates detailed textures from aerial imagery using a UNetFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib15" title="">15</a>]</cite> with a Multi-Axis Vision Transformer (MaxViT) encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib16" title="">16</a>]</cite>, effectively capturing high-resolution spatial details. The second branch focuses on capturing complex spatio-temporal dynamics from the Sentinel-2 satellite image time series by employing a U-Net with Temporal Attention Encoder (U-TAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib17" title="">17</a>]</cite>, which processes and interprets temporal information. In the late fusion deep learning model, the probability scores from each branch are combined using a weighted geometric mean to obtain the final segmentation map. This dual-branch approach enables the model to leverage both spatial and temporal data for enhanced semantic segmentation performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The first branch of the LF-DLM model builds upon the Unet-like transformer UNetFormer. Originally it consists of a convolutional neural network (CNN) based encoder and a transformer-based decoder. The transformer-based decoder is constructed using global-local Transformer blocks (GLTB) that employ an efficient global-local attention mechanism with an attentional global branch and a convolutional local branch, enabling the capture of both global and local contexts for enhanced visual perception. We propose a modification to UNetFormer by replacing its CNN encoder with MaxViT, a hybrid vision transformer architecture. MaxViT introduces a novel building block called Multi-axis Self-Attention (Max-SA). This block allows the model to attend to information along multiple axes within an image feature map, including spatial, channel-wise axes, or combination. Compared to standard full self-attention in ViTs, Max-SA captures long-range dependencies (global information) more efficiently without requiring complex computations. MaxViT utilizes a hierarchical architecture where each stage in the hierarchy consists of a MaxViT block, which combines Max-SA with a convolutional layer. This combination leverages the strengths of both approaches: Max-SA for global context and convolutions for efficient local feature extraction. The network begins by downsampling the input through Conv3x3 layers in the stem stage (S0). The body of the network contains four stages (S1-S4), with each stage having half the resolution of the previous one with a doubled number of channels (hidden dimension). The feature maps generated by each stage are fused with the corresponding feature maps generated by the GLTB of the decoder using a weighted sum operation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The MaxViT model can be scaled up by increasing the number of blocks per stage and the channel dimension. There are several MaxViT variants including MaxViT-T, MaxViT-S, MaxViT-B, MaxViT-L, and MaxViT-XL. These variants progressively increase in complexity (number of blocks and channels) and likely performance, potentially reaching a trade-off between accuracy and efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib16" title="">16</a>]</cite>. In this study, we are using MaxViT-T as an encoder in the UNetFormer architecture. We are utilizing MaxViT-T, pre-trained on the ImageNet-1K dataset, to leverage its learned general visual features, which can be highly beneficial for semantic segmentation tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">To effectively analyze both spatial and temporal information within the Sentinel-2 satellite image time series, we leverage a U-Net with temporal attention (U-TAE) model, which serves as the second branch in the LF-DLM model. This branch extracts multi-scale spatio-temporal feature maps from SITS using a combination of spatial convolution and temporal attention. U-TAE encodes a given sequence in three key steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib17" title="">17</a>]</cite>. First, each image in the sequence is embedded simultaneously and independently by a shared multi-level spatial convolutional encoder. Next, a temporal attention encoder collapses the temporal dimension of the resulting sequence of feature maps into a single map for each level. Finally, a spatial convolutional decoder produces a single feature map with the same resolution as the input images. By combining these steps, U-TAE allows effective exploitation of the rich spatio-temporal information present in the SITS, leading to a more comprehensive understanding of the scene dynamics.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Design and Setup</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The primary objective of our study is to develop, evaluate, and compare a late fusion deep learning model (LF-DLM) for semantic segmentation of remote sensing imagery by leveraging the complementary strengths of VHR aerial imagery and SITS. Our experimental design is structured around the main hypothesis, that the fusion of these multi-source optical images will improve the semantic segmentation performance compared to using either data source alone. To test the hypothesis, we use VHR aerial imagery processed through the UNetFormer with MaxViT-S backbone to capture detailed spatial features, and SITS processed through a U-Net with Temporal Attention Encoder (U-TAE) to capture complex spatio-temporal dynamics. Our evaluation strategy involves training and assessing each model separately to determine their performances and conducting a comparative analysis to highlight the benefits of combining these data sources with weighted late fusion as a strategy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The experimental setup involves data pre-processing, a configuration of the models, and hyperparameter selection. While no additional pre-processing is applied to the aerial patches, we address the potential influence of clouds and snow in the Sentinel-2 time series by implementing two pre-processing strategies using the provided mask files. Cloud filtering focuses on the probability of cloud or snow occurrence in the masks. We exclude images from the training process where the number of pixels exceeding a specific probability threshold (set to 0.5 in our experiments) surpasses a designated percentage of the total image pixels. This approach mitigates the impact of cloudy or snowy data on the training process. Additionally, we apply temporal monthly averaging to address challenges posed by the large number of dates within the time series. Here, a monthly average is computed using only cloudless dates within each month. If no cloudless dates are available for a particular month, the U-TAE branch might receive less than 12 images as input.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.5">The UNetFormer model leverages a pre-trained MaxViT-T encoder on the ImageNet-1K dataset. This encoder receives five-channel aerial patches containing red, green, blue, near-infrared, and elevation data. The resolution of the aerial patches is <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">512</mn><mo id="S4.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn id="S4.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.p3.1.m1.1.1.2">512</cn><cn id="S4.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.p3.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">512 × 512</annotation></semantics></math> pixels. We add two channels to the initial layers to accommodate the near-infrared and elevation pixel values, with the weights of these added channels initialized randomly. The number of learnable parameters in this UNetFormer model is approximately 31 million. We use the default U-TAE parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib17" title="">17</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib12" title="">12</a>]</cite>, with the only modification being the widths of the encoder and decoder, which we adjusted to [64, 64, 128, 128]. This list specifies the number of channels for the successive layers of the convolutional encoder, and the same configuration applies to the decoder. The input to this model is the SITS data with dimensions <math alttext="T\times 10\times 40\times 40" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">T</mi><mo id="S4.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">10</mn><mo id="S4.p3.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S4.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.p3.2.m2.1.1.4" xref="S4.p3.2.m2.1.1.4.cmml">40</mn><mo id="S4.p3.2.m2.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S4.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.p3.2.m2.1.1.5" xref="S4.p3.2.m2.1.1.5.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝑇</ci><cn id="S4.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.p3.2.m2.1.1.3">10</cn><cn id="S4.p3.2.m2.1.1.4.cmml" type="integer" xref="S4.p3.2.m2.1.1.4">40</cn><cn id="S4.p3.2.m2.1.1.5.cmml" type="integer" xref="S4.p3.2.m2.1.1.5">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">T\times 10\times 40\times 40</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_T × 10 × 40 × 40</annotation></semantics></math>, where <math alttext="T" class="ltx_Math" display="inline" id="S4.p3.3.m3.1"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.p3.3.m3.1d">italic_T</annotation></semantics></math> represents the number of images in the time series (with a maximum of 12), 10 is the number of spectral bands, and <math alttext="40\times 40" class="ltx_Math" display="inline" id="S4.p3.4.m4.1"><semantics id="S4.p3.4.m4.1a"><mrow id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mn id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">40</mn><mo id="S4.p3.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.p3.4.m4.1.1.1.cmml">×</mo><mn id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><times id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1.1"></times><cn id="S4.p3.4.m4.1.1.2.cmml" type="integer" xref="S4.p3.4.m4.1.1.2">40</cn><cn id="S4.p3.4.m4.1.1.3.cmml" type="integer" xref="S4.p3.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">40\times 40</annotation><annotation encoding="application/x-llamapun" id="S4.p3.4.m4.1d">40 × 40</annotation></semantics></math> is the pixel resolution. The number of learnable parameters in this U-TAE model is approximately 2.9 million. To ensure spatial alignment, the U-TAE outputs are first cropped to match the size of the corresponding aerial patch. Then, they are upsampled to the same resolution (<math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.p3.5.m5.1"><semantics id="S4.p3.5.m5.1a"><mrow id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml"><mn id="S4.p3.5.m5.1.1.2" xref="S4.p3.5.m5.1.1.2.cmml">512</mn><mo id="S4.p3.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.p3.5.m5.1.1.1.cmml">×</mo><mn id="S4.p3.5.m5.1.1.3" xref="S4.p3.5.m5.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><apply id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1"><times id="S4.p3.5.m5.1.1.1.cmml" xref="S4.p3.5.m5.1.1.1"></times><cn id="S4.p3.5.m5.1.1.2.cmml" type="integer" xref="S4.p3.5.m5.1.1.2">512</cn><cn id="S4.p3.5.m5.1.1.3.cmml" type="integer" xref="S4.p3.5.m5.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.p3.5.m5.1d">512 × 512</annotation></semantics></math> pixels) as the aerial mask files. We experimented with different weight combinations for the late fusion, and the best performance was achieved when the UNetFormer branch was assigned a weight of 0.7 and the U-TAE branch had a weight of 0.3 in the weighted geometric mean.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.4">To train both the UNetFormer and U-TAE models effectively, we employed several common deep learning techniques. To prevent overfitting and optimize hyperparameters, we employed a training process with hyperparameter selection and early stopping. The training data was used to train the model. Hyperparameter selection was performed on the validation split to identify the optimal configuration for the model’s hyperparameters. To prevent overfitting, we implemented early stopping using the validation loss. If the validation loss did not improve for a predefined patience period (15 epochs in this case), training was terminated. The model with the best performance on the validation set, determined by the chosen evaluation metric, was then saved as the final model. This model was subsequently evaluated on the unseen test data to obtain an unbiased assessment of its predictive performance. The maximum training duration was set to 30 epochs. To improve the robustness and generalization ability of our model, we employ data augmentation techniques during training. This process involves applying random geometric transformations to the training data. Specifically, we utilize horizontal flips, vertical flips, and random rotations at predefined angles (0, 90, 180, and 270 degrees). We fixed the batch size at 12 for our experiments. We employed the AdamW optimizer with a learning rate of 0.0001 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib18" title="">18</a>]</cite>. A polynomial decay scheduler was used to gradually decrease the learning rate throughout training. This approach, with a carefully chosen decay rate, has been shown to improve model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib19" title="">19</a>]</cite>. The scheduler applies a polynomial function to the AdamW optimizer, starting with an initial learning rate of (1 <math alttext="\times" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><mo id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><times id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">×</annotation></semantics></math> 10<sup class="ltx_sup" id="S4.p4.4.1"><span class="ltx_text ltx_font_italic" id="S4.p4.4.1.1">-4</span></sup>) and reaching a final learning rate of (1 <math alttext="\times" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mo id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><times id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">×</annotation></semantics></math> 10<sup class="ltx_sup" id="S4.p4.4.2"><span class="ltx_text ltx_font_italic" id="S4.p4.4.2.1">-7</span></sup>) within the specified number of decay steps. To achieve a balanced approach to semantic segmentation, we combined Cross Entropy Loss and Dice Loss. Cross Entropy Loss measures the similarity between predicted and ground truth masks at each pixel, while Dice Loss focuses on accurate boundary localization. This combination effectively addresses both object localization and overall segmentation accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.2">All models were trained on NVIDIA A100-PCIe GPUs with 40 GB of memory running CUDA version 11.5. We configured and ran the experiments using the deep learning framework PyTorch Lightning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib20" title="">20</a>]</cite>. In the experimental setup, we carefully considered the constraints and requirements of the FLAIR dataset, as it is part of the FLAIR #2 challenge. Our solution was designed to effectively utilize both data sources while adhering to the allowed inference time for the FLAIR #2 challenge. The FLAIR #2 challenge specifies a maximum inference time that cannot exceed 2.5 times the baseline method. By measuring the inference time of the provided FLAIR #2 challenge baseline code on our environment, we determined that it takes approximately 396 seconds to generate segmentation maps for all images in the test set. Since the challenge restricts inference time to a maximum of 2.5 times the baseline, our model’s inference time must not exceed 2.5 times 396 seconds, which translates to approximately 990 seconds. We assess the model performance using label-wise intersection over union (<math alttext="IoU" class="ltx_Math" display="inline" id="S4.p5.1.m1.1"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">I</mi><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">o</mi><mo id="S4.p5.1.m1.1.1.1a" xref="S4.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p5.1.m1.1.1.4" xref="S4.p5.1.m1.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><times id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1"></times><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">𝐼</ci><ci id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">𝑜</ci><ci id="S4.p5.1.m1.1.1.4.cmml" xref="S4.p5.1.m1.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">IoU</annotation><annotation encoding="application/x-llamapun" id="S4.p5.1.m1.1d">italic_I italic_o italic_U</annotation></semantics></math>) which denotes the area of the overlap between the ground truth and predicted label divided by the total area. We also report the mean intersection over union (<math alttext="mIoU" class="ltx_Math" display="inline" id="S4.p5.2.m2.1"><semantics id="S4.p5.2.m2.1a"><mrow id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mi id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">m</mi><mo id="S4.p5.2.m2.1.1.1" xref="S4.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml">I</mi><mo id="S4.p5.2.m2.1.1.1a" xref="S4.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.p5.2.m2.1.1.4" xref="S4.p5.2.m2.1.1.4.cmml">o</mi><mo id="S4.p5.2.m2.1.1.1b" xref="S4.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.p5.2.m2.1.1.5" xref="S4.p5.2.m2.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><times id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1"></times><ci id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">𝑚</ci><ci id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3">𝐼</ci><ci id="S4.p5.2.m2.1.1.4.cmml" xref="S4.p5.2.m2.1.1.4">𝑜</ci><ci id="S4.p5.2.m2.1.1.5.cmml" xref="S4.p5.2.m2.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S4.p5.2.m2.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math>) averaged across the different labels. The evaluation metrics are computed for the first 12 classes, excluding the ’other’ class.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S5.T1" title="Table 1 ‣ 5 Results ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the performance of each model on the FLAIR dataset. Label-wise Intersection over Union (IoU) and mean IoU (mIoU) are reported in percentage. As expected, the U-TAE model achieved the lowest mIoU (39.68%) due to the limited spatial resolution of the satellite image time series. The UNetFormer model, leveraging the high spatial detail of aerial imagery, significantly improved upon this with a mIoU of 62.81%, representing a 23.13% increase. This outcome is expected given that the satellite imagery has a spatial resolution 50 times lower than the aerial imagery (10 m versus 0.2 m). Notably, the Late Fusion Deep Learning Model (LF-DLM) achieved the best overall performance with a mIoU of 63.10%. This represents an improvement of 0.29% compared to the UNetFormer alone. These findings support our hypothesis that combining information from both aerial imagery and satellite time series data through late fusion leads to improved semantic segmentation performance.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean intersection over union (mIoU %) and Intersection over Union (IoU %) for each label of the UNetFormer, U-TAE, and LF-DLM models over the FLAIR dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.1">Label \ Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.2">UNetFormer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.3">U-TAE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.4">LF-DLM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.2.1.1">building</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2">85.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.3">35.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.4">85.14</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.3.2.1">pervious surface</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2">57.69</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.3">31.36</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.4">58.31</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.4.3.1">impervious surface</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.2">74.95</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.3">38.67</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.4">74.66</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.5.4.1">bare soil</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.2">63.89</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.3">39.02</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.4">65.01</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.6.5.1">water</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.2">90.77</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.3">74.75</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.4">91.08</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.7.6.1">coniferous</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.7.6.2">65.67</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.7.6.3">54.74</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.7.6.4">66.89</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.8.7.1">deciduous</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.8.7.2">73.83</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.8.7.3">56.36</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.8.7.4">74.32</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.9.8.1">brushwood</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.9.8.2">27.68</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.9.8.3">11.35</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.9.8.4">26.77</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.10.9.1">vineyard</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.10.9.2">67.19</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.10.9.3">49.48</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.10.9.4">67.59</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.11.10.1">herbaceous vegetation</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.11.10.2">50.93</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.11.10.3">26.80</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.11.10.4">50.86</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.12.11.1">agricultural land</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.12.11.2">56.16</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.12.11.3">45.43</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.12.11.4">56.59</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.13.12.1">plowed land</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.13.12.2">39.55</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.13.12.3">12.67</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.13.12.4">39.96</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.14.13.1">mIoU</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.14.13.2">62.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.14.13.3">39.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.14.13.4">63.10</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The LF-DLM shows improvement in IoU for most labels compared to the UNetFormer model. This is evident in labels like ’pervious surface’ (0.62%), ’bare soil’ (1.12%), ’water’ (0.32%), ’coniferous’ (1.22%), ’deciduous’ (0.5%), ’vineyard’ (0.41%), ’agricultural land’ (0.43%), and ’plowed land’ (0.41%). This suggests that the late fusion strategy effectively combines the strengths of both U-TAE (capturing spatio-temporal information) and UNetFormer (capturing high spatial details) to improve segmentation accuracy across various land cover types. This is particularly evident for labels like ’coniferous’ where SITS data, containing temporal information, might be crucial to distinguish them from ’deciduous’ trees exhibiting seasonal changes in spectral properties. The improvement is also notable for the ’bare soil’ label, potentially benefiting from the complementary information provided by SITS data. However, the LF-DLM shows a slight decrease in IoU for labels like ’building’ (-0.26%), ’impervious surface’ (-0.29%), ’brushwood’ (-0.91%), and ’herbaceous vegetation’ (-0.07%). This could be due to several factors like class/label imbalance or fusion complexity where the late fusion process might introduce additional complexity for these specific labels, leading to slight performance drops compared to the UNetFormer model. Potentially we can explore the possibility of employing label-specific weighting or fusion techniques during late fusion to potentially address challenges faced by specific land cover types.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Examining the confusion matrix depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S5.F3" title="Figure 3 ‣ 5 Results ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">3</span></a> reveals that the best LF-DLM model achieves high prediction accuracy, with minimal misclassification in the majority of labels. However, it tends to confuse the labels "coniferous" and "deciduous", "brushwood" and "herbaceous vegetation", "brushwood" and "deciduous", as well as "agricultural land" and "herbaceous vegetation". This is rather expected given the semantic similarity between these labels.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="331" id="S5.F3.g1" src="x3.png" width="418"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Confusion matrix for LF-DLM on the FLAIR dataset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S5.F4" title="Figure 4 ‣ 5 Results ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">4</span></a> shows several example images, ground truth masks, and predicted masks from the FLAIR dataset. Obtaining accurate segmentation maps is very challenging due to factors like complex scenes, occlusion between different land cover areas, and the semantic similarity between land cover types.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="648" id="S5.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example images, ground-truth masks, and inference masks from the FLAIR dataset. The first row shows example images. The second row shows the corresponding ground-truth masks. The third row shows the prediction results of the LF-DLM.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">To ensure compliance with the inference time constraints of the FLAIR #2 challenge, we measured the inference times of our proposed models. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#S5.T2" title="Table 2 ‣ 5 Results ‣ Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data"><span class="ltx_text ltx_ref_tag">2</span></a> presents these measurements along with their corresponding ratios compared to the baseline model inference time. All measurements were conducted on the same machine equipped with an NVIDIA A100-PCIe GPU with 40 GB of memory. The challenge restricts inference time to a maximum of 2.5 times that of the baseline model. As the table indicates, our models currently operate within this limit. This suggests potential for further improvement in our model’s predictive performance while maintaining compliance with the challenge’s constraints.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Inference times and their corresponding ratios compared to the baseline model inference time for our proposed models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2">Inference time (sec.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3">Relative time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.1.2.1.1">U-TAE</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">229</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.3">0.58</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.3.2.1">UNetFormer</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.2">429</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.3">1.08</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.1.4.3.1">LF-DLM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.4.3.2">594</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.4.3.3">1.5</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">To enhance predictive performance while adhering to the FLAIR #2 challenge’s inference time constraint, we incorporated a second UNetFormer model into the late fusion scheme. This second model was trained using identical parameters, with the sole difference being a variation in the random seed. The resulting late fusion deep learning model comprises the U-TAE model, two UNetFormer models (with different random seeds), and the late fusion layer. This configuration achieved a mIoU value of 64.52%, an inference time of 943 seconds, and a relative inference time ratio of 2.38. Importantly, this remains well within the challenge’s allowed constraints.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">To comprehensively evaluate our best model configuration’s predictive performance, we compared it with previously employed methods on the FLAIR dataset. The challenge organizers provided a U-Net baseline with a ResNet34 backbone in combination with a U-TAE model using a mid-stage fusion of features from both models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib12" title="">12</a>]</cite>, achieving a mIoU of 57.58%. Our best model surpasses the baseline by a significant margin of 6.94%. The current state-of-the-art on this dataset was an ensemble model consisting of four base models. The base models are similar to the baseline model provided by the challenge organizers, the only modification is the replacement of the ResNet34 backbone with MiT and ResNeXt backbones in the U-Net model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00469v1#bib.bib21" title="">21</a>]</cite>. Additionally, a two-stage training procedure is proposed to boost the predictive performance. This ensemble model achieved a mIoU of 64.13% and ranked first in the competition. Notably, our proposed model with a mIoU of 64.52% outperforms this previous best result, establishing a new state-of-the-art for semantic segmentation on the FLAIR dataset.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work investigated the effectiveness of late fusion for semantic segmentation of remote sensing imagery, leveraging the complementary strengths of Very High Resolution (VHR) aerial imagery and Satellite Image Time Series (SITS) data. We proposed a Late Fusion Deep Learning Model (LF-DLM) that integrates a UNetFormer branch for capturing spatial details from aerial imagery and a U-TAE branch for capturing spatio-temporal dynamics from SITS data. The LF-DLM achieved state-of-the-art performance on the FLAIR dataset, a large-scale benchmark for land cover segmentation using multi-source optical imagery. Compared to the UNetFormer model alone, the LF-DLM achieved an improved mIoU of 0.29%. This signifies the effectiveness of late fusion in combining information from both data sources to enhance segmentation accuracy across various land cover types.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Furthermore, our best model configuration with a mIoU of 64.52% surpasses the previous state-of-the-art on the FLAIR dataset, demonstrating its robustness and efficiency while adhering to the challenge’s inference time constraints. These findings highlight the potential of late fusion deep learning models for improving the accuracy and robustness of semantic segmentation in remote sensing applications. Future work can explore label-specific fusion techniques and class imbalance mitigation strategies to address remaining challenges and further enhance performance for specific land cover types.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Charles Toth and Grzegorz Jóźków.

</span>
<span class="ltx_bibblock">Remote sensing platforms and sensors: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 115:22–36, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Vlatko Spasev, Ivica Dimitrovski, Ivan Kitanovski, and Ivan Chorbev.

</span>
<span class="ltx_bibblock">Semantic segmentation of remote sensing images: Definition, methods, datasets and applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">ICT Innovations 2023. Learning: Humans, Theory, Machines, and Data</span>, pages 127–140, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, and Nikola Simidjievski.

</span>
<span class="ltx_bibblock">Current trends in deep learning for earth observation: An open-source benchmark arena for image classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 197:18–35, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shijie Hao, Yuan Zhou, and Yanrong Guo.

</span>
<span class="ltx_bibblock">A brief survey on semantic segmentation with deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Neurocomputing</span>, 406:302–321, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.

</span>
<span class="ltx_bibblock">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 40(4):834–848, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.

</span>
<span class="ltx_bibblock">Unified perceptual parsing for scene understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 418–434, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Encoder-decoder with atrous separable convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 801–818, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Psanet: Point-wise spatial attention network for scene parsing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 267–283, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Hanqing Lu.

</span>
<span class="ltx_bibblock">Scene segmentation with dual relation-aware attention network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Neural Networks and Learning Systems</span>, 32(6):2547–2560, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.

</span>
<span class="ltx_bibblock">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 6881–6890, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yifei Zhang, Désiré Sidibé, Olivier Morel, and Fabrice Mériaudeau.

</span>
<span class="ltx_bibblock">Deep multimodal fusion for semantic image segmentation: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Image and Vision Computing</span>, 105:104042, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poupée, Sébastien Giordano, et al.

</span>
<span class="ltx_bibblock">Flair: a country-scale land cover semantic segmentation dataset from multi-source optical imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Drusch, U. Del Bello, S. Carlier, O. Colin, V. Fernandez, F. Gascon, B. Hoersch, C. Isola, P. Laberinti, P. Martimort, A. Meygret, F. Spoto, O. Sy, F. Marchese, and P. Bargellini.

</span>
<span class="ltx_bibblock">Sentinel-2: Esa’s optical high-resolution mission for gmes operational services.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Remote Sensing of Environment</span>, 120:25–36, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Magdalena Main-Knorn, Bringfried Pflug, Jerome Louis, Vincent Debaecker, Uwe Müller-Wilm, and Ferran Gascon.

</span>
<span class="ltx_bibblock">Sen2cor for sentinel-2.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Image and signal processing for remote sensing XXIII</span>, volume 10427, pages 37–48, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Libo Wang, Rui Li, Ce Zhang, Shenghui Fang, Chenxi Duan, Xiaoliang Meng, and Peter M Atkinson.

</span>
<span class="ltx_bibblock">Unetformer: A unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 190:196–214, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li.

</span>
<span class="ltx_bibblock">Maxvit: Multi-axis vision transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Computer Vision – ECCV 2022</span>, pages 459–479, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Vivien Sainte Fare Garnot and Loic Landrieu.

</span>
<span class="ltx_bibblock">Panoptic segmentation of satellite image time series with convolutional temporal attention networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4872–4881, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1711.05101</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ivica Dimitrovski, Vlatko Spasev, Suzana Loshkovska, and Ivan Kitanovski.

</span>
<span class="ltx_bibblock">U-net ensemble for enhanced semantic segmentation in remote sensing imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Remote Sensing</span>, 16(12), 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
William Falcon and The PyTorch Lightning team.

</span>
<span class="ltx_bibblock">PyTorch Lightning, March 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jakub Straka and Ivan Gruber.

</span>
<span class="ltx_bibblock">Modernized training of u-net for aerial semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span>, pages 776–784, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 07:48:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
