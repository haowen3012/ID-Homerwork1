<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System.</title>
<!--Generated on Thu Apr 25 11:56:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.16548v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S1" title="In Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S2" title="In Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S2.SS1" title="In 2 Related Work ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Camera object detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S2.SS2" title="In 2 Related Work ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pointcloud object detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S2.SS3" title="In 2 Related Work ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Fusion architectures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3" title="In Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.SS1" title="In 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Image network architecture:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.SS2" title="In 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pointcloud network architecture:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.SS3" title="In 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>CDSM fusion:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4" title="In Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments &amp; Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.SS1" title="In 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.SS2" title="In 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.SS3" title="In 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S5" title="In Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System.</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Dworak
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ddworak@agh.edu.pl">ddworak@agh.edu.pl</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mateusz Komorkiewicz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mateusz.komorkiewicz@aptiv.com">mateusz.komorkiewicz@aptiv.com</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paweł Skruch
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:skruch@agh.edu.pl">skruch@agh.edu.pl</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jerzy Baranowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jb@agh.edu.pl">jb@agh.edu.pl</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems.
Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance. Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space. We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation.
To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset. We compare our approach to both single-sensor performance and current state-of-the-art fusion methods. Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
camera , radar , fusion , low-level , neural network , deep learning , object detection , autonomous vehicle

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Expert Systems with Applications</span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\affiliation</span>
<p class="ltx_p" id="p1.2">[agh]organization=AGH University of Kraków,
addressline=Al. Mickiewicza 30,
city=Kraków,
postcode=30-059,
country=Poland</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\affiliation</span>
<p class="ltx_p" id="p2.2">[label2]organization=Aptiv Services,
addressline=Ul. Podgórki tynieckie 2,
city=Kraków,
postcode=30-399,
country=Poland</p>
</div>
<div class="ltx_para" id="p3">
<span class="ltx_ERROR undefined" id="p3.1">{graphicalabstract}</span>
</div>
<figure class="ltx_figure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S0.F1.g1" src="extracted/5558933/preview.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S0.F1.g2" src="extracted/5558933/preview2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example results of CDSM fusion method predictions on NuScenes test data. Predicted objects are marked in blue, both in a camera and an enhanced BEV view. Green cuboids represent matched groundtruth labels. LiDAR pointcloud added for reference in BEV view.</figcaption>
</figure>
<div class="ltx_para" id="p4">
<span class="ltx_ERROR undefined" id="p4.1">{highlights}</span>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">New method of Low Level Fusion of camera and radar data within neural network structure</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">Projection-less approach based on tensor orientation matching</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p" id="p7.1">Lightweight solution, competitive with current SOTA approaches</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Modern cars become more and more autonomous every day. Although they are still far from achieving full level 5 of autonomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib1" title="">1</a>]</cite>, we see significant progress in that research field. One of the major reasons for that is an advancement in artificial perception systems. In autonomous vehicles (AV), the perception system is responsible for recognising the surrounding environment: filtering out the background, detecting other road users (cars, pedestrians etc.) and important infrastructure landmarks (lane markings, traffic signs, traffic lights etc.).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To perform a perception task, the vehicle is provided with a versatile sensors suite <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib2" title="">2</a>]</cite>. A typical configuration includes a high-resolution front camera that is used for general object detection, supplemented by lower-resolution surrounding cameras to provide a 360-degree field of view for detecting objects in close proximity to the car. Additionally, high-density LiDAR sensors are employed for precise distance measurements, while a combination of close and long-range radars is utilized to obtain accurate distance and velocity readings.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Raw data from those sensors, in a form of an image or a poincloud,</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S1.F2.g1" src="extracted/5558933/preview.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S1.F2.g2" src="extracted/5558933/preview2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example results of CDSM fusion method predictions on NuScenes test data. Predicted objects are marked in blue, both in a camera and an enhanced BEV view. Green cuboids represent matched groundtruth labels. LiDAR pointcloud added for reference in BEV view.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">is then processed to obtain a model of an environment, used for example in path planning algorithms and safety systems.
Creating such a model from raw sensor readings is a complicated task. It is complex to the point, where traditional algorithms could not handle the variety and the amount of data collected during different real-life road scenarios, thus machine learning techniques are used to process sensor inputs.
Especially neural networks have proven to be more than capable of performing object detection tasks. They surpass human abilities in recognizing objects in the images. Similarly, pointclouds from LiDAR and radar sensors might be difficult to interpret by humans, whereas neural networks can easily find patterns in them.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To assure an even higher performance of the AV perception system, the fusion algorithm combines single sensor data and yields the final perception outcome. The fusion results should be more robust and benefit from each sensor’s advantageous aspects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib3" title="">3</a>]</cite>. Also, in case of partial sensor blockage or other failure modes, the fusion algorithm provides an additional layer of safety. It could mitigate hazardous effects by relying on more confident sensor readings. Fusion algorithms can be classified as a high or a low level<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib4" title="">4</a>]</cite>. A high-level fusion utilises information about detected objects from separate sensors and fuses them at the object level. A low-level fusion operates closer to raw input data, using information directly from each data stream. The main difference between them is that the high-level fusion operates on already processed sensor detections, whereas the low-level fusion operates on the raw or minimally processed data streams themselves. Therefore low-level fusion neural networks could find patterns in cross-sensor data, that would not be accessible at a higher (object) level.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In case of the autonomous vehicles, low-level fusion is typically done on images and pointcloud data. Images come from automotive-grade cameras and are proven to be vital for the perception system in many ways. But when it comes to pointcloud data, there are two sensors, LiDAR and radar, which may seem very similar in the output they produce, but there are major differences between them. Both sensors produce pointcloud data with accurate distance readings in 3D space, but the sparsity of LiDAR pointcloud (a few hundred thousand points) is much denser than radar (a few hundred points), hence it contains more information. That comes with a price, as rotation LiDARs are much more expensive sensors and thus are not suitable for mass production from the manufacturer’s point of view. Solid-state LiDARs are cheaper, but the development of such sensors does not yet achieve the technological readiness level required for automotive-grade sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib5" title="">5</a>]</cite>. On the other hand, radars, which operate on radio frequencies instead of light weaves, are much more resilient to environmental effects on the road. They also provide additional information about velocities for each detected point, which could be very useful for traffic environment modelling.
Taking into account those differences, both sensors seem suitable for fusion with camera images, as they can provide complementary information. That being said, in the deep learning sensor fusion domain, there is only a handful of camera-radar fusion solutions when compare to camera-LiDAR ones.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In this article, we address the problem of low-level camera-radar fusion perception systems based on neural networks by proposing a new method of fusing data from these sensors. Based on the research presented in a related work section, we adopt a multi-view approach used for camera-LiDAR solutions, using separate single-stage architectures for camera processing and a voxelwise radar pointcloud processing. Obtained feature maps are then fused together in a novel Cross-Domain Spatial Matching (CDSM) low-level fusion block to produce an enhanced bird eye view internal representation. Based on this representation, detection heads yield 3D objects bounding boxes with related parameters. We perform experiments on the nuScenes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib6" title="">6</a>]</cite> dataset and show the advantages and disadvantages of our solution.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">This paper is structured as follows. In section 2 we go over related work regarding single-sensor perception as well as different fusion techniques. In section 3 we present our approach to camera-radar based perception system, with detailed network architectures and the CDSM fusion method. Section 4 contains the description of conducted experiments and the results we obtained in the form of perception KPIs and improvement gain of fusion over single sensor systems respectively. Finally, we draw our conclusions in section 5.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Camera object detection</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">An object detection task in camera images was the first field to successfully apply convolutional neural network solutions. Ever since then, researchers are constantly improving the algorithms by applying novel architectures and mechanisms to increase performance. We can divide object detection methods into two major groups: 2D image plane and monocular 3D domain detectors.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">One of the most recognizable architectures among 2D detectors is the single-shot YOLO (You Only Look Once) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib7" title="">7</a>]</cite> network. Over time, improvements have been proposed to increase initial network performance. YOLOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib8" title="">8</a>]</cite> utilises an anchor box mechanism, where instead of predicting raw bounding boxes size, it is done relative to the most suitable, predefined anchor size. YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib9" title="">9</a>]</cite> introduces multi-scale training for small, medium and large objects at different levels of neural network Feature Pyramid Network (FPN), which are then concatenated right before the Non-Max Suppression (NMS) algorithm. YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib10" title="">10</a>]</cite> refine the network architecture by applying Cross-stage Partial Connections (CPS) backbone, Path Aggregation Network (PAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib11" title="">11</a>]</cite>, attention in the form of Convolutional Block Attention Mechanism (CBAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib12" title="">12</a>]</cite>, CIoU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib13" title="">13</a>]</cite> metric for improved loss calculation and mish activation function. Based on the same concept, RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib14" title="">14</a>]</cite> architecture introduces improvements in a form of a focal loss. The new loss function refines class imbalance problem and overall training speed and stability.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Further optimizing of single-shot detectors architecture, researchers from Google took a closer look at model scaling, namely the width, the depth and the resolution of the model. In their paper EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib15" title="">15</a>]</cite>, through compound coefficient, they designed smaller and faster, yet better-performing architecture, which was chosen from a set of models with various width, depth and resolution parameters. Directly following that idea, the detection network architecture EfficientDet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib16" title="">16</a>]</cite> was introduced, which uses EfficientNet as a backbone. Additionally, expanding on the idea of multi-scale feature fusion, they proposed a weighted bi-directional feature pyramid network (BiFPN) to propagate internal network representation from various layers even more effectively.
In EfficientNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib17" title="">17</a>]</cite>, the backbone architecture was optimized even more to achieve better training speed and parameter efficiency in terms of the model size.
Different backbone architecture is shown in Deep Layer Aggregation (DLA)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib18" title="">18</a>]</cite>. Authors propose deeper features aggregation to improve fuse information sharing across layers of the backbone. This includes both Iterative Deep Aggregation (IDA) and Hierarchical Deep Aggregation (HDA) novel features aggregation methods.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Although object detection in 3D space from a single monocular camera image is a much more complicated task, recent studies show, that specific neural network architectures are also capable of achieving meaningful results. In CenterNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib19" title="">19</a>]</cite>, the extension of 2D model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib20" title="">20</a>]</cite>, the proposed approach is to divide 3D object detection into two steps. The first is an anchorless prediction of the centre for a given cuboid in the image and the second step is the regression of all 3D parameters such as depth, 3D dimensions, and rotation angles. After a projection of the predicted centre, 3D results are obtained. Similarly, in FCOS3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib21" title="">21</a>]</cite>, authors use a set of predefined landmark 3D points in the image to perform 2D centreness prediction and based on 2D position and depth project it to 2.5D space. The rest of the parameters are regressed in 3D space as well, to yield final object predictions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pointcloud object detection</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">An input sensor data from LiDAR and radar comes in a form of a list of points coordinates and corresponding features. Depending on the sensor, the features can be reflection intensity for LiDAR or cross-section and velocities for radar. The list of points is often referred to as a pointcloud. Pointcloud processing poses some challenges with regard to neural networks. The network must be invariant to all permutations of input points; changing the order of the data in the input list should yield the same results. The length of such a list could also vary depending on the sensor reading, but neural network architecture, due to how it is structured, tends to except fixed input size.
On the other hand, when scattered in 3-dimensional space, pointcloud data is very sparse (95% of that space remains empty). Those issues resulted in two approaches to processing pointclouds with neural networks: pointwise and voxelwise.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">A pointwise approach in classification network Point-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib22" title="">22</a>]</cite> uses transformation dense layers to extract features for each point individually, although the weights of those layers are shared. To assure invariance to points order, the max pooling layer is used to extract global features. Another pointwise architecture, PointRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib23" title="">23</a>]</cite> takes a two-stage approach for 3D object detection tasks. Stage one, segments points from the background and generates a small number of detections in a bottom-up manner. In stage two, those detections are refined pointwise with respect to local spatial features and global semantic features, which results in accurate bounding boxes and confidence scores.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">For a detection problem, voxelwise methods are more commonly used. Introduced in VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib24" title="">24</a>]</cite>, the idea is to scatter points in 3D space, to minimize computational effort and data sparsity, whole space is divided into smaller cuboids called voxels. Each voxel has features calculated by Voxel Feature Extractor (VFE) layers based on points inside it. After feature extraction, a fixed-size output tensor is processed by 3D convolutional layers to produce 3D detections. Pointpillars <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib25" title="">25</a>]</cite> algorithm
changes feature extraction by stacking voxels vertically (along the z-axis) into pillars. By doing so, the output of the extraction is a three-dimensional tensor rather than a four-dimensional one, like in VoxelNet. This enables the usage of 2D convolutions instead of 3D ones, and as shown in the paper tremendously increases inference time (up to real-time).</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">In a recent work called PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib26" title="">26</a>]</cite>, authors propose both pointwise and voxelwise processing methods combined into one network architecture. In addition to normal voxel feature extraction, feature maps from voxelwise subnetworks are fused with pointwise features in the original Voxel Set Abstraction Module. As a result of the fusion, Keypoint Features are produced and passed to the detection head to amplify certain regions in the output grid.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Radar-only 3D object detection, on the other hand, is a less popular subject of research. In recent NVRadarNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib27" title="">27</a>]</cite>, authors use sensor peak detections, rather than raw antennas signal, to create a sparse radar pointcloud. Such pointcloud is scattered onto a BEV grid and processed via an encoder-decoder model to yield 3D objects. They present the results on the NuScenes dataset, but they are far from what LiDAR sensors achieve in terms of KPI metrics for 3D object detection tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion architectures</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Sensor fusion algorithms fuse the data from different sensors to obtain improved performance. This is especially true for image and pointcloud data, as cameras and LiDARs or radars perceive the environment in a completely different, but complementary manner. Due to significant differences in sensor readings domains, image camera view and pointcloud 3D surrounding view, the fusion poses a problem of incorporating those two sources of information together.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In multi-view setups (AVOD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib28" title="">28</a>]</cite>, MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib29" title="">29</a>]</cite>, PointFusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib30" title="">30</a>]</cite>), each sensor input is processed by a distinct subnetwork to obtain view-specific feature maps. Those views are typically a Bird’s Eye pointcloud View (BEV), a front pointcloud view (3D points projected to camera plain view) and a camera view. Based on concatenated feature maps, the fusion region proposal network determines Regions of Interest (ROIs) for detection heads. In those solutions, the fusion process is typically performed in an end-to-end manner, where the specific method of merging detailed information is determined by the distribution of network weights learned during the training process.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">A different approach to fusion is shown in PointPillars++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib31" title="">31</a>]</cite> and Joint 3D Object Detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib32" title="">32</a>]</cite>. Authors enhance pointcloud data in LiDAR front view by incorporating camera pixel information into corresponding points. The fused front view is then processed as a pointcloud with additional features by a neural network. By performing this deterministic step during input processing, a certain fusion method is forced upon the neural network architecture and the usage of both information sources is better conditioned.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Novel solutions for the drivable area and road detection field can be also applied to object detection fusion. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib33" title="">33</a>]</cite>, besides projecting LiDAR pointcloud onto a camera image, authors apply a depth-completion algorithm to create dense depth maps from sparse points in a front view, thus providing more information to the fusion algorithm. The opposite solution was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib34" title="">34</a>]</cite>. Instead of projecting points onto the image, the pixels data was projected onto a pointcloud BEV occupancy grid, resulting in a completely different fusion domain, but achieving comparable results.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">All the fusion methods discussed were designed with respect to pointcloud data from a LiDAR sensor. There are only a handful of fusion deep learning solutions that utilize camera images and a radar pointcloud, as mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib35" title="">35</a>]</cite>. In the same paper, authors propose a base multi-view network architecture and present their results, which are however inferior in terms of performance to state-of-the-art camera-LiDAR fusion methods. They explain possible reasons for this gap to be a matter of a small training dataset, as well as pointcloud data differences specific for both sensors. On the other hand, CRF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib36" title="">36</a>]</cite> achieves satisfying results for a camera-radar fusion on a newer, much larger NuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib6" title="">6</a>]</cite>. The approach for the fusion is to enhance a camera image with projected radar points but in the form of vertical lines in the image. Authors show improvements over a baseline camera-only object detection network. Regardless, the objects are detected in a 2D camera image space, rather than a 3D domain.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">Predictions in 3D space are obtained in recent
CenterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib37" title="">37</a>]</cite> architecture. The fusion is done on a camera image processed similarly to the CenterNet vision-only model approach but with additional information from radar detections. First, a 2D centre point and object features are predicted in the image, which is then associated with extracted radar features via the frustum association mechanism. The fusion of two sensor feature maps leads to final 3D predictions.
In FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib38" title="">38</a>]</cite>, authors proposed a framework to fuse camera images with both LiDAR and radar pointclouds. They employ a query-based modality agnostic feature sampler to fuse all sensor features and accommodate a transformer decoder to predict 3D objects directly.</p>
</div>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1">Those fusion solutions are currently achieving top positions in the official NuScenes 3D object detection ranking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib39" title="">39</a>]</cite>. In the next section, we present our approach to camera and radar sensor data fusion. We target the full 3D prediction domain, as such predictions are more desirable input for perception systems, although they are also more challenging to obtain. To that end, we propose a new simple yet effective method to fuse image and pointcloud feature maps to accomplish said task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our approach to fusion adopts a multi-view setup concept. We use separate network architectures to process both camera images and radar pointcloud data (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.F3" title="Figure 3 ‣ 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">3</span></a>). Input from the camera is processed in a 2D image domain, whereas radar pointcloud is processed in a 3D space in an enhanced BEV. Both neural networks could produce their respective output, predictions in related domains. Additionally, for the purpose of low-level sensor fusion, we also introduce novel Cross-Domain Spatial Matching (CDSM) fusion block. Our goal is to fuse feature maps from intermediate networks layers to create a single fusion output in a 3D space. The main issue with those feature maps is that they come from completely different domains (2D camera and 3D BEV), thus in order to benefit from both sources we need to spatially align them before the fusion, which is done in the CDSM block.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S3.F3.g1" src="extracted/5558933/pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Whole solution pipeline with camera image and pointcloud list inputs, image processing network in blue, pointcloud processing network in yellow, both with optional outputs and CDSM fusion in green with main fusion predictions output.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Image network architecture:</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For camera image processing we designed a single-stage detector based on EfficientDet network structure. There are 3 main elements that constitute our model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.F4" title="Figure 4 ‣ 3.1 Image network architecture: ‣ 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">4</span></a>): an EfficientNetV2 backbone for initial features extraction, a BiFPN that aggregates and merges features across different levels of abstraction and finally classification and regression heads, which predict the final outcome.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S3.F4.g1" src="extracted/5558933/vis_net.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Camera network architecture.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">While the core concept remains unchanged, we made modifications to the network structure in order to optimize it for our specific purpose. An input resolution was changed to 512x384 pixels to better suit the NuScenes dataset image aspect ratio. We extract features from the backbone network at 3 stages corresponding to official P3-P5 levels (1/8, 1/16 and 1/32 of an input size). Then, we artificially added levels P6 and P7 to match the required input of the BiFPN block. After 4 repeats of BiFPN, refined features are passed to the classification head for object class prediction and score, as well as the regression head, for bounding boxes coordinates and sizes.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We used ImageNet pretrained weights for the EfficientNetV2 backbone and randomly initialized ones for the BiFPN and prediction heads. Through experiments, we decided to choose a mix of LeakyRelu and Mish activation functions across all layers. We also tried different normalization layers: BatchNorm, GroupNorm, InstanceNorm and LayerNorm, where the last one proves to work best for us.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Finally, the model predicts objects on 5 different scales (output grid size 1/8, 1/16, 1/32, 1/64 and 1/128 of an input size) with respect to corresponding anchors. Anchors have been automatically generated based on each grid size and combinations of 3 scale factors and 3 ratio factors, which resulted in 9 anchors per grid cell.
In order to yield final results, we use the Non-Max Suppression algorithm on detections from all 5 scales simultaneously to remove duplicates and overlapping detections.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pointcloud network architecture:</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">For the radar pointcloud processing network, we took inspiration from architectures created to process LiDAR pointclouds. We process radar data with a voxel-wise approach, we divided the whole 3D space into a voxel grid of size 1m x 1m x 1m, due to high data sparsity. Then, in Voxel Feature Extractor (VFE), based on radar points in each voxel, we calculate its features. Maximum points per voxel are limited to 5, as VFE requires a fixed amount of points. To ensure sufficient inference time, we stack voxels along Z-axis to transform voxel feature tensors from 4D to 3D.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S3.F5.g1" src="extracted/5558933/pc_net.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Radar network architecture.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">After Voxel features extraction, the pointcloud network architecture is similar to the previously described image network: it has a backbone, a BiFPN block and prediction heads (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.F5" title="Figure 5 ‣ 3.2 Pointcloud network architecture: ‣ 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">5</span></a>). Because we are no longer using ImageNet pretrained weights, we changed the backbone to DLA34, which we further significantly modified for a pointcloud processing purpose. Our new backbone is much smaller than EfficienNet but still provides aggregation functions of a DLA architecture. The BiFPN blocks and prediction heads have also been reduced in terms of the number of layers. The reason for this is the sparse nature of radar pointcloud and a rather low amount of information to process (compare to the camera or the LiDAR).</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The output of the pointcloud processing network is a set of 3 BEV grids of sizes 80x80, 40x40 and 20x20 (cell size of 1m, 2m and 4m respectively), that covers 80x80m ROI. The objects are also predicted with respect to auto-generated anchors per each scale. The difference is in encoding an additional Z dimension of the centre and height of a bounding box, as well as yaw rotation angle in 3D for each prediction. Combining predictions in an NMS algorithm yields final results.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>CDSM fusion:</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The main innovation proposed in our solution is a fusion block called Cross Domain Spatial Matching (CDSM). The core concept of this fusion block is based on the spatial alignment of the information contained in sensor readings from the camera image and radar pointcloud, as respective feature maps from each network intermediate layer are initially misaligned. CDSM consists of 2 major elements: Domain Alignment and Aligned Features Fusion.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To better understand this idea, we first introduce a vehicle coordinate system (VCS). The VCS is centred on the car’s front axle, with X-axis pointing forward, Y-axis pointing to the left of the car and the Z-axis straight up. Considering the VCS, we can position sensor readings, namely the image and the pointcloud voxel grid in this one, unified space.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.F6" title="Figure 6 ‣ 3.3 CDSM fusion: ‣ 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">6</span></a>, related 3D tensors for both inputs have different orientations. For a camera image, the first 2 dimensions correspond to VCS ZY-plane and learned features (initially RGB values) span throughout X-axis. In the case of a pointcloud voxel grid, the first 2 dimensions correspond to VCS XY-plain and features (initially stacked VFE outputs) span along Z-axis.
The latter representation is consistent with the expected single-shot perception network output, that is a BEV grid (in XY-plain) with detected objects and its parameters. However, fusing information from the camera poses a problem, as those tensors contain features from different perspectives.
In the CDSM fusion block, we address this fusion problem with the following solution.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="329" id="S3.F6.g1" src="extracted/5558933/cdsm_align.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>CDSM Domain alignment diagram. Image features are marked in blue, pointcloud features in yellow and the
fusion elements in green. As we can observe, the initial feature maps orientation in the VCS coordinate system is mismatched. We apply a custom CDMS align layer, rotating 2D image features to match those of the pointcloud, in preparation for the fusion architecture.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.p3.1.1">Domain alignment.</span> Before fusing information from both views, we align the tensors to match their spatial orientation in VCS. To do so, we implement a custom CDSM rotation layer to perform such an operation. In principle, we used a chain of quaternion rotations to calculate the final rotation matrix and apply it (via matrix multiplication) to tensor indexes. We also shift the new indexes by calculated offset to align the (0,0,0) tensor index (as some rotations result in negative index values). Finally, we gather all values from old indexes and scatter them across rotated output tensor according to new indexes. We use the CDSM rotation layer to match camera and radar feature maps tensors with respect to the VCS. The parameters and order of camera features tensor rotations are as follows: firstly rotating by 180 degrees around the first dimension (VCS Z-axis) and then by 90 degrees around the second dimension (VCS Y-axis).
It is worth mentioning that chosen combination and order of rotate operations not only assures that both tensors are in the same orientation with respect to VCS but also both centres of the VCS are aligned in the same position. Such alignment can not be achieved with any combination of permutations and/or transpositions of the input tensor dimensions.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S3.F7.g1" src="extracted/5558933/cdsm_fusion.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>CDSM fusion architecture. Aligned image features are marked in blue, pointcloud features in yellow and the
fusion elements in green. We apply aggregation and refinement steps to image feature maps and fuse them together with pointcloud data using tensor concatenation. Then, after a BiFPN processing, prediction heads output final fused 3D predictions.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.p4.1.1">Aligned features fusion.</span> With both tensors spatially aligned we are able to merge information from both views in a fusion block. Our proposed CDSM fusion method (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S3.F7" title="Figure 7 ‣ 3.3 CDSM fusion: ‣ 3 Proposed Approach ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">7</span></a>) can be divided into three following stages.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">At first, we take camera feature maps from different scale levels and we aggregate them on a single BEV map. The reason to do so is that those feature maps are responsible for detecting objects of different sizes in a camera plane, and thus due to perspective mapping, they correspond to particular regions in the BEV domain at a certain distance from the camera sensor. We used Grad-CAM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib40" title="">40</a>]</cite> visualization method to determine the distance ranges for features from each scale level. Additionally, when aggregating feature maps, we take into account the relation of the camera sensor field of view with respect to the output BEV grid. This assures that the features are not placed in a 3D space not visible in the image.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">After the aggregation, we propose a features refinement step. It consists of several 2D convolutional layers in the BEV domain. Similarly to the backbone concept, we process the features from detailed to more general ones, creating smaller grid representations of the same BEV area. This step allows us to obtain the relations between different feature maps throughout the training process in an end-to-end manner, instead of manually invoking them onto the model. It also creates higher-level features, that capture larger areas in a BEV. Finally, the result of the refinement is a set of 3 different BEV grid feature maps from the camera sensor in a 3D domain, which we could directly fuse with pointcloud feature maps.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">The fusion of camera and pointcloud features is a relatively simple task now, as we converted them into the same coordinate system. Aggregated and refined camera feature maps in BEV correspond spatially to those obtained during radar data processing. During architecture design we assure compatible grid sizes so that we could concatenate both grid tensors together, stacking camera and radar features for each grid cell along the channel dimension.
We apply another BiFPN block to concatenated feature maps at different levels in order to further combine both sensors’ information into a single 3D internal representation. This representation is used in prediction heads to yield final 3D object predictions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments &amp; Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We train our fusion solution on a popular automotive dataset called NuScenes, published in 2019. Recorded scenes in this dataset come from real-world test drives across different environments and cities. Detailed information about the dataset is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#bib.bib6" title="">6</a>]</cite> paper. We used NuScenes version 1.0.
Regarding sensors setup, the car was equipped with 6 cameras, 1 top LiDAR and 5 radar sensors. For the purpose of this research, we only use a front view RGB camera along with LiDAR and radar readings (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.F8" title="Figure 8 ‣ 4.1 Dataset ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.F9" title="Figure 9 ‣ 4.1 Dataset ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">9</span></a>) within the chosen field of view (FOV). As a FOV we decided to take into account only the area where pointcloud data and camera view overlap, in the previously mentioned VCS this area is bounded from 0m to 80m in the X-axis (in front of a car), from -40m to 40m in Y-axis (from right to left) and from 0m to 5m in Z-axis (the height). Dataset split follows common train, validation and test sets division and the sizes of those are 19872, 8111 and 4485 samples respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S4.F8.g1" src="extracted/5558933/nu_img.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>NuScenes camera view with projected LiDAR points (blue), radar points (red) and labels. Labels with visibility over 50% are marked in green, otherwise in yellow. Additional overlays are shown to present data, camera processing network is fed with raw RGB image. Best viewed in colour.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p2.1.1">Image data preprocessing.</span> NuScenes front RGB camera has a resolution of 1600x900 pixels. This is quite a large size to be processed by the neural network. To mitigate computing requirements for our model we decide to resize images to 512x384 pixels resolution. We also used a letterbox resizing mechanism to keep the image aspect ratio and normalization of pixel values from 0 to 1 across all RGB channels.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="477" id="S4.F9.g1" src="extracted/5558933/nu_bev.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>NuScenes Bird’s Eye View with projected LiDAR points (blue), radar points (red) and labels. In addition to colour-coded label visibility, solid line style indicates both LiDAR and radar points within the labelled object, dashed line only LiDAR points and dotted line neither LiDAR nor radar points within the labelled object. Best viewed in colour.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p3.1.1">Pointcloud data preprocessing.</span> Pointcloud data comes in the form of a list of points with XYZ coordinates in the sensor coordinate system along with sensors’ specific readings, intensity for LiDAR and parameters like velocities, cross-section etc. for radar. Firstly we map those coordinates to the defined VCS. Additionally, we remove points that fall outside our defined FOV, as they pose no useful information to the fusion algorithm. Meanwhile, this step helps with the network inference speed, as fewer data points need to be processed. Clipping pointclouds to the FOV results in fewer points from LiDAR and radar sensors for further processing. The average number of points in pointcloud per sample is 13567 for LiDAR and 45 for radar data.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Labels detailed information regarding label visibility for the camera, LiDAR and radar sensors across the whole dataset. Statistics were captured using NuScenes human annotation information per each labelled object present in the data.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">All classes</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Cars</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Pedestrians</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.2.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Total labels count</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">549289</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">219328</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">116952</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.3.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Camera visibility over 40%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">346475 (63%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">126355 (58%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">72459 (62%)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.4.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Labels with LiDAR points</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">451621 (82%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">170519 (78%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">102295 (87%)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.5.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Labels with radar points</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.5.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">173836 (32%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.5.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">101049 (46%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.5.5.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">12839 (11%)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.6.6.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Mean LiDAR points per label</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.6.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.6.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">127</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.6.6.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">14</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.7.7.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Mean radar points per label</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.7.7.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">2.26</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.7.7.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">1.96</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.7.7.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">1.14</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p4.1.1">Label data preprocessing.</span> NuScenes dataset was annotated manually by humans in 3D space, based on LiDAR pointcloud and camera images. Labels are divided into classes like cars, pedestrians, trucks etc. Each class has sub-classes i.e. sitting pedestrian, walking pedestrian etc. For the purpose of object detection, we only distinguish top-level classes.
In the camera object detection, we transform corners of 3D labels onto the camera image plane and draw the smallest rectangle bounding box containing all projected points. We also scale those bounding boxes according to the original image resize coefficients. For pointcloud and fusion detection, we took labels straight from the NuScenes database, as they are placed in the same space, but the following postprocessing was done regarding label filtering.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">NuScenes labels provide additional information about the visibility of the objects in the camera image, as well as a number of LiDAR and radar points that belong to a given labelled object. This information allowed us to filter some of the labels, as there were no data required to detect those objects in this particular sensor setup. Based on characteristics from Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">1</span></a>, we decided to use only labels with visibility over 40% as camera object detection groundtruth and labels that have at least one radar detection as 3D enhanced BEV object detection groundtruth. For the fusion we want to prove its robustness, thus groundtruth should be either visible in the camera or has radar detection or both.
Lastly, we decided to focus on car objects solely, as the radar detections for that class are reliable enough and true benefits of fusion with that sensor could be observed.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In order to prove our method and show fusion benefits over single-sensor solutions, we trained both camera and radar detection networks separately, as well as combined a multi-sensor fusion model with the CDSM block.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.3">Starting from single sensor architectures, we trained camera 2D and radar 3D processing models. Apart from ImageNet pretrained weights for the camera EfficientNetV2 backbone, we used the random Xavier initialization method for the DLA backbone, BiFPNs and prediction heads. For classification heads, we utilized focal loss with fine-tuned hyperparameters <math alttext="\alpha=0.25" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">α</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝛼</ci><cn id="S4.SS2.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p2.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\alpha=0.25</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_α = 0.25</annotation></semantics></math> and <math alttext="\gamma=1.5" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">γ</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝛾</ci><cn id="S4.SS2.p2.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.p2.2.m2.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\gamma=1.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_γ = 1.5</annotation></semantics></math> and weighted mean square error loss for regression heads across both models. The optimization process was done with Adam, the initial learning rate of <math alttext="lr=3e^{-5}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mrow id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2.2" xref="S4.SS2.p2.3.m3.1.1.2.2.cmml">l</mi><mo id="S4.SS2.p2.3.m3.1.1.2.1" xref="S4.SS2.p2.3.m3.1.1.2.1.cmml">⁢</mo><mi id="S4.SS2.p2.3.m3.1.1.2.3" xref="S4.SS2.p2.3.m3.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mn id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">3</mn><mo id="S4.SS2.p2.3.m3.1.1.3.1" xref="S4.SS2.p2.3.m3.1.1.3.1.cmml">⁢</mo><msup id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.3.3.2" xref="S4.SS2.p2.3.m3.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p2.3.m3.1.1.3.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.3.cmml"><mo id="S4.SS2.p2.3.m3.1.1.3.3.3a" xref="S4.SS2.p2.3.m3.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p2.3.m3.1.1.3.3.3.2" xref="S4.SS2.p2.3.m3.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><apply id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2"><times id="S4.SS2.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.p2.3.m3.1.1.2.1"></times><ci id="S4.SS2.p2.3.m3.1.1.2.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2.2">𝑙</ci><ci id="S4.SS2.p2.3.m3.1.1.2.3.cmml" xref="S4.SS2.p2.3.m3.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><times id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.1"></times><cn id="S4.SS2.p2.3.m3.1.1.3.2.cmml" type="integer" xref="S4.SS2.p2.3.m3.1.1.3.2">3</cn><apply id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p2.3.m3.1.1.3.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3.3"><minus id="S4.SS2.p2.3.m3.1.1.3.3.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3.3"></minus><cn id="S4.SS2.p2.3.m3.1.1.3.3.3.2.cmml" type="integer" xref="S4.SS2.p2.3.m3.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">lr=3e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_l italic_r = 3 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and the cosine annealing learning rate scheduler for runtime adjustments. We trained models until early stopping, monitoring validation loss, and did not show any improvements in 5 consecutive epochs.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Both trained models achieve decent results, making them suitable sub-models for the CDSM fusion. However, we find it rather unrelated to compare 2D camera metrics with 3D radar and fusion ones. To that end, we trained another vision-only model, that predicts objects in 3D space based on monocular camera images. It was trained on top of the existing 2D model, but after obtaining 2D feature maps from BiFPN we applied CDSM alignment and aggregation layers, without any fusion with radar data. Such transformation of 2D features into 3D space enables direct prediction of objects in that domain, similar to the CDSM fusion concept.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Lastly, we took both single-sensor models and conducted end-to-end CDSM fusion model training. We used previously pretrained sub-models to obtain sensor-specific feature maps from camera and radar data and apply CDSM alignment, aggregation and fusion to them. Training hyperparameters were similar to single-sensor ones. We also experiment with fine-tuning the pretrained networks. At first, we freeze them and trained only the fusion part of the architecture. Afterwards, we optimized them as well during the training, making adjustments precisely for the fusion purpose.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Evaluation of obtained results was done on part of the NuScenes dataset, which consists of particular scene sequences not used in the training and validation process. We utilized the most popular performance metric for object detection tasks called mean average precision score (mAP), which is based upon precision-recall relation at different threshold levels. Moreover, mAP is highly dependent on the true positive association method, thus we explicitly state what method we used in each experiment, whether it is intersection over union (IoU) or absolute distance (DIST) between centres of 3D cuboids.</p>
</div>
<figure class="ltx_figure" id="S4.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F10.g1" src="extracted/5558933/results_cam.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F10.g2" src="extracted/5558933/results_pc.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F10.g3" src="extracted/5558933/results_fus.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Prediction results for the same test dataset scene for camera-only, radar-only and CDMS fusion models respectively from top to bottom. On both. images and corresponding 3D views, we marked predicted bounding boxes matched as true positives (predictions in blue, matched targets in green), false detections (magenta) and missed objects (yellow). LiDAR pointcloud added for reference in BEV view.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.F10" title="Figure 10 ‣ 4.3 Results ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">10</span></a>, we present results for the same dataset sample obtained from a single camera and radar sensor networks, as well as from the fusion one. For the camera-only model, we can observe a high object detection rate, as well as accurate general size estimation. On the other hand, depth distance prediction in 3D is rather inaccurate, which results in mismatched detections due to failed association process. The radar-only model, on the contrary, predicts accurate positions, but due to the low amount of detections, it struggles to forecast objects’ dimensions and orientation properly. Finally, the fusion model makes use of both sensors’ advantages and mitigates their weaknesses. The fusion of radar data precise position readings and camera ability to predict the accurate size, orientation and class results in the CDSM model vastly outperforms single sensor models.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.T2" title="Table 2 ‣ 4.3 Results ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">2</span></a>, we arranged the results metrics for trained single sensor and fusion models, along with modality, predictions domain and association method used to calculate the mAP score.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>mAP performance metric comparison for all of our single sensor models as well as fusion ones. Modality corresponds to C - camera, and R - radar sensors. The detection domain is either a 2D image space or a 3D-enhanced BEV grid. The association method for the image network is intersection over union with a 0.2 threshold value (IOU20), whereas 3D predictions were matched with 2m distance criteria (DIST2).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.2">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.3">Modality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.4">Association</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1">mAP<sub class="ltx_sub" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.1.1.1">car</span></sub>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1">Vision model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2">C (2D)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3">IOU20</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.2.1.4">0.741</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.1.1">Vision model</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.1.2">C (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.1.3" rowspan="4"><span class="ltx_text" id="S4.T2.1.3.1.3.1">DIST2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.1.4">0.461</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.2.1">Pointcloud model</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.2.2">R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.2.3">0.324</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.5.3.1">CDSM Fusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.3.2">C+R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.3.3">0.523</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.6.4.1">CDSM Fusion (FT)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.4.2">C+R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.4.3">0.681</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Although the vision-only model in 2D has the highest mAP score, it is the only solution that yields predictions in a 2D image space, which is a far less complex task than a 3D object detection. When we consider the vision-only model, but in a 3D domain, the mAP score is significantly lower, as a result of an additional depth estimation task for each object, based on a single image frame only. The radar pointcloud-based model score is even lower, due to discussed high radar detections sparsity issue. The predictions, even if present, are often considered false positives, as the association conditions are not met, mostly because of poor size estimation.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">The fusion model outperforms both single sensors by a large margin. Considering the same association metric, the mAP is much higher, which indicates that more objects are detected correctly with better overall accuracy. On top of that, a fine-tuned version, where we do not freeze single sensor submodels and adjust their parameters during the</p>
</div>
<figure class="ltx_figure" id="S4.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F11.g1" src="extracted/5558933/corner_cam.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F11.g2" src="extracted/5558933/corner_pc.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="S4.F11.g3" src="extracted/5558933/corner_fus.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Corner case example for camera, radar and fusion models respectively from top to bottom. Same bounding box color coding as in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.F10" title="Figure 10 ‣ 4.3 Results ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">10</span></a>. Even when single-sensor models fail to predict all objects in the scene, a fusion model utilizes both sensors’ data to improve upon each of them.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">training, achieves even better results, as the internal representation of camera and radar data is accommodated for fusion purposes.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">Another situations, where fusion benefits could be observed, are single sensor failures to detect particular objects. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.F11" title="Figure 11 ‣ 4.3 Results ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">11</span></a> we showcase such a corner case, in which the camera model positively identified parked cars on the right, but the proceeding vehicle is detected too far from the groundtruth position. On the other hand, the radar model detection of the car in front is really precise, while parked cars are missed completely. The fusion model predicts all objects and even improves on both sensors’ position and size estimation.</p>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.1">Finally, we compare our results to other state-of-the-art solutions in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16548v1#S4.T3" title="Table 3 ‣ 4.3 Results ‣ 4 Experiments &amp; Results ‣ Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System."><span class="ltx_text ltx_ref_tag">3</span></a>. In order to do so, we calculated mAP score according to the official NuScenes ranking, which is a mean of mAP for four different association methods, namely DIST 0.5m, 1m, 2m and 4m.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of different state-of-the-art solutions to our approach using mAP performance metric. Modality corresponds to C - camera, L - LiDAR and R - radar sensors. The detection domain is either a 2D image space or a 3D-enhanced BEV grid. We used the official NuScenes association method.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.1.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Modality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">Association</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">mAP<sub class="ltx_sub" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.1.1">car</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.2.1.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">PointPillars</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.2.1.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">L (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.2.1.3" rowspan="6" style="padding-top:-1.5pt;padding-bottom:-1.5pt;"><span class="ltx_text" id="S4.T3.1.2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.3.1.1">
<span class="ltx_p" id="S4.T3.1.2.1.3.1.1.1">Average mAP</span>
<span class="ltx_p" id="S4.T3.1.2.1.3.1.1.2">over all DIST</span>
<span class="ltx_p" id="S4.T3.1.2.1.3.1.1.3">0.5,1,2 and 4</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.2.1.4" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.684</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.3.2.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">FCOS3D</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.2.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">C (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.2.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.524</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.4.3.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">CRFNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.4.3.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">C+R (2D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.4.3.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.559</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.5.4.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">CenterFusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.5.4.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">C+R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.5.4.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.509</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.6.5.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">FUTR3D</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.6.5.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">C+R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.6.5.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.52-0.54*</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.7.6.1" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">CDSM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.7.6.2" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">C+R (3D)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.7.6.3" style="padding-top:-1.5pt;padding-bottom:-1.5pt;">0.535</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S4.I1">
<li class="ltx_item" id="S4.I1.1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I1.1.p1">
<p class="ltx_p" id="S4.I1.1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.1.p1.1.1" style="font-size:70%;">*FUTR3D paper provides only general mAP for C+R=0.35. Based on the comparison of C+L (single-beam) general and car performance, we estimate C+R car class mAP to be somewhere between 0.52-0.54.<span class="ltx_text ltx_font_upright" id="S4.I1.1.p1.1.1.1"></span></span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS3.p9">
<p class="ltx_p" id="S4.SS3.p9.1">The most related similar 3D camera and radar fusion solutions are CenterFusion and FUTR3D. With the final mAP score calculated for car class objects, we see improvements over those two methods in our CDSM model. Additionally, even though our camera-only model achieves a lower score than a similar FCOS3D model, the application of fusion bridges the gap and surpasses both vision-only methods.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this article, we focused on sensor data fusion from camera and radar devices in autonomous vehicle applications. We presented related work for single sensor methods, as well as fusion solutions available for the given sensors suite. On top of that, we thoroughly described our novel approach to the problem with the use of proposed Cross-Domain Spatial Matching transformation and fusion.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In order to justify CDSM fusion benefits, we conducted experiments on the open NuScenes dataset. We trained both single-sensor models and proposed fusion architecture. The presented results show significant improvements in the latter one, both in general mAP metric and specific corner cases. Finally, we compared our approach to other state-of-the-art solutions in the 3D object detection domain, achieving outstanding performance for the camera and radar setup.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Although we are satisfied with the current results, we observe a gap between the camera and the radar single sensor contribution to the fusion. We believe that applying a machine learning approach to raw radar antennas signal, rather than internally post-processed detections, could improve the perception of these sensors and, with our approach, of the entire fusion system.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The research was carried out in cooperation
with Aptiv Services Poland S.A. – Technical Center Kraków and AGH University of Krakow.
It is funded partially by the Polish Ministry of Science and Higher Education (MNiSW) Project No. 0014/DW/2018/02 and partially by National Science Centre, contract no. UMO-2021/41/B/ST7/03851.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. of Automotive Engineers, Taxonomy and Definitions for Terms Related to
Driving Automation Systems for On-Road Motor Vehicles, SAE International,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Kocić, N. Jovičić, V. Drndarević, Sensors and sensor fusion in
autonomous vehicles, in: 2018 26th Telecommunications Forum (TELFOR), 2018,
pp. 420–425.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TELFOR.2018.8612054" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TELFOR.2018.8612054</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Wei, F. Zhang, S. Chang, Y. Liu, H. Wu, Z. Feng,
<a class="ltx_ref ltx_href" href="https://www.mdpi.com/1424-8220/22/7/2542" title="">Mmwave radar and vision
fusion for object detection in autonomous driving: A review</a>, Sensors 22 (7)
(2022).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3390/s22072542" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.3390/s22072542</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1424-8220/22/7/2542" title="">https://www.mdpi.com/1424-8220/22/7/2542</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E. Blasch, J. Llinas, D. Lambert, P. Valin, S. Das, C. Chong, M. Kokar,
E. Shahbazian, High level information fusion developments, issues, and grand
challenges: Fusion 2010 panel discussion, in: 2010 13th International
Conference on Information Fusion, 2010, pp. 1–8.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICIF.2010.5712116" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICIF.2010.5712116</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.-P. Hsu, B. Li, B. Solano-Rivas, A. R. Gohil, P. H. Chan, A. D. Moore,
V. Donzella, A review and perspective on optical phased array for automotive
lidar, IEEE Journal of Selected Topics in Quantum Electronics 27 (1) (2021)
1–16.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/JSTQE.2020.3022948" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/JSTQE.2020.3022948</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
Y. Pan, G. Baldan, O. Beijbom, nuscenes: A multimodal dataset for autonomous
driving, in: 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020, pp. 11618–11628.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR42600.2020.01164" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.01164</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified,
real-time object detection, in: 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016, pp. 779–788.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2016.91" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2016.91</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Redmon, A. Farhadi, Yolo9000: Better, faster, stronger, in: 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp.
6517–6525.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2017.690" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2017.690</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Redmon, A. Farhadi, <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1804.02767" title="">Yolov3: An
incremental improvement</a>, CoRR abs/1804.02767 (2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1804.02767" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1804.02767</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1804.02767" title="">http://arxiv.org/abs/1804.02767</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C. Wang, H. M. Liao,
<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.10934" title="">Yolov4: Optimal speed and accuracy of
object detection</a>, CoRR abs/2004.10934 (2020).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.10934" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2004.10934</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2004.10934" title="">https://arxiv.org/abs/2004.10934</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Liu, L. Qi, H. Qin, J. Shi, J. Jia, Path aggregation network for instance
segmentation, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2018, pp. 8759–8768.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2018.00913" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2018.00913</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Woo, J. Park, J. Lee, I. S. Kweon,
<a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1807.06521" title="">CBAM: convolutional block attention
module</a>, CoRR abs/1807.06521 (2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1807.06521" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1807.06521</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1807.06521" title="">http://arxiv.org/abs/1807.06521</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, D. Ren,
<a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1911.08287" title="">Distance-iou loss: Faster and better
learning for bounding box regression</a>, CoRR abs/1911.08287 (2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1911.08287" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1911.08287</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1911.08287" title="">http://arxiv.org/abs/1911.08287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollár, Focal loss for dense
object detection, in: 2017 IEEE International Conference on Computer Vision
(ICCV), 2017, pp. 2999–3007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV.2017.324" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCV.2017.324</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Tan, Q. V. Le, <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1905.11946" title="">Efficientnet:
Rethinking model scaling for convolutional neural networks</a>, CoRR
abs/1905.11946 (2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1905.11946" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1905.11946</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.11946" title="">http://arxiv.org/abs/1905.11946</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object
detection, in: 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020, pp. 10778–10787.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR42600.2020.01079" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.01079</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Tan, Q. V. Le, <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2104.00298" title="">Efficientnetv2:
Smaller models and faster training</a>, CoRR abs/2104.00298 (2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.00298" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2104.00298</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.00298" title="">https://arxiv.org/abs/2104.00298</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Yu, D. Wang, T. Darrell, <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1707.06484" title="">Deep layer
aggregation</a>, CoRR abs/1707.06484 (2017).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1707.06484" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1707.06484</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.06484" title="">http://arxiv.org/abs/1707.06484</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Zhou, V. Koltun, P. Krähenbühl, Tracking objects as points, in:
A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer Vision – ECCV
2020, Springer International Publishing, Cham, 2020, pp. 474–490.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian, Centernet: Keypoint triplets
for object detection, in: 2019 IEEE/CVF International Conference on Computer
Vision (ICCV), 2019, pp. 6568–6577.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV.2019.00667" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCV.2019.00667</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Wang, X. Zhu, J. Pang, D. Lin, Fcos3d: Fully convolutional one-stage
monocular 3d object detection, in: 2021 IEEE/CVF International Conference on
Computer Vision Workshops (ICCVW), 2021, pp. 913–922.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCVW54120.2021.00107" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCVW54120.2021.00107</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Q. Charles, H. Su, M. Kaichun, L. J. Guibas, Pointnet: Deep learning on
point sets for 3d classification and segmentation, in: 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77–85.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2017.16" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2017.16</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Shi, X. Wang, H. Li, Pointrcnn: 3d object proposal generation and detection
from point cloud, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2019, pp. 770–779.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2019.00086" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2019.00086</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Zhou, O. Tuzel, Voxelnet: End-to-end learning for point cloud based 3d
object detection, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2018, pp. 4490–4499.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2018.00472" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2018.00472</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Beijbom, Pointpillars:
Fast encoders for object detection from point clouds, in: 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp.
12689–12697.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2019.01298" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2019.01298</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, H. Li, Pv-rcnn: Point-voxel
feature set abstraction for 3d object detection, in: 2020 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 10526–10535.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR42600.2020.01054" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.01054</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Popov, P. Gebhardt, K. Chen, R. Oldja, H. Lee, S. Murray, R. Bhargava,
N. Smolyanskiy, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2209.14499" title="">Nvradarnet:
Real-time radar obstacle and free space detection for autonomous driving</a>,
CoRR abs/2209.14499 (2022).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2209.14499" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2209.14499</span></a>,
<a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2209.14499" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2209.14499</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2209.14499" title="">https://doi.org/10.48550/arXiv.2209.14499</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Ku, M. Mozifian, J. Lee, A. Harakeh, S. L. Waslander, Joint 3d proposal
generation and object detection from view aggregation, in: 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), 2018, pp.
1–8.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/IROS.2018.8594049" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IROS.2018.8594049</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. Chen, H. Ma, J. Wan, B. Li, T. Xia, Multi-view 3d object detection network
for autonomous driving, in: 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017, pp. 6526–6534.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2017.691" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2017.691</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Xu, D. Anguelov, A. Jain, Pointfusion: Deep sensor fusion for 3d bounding
box estimation, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2018, pp. 244–253.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR.2018.00033" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2018.00033</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Vora, A. H. Lang, B. Helou, O. Beijbom, Pointpainting: Sequential fusion for
3d object detection, in: 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2020, pp. 4603–4611.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR42600.2020.00466" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.00466</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
G. P. Meyer, J. Charland, D. Hegde, A. Laddha, C. Vallespi-Gonzalez, Sensor
fusion for joint 3d object detection and semantic segmentation, in: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), 2019, pp. 1230–1237.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPRW.2019.00162" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPRW.2019.00162</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
L. Caltagirone, M. Bellone, L. Svensson, M. Wahde, Lidar-camera fusion for road
detection using fully convolutional neural networks, Robotics and Autonomous
Systems 111 (11 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.robot.2018.11.002" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1016/j.robot.2018.11.002</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
F. Wulff, B. Schäufele, O. Sawade, D. Becker, B. Henke, I. Radusch, Early
fusion of camera and lidar for robust road detection based on u-net fcn, in:
2018 IEEE Intelligent Vehicles Symposium (IV), 2018, pp. 1426–1431.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/IVS.2018.8500549" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IVS.2018.8500549</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Meyer, G. Kuschk, Deep learning based 3d object detection for automotive
radar and camera, in: 2019 16th European Radar Conference (EuRAD), 2019, pp.
133–136.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
F. Nobis, M. Geisslinger, M. Weber, J. Betz, M. Lienkamp, A deep learning-based
radar and camera sensor fusion architecture for object detection, in: 2019
Sensor Data Fusion: Trends, Solutions, Applications (SDF), 2019, pp. 1–7.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/SDF.2019.8916629" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/SDF.2019.8916629</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
R. Nabati, H. Qi, Centerfusion: Center-based radar and camera fusion for 3d
object detection, in: 2021 IEEE Winter Conference on Applications of Computer
Vision (WACV), 2021, pp. 1526–1535.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/WACV48630.2021.00157" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/WACV48630.2021.00157</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
X. Chen, T. Zhang, Y. Wang, Y. Wang, H. Zhao, Futr3d: A unified sensor fusion
framework for 3d detection, arXiv preprint arXiv:2203.10642 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Nuscenes object detection ranking,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nuscenes.org/object-detection?externalData=all&amp;mapData=all&amp;modalities=Any" title="">https://www.nuscenes.org/object-detection?externalData=all&amp;mapData=all&amp;modalities=Any</a>,
accessed: 2023-05-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,
Grad-cam: Visual explanations from deep networks via gradient-based
localization, in: 2017 IEEE International Conference on Computer Vision
(ICCV), 2017, pp. 618–626.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV.2017.74" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCV.2017.74</span></a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Apr 25 11:56:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
