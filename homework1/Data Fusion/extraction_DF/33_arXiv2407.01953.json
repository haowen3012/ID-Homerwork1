{
    "S4.T1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1\">Dataset</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.2.1\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.3.1\">F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.2.1.1\">No Fine-tune</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.2.2\">0.4997</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.2.3\">0.1581</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.3.1.1\">Task 1</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.3.2\">0.3490</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.3.3\">0.3913</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.4.4.1.1\">Task 1 + Task 2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.1.4.4.2.1\">\\ul</span>0.6259</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.4.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.1.4.4.3.1\">\\ul</span>0.5634</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1: The performance for two models tasked with classifying sentences as either \"premise\" or \"claim\". It includes two key metrics: Accuracy (ACC) and F1 Score (F1). Model \"Task 1\" was fine-tuned using only the dataset from Task 1, while Model Task 1 + Task 2 used datasets from both Task 1 and Task 2 for fine-tuning.",
        "footnotes": [],
        "references": [
            "Table 1 illustrates that the fine-tuned LLMs have significantly improved reasoning for downstream-specific tasks. Furthermore, the LLMs, fine-tuned using the fused dataset, exhibit significant performance enhancements, where it achieves a 0.5634 F1 score. This evidence supports the notion that integrating different tasks can substantially enhance the reasoning capabilities of LLMs."
        ]
    },
    "S4.T2.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.1\">Rouge-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.1\">Rouge-2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.4.1\">BertScore</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.2.2.1.1\">Task 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.2.2.2\">0.4847</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.2.2.3\">0.2921</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.2.2.4\">0.6904</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.3.1.1\">Task 1 + Task 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T2.1.3.3.2.1\">\\ul</span>0.4920</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T2.1.3.3.3.1\">\\ul</span>0.3015</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T2.1.3.3.4.1\">\\ul</span>0.6946</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2: The performance results for two models tasked with summarizing. It includes metrics for evaluating summarization: Rouge and Bert Score. Model \"Task 1\" was fine-tuned using only the dataset from Task 1, while Model Task 1 + Task 2 used datasets from both Task 1 and Task 2 for fine-tuning.",
        "footnotes": [],
        "references": [
            "Table 2 also demonstrates that the fine-tuned LLMs,by using the fused dataset, achieved significant performance gains in the text summarization task. This reinforces the idea that integrating various tasks can notably enhance the generalization capabilities of LLMs across different applications."
        ]
    }
}