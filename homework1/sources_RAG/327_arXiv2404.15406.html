<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs</title>
<!--Generated on Fri May 24 16:55:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.15406v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S1" title="In Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S2" title="In Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S3" title="In Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S3.SS1" title="In 3 Proposed Method ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Knowledge-based Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S3.SS2" title="In 3 Proposed Method ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4" title="In Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS1" title="In 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS2" title="In 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS3" title="In 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation Protocol</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS4" title="In 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS5" title="In 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Limitations and Future Works</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S5" title="In Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Wiki-LLaVA:
<br class="ltx_break"/>Hierarchical Retrieval-Augmented Generation for Multimodal LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Davide Caffagni<sup class="ltx_sup" id="id13.5.id1"><span class="ltx_text ltx_font_italic" id="id13.5.id1.1">1,∗</span></sup>  Federico Cocchi<sup class="ltx_sup" id="id14.6.id2"><span class="ltx_text ltx_font_italic" id="id14.6.id2.1">1,2,∗</span></sup>  Nicholas Moratelli<sup class="ltx_sup" id="id15.7.id3"><span class="ltx_text ltx_font_italic" id="id15.7.id3.1">1,∗</span></sup>  Sara Sarto<sup class="ltx_sup" id="id16.8.id4"><span class="ltx_text ltx_font_italic" id="id16.8.id4.1">1,∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcella Cornia<sup class="ltx_sup" id="id17.9.id1">1</sup>  Lorenzo Baraldi<sup class="ltx_sup" id="id18.10.id2">1</sup>  Rita Cucchiara<sup class="ltx_sup" id="id19.11.id3"><span class="ltx_text ltx_font_italic" id="id19.11.id3.1">1,3</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id20.12.id4">1</sup>University of Modena and Reggio Emilia, Italy  <sup class="ltx_sup" id="id21.13.id5">2</sup>University of Pisa, Italy  <sup class="ltx_sup" id="id22.14.id6">3</sup>IIT-CNR, Italy
<br class="ltx_break"/><sup class="ltx_sup" id="id23.15.id7"><span class="ltx_text" id="id23.15.id7.1" style="font-size:90%;">1</span></sup><span class="ltx_text ltx_font_typewriter" id="id24.16.id8" style="font-size:90%;">{name.surname}@unimore.it</span>  <sup class="ltx_sup" id="id25.17.id9"><span class="ltx_text" id="id25.17.id9.1" style="font-size:90%;">2</span></sup><span class="ltx_text ltx_font_typewriter" id="id26.18.id10" style="font-size:90%;">{name.surname}@phd.unipi.it</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id27.id1">Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup class="ltx_sup" id="footnote1.1">∗</sup>Equal contribution.</span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recently, Large Language Models (LLMs) have demonstrated impressive performance in zero-shot textual tasks. Specifically, recent literature has devised models capable of tackling diverse tasks, as instructed by the user <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. In this context, the classical approach is that of fine-tuning a model on varied tasks that are described through natural language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, thus empowering the model to assimilate externally provided instructions and facilitating robust generalization across multiple domains. Following these advancements, the computer vision community has started to investigate the extension of such models to vision-and-language contexts, thus generating Multimodal Large Language Models (MLLMs). On this line, the fusion of visual features into LLM backbones through vision-to-language adapters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> has induced notable performance improvements, enabling extensive generalization to vision-and-language tasks requiring elaborate visual descriptions.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Comparison between a standard multimodal LLM and Wiki-LLaVa. Our model integrates knowledge retrieved from an external knowledge base of documents through a hierarchical retrieval pipeline. As a result, it provides more precise answers when tasked with questions that require external knowledge.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this context, MLLMs excel by simply including a small module (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">i.e.</em>, an adapter) that aligns visual features with textual ones. However, despite these models being built upon LLMs trained on large-scale data, they exhibit notable limitations when confronted with highly specific user queries or when a certain degree of compositional reasoning is required to formulate the response. Moreover, certain knowledge proves itself challenging to be encoded within the parameters of an MLLM, due to the scarcity of long-tail information in the training data. In response to this challenge, different benchmarks have been recently introduced for evaluating the capabilities of MLLM to tackle queries related to external data, such as InfoSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and Encyclopedic-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. While different works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> have been testing on these benchmarks, underscoring the significance of this area, none of them has developed architectures specifically designed for tackling external knowledge.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Driving from these considerations, in this paper we propose the first MLLM augmented with a retrieval module, thus shifting the focus towards teaching the model to leverage diverse information in its responses and learning to discern the relative importance of each. In particular, our model retrieves appropriate information from an external knowledge base of documents and employs a hierarchical retrieval approach to identify relevant passages. This additional knowledge is then fed to an MLLM, without changing its structure but improving its answering capabilities. To the best of our knowledge, our work represents the first MLLM to harness the retrieval capability of external sources. We assess the quality of the proposed approach by conducting extensive experiments and comparisons with respect to recent MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> and by showcasing the effectiveness of our design choices. Experimental results demonstrate the advantage of retrieving from external sources and the appropriateness of our model design. Overall, we conceive our work as a first step in the direction of retrieval-augmented MLLMs, which could foster future works in the same area.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Multimodal LLMs.</span>
LLMs have significantly reshaped the landscape of AI research and applications, spearheaded by notable examples like OpenAI’s ChatGPT and GPT-4. These models leverage alignment techniques such as instruction tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> and reinforcement learning from human feedback <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> and achieve remarkable capabilities in language understanding and reasoning. Open-source LLMs like Flan-T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, and Alpaca <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> have further accelerated the advancement within the research community. This surge in the development of LLMs subsequently led to the emergence of MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, which can combine the understating of visual inputs with natural language generation.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Early attempts of building MLLMs such as VisualGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> and Frozen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> used pre-trained language models to enhance vision-and-language models specifically for tasks like image captioning and visual question answering. This initial investigation paved the way for subsequent research in this domain, with the introduction of solutions such as Flamingo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> or BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> which allowed the integration of image features into LLMs respectively through trainable cross-attention layers directly within the LLM or Q-Former blocks that instead combine image and textual features via learnable queries. Building upon these advancements, subsequent models like FROMAGe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, Kosmos-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, and MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> have been introduced to further refine the interplay between visual and language modalities within the LLM architecture.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Concurrently, the LLaVA family of models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> introduced the usage of instruction tuning in the multimodal domain, by training on a curated dataset collected with GPT-4. This strategy is now among the most promising recipes for building MLLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Retrieval-augmented language models.</span>
In recent years, retrieval-augmentation has been applied to language models by expanding their input space with relevant text passages extracted from external sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> or eventually retrieved directly from the web <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. These techniques have demonstrated large
improvements in knowledge-intensive tasks and significant savings in terms of model size.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Traditionally, the integration of external knowledge into textual generation has been confined to the initial stages. Different solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> proposed to adaptively retrieve passages for generation on top of a proprietary LLM. Some works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, instead, focused on capturing knowledge in a more modular and interpretable way, by augmenting the language model pre-training with a latent knowledge retriever. This
allows the model to retrieve and attend documents taken from a large corpus such as Wikipedia.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">While much attention has been directed towards textual augmentation, similar research efforts have recently been dedicated in the context of vision-and-language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. Following this direction, the work presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> proposed a retrieval-augmented visual-language model that encodes world knowledge into a large-scale memory. Other approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> also apply retrieval to specific downstream tasks such as image captioning. Differently from all the aforementioned approaches, our work is the first to apply retrieval-augmentation to MLLMs. We do this by applying a hierarchical retrieval strategy on top of a knowledge base made of multimodal documents.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S2.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Overview of the architecture of Wiki-LLaVA, which augments a multimodal LLM with external knowledge through a hierarchical retrieval pipeline.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold" id="S2.p7.1.1">Knowledge-based visual question answering.</span>
Recently, the emergence of new benchmarks like Encyclopedic-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and InfoSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> has raised the difficulty of standard knowledge-based VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> with questions that require intensive knowledge about specific entities, such that even LLM-based models perform poorly without retrieving information from external sources. Often, contrastive image-text encoders are employed to retrieve the target entity given the query image <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. Then, the entity name is used as a key to access an external knowledge base, which is typically composed of several text passages that encompass the correct answer. In this work, we design a hierarchical retrieval scheme based on CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> and the Contriever model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> to extrapolate relevant passages, and we feed them to an MLLM to help the answer generation.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our goal is to equip Multimodal LLMs (MLLMs) with the ability to answer complex and specific questions that cannot be addressed solely through the image content and pre-trained knowledge. To achieve this, we propose Wiki-LLaVA, which integrates external knowledge derived from an external memory into the LLaVA model, without significantly altering its design. Instead, we augment the capabilities of the model by incorporating retrieval information as additional input context.
Overall, Wiki-LLaVA comprises three components, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>: a visual encoder, which is employed to provide the MLLM with visual context and as a query to retrieve from an external knowledge base, the knowledge base itself (<em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">e.g.</em>, Wikipedia), and a hierarchical retrieval module which retrieves relevant documents and passages from the external knowledge base, to be employed as additional context for the MLLM.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Knowledge-based Augmentation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.5.1">Multimodal integration and autoregressive generation.</span>
An MLLM usually takes as input a multimodal input query, comprising both image and text, and generates a textual output in an autoregressive manner. Formally, the architecture is trained to model a probability distribution <math alttext="p(w_{t}|I,w_{0},w_{1},...,w_{t-1},\theta)" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.4"><semantics id="S3.SS1.p1.1.m1.4a"><mrow id="S3.SS1.p1.1.m1.4.4" xref="S3.SS1.p1.1.m1.4.4.cmml"><mi id="S3.SS1.p1.1.m1.4.4.3" xref="S3.SS1.p1.1.m1.4.4.3.cmml">p</mi><mo id="S3.SS1.p1.1.m1.4.4.2" xref="S3.SS1.p1.1.m1.4.4.2.cmml">⁢</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.4.4.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.4.4.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.5" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.5.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5.2.cmml">w</mi><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.5.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.1.m1.4.4.1.1.1.4" xref="S3.SS1.p1.1.m1.4.4.1.1.1.4.cmml">|</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.2.cmml">w</mi><mn id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.5" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.2.cmml">w</mi><mn id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.6" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" mathvariant="normal" xref="S3.SS1.p1.1.m1.2.2.cmml">…</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.7" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.2.cmml">w</mi><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.2.cmml">t</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.8" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml">,</mo><mi id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml">θ</mi></mrow></mrow><mo id="S3.SS1.p1.1.m1.4.4.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.4b"><apply id="S3.SS1.p1.1.m1.4.4.cmml" xref="S3.SS1.p1.1.m1.4.4"><times id="S3.SS1.p1.1.m1.4.4.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2"></times><ci id="S3.SS1.p1.1.m1.4.4.3.cmml" xref="S3.SS1.p1.1.m1.4.4.3">𝑝</ci><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.4.4.1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.4">conditional</csymbol><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.5.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.5.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.5.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5.2">𝑤</ci><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.5.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.5.3">𝑡</ci></apply><list id="S3.SS1.p1.1.m1.4.4.1.1.1.3.4.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐼</ci><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.2">𝑤</ci><cn id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.2">𝑤</ci><cn id="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">…</ci><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.2">𝑤</ci><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3"><minus id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.1"></minus><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.2">𝑡</ci><cn id="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.4.4.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">𝜃</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.4c">p(w_{t}|I,w_{0},w_{1},...,w_{t-1},\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.4d">italic_p ( italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_I , italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_θ )</annotation></semantics></math>, where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_θ</annotation></semantics></math> denotes the parameters of the model, <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_I</annotation></semantics></math> represents an input image, and <math alttext="{w_{0},..,w_{t-1}}" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1b"><msub id="S3.SS1.p1.4.m4.1.1"><mi id="S3.SS1.p1.4.m4.1.1.2">w</mi><mn id="S3.SS1.p1.4.m4.1.1.3">0</mn></msub><mo id="S3.SS1.p1.4.m4.1.2">,</mo><mo id="S3.SS1.p1.4.m4.1.3" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.SS1.p1.4.m4.1.4" lspace="0.0835em" rspace="0.167em">.</mo><mo id="S3.SS1.p1.4.m4.1.5">,</mo><msub id="S3.SS1.p1.4.m4.1.6"><mi id="S3.SS1.p1.4.m4.1.6.2">w</mi><mrow id="S3.SS1.p1.4.m4.1.6.3"><mi id="S3.SS1.p1.4.m4.1.6.3.2">t</mi><mo id="S3.SS1.p1.4.m4.1.6.3.1">−</mo><mn id="S3.SS1.p1.4.m4.1.6.3.3">1</mn></mrow></msub></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">{w_{0},..,w_{t-1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , . . , italic_w start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> denotes the textual prompt. The textual prompt usually includes a pre-defined system-level prompt and a question related to the input image, given by the user. Clearly, a standard MLLM can only rely on the user prompt, the input image, and the knowledge stored in its internal parameters (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.5.2">i.e.</em>, <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_θ</annotation></semantics></math>) to accommodate requests, thus limiting its ability to answer questions that rely on external knowledge.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">In the rest of the paper, we employ LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> as our reference MLLM. LLaVA exploits the capabilities of a pre-trained LLM (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.4.1">i.e.</em>, Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>) and a pre-trained visual model (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.4.2">i.e.</em>, a CLIP-based visual encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>), which are interconnected through an MLP adapter, in charge of converting CLIP features to dense input tokens. For an input image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_I</annotation></semantics></math>, therefore, LLaVA utilizes a pre-trained CLIP visual encoder <math alttext="E_{v}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">E</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐸</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">E_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, extracts a dense grid of visual features <math alttext="Z_{v}=E_{v}(I)" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.2" xref="S3.SS1.p2.3.m3.1.2.cmml"><msub id="S3.SS1.p2.3.m3.1.2.2" xref="S3.SS1.p2.3.m3.1.2.2.cmml"><mi id="S3.SS1.p2.3.m3.1.2.2.2" xref="S3.SS1.p2.3.m3.1.2.2.2.cmml">Z</mi><mi id="S3.SS1.p2.3.m3.1.2.2.3" xref="S3.SS1.p2.3.m3.1.2.2.3.cmml">v</mi></msub><mo id="S3.SS1.p2.3.m3.1.2.1" xref="S3.SS1.p2.3.m3.1.2.1.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.1.2.3" xref="S3.SS1.p2.3.m3.1.2.3.cmml"><msub id="S3.SS1.p2.3.m3.1.2.3.2" xref="S3.SS1.p2.3.m3.1.2.3.2.cmml"><mi id="S3.SS1.p2.3.m3.1.2.3.2.2" xref="S3.SS1.p2.3.m3.1.2.3.2.2.cmml">E</mi><mi id="S3.SS1.p2.3.m3.1.2.3.2.3" xref="S3.SS1.p2.3.m3.1.2.3.2.3.cmml">v</mi></msub><mo id="S3.SS1.p2.3.m3.1.2.3.1" xref="S3.SS1.p2.3.m3.1.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p2.3.m3.1.2.3.3.2" xref="S3.SS1.p2.3.m3.1.2.3.cmml"><mo id="S3.SS1.p2.3.m3.1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p2.3.m3.1.2.3.cmml">(</mo><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">I</mi><mo id="S3.SS1.p2.3.m3.1.2.3.3.2.2" stretchy="false" xref="S3.SS1.p2.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.2.cmml" xref="S3.SS1.p2.3.m3.1.2"><eq id="S3.SS1.p2.3.m3.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.1"></eq><apply id="S3.SS1.p2.3.m3.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.2.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.2.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.2.2">𝑍</ci><ci id="S3.SS1.p2.3.m3.1.2.2.3.cmml" xref="S3.SS1.p2.3.m3.1.2.2.3">𝑣</ci></apply><apply id="S3.SS1.p2.3.m3.1.2.3.cmml" xref="S3.SS1.p2.3.m3.1.2.3"><times id="S3.SS1.p2.3.m3.1.2.3.1.cmml" xref="S3.SS1.p2.3.m3.1.2.3.1"></times><apply id="S3.SS1.p2.3.m3.1.2.3.2.cmml" xref="S3.SS1.p2.3.m3.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.2.3.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.3.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.2.3.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.3.2.2">𝐸</ci><ci id="S3.SS1.p2.3.m3.1.2.3.2.3.cmml" xref="S3.SS1.p2.3.m3.1.2.3.2.3">𝑣</ci></apply><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">Z_{v}=E_{v}(I)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_Z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_I )</annotation></semantics></math>, which is then projected via a learnable MLP to produce a sequence of dense embedding tokens <math alttext="v_{o},v_{1},...,v_{N}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.4"><semantics id="S3.SS1.p2.4.m4.4a"><mrow id="S3.SS1.p2.4.m4.4.4.3" xref="S3.SS1.p2.4.m4.4.4.4.cmml"><msub id="S3.SS1.p2.4.m4.2.2.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.1.2" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml">v</mi><mi id="S3.SS1.p2.4.m4.2.2.1.1.3" xref="S3.SS1.p2.4.m4.2.2.1.1.3.cmml">o</mi></msub><mo id="S3.SS1.p2.4.m4.4.4.3.4" xref="S3.SS1.p2.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.3.3.2.2" xref="S3.SS1.p2.4.m4.3.3.2.2.cmml"><mi id="S3.SS1.p2.4.m4.3.3.2.2.2" xref="S3.SS1.p2.4.m4.3.3.2.2.2.cmml">v</mi><mn id="S3.SS1.p2.4.m4.3.3.2.2.3" xref="S3.SS1.p2.4.m4.3.3.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p2.4.m4.4.4.3.5" xref="S3.SS1.p2.4.m4.4.4.4.cmml">,</mo><mi id="S3.SS1.p2.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p2.4.m4.1.1.cmml">…</mi><mo id="S3.SS1.p2.4.m4.4.4.3.6" xref="S3.SS1.p2.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.4.4.3.3" xref="S3.SS1.p2.4.m4.4.4.3.3.cmml"><mi id="S3.SS1.p2.4.m4.4.4.3.3.2" xref="S3.SS1.p2.4.m4.4.4.3.3.2.cmml">v</mi><mi id="S3.SS1.p2.4.m4.4.4.3.3.3" xref="S3.SS1.p2.4.m4.4.4.3.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.4b"><list id="S3.SS1.p2.4.m4.4.4.4.cmml" xref="S3.SS1.p2.4.m4.4.4.3"><apply id="S3.SS1.p2.4.m4.2.2.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.2">𝑣</ci><ci id="S3.SS1.p2.4.m4.2.2.1.1.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3">𝑜</ci></apply><apply id="S3.SS1.p2.4.m4.3.3.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.2.2.1.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2.2">𝑣</ci><cn id="S3.SS1.p2.4.m4.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.3.3.2.2.3">1</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">…</ci><apply id="S3.SS1.p2.4.m4.4.4.3.3.cmml" xref="S3.SS1.p2.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.4.4.3.3.1.cmml" xref="S3.SS1.p2.4.m4.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.4.4.3.3.2.cmml" xref="S3.SS1.p2.4.m4.4.4.3.3.2">𝑣</ci><ci id="S3.SS1.p2.4.m4.4.4.3.3.3.cmml" xref="S3.SS1.p2.4.m4.4.4.3.3.3">𝑁</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.4c">v_{o},v_{1},...,v_{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.4d">italic_v start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>. Finally, these are prepended to the system prompt, and the full sequence of visual and textual tokens is then given as input to the LLM component of the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.2.1">Augmentation with external knowledge.</span>
To augment the MLLM with external knowledge, we enrich the input context by injecting relevant textual data from an external memory composed of documents. Formally, the distribution of the MLLM is conditioned on additional textual retrieval-knowledge tokens, leading to</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(w_{t}|\overbracket{v_{o},v_{1},...,v_{N}}^{\text{Visual tokens}},\ \ \ \ %
\underbracket{w_{0},w_{1},...,w_{t-1}}_{\text{{\color[rgb]{1,0,0}System + user%
 prompt}}},\overbracket{e_{0},e_{1},...,e_{\tau}}^{\text{{\color[rgb]{0,0,1}%
External memory tokens}}})," class="ltx_Math" display="block" id="S3.E1.m1.13"><semantics id="S3.E1.m1.13a"><mrow id="S3.E1.m1.13.13.1" xref="S3.E1.m1.13.13.1.1.cmml"><mrow id="S3.E1.m1.13.13.1.1" xref="S3.E1.m1.13.13.1.1.cmml"><mi id="S3.E1.m1.13.13.1.1.3" xref="S3.E1.m1.13.13.1.1.3.cmml">p</mi><mo id="S3.E1.m1.13.13.1.1.2" xref="S3.E1.m1.13.13.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.13.13.1.1.1.1" xref="S3.E1.m1.13.13.1.1.1.1.1.cmml"><mo id="S3.E1.m1.13.13.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.13.13.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.13.13.1.1.1.1.1" xref="S3.E1.m1.13.13.1.1.1.1.1.cmml"><msub id="S3.E1.m1.13.13.1.1.1.1.1.5" xref="S3.E1.m1.13.13.1.1.1.1.1.5.cmml"><mi id="S3.E1.m1.13.13.1.1.1.1.1.5.2" xref="S3.E1.m1.13.13.1.1.1.1.1.5.2.cmml">w</mi><mi id="S3.E1.m1.13.13.1.1.1.1.1.5.3" xref="S3.E1.m1.13.13.1.1.1.1.1.5.3.cmml">t</mi></msub><mo fence="false" id="S3.E1.m1.13.13.1.1.1.1.1.4" xref="S3.E1.m1.13.13.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E1.m1.13.13.1.1.1.1.1.3.3" xref="S3.E1.m1.13.13.1.1.1.1.1.3.4.cmml"><mover id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.5.cmml"><msub id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">v</mi><mi id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.3.cmml">o</mi></msub><mo id="S3.E1.m1.4.4.4.4.4" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><msub id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.2.cmml"><mi id="S3.E1.m1.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.2.2.cmml">v</mi><mn id="S3.E1.m1.3.3.3.3.2.3" xref="S3.E1.m1.3.3.3.3.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.4.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><mi id="S3.E1.m1.1.1.1.1" mathvariant="normal" xref="S3.E1.m1.1.1.1.1.cmml">…</mi><mo id="S3.E1.m1.4.4.4.4.6" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><msub id="S3.E1.m1.4.4.4.4.3" xref="S3.E1.m1.4.4.4.4.3.cmml"><mi id="S3.E1.m1.4.4.4.4.3.2" xref="S3.E1.m1.4.4.4.4.3.2.cmml">v</mi><mi id="S3.E1.m1.4.4.4.4.3.3" xref="S3.E1.m1.4.4.4.4.3.3.cmml">N</mi></msub></mrow><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml">﹇</mo></mover><mtext id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2a.cmml">Visual tokens</mtext></mover><mo id="S3.E1.m1.13.13.1.1.1.1.1.3.3.4" rspace="2.167em" xref="S3.E1.m1.13.13.1.1.1.1.1.3.4.cmml">,</mo><munder id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.cmml"><munder accentunder="true" id="S3.E1.m1.8.8" xref="S3.E1.m1.8.8.cmml"><mrow id="S3.E1.m1.8.8.4.4" xref="S3.E1.m1.8.8.4.5.cmml"><msub id="S3.E1.m1.6.6.2.2.1" xref="S3.E1.m1.6.6.2.2.1.cmml"><mi id="S3.E1.m1.6.6.2.2.1.2" xref="S3.E1.m1.6.6.2.2.1.2.cmml">w</mi><mn id="S3.E1.m1.6.6.2.2.1.3" xref="S3.E1.m1.6.6.2.2.1.3.cmml">0</mn></msub><mo id="S3.E1.m1.8.8.4.4.4" xref="S3.E1.m1.8.8.4.5.cmml">,</mo><msub id="S3.E1.m1.7.7.3.3.2" xref="S3.E1.m1.7.7.3.3.2.cmml"><mi id="S3.E1.m1.7.7.3.3.2.2" xref="S3.E1.m1.7.7.3.3.2.2.cmml">w</mi><mn id="S3.E1.m1.7.7.3.3.2.3" xref="S3.E1.m1.7.7.3.3.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.8.8.4.4.5" xref="S3.E1.m1.8.8.4.5.cmml">,</mo><mi id="S3.E1.m1.5.5.1.1" mathvariant="normal" xref="S3.E1.m1.5.5.1.1.cmml">…</mi><mo id="S3.E1.m1.8.8.4.4.6" xref="S3.E1.m1.8.8.4.5.cmml">,</mo><msub id="S3.E1.m1.8.8.4.4.3" xref="S3.E1.m1.8.8.4.4.3.cmml"><mi id="S3.E1.m1.8.8.4.4.3.2" xref="S3.E1.m1.8.8.4.4.3.2.cmml">w</mi><mrow id="S3.E1.m1.8.8.4.4.3.3" xref="S3.E1.m1.8.8.4.4.3.3.cmml"><mi id="S3.E1.m1.8.8.4.4.3.3.2" xref="S3.E1.m1.8.8.4.4.3.3.2.cmml">t</mi><mo id="S3.E1.m1.8.8.4.4.3.3.1" xref="S3.E1.m1.8.8.4.4.3.3.1.cmml">−</mo><mn id="S3.E1.m1.8.8.4.4.3.3.3" xref="S3.E1.m1.8.8.4.4.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.E1.m1.8.8.5" xref="S3.E1.m1.8.8.5.cmml">﹈</mo></munder><mtext id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2" mathcolor="#FF0000" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2a.cmml">System + user prompt</mtext></munder><mo id="S3.E1.m1.13.13.1.1.1.1.1.3.3.5" xref="S3.E1.m1.13.13.1.1.1.1.1.3.4.cmml">,</mo><mover id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.cmml"><mover accent="true" id="S3.E1.m1.12.12" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.12.12.4.4" xref="S3.E1.m1.12.12.4.5.cmml"><msub id="S3.E1.m1.10.10.2.2.1" xref="S3.E1.m1.10.10.2.2.1.cmml"><mi id="S3.E1.m1.10.10.2.2.1.2" xref="S3.E1.m1.10.10.2.2.1.2.cmml">e</mi><mn id="S3.E1.m1.10.10.2.2.1.3" xref="S3.E1.m1.10.10.2.2.1.3.cmml">0</mn></msub><mo id="S3.E1.m1.12.12.4.4.4" xref="S3.E1.m1.12.12.4.5.cmml">,</mo><msub id="S3.E1.m1.11.11.3.3.2" xref="S3.E1.m1.11.11.3.3.2.cmml"><mi id="S3.E1.m1.11.11.3.3.2.2" xref="S3.E1.m1.11.11.3.3.2.2.cmml">e</mi><mn id="S3.E1.m1.11.11.3.3.2.3" xref="S3.E1.m1.11.11.3.3.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.12.12.4.4.5" xref="S3.E1.m1.12.12.4.5.cmml">,</mo><mi id="S3.E1.m1.9.9.1.1" mathvariant="normal" xref="S3.E1.m1.9.9.1.1.cmml">…</mi><mo id="S3.E1.m1.12.12.4.4.6" xref="S3.E1.m1.12.12.4.5.cmml">,</mo><msub id="S3.E1.m1.12.12.4.4.3" xref="S3.E1.m1.12.12.4.4.3.cmml"><mi id="S3.E1.m1.12.12.4.4.3.2" xref="S3.E1.m1.12.12.4.4.3.2.cmml">e</mi><mi id="S3.E1.m1.12.12.4.4.3.3" xref="S3.E1.m1.12.12.4.4.3.3.cmml">τ</mi></msub></mrow><mo id="S3.E1.m1.12.12.5" xref="S3.E1.m1.12.12.5.cmml">﹇</mo></mover><mtext id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2" mathcolor="#0000FF" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2a.cmml">External memory tokens</mtext></mover></mrow></mrow><mo id="S3.E1.m1.13.13.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.13.13.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.13.13.1.2" xref="S3.E1.m1.13.13.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.13b"><apply id="S3.E1.m1.13.13.1.1.cmml" xref="S3.E1.m1.13.13.1"><times id="S3.E1.m1.13.13.1.1.2.cmml" xref="S3.E1.m1.13.13.1.1.2"></times><ci id="S3.E1.m1.13.13.1.1.3.cmml" xref="S3.E1.m1.13.13.1.1.3">𝑝</ci><apply id="S3.E1.m1.13.13.1.1.1.1.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.13.13.1.1.1.1.1.4.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E1.m1.13.13.1.1.1.1.1.5.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.13.13.1.1.1.1.1.5.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E1.m1.13.13.1.1.1.1.1.5.2.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.5.2">𝑤</ci><ci id="S3.E1.m1.13.13.1.1.1.1.1.5.3.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.5.3">𝑡</ci></apply><list id="S3.E1.m1.13.13.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3"><apply id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><ci id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5">﹇</ci><list id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.4"><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2">𝑣</ci><ci id="S3.E1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.3">𝑜</ci></apply><apply id="S3.E1.m1.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.2.2">𝑣</ci><cn id="S3.E1.m1.3.3.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.3.3.3.3.2.3">1</cn></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">…</ci><apply id="S3.E1.m1.4.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.3.1.cmml" xref="S3.E1.m1.4.4.4.4.3">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.3.2.cmml" xref="S3.E1.m1.4.4.4.4.3.2">𝑣</ci><ci id="S3.E1.m1.4.4.4.4.3.3.cmml" xref="S3.E1.m1.4.4.4.4.3.3">𝑁</ci></apply></list></apply><ci id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2a.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2"><mtext id="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2.cmml" mathsize="70%" xref="S3.E1.m1.13.13.1.1.1.1.1.1.1.1.2">Visual tokens</mtext></ci></apply><apply id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S3.E1.m1.8.8.cmml" xref="S3.E1.m1.8.8"><ci id="S3.E1.m1.8.8.5.cmml" xref="S3.E1.m1.8.8.5">﹈</ci><list id="S3.E1.m1.8.8.4.5.cmml" xref="S3.E1.m1.8.8.4.4"><apply id="S3.E1.m1.6.6.2.2.1.cmml" xref="S3.E1.m1.6.6.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.2.2.1.1.cmml" xref="S3.E1.m1.6.6.2.2.1">subscript</csymbol><ci id="S3.E1.m1.6.6.2.2.1.2.cmml" xref="S3.E1.m1.6.6.2.2.1.2">𝑤</ci><cn id="S3.E1.m1.6.6.2.2.1.3.cmml" type="integer" xref="S3.E1.m1.6.6.2.2.1.3">0</cn></apply><apply id="S3.E1.m1.7.7.3.3.2.cmml" xref="S3.E1.m1.7.7.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.3.3.2.1.cmml" xref="S3.E1.m1.7.7.3.3.2">subscript</csymbol><ci id="S3.E1.m1.7.7.3.3.2.2.cmml" xref="S3.E1.m1.7.7.3.3.2.2">𝑤</ci><cn id="S3.E1.m1.7.7.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.7.7.3.3.2.3">1</cn></apply><ci id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1">…</ci><apply id="S3.E1.m1.8.8.4.4.3.cmml" xref="S3.E1.m1.8.8.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.4.4.3.1.cmml" xref="S3.E1.m1.8.8.4.4.3">subscript</csymbol><ci id="S3.E1.m1.8.8.4.4.3.2.cmml" xref="S3.E1.m1.8.8.4.4.3.2">𝑤</ci><apply id="S3.E1.m1.8.8.4.4.3.3.cmml" xref="S3.E1.m1.8.8.4.4.3.3"><minus id="S3.E1.m1.8.8.4.4.3.3.1.cmml" xref="S3.E1.m1.8.8.4.4.3.3.1"></minus><ci id="S3.E1.m1.8.8.4.4.3.3.2.cmml" xref="S3.E1.m1.8.8.4.4.3.3.2">𝑡</ci><cn id="S3.E1.m1.8.8.4.4.3.3.3.cmml" type="integer" xref="S3.E1.m1.8.8.4.4.3.3.3">1</cn></apply></apply></list></apply><ci id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2a.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2"><mtext id="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2.cmml" mathcolor="#FF0000" mathsize="70%" xref="S3.E1.m1.13.13.1.1.1.1.1.2.2.2.2">System + user prompt</mtext></ci></apply><apply id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3">superscript</csymbol><apply id="S3.E1.m1.12.12.cmml" xref="S3.E1.m1.12.12"><ci id="S3.E1.m1.12.12.5.cmml" xref="S3.E1.m1.12.12.5">﹇</ci><list id="S3.E1.m1.12.12.4.5.cmml" xref="S3.E1.m1.12.12.4.4"><apply id="S3.E1.m1.10.10.2.2.1.cmml" xref="S3.E1.m1.10.10.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.2.2.1.1.cmml" xref="S3.E1.m1.10.10.2.2.1">subscript</csymbol><ci id="S3.E1.m1.10.10.2.2.1.2.cmml" xref="S3.E1.m1.10.10.2.2.1.2">𝑒</ci><cn id="S3.E1.m1.10.10.2.2.1.3.cmml" type="integer" xref="S3.E1.m1.10.10.2.2.1.3">0</cn></apply><apply id="S3.E1.m1.11.11.3.3.2.cmml" xref="S3.E1.m1.11.11.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.3.3.2.1.cmml" xref="S3.E1.m1.11.11.3.3.2">subscript</csymbol><ci id="S3.E1.m1.11.11.3.3.2.2.cmml" xref="S3.E1.m1.11.11.3.3.2.2">𝑒</ci><cn id="S3.E1.m1.11.11.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.11.11.3.3.2.3">1</cn></apply><ci id="S3.E1.m1.9.9.1.1.cmml" xref="S3.E1.m1.9.9.1.1">…</ci><apply id="S3.E1.m1.12.12.4.4.3.cmml" xref="S3.E1.m1.12.12.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.4.4.3.1.cmml" xref="S3.E1.m1.12.12.4.4.3">subscript</csymbol><ci id="S3.E1.m1.12.12.4.4.3.2.cmml" xref="S3.E1.m1.12.12.4.4.3.2">𝑒</ci><ci id="S3.E1.m1.12.12.4.4.3.3.cmml" xref="S3.E1.m1.12.12.4.4.3.3">𝜏</ci></apply></list></apply><ci id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2a.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2"><mtext id="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2.cmml" mathcolor="#0000FF" mathsize="70%" xref="S3.E1.m1.13.13.1.1.1.1.1.3.3.3.2">External memory tokens</mtext></ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.13c">p(w_{t}|\overbracket{v_{o},v_{1},...,v_{N}}^{\text{Visual tokens}},\ \ \ \ %
\underbracket{w_{0},w_{1},...,w_{t-1}}_{\text{{\color[rgb]{1,0,0}System + user%
 prompt}}},\overbracket{e_{0},e_{1},...,e_{\tau}}^{\text{{\color[rgb]{0,0,1}%
External memory tokens}}}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.13d">italic_p ( italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | over﹇ start_ARG italic_v start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_ARG start_POSTSUPERSCRIPT Visual tokens end_POSTSUPERSCRIPT , under﹈ start_ARG italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT System + user prompt end_POSTSUBSCRIPT , over﹇ start_ARG italic_e start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT end_ARG start_POSTSUPERSCRIPT External memory tokens end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.1">where <math alttext="e_{0},...,e_{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.3"><semantics id="S3.SS1.p3.1.m1.3a"><mrow id="S3.SS1.p3.1.m1.3.3.2" xref="S3.SS1.p3.1.m1.3.3.3.cmml"><msub id="S3.SS1.p3.1.m1.2.2.1.1" xref="S3.SS1.p3.1.m1.2.2.1.1.cmml"><mi id="S3.SS1.p3.1.m1.2.2.1.1.2" xref="S3.SS1.p3.1.m1.2.2.1.1.2.cmml">e</mi><mn id="S3.SS1.p3.1.m1.2.2.1.1.3" xref="S3.SS1.p3.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS1.p3.1.m1.3.3.2.3" xref="S3.SS1.p3.1.m1.3.3.3.cmml">,</mo><mi id="S3.SS1.p3.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p3.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.p3.1.m1.3.3.2.4" xref="S3.SS1.p3.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS1.p3.1.m1.3.3.2.2" xref="S3.SS1.p3.1.m1.3.3.2.2.cmml"><mi id="S3.SS1.p3.1.m1.3.3.2.2.2" xref="S3.SS1.p3.1.m1.3.3.2.2.2.cmml">e</mi><mi id="S3.SS1.p3.1.m1.3.3.2.2.3" xref="S3.SS1.p3.1.m1.3.3.2.2.3.cmml">τ</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.3b"><list id="S3.SS1.p3.1.m1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.3.3.2"><apply id="S3.SS1.p3.1.m1.2.2.1.1.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1.2">𝑒</ci><cn id="S3.SS1.p3.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.2.2.1.1.3">0</cn></apply><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">…</ci><apply id="S3.SS1.p3.1.m1.3.3.2.2.cmml" xref="S3.SS1.p3.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p3.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p3.1.m1.3.3.2.2.2">𝑒</ci><ci id="S3.SS1.p3.1.m1.3.3.2.2.3.cmml" xref="S3.SS1.p3.1.m1.3.3.2.2.3">𝜏</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.3c">e_{0},...,e_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.3d">italic_e start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math> represents the added tokens retrieved from the external memory. Differently from the standard formulation of MLLMs, by enriching the input context we allow the model to generate more specific answers by exploiting tokens retrieved from the memory.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Hierarchical retrieval from an external memory.</span>
The external memory comprises a collection of (document, image, text-title) triplets taken from documents, denoted as <math alttext="\mathcal{D}=\{(d_{i},t_{i})_{i}\}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">𝒟</mi><mo id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS1.p4.1.m1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.2.cmml"><mo id="S3.SS1.p4.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml"><mrow id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml"><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.3" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.2.cmml">d</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.4" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.2.cmml">t</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.5" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml">)</mo></mrow><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.4" xref="S3.SS1.p4.1.m1.1.1.1.1.1.4.cmml">i</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><eq id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2"></eq><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝒟</ci><set id="S3.SS1.p4.1.m1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1"><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1">subscript</csymbol><interval closure="open" id="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2"><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.2">𝑑</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.2">𝑡</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></interval><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.4.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.4">𝑖</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathcal{D}=\{(d_{i},t_{i})_{i}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">caligraphic_D = { ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math>. Within this memory, we conduct a hierarchical two-step search to retrieve appropriate information. Initially, we locate the most pertinent document, followed by identifying the relevant passage inside a particular document, which is subsequently exploited as additional input context in the MLLM.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.4">In the first stage, given an input query image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">italic_I</annotation></semantics></math> we perform an approximate <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">italic_k</annotation></semantics></math>-nearest neighbor search into the external memory, using document titles as retrievable keys. The similarity between the query image and the text titles is modeled as the inner product between their respective embeddings, which are computed through the visual and textual CLIP encoders (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.4.1">i.e.</em>, <math alttext="E_{v}" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><msub id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">E</mi><mi id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">𝐸</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">E_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="E_{t}" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4.1"><semantics id="S3.SS1.p5.4.m4.1a"><msub id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">E</mi><mi id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">𝐸</ci><ci id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">E_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.4.m4.1d">italic_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>), as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{sim}(I_{i},t_{i})=E_{v}(I)\cdot E_{t}(t_{i})^{T}." class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mtext id="S3.E2.m1.2.2.1.1.2.4" xref="S3.E2.m1.2.2.1.1.2.4a.cmml">sim</mtext><mo id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.E2.m1.2.2.1.1.2.2.2.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.2.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml">t</mi><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.2.2.2.5" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><msub id="S3.E2.m1.2.2.1.1.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.2.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2.cmml">E</mi><mi id="S3.E2.m1.2.2.1.1.3.3.2.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.2.3.cmml">v</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.3.2.1" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.2.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><mo id="S3.E2.m1.2.2.1.1.3.3.2.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">I</mi><mo id="S3.E2.m1.2.2.1.1.3.3.2.3.2.2" rspace="0.055em" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.1" rspace="0.222em" xref="S3.E2.m1.2.2.1.1.3.3.1.cmml">⋅</mo><msub id="S3.E2.m1.2.2.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.3.2.cmml">E</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.cmml">t</mi></msub></mrow><mo id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">⁢</mo><msup id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.1.1.1" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.3.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.3.1.1.1.1" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.3.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.2.cmml">t</mi><mi id="S3.E2.m1.2.2.1.1.3.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E2.m1.2.2.1.1.3.1.3" xref="S3.E2.m1.2.2.1.1.3.1.3.cmml">T</mi></msup></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" lspace="0em" xref="S3.E2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4"></eq><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><times id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3"></times><ci id="S3.E2.m1.2.2.1.1.2.4a.cmml" xref="S3.E2.m1.2.2.1.1.2.4"><mtext id="S3.E2.m1.2.2.1.1.2.4.cmml" xref="S3.E2.m1.2.2.1.1.2.4">sim</mtext></ci><interval closure="open" id="S3.E2.m1.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2">𝑡</ci><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"></times><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3"><ci id="S3.E2.m1.2.2.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1">⋅</ci><apply id="S3.E2.m1.2.2.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><times id="S3.E2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.1"></times><apply id="S3.E2.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2">𝐸</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.3">𝑣</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐼</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.2">𝐸</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3">𝑡</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.2.2.1.1.3.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3.1.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\text{sim}(I_{i},t_{i})=E_{v}(I)\cdot E_{t}(t_{i})^{T}.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">sim ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_I ) ⋅ italic_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p5.5">Then, the knowledge retriever returns the top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p5.5.m1.1"><semantics id="S3.SS1.p5.5.m1.1a"><mi id="S3.SS1.p5.5.m1.1.1" xref="S3.SS1.p5.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m1.1b"><ci id="S3.SS1.p5.5.m1.1.1.cmml" xref="S3.SS1.p5.5.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.5.m1.1d">italic_k</annotation></semantics></math> documents associated with the most relevant items retrieved using the aforementioned procedure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.3.1">Retrieving document passages.</span>
In the second step, we analyze each of the retrieved documents to identify the most relevant passages corresponding to the user’s question.
Each document is defined as a sequence of chunks, denoted as <math alttext="d_{i}=[c_{i_{0}},..,c_{i_{T}}]" class="ltx_math_unparsed" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1b"><msub id="S3.SS1.p6.1.m1.1.1"><mi id="S3.SS1.p6.1.m1.1.1.2">d</mi><mi id="S3.SS1.p6.1.m1.1.1.3">i</mi></msub><mo id="S3.SS1.p6.1.m1.1.2">=</mo><mrow id="S3.SS1.p6.1.m1.1.3"><mo id="S3.SS1.p6.1.m1.1.3.1" stretchy="false">[</mo><msub id="S3.SS1.p6.1.m1.1.3.2"><mi id="S3.SS1.p6.1.m1.1.3.2.2">c</mi><msub id="S3.SS1.p6.1.m1.1.3.2.3"><mi id="S3.SS1.p6.1.m1.1.3.2.3.2">i</mi><mn id="S3.SS1.p6.1.m1.1.3.2.3.3">0</mn></msub></msub><mo id="S3.SS1.p6.1.m1.1.3.3">,</mo><mo id="S3.SS1.p6.1.m1.1.3.4" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.SS1.p6.1.m1.1.3.5" lspace="0.0835em" rspace="0.167em">.</mo><mo id="S3.SS1.p6.1.m1.1.3.6">,</mo><msub id="S3.SS1.p6.1.m1.1.3.7"><mi id="S3.SS1.p6.1.m1.1.3.7.2">c</mi><msub id="S3.SS1.p6.1.m1.1.3.7.3"><mi id="S3.SS1.p6.1.m1.1.3.7.3.2">i</mi><mi id="S3.SS1.p6.1.m1.1.3.7.3.3">T</mi></msub></msub><mo id="S3.SS1.p6.1.m1.1.3.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">d_{i}=[c_{i_{0}},..,c_{i_{T}}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_c start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , . . , italic_c start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math>, and, given the input question, we retrieve the chunks with the highest similarity to the question. We employ the Contriever architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> to embed each chunk of the selected document, along with the query (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p6.3.2">i.e.</em>, the question provided by the user), and compute the similarity as an inner product between embeddings. By retrieving the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2.1"><semantics id="S3.SS1.p6.2.m2.1a"><mi id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><ci id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m2.1d">italic_n</annotation></semantics></math> most appropriate passages inside each of the retrieved documents, overall we obtain <math alttext="k\cdot n" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m3.1"><semantics id="S3.SS1.p6.3.m3.1a"><mrow id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><mi id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml">k</mi><mo id="S3.SS1.p6.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p6.3.m3.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><ci id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1.1">⋅</ci><ci id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2">𝑘</ci><ci id="S3.SS1.p6.3.m3.1.1.3.cmml" xref="S3.SS1.p6.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">k\cdot n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.3.m3.1d">italic_k ⋅ italic_n</annotation></semantics></math> passages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Context enrichment.</span>
Once we find the most relevant chunks, we employ their raw contents as an additional input to the MLLM. Specifically, the final prompt that we employ includes the image tokens, the retrieved raw chunks, the system-level prompt, and the user question. Formally, considering three retrieved passages, the final prompt is defined as follows:</p>
<table class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table" id="Sx1.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="S3.Ex1.1" style="font-size:90%;">&lt;IMAGE&gt;\nGiven the following context:\n</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="S3.Ex2.1" style="font-size:90%;"> &lt;R1&gt;\n&lt;R2&gt;\&lt;R3&gt;\n &lt;QUESTION&gt;</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="S3.E3.1" style="font-size:90%;">Give a short answer. ASSISTANT:</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="0"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While the aforementioned approach could work in a zero-shot fashion, using the original weights <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_θ</annotation></semantics></math> of the pre-trained MLLM, we also investigate the case of fine-tuning the model to augment its capabilities of exploiting retrieved passages. In particular, in this case, the model is trained on pairs of questions and ground-truth answers requiring external knowledge. As this would potentially reduce the capabilities of the MLLM on tasks not requiring external knowledge (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">i.e.</em>, all the other tasks on which the model has been originally trained), we apply a data mixing approach in which ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we first introduce the experimental settings, describing the datasets employed, the evaluation protocol, and the implementation and training details used to perform the experiments. Then, we present our experimental results, analyzing the effectiveness of CLIP fine-tuning and evaluating how it is possible to incorporate retrieved knowledge in an MLLM. Finally, limitations of the proposed approach and possible future works are reported.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Encyclopedic-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</span>
The dataset contains around 221k question-answer pairs associated with 16.7k different fine-grained entities, with up to 5 images representing the same entity. Overall, there are more than 1M triplets composed of an image, a question, and the corresponding answer. Fine-grained entities and related images are extracted from iNaturalist 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> and Google Landmarks Dataset V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, which are associated with the corresponding Wikipedia article. Questions are divided into four different categories, namely single-hop, automatically generated, multi-answer, and two-hop. In particular, single-hop questions have been manually annotated and a single Wikipedia article is needed to answer them. Automatically generated questions are similar to the single-hop questions but have been generated by automatic models. Multi-answer questions, instead, can be answered with a list of terms, but always refer to a single fine-grained entity. Finally, two-hop questions require two retrieval steps to answer them. The dataset also comes with a knowledge base composed of 2M Wikipedia articles, suitable for answering dataset questions.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Dataset triplets are divided into training, validation, and test splits respectively composed of 1M, 13.6k, and 5.8k samples. In our experiments, we employ the training split to fine-tune the LLaVA model and report the results on the test set of the dataset. During testing, we filter out two-hop questions resulting in 4,750 test triplets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">InfoSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.</span> The dataset contains 1.3M image-question-answer triplets corresponding to around 11k different entities (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.2">i.e.</em>, Wikipedia articles). The vast majority of questions have been obtained with an almost entirely automatic procedure, by filling human-authored templates with knowledge triples from Wikidata. In this case, images are derived from the OVEN dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. Triplets are divided into training, validation, and test sets, with around 934k, 73k, and 348k samples respectively. At the time of the submission, the ground-truth answers and entities from the test set were not available. Therefore, we report our results on the validation split. Both validation and test sets contain questions related to new entities not included in the training split and questions not seen during training.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Along with image-question-answer triplets, a knowledge base composed of 6M Wikipedia entities is provided. In our experiments, we consider a randomly extracted subset of 100k entities, in which we guarantee the presence of the 6,741 entities associated with questions from the training and validation splits.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">LLaVA fine-tuning.</span> We employ two distinct fine-tuning approaches, with each being exclusively applied to one of the datasets. In order to maintain the performance of the LLaVA model on well-established MLLM datasets, we supplement fine-tuning data with samples from the LLaVA-Instruct dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. Specifically, given its size of 158k, we double the probability of having examples from this dataset in each mini-batch. To reduce the number of trainable parameters, we train using low-rank adapters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> with a total batch size of 512 samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Retrieval.</span> Textual documents sourced from Wikipedia content are embedded using the Contriever architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, segmenting the text into chunks of 600 characters each. Furthermore, for streamlined efficiency, the process involves utilizing a single visual encoder. Specifically, following the LLaVA architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, we employ the CLIP ViT-L/14@336 backbone to embed images to give as input to the MLLM, while simultaneously leveraging it to extract query visual features in the initial hierarchical retrieval step, facilitating the integration of an external memory component.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To perform entity retrieval, we employ approximate <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">k</span>NN search rather than exact <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">k</span>NN search because it significantly improves
the computational speed of the entire pipeline. To this aim, we employ the Faiss library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> and a graph-based HNSW index with 32 links per vertex.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Protocol</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We evaluate our models in two settings: without external knowledge base and with external knowledge base. The former means that we ask the model to directly answer a visual question, by solely relying on the competencies learned during pre-training and/or fine-tuning. On the other hand, in the latter setting, we leverage the proposed hierarchical retrieval method to search for additional information in the external knowledge base. In practice, this is represented by two dumps of Wikipedia comprehending 2M and 100k pages, respectively for Encyclopedic-VQA and InfoSeek. Concerning the evaluation metrics, we report the accuracy over the Encyclopedic-VQA test split and the InfoSeek validation split, following the official evaluation scripts provided along with the datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Analyzing CLIP performance.</span> We start by evaluating entity retrieval results using CLIP. In this setting, we consider images from the Encyclopedic-VQA test set and InfoSeek validation set and measure the CLIP ability to find the correct entity within the knowledge base of each respective dataset (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.2">i.e.</em>, composed of 2M entries for Encyclopedic-VQA and 100k entries for InfoSeek). As previously mentioned, we perform retrieval using images as queries and Wikipedia titles as retrievable items.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.4">Results are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.T1" title="Table 1 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> in terms of recall@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_k</annotation></semantics></math> (R@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.1d">italic_k</annotation></semantics></math>) with <math alttext="k=1,10,20,50" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.4"><semantics id="S4.SS4.p2.3.m3.4a"><mrow id="S4.SS4.p2.3.m3.4.5" xref="S4.SS4.p2.3.m3.4.5.cmml"><mi id="S4.SS4.p2.3.m3.4.5.2" xref="S4.SS4.p2.3.m3.4.5.2.cmml">k</mi><mo id="S4.SS4.p2.3.m3.4.5.1" xref="S4.SS4.p2.3.m3.4.5.1.cmml">=</mo><mrow id="S4.SS4.p2.3.m3.4.5.3.2" xref="S4.SS4.p2.3.m3.4.5.3.1.cmml"><mn id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">1</mn><mo id="S4.SS4.p2.3.m3.4.5.3.2.1" xref="S4.SS4.p2.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS4.p2.3.m3.2.2" xref="S4.SS4.p2.3.m3.2.2.cmml">10</mn><mo id="S4.SS4.p2.3.m3.4.5.3.2.2" xref="S4.SS4.p2.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS4.p2.3.m3.3.3" xref="S4.SS4.p2.3.m3.3.3.cmml">20</mn><mo id="S4.SS4.p2.3.m3.4.5.3.2.3" xref="S4.SS4.p2.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS4.p2.3.m3.4.4" xref="S4.SS4.p2.3.m3.4.4.cmml">50</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.4b"><apply id="S4.SS4.p2.3.m3.4.5.cmml" xref="S4.SS4.p2.3.m3.4.5"><eq id="S4.SS4.p2.3.m3.4.5.1.cmml" xref="S4.SS4.p2.3.m3.4.5.1"></eq><ci id="S4.SS4.p2.3.m3.4.5.2.cmml" xref="S4.SS4.p2.3.m3.4.5.2">𝑘</ci><list id="S4.SS4.p2.3.m3.4.5.3.1.cmml" xref="S4.SS4.p2.3.m3.4.5.3.2"><cn id="S4.SS4.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS4.p2.3.m3.1.1">1</cn><cn id="S4.SS4.p2.3.m3.2.2.cmml" type="integer" xref="S4.SS4.p2.3.m3.2.2">10</cn><cn id="S4.SS4.p2.3.m3.3.3.cmml" type="integer" xref="S4.SS4.p2.3.m3.3.3">20</cn><cn id="S4.SS4.p2.3.m3.4.4.cmml" type="integer" xref="S4.SS4.p2.3.m3.4.4">50</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.4c">k=1,10,20,50</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.4d">italic_k = 1 , 10 , 20 , 50</annotation></semantics></math> which measures the percentage of times the correct entity is found in the top-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m4.1"><semantics id="S4.SS4.p2.4.m4.1a"><mi id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><ci id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.4.m4.1d">italic_k</annotation></semantics></math> retrieved elements. Notably, correctly retrieving the Wikipedia entity associated with the input image strongly depends on the size of the employed knowledge base. In fact, when using 100k items, as in the case of InfoSeek, the correct entity is retrieved as the first item 36.9% of the time and among the top-10 66.1% of the time. Instead, when using a significantly larger knowledge base as in the case of Encyclopedic-VQA, which contains 2M items, retrieval results are significantly lower with 3.3% and 9.9% respectively in terms of R@1 and R@10.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:411.9pt;height:82.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(71.6pt,-14.4pt) scale(1.53310816643156,1.53310816643156) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.2.1.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.2.1">KB</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.3" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">R@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">R@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.6" style="padding-left:4.5pt;padding-right:4.5pt;">R@20</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.7" style="padding-left:4.5pt;padding-right:4.5pt;">R@50</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.1.2.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">Encyclopedic-VQA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.2" style="padding-left:4.5pt;padding-right:4.5pt;">2M</td>
<td class="ltx_td ltx_border_t" id="S4.T1.2.1.2.1.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">3.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">9.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.6" style="padding-left:4.5pt;padding-right:4.5pt;">13.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.7" style="padding-left:4.5pt;padding-right:4.5pt;">17.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.2.1.3.2.1" style="padding-left:4.5pt;padding-right:4.5pt;">InfoSeek</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.1.3.2.2" style="padding-left:4.5pt;padding-right:4.5pt;">100k</td>
<td class="ltx_td ltx_border_bb" id="S4.T1.2.1.3.2.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.1.3.2.4" style="padding-left:4.5pt;padding-right:4.5pt;">36.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.1.3.2.5" style="padding-left:4.5pt;padding-right:4.5pt;">66.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.1.3.2.6" style="padding-left:4.5pt;padding-right:4.5pt;">71.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.1.3.2.7" style="padding-left:4.5pt;padding-right:4.5pt;">78.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Entity retrieval results on the Encyclopedic-VQA test set and InfoSeek validation set. To comply with the visual encoder employed in LLaVA, all results are obtained using CLIP ViT-L/14@336.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.4" style="width:329.5pt;height:166pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.1pt,61.3pt) scale(0.574309429912296,0.574309429912296) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.4.5.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.4.4.5.1.1" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.2" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.3" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.4" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.5" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.6" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.7" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.4.4.5.1.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.5.1.8.1">Enc-VQA</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.5.1.9" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.4.4.5.1.10" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.5.1.10.1">InfoSeek</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.2.2.2.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.2.2.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.4.1">LLM</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T2.2.2.2.5" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.2.2.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.6.1">KB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;"><math alttext="k" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">italic_k</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.2.2.2" style="padding-left:4.5pt;padding-right:4.5pt;"><math alttext="n" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mi id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">italic_n</annotation></semantics></math></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T2.2.2.2.7" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.8" style="padding-left:4.5pt;padding-right:4.5pt;">Single-Hop</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.9" style="padding-left:4.5pt;padding-right:4.5pt;">All</th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T2.2.2.2.10" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.11" style="padding-left:4.5pt;padding-right:4.5pt;">Unseen-Q</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.12" style="padding-left:4.5pt;padding-right:4.5pt;">Unseen-E</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.13" style="padding-left:4.5pt;padding-right:4.5pt;">All</th>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.4.6.2.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.6.2.1.1">Zero-shot Models</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.2" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.4" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.5" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.6" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.8" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.9" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.11" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.12" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.6.2.13" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_align_left" id="S4.T2.3.3.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">      BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.1" style="padding-left:4.5pt;padding-right:4.5pt;">Flan-T5<math alttext="{}_{\text{XL}}" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><msub id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml"><mi id="S4.T2.3.3.3.1.m1.1.1a" xref="S4.T2.3.3.3.1.m1.1.1.cmml"></mi><mtext id="S4.T2.3.3.3.1.m1.1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.1a.cmml">XL</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><apply id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1"><ci id="S4.T2.3.3.3.1.m1.1.1.1a.cmml" xref="S4.T2.3.3.3.1.m1.1.1.1"><mtext id="S4.T2.3.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="S4.T2.3.3.3.1.m1.1.1.1">XL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">{}_{\text{XL}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT XL end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td" id="S4.T2.3.3.3.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.5" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.6" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td" id="S4.T2.3.3.3.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.8" style="padding-left:4.5pt;padding-right:4.5pt;">12.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.9" style="padding-left:4.5pt;padding-right:4.5pt;">12.4</td>
<td class="ltx_td" id="S4.T2.3.3.3.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.11" style="padding-left:4.5pt;padding-right:4.5pt;">12.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.12" style="padding-left:4.5pt;padding-right:4.5pt;">12.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.13" style="padding-left:4.5pt;padding-right:4.5pt;">12.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.4.2" style="padding-left:4.5pt;padding-right:4.5pt;">      InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.1" style="padding-left:4.5pt;padding-right:4.5pt;">Flan-T5<math alttext="{}_{\text{XL}}" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><msub id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml"><mi id="S4.T2.4.4.4.1.m1.1.1a" xref="S4.T2.4.4.4.1.m1.1.1.cmml"></mi><mtext id="S4.T2.4.4.4.1.m1.1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.1a.cmml">XL</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><apply id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1"><ci id="S4.T2.4.4.4.1.m1.1.1.1a.cmml" xref="S4.T2.4.4.4.1.m1.1.1.1"><mtext id="S4.T2.4.4.4.1.m1.1.1.1.cmml" mathsize="70%" xref="S4.T2.4.4.4.1.m1.1.1.1">XL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">{}_{\text{XL}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">start_FLOATSUBSCRIPT XL end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td" id="S4.T2.4.4.4.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.4" style="padding-left:4.5pt;padding-right:4.5pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.5" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.6" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td" id="S4.T2.4.4.4.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.8" style="padding-left:4.5pt;padding-right:4.5pt;">11.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.9" style="padding-left:4.5pt;padding-right:4.5pt;">12.0</td>
<td class="ltx_td" id="S4.T2.4.4.4.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.11" style="padding-left:4.5pt;padding-right:4.5pt;">8.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.12" style="padding-left:4.5pt;padding-right:4.5pt;">7.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.13" style="padding-left:4.5pt;padding-right:4.5pt;">8.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.7.3">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.7.3.1" style="padding-left:4.5pt;padding-right:4.5pt;">      LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">Vicuna-7B</td>
<td class="ltx_td" id="S4.T2.4.4.7.3.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.5" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.6" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td" id="S4.T2.4.4.7.3.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.8" style="padding-left:4.5pt;padding-right:4.5pt;">16.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.9" style="padding-left:4.5pt;padding-right:4.5pt;">16.9</td>
<td class="ltx_td" id="S4.T2.4.4.7.3.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.11" style="padding-left:4.5pt;padding-right:4.5pt;">9.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.12" style="padding-left:4.5pt;padding-right:4.5pt;">9.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.13" style="padding-left:4.5pt;padding-right:4.5pt;">9.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.8.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.4.8.4.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.1.1">Fine-tuned Models</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.2" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.4" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.5" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.6" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.8" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.9" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.11" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.12" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.8.4.13" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.9.5">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.9.5.1" style="padding-left:4.5pt;padding-right:4.5pt;">      LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.2" style="padding-left:4.5pt;padding-right:4.5pt;">Vicuna-7B</td>
<td class="ltx_td" id="S4.T2.4.4.9.5.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.4" style="padding-left:4.5pt;padding-right:4.5pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.5" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.6" style="padding-left:4.5pt;padding-right:4.5pt;">-</td>
<td class="ltx_td" id="S4.T2.4.4.9.5.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.8" style="padding-left:4.5pt;padding-right:4.5pt;">23.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.9" style="padding-left:4.5pt;padding-right:4.5pt;">28.5</td>
<td class="ltx_td" id="S4.T2.4.4.9.5.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.11" style="padding-left:4.5pt;padding-right:4.5pt;">19.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.12" style="padding-left:4.5pt;padding-right:4.5pt;">16.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.9.5.13" style="padding-left:4.5pt;padding-right:4.5pt;">17.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.10.6" style="background-color:#FAF0BF;">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.10.6.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.1.1" style="background-color:#FAF0BF;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.10.6.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.2.1" style="background-color:#FAF0BF;">Vicuna-7B</span></td>
<td class="ltx_td" id="S4.T2.4.4.10.6.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.4.1" style="background-color:#FAF0BF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.5.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.6.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td" id="S4.T2.4.4.10.6.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.8.1" style="background-color:#FAF0BF;">21.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.9.1" style="background-color:#FAF0BF;">26.4</span></td>
<td class="ltx_td" id="S4.T2.4.4.10.6.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.11.1" style="background-color:#FAF0BF;">26.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.12.1" style="background-color:#FAF0BF;">24.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.10.6.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.10.6.13.1" style="background-color:#FAF0BF;">25.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.11.7" style="background-color:#FAF0BF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.4.11.7.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.1.1" style="background-color:#FAF0BF;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.11.7.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.2.1" style="background-color:#FAF0BF;">Vicuna-7B</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.11.7.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.4.1" style="background-color:#FAF0BF;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.5.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.6.1" style="background-color:#FAF0BF;">2</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.11.7.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.8.1" style="background-color:#FAF0BF;">19.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.9.1" style="background-color:#FAF0BF;">23.2</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.11.7.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.11.1" style="background-color:#FAF0BF;">29.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.12.1" style="background-color:#FAF0BF;">26.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.11.7.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.11.7.13.1" style="background-color:#FAF0BF;">27.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.12.8" style="background-color:#FAF0BF;">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.12.8.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.1.1" style="background-color:#FAF0BF;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.12.8.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.2.1" style="background-color:#FAF0BF;">Vicuna-7B</span></td>
<td class="ltx_td" id="S4.T2.4.4.12.8.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.4.1" style="background-color:#FAF0BF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.5.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.6.1" style="background-color:#FAF0BF;">3</span></td>
<td class="ltx_td" id="S4.T2.4.4.12.8.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.8.1" style="background-color:#FAF0BF;">17.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.9.1" style="background-color:#FAF0BF;">20.3</span></td>
<td class="ltx_td" id="S4.T2.4.4.12.8.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.11.1" style="background-color:#FAF0BF;">30.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.12.1" style="background-color:#FAF0BF;">27.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.12.8.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.12.8.13.1" style="background-color:#FAF0BF;">28.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.13.9" style="background-color:#FAF0BF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.4.13.9.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.1.1" style="background-color:#FAF0BF;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.13.9.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.2.1" style="background-color:#FAF0BF;">Vicuna-7B</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.13.9.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.4.1" style="background-color:#FAF0BF;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.5.1" style="background-color:#FAF0BF;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.6.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.13.9.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.8.1" style="background-color:#FAF0BF;">21.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.9.1" style="background-color:#FAF0BF;">25.4</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.4.4.13.9.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.11.1" style="background-color:#FAF0BF;">27.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.12.1" style="background-color:#FAF0BF;">24.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.13.9.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.13.9.13.1" style="background-color:#FAF0BF;">26.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.14.10" style="background-color:#FAF0BF;">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.14.10.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.1.1" style="background-color:#FAF0BF;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.14.10.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.2.1" style="background-color:#FAF0BF;">Vicuna-7B</span></td>
<td class="ltx_td" id="S4.T2.4.4.14.10.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.4.1" style="background-color:#FAF0BF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.5.1" style="background-color:#FAF0BF;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.6.1" style="background-color:#FAF0BF;">1</span></td>
<td class="ltx_td" id="S4.T2.4.4.14.10.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.8.1" style="background-color:#FAF0BF;">20.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.9.1" style="background-color:#FAF0BF;">24.3</span></td>
<td class="ltx_td" id="S4.T2.4.4.14.10.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.11.1" style="background-color:#FAF0BF;">27.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.12.1" style="background-color:#FAF0BF;">24.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.14.10.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.14.10.13.1" style="background-color:#FAF0BF;">25.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.15.11" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.4.4.15.11.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.1.1" style="background-color:#E6E6E6;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.15.11.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.2.1" style="background-color:#E6E6E6;">Vicuna-7B</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.4.4.15.11.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.4.1" style="background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.5.1" style="background-color:#E6E6E6;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.6.1" style="background-color:#E6E6E6;">1</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.4.4.15.11.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.8.1" style="background-color:#E6E6E6;">34.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.9.1" style="background-color:#E6E6E6;">37.2</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.4.4.15.11.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.11.1" style="background-color:#E6E6E6;">41.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.12.1" style="background-color:#E6E6E6;">41.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.15.11.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.15.11.13.1" style="background-color:#E6E6E6;">41.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.16.12" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.16.12.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.1.1" style="background-color:#E6E6E6;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.16.12.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.2.1" style="background-color:#E6E6E6;">Vicuna-7B</span></td>
<td class="ltx_td" id="S4.T2.4.4.16.12.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.4.1" style="background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.5.1" style="background-color:#E6E6E6;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.6.1" style="background-color:#E6E6E6;">2</span></td>
<td class="ltx_td" id="S4.T2.4.4.16.12.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.8.1" style="background-color:#E6E6E6;">39.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.9.1" style="background-color:#E6E6E6;">40.2</span></td>
<td class="ltx_td" id="S4.T2.4.4.16.12.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.11.1" style="background-color:#E6E6E6;">49.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.12.1" style="background-color:#E6E6E6;">46.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.16.12.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.16.12.13.1" style="background-color:#E6E6E6;">47.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.17.13" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.4.4.17.13.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.1.1" style="background-color:#E6E6E6;">      <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.17.13.1.1.1">Wiki-LLaVA</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.2.1" style="background-color:#E6E6E6;">Vicuna-7B</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.4.4.17.13.3" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.4.1" style="background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.5.1" style="background-color:#E6E6E6;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.6.1" style="background-color:#E6E6E6;">3</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.4.4.17.13.7" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.8" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.8.1" style="background-color:#E6E6E6;">38.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.9" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.9.1" style="background-color:#E6E6E6;">38.6</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.4.4.17.13.10" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.11" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.11.1" style="background-color:#E6E6E6;">52.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.12" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.12.1" style="background-color:#E6E6E6;">50.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.17.13.13" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.4.4.17.13.13.1" style="background-color:#E6E6E6;">51.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.13.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.8.2" style="font-size:90%;">Accuracy results on the Encyclopedic-VQA test set and InfoSeek validation set. <span class="ltx_text ltx_font_bold" id="S4.T2.8.2.1" style="background-color:#FAF0BF;">Yellow color</span> indicates models employing the CLIP model to perform entity retrieval, while <span class="ltx_text ltx_font_bold" id="S4.T2.8.2.2" style="background-color:#E6E6E6;">gray color</span> indicates the use of ground-truth entities (<em class="ltx_emph ltx_font_italic" id="S4.T2.8.2.3">i.e.</em>, oracle). <math alttext="k" class="ltx_Math" display="inline" id="S4.T2.7.1.m1.1"><semantics id="S4.T2.7.1.m1.1b"><mi id="S4.T2.7.1.m1.1.1" xref="S4.T2.7.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T2.7.1.m1.1c"><ci id="S4.T2.7.1.m1.1.1.cmml" xref="S4.T2.7.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.1.m1.1e">italic_k</annotation></semantics></math> denotes the number of retrieved entities, and <math alttext="n" class="ltx_Math" display="inline" id="S4.T2.8.2.m2.1"><semantics id="S4.T2.8.2.m2.1b"><mi id="S4.T2.8.2.m2.1.1" xref="S4.T2.8.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T2.8.2.m2.1c"><ci id="S4.T2.8.2.m2.1.1.cmml" xref="S4.T2.8.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.2.m2.1d">n</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.2.m2.1e">italic_n</annotation></semantics></math> represents the number of textual chunks retrieved for each entity that are given to the MLLM as additional context.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.5"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.5.1">Results on Encyclopedic-VQA and InfoSeek.</span>
We then report visual question-answering results in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.T2" title="Table 2 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>. We include the performance of zero-shot models like BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, and the LLaVA-1.5 baseline model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, which are not fine-tuned on the considered datasets and that do not leverage the external knowledge base. Moreover, we consider the accuracy results of LLaVA-1.5 when fine-tuned on the training set of Encyclopedic-VQA and InfoSeek, but not augmented with retrieved context. The results of our approach (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.5.2">i.e.</em>, Wiki-LLaVA) are reported both in the standard setting in which CLIP is used to retrieve the most representative entity from the knowledge base and in its <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.5.3">oracle</span> version, which employs the entity corresponding to the input image-question pair. For both cases, we consider a different number <math alttext="n" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">italic_n</annotation></semantics></math> of retrieved textual chunks, all corresponding to the top-1 (or ground-truth) entity. When employing CLIP, we also vary the number <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mi id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><ci id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">italic_k</annotation></semantics></math> of retrieved entities (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.5.4">i.e.</em>, <math alttext="k=1,2,3" class="ltx_Math" display="inline" id="S4.SS4.p3.3.m3.3"><semantics id="S4.SS4.p3.3.m3.3a"><mrow id="S4.SS4.p3.3.m3.3.4" xref="S4.SS4.p3.3.m3.3.4.cmml"><mi id="S4.SS4.p3.3.m3.3.4.2" xref="S4.SS4.p3.3.m3.3.4.2.cmml">k</mi><mo id="S4.SS4.p3.3.m3.3.4.1" xref="S4.SS4.p3.3.m3.3.4.1.cmml">=</mo><mrow id="S4.SS4.p3.3.m3.3.4.3.2" xref="S4.SS4.p3.3.m3.3.4.3.1.cmml"><mn id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml">1</mn><mo id="S4.SS4.p3.3.m3.3.4.3.2.1" xref="S4.SS4.p3.3.m3.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p3.3.m3.2.2" xref="S4.SS4.p3.3.m3.2.2.cmml">2</mn><mo id="S4.SS4.p3.3.m3.3.4.3.2.2" xref="S4.SS4.p3.3.m3.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p3.3.m3.3.3" xref="S4.SS4.p3.3.m3.3.3.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.3b"><apply id="S4.SS4.p3.3.m3.3.4.cmml" xref="S4.SS4.p3.3.m3.3.4"><eq id="S4.SS4.p3.3.m3.3.4.1.cmml" xref="S4.SS4.p3.3.m3.3.4.1"></eq><ci id="S4.SS4.p3.3.m3.3.4.2.cmml" xref="S4.SS4.p3.3.m3.3.4.2">𝑘</ci><list id="S4.SS4.p3.3.m3.3.4.3.1.cmml" xref="S4.SS4.p3.3.m3.3.4.3.2"><cn id="S4.SS4.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS4.p3.3.m3.1.1">1</cn><cn id="S4.SS4.p3.3.m3.2.2.cmml" type="integer" xref="S4.SS4.p3.3.m3.2.2">2</cn><cn id="S4.SS4.p3.3.m3.3.3.cmml" type="integer" xref="S4.SS4.p3.3.m3.3.3">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.3c">k=1,2,3</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.m3.3d">italic_k = 1 , 2 , 3</annotation></semantics></math>) using <math alttext="n=1" class="ltx_Math" display="inline" id="S4.SS4.p3.4.m4.1"><semantics id="S4.SS4.p3.4.m4.1a"><mrow id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mi id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">n</mi><mo id="S4.SS4.p3.4.m4.1.1.1" xref="S4.SS4.p3.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS4.p3.4.m4.1.1.3" xref="S4.SS4.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><eq id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1.1"></eq><ci id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">𝑛</ci><cn id="S4.SS4.p3.4.m4.1.1.3.cmml" type="integer" xref="S4.SS4.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">n=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.4.m4.1d">italic_n = 1</annotation></semantics></math> when <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p3.5.m5.1"><semantics id="S4.SS4.p3.5.m5.1a"><mi id="S4.SS4.p3.5.m5.1.1" xref="S4.SS4.p3.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.5.m5.1b"><ci id="S4.SS4.p3.5.m5.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.5.m5.1d">italic_k</annotation></semantics></math> is greater than 1. This choice is given by the maximum context length that Vicuna takes as input, which is set to 2,048 tokens.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">As it can be seen, zero-shot MLLMs face difficulties in correctly answering the given questions as these models can only rely on the knowledge embedded inside the LLM. When instead using an external knowledge base, the accuracy results significantly increase especially on the InfoSeek dataset with 100k retrievable items. The limited performance of the CLIP model in retrieving the correct entity on larger knowledge bases, instead, leads to a slight degradation of accuracy scores. This is due to the noisy textual passages that are provided to the MLLM as additional external context which, being related to a different entity, often do not contain informative content.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.10">Overall, retrieving passages from different entities does not always help increase the results. Instead, using more than one textual chunk as additional context for the MLLM generally improves the final accuracy on the InfoSeek validation set with an overall improvement of 2.1 and 3.4 accuracy points with <math alttext="n=2" class="ltx_Math" display="inline" id="S4.SS4.p5.1.m1.1"><semantics id="S4.SS4.p5.1.m1.1a"><mrow id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml"><mi id="S4.SS4.p5.1.m1.1.1.2" xref="S4.SS4.p5.1.m1.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.1.m1.1.1.1" xref="S4.SS4.p5.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.1.m1.1.1.3" xref="S4.SS4.p5.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><apply id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1"><eq id="S4.SS4.p5.1.m1.1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1.1"></eq><ci id="S4.SS4.p5.1.m1.1.1.2.cmml" xref="S4.SS4.p5.1.m1.1.1.2">𝑛</ci><cn id="S4.SS4.p5.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">n=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.1.m1.1d">italic_n = 2</annotation></semantics></math> and <math alttext="n=3" class="ltx_Math" display="inline" id="S4.SS4.p5.2.m2.1"><semantics id="S4.SS4.p5.2.m2.1a"><mrow id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml"><mi id="S4.SS4.p5.2.m2.1.1.2" xref="S4.SS4.p5.2.m2.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.2.m2.1.1.1" xref="S4.SS4.p5.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.2.m2.1.1.3" xref="S4.SS4.p5.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.1b"><apply id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1"><eq id="S4.SS4.p5.2.m2.1.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1.1"></eq><ci id="S4.SS4.p5.2.m2.1.1.2.cmml" xref="S4.SS4.p5.2.m2.1.1.2">𝑛</ci><cn id="S4.SS4.p5.2.m2.1.1.3.cmml" type="integer" xref="S4.SS4.p5.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.1c">n=3</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.2.m2.1d">italic_n = 3</annotation></semantics></math> respectively. Furthermore, it is worth noting that employing oracle entities significantly boosts the final accuracy. In particular, oracle entities lead to an improvement of 13.8% on Encyclopedic-VQA and 22.6% on InfoSeek, comparing the best-performing configuration with CLIP-based entity retrieval (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p5.10.1">i.e.</em>, <math alttext="k=1" class="ltx_Math" display="inline" id="S4.SS4.p5.3.m3.1"><semantics id="S4.SS4.p5.3.m3.1a"><mrow id="S4.SS4.p5.3.m3.1.1" xref="S4.SS4.p5.3.m3.1.1.cmml"><mi id="S4.SS4.p5.3.m3.1.1.2" xref="S4.SS4.p5.3.m3.1.1.2.cmml">k</mi><mo id="S4.SS4.p5.3.m3.1.1.1" xref="S4.SS4.p5.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.3.m3.1.1.3" xref="S4.SS4.p5.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.3.m3.1b"><apply id="S4.SS4.p5.3.m3.1.1.cmml" xref="S4.SS4.p5.3.m3.1.1"><eq id="S4.SS4.p5.3.m3.1.1.1.cmml" xref="S4.SS4.p5.3.m3.1.1.1"></eq><ci id="S4.SS4.p5.3.m3.1.1.2.cmml" xref="S4.SS4.p5.3.m3.1.1.2">𝑘</ci><cn id="S4.SS4.p5.3.m3.1.1.3.cmml" type="integer" xref="S4.SS4.p5.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.3.m3.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.3.m3.1d">italic_k = 1</annotation></semantics></math> and <math alttext="n=1" class="ltx_Math" display="inline" id="S4.SS4.p5.4.m4.1"><semantics id="S4.SS4.p5.4.m4.1a"><mrow id="S4.SS4.p5.4.m4.1.1" xref="S4.SS4.p5.4.m4.1.1.cmml"><mi id="S4.SS4.p5.4.m4.1.1.2" xref="S4.SS4.p5.4.m4.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.4.m4.1.1.1" xref="S4.SS4.p5.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.4.m4.1.1.3" xref="S4.SS4.p5.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.4.m4.1b"><apply id="S4.SS4.p5.4.m4.1.1.cmml" xref="S4.SS4.p5.4.m4.1.1"><eq id="S4.SS4.p5.4.m4.1.1.1.cmml" xref="S4.SS4.p5.4.m4.1.1.1"></eq><ci id="S4.SS4.p5.4.m4.1.1.2.cmml" xref="S4.SS4.p5.4.m4.1.1.2">𝑛</ci><cn id="S4.SS4.p5.4.m4.1.1.3.cmml" type="integer" xref="S4.SS4.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.4.m4.1c">n=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.4.m4.1d">italic_n = 1</annotation></semantics></math> for Encyclopedic-VQA and <math alttext="k=1" class="ltx_Math" display="inline" id="S4.SS4.p5.5.m5.1"><semantics id="S4.SS4.p5.5.m5.1a"><mrow id="S4.SS4.p5.5.m5.1.1" xref="S4.SS4.p5.5.m5.1.1.cmml"><mi id="S4.SS4.p5.5.m5.1.1.2" xref="S4.SS4.p5.5.m5.1.1.2.cmml">k</mi><mo id="S4.SS4.p5.5.m5.1.1.1" xref="S4.SS4.p5.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.5.m5.1.1.3" xref="S4.SS4.p5.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.5.m5.1b"><apply id="S4.SS4.p5.5.m5.1.1.cmml" xref="S4.SS4.p5.5.m5.1.1"><eq id="S4.SS4.p5.5.m5.1.1.1.cmml" xref="S4.SS4.p5.5.m5.1.1.1"></eq><ci id="S4.SS4.p5.5.m5.1.1.2.cmml" xref="S4.SS4.p5.5.m5.1.1.2">𝑘</ci><cn id="S4.SS4.p5.5.m5.1.1.3.cmml" type="integer" xref="S4.SS4.p5.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.5.m5.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.5.m5.1d">italic_k = 1</annotation></semantics></math> and <math alttext="n=3" class="ltx_Math" display="inline" id="S4.SS4.p5.6.m6.1"><semantics id="S4.SS4.p5.6.m6.1a"><mrow id="S4.SS4.p5.6.m6.1.1" xref="S4.SS4.p5.6.m6.1.1.cmml"><mi id="S4.SS4.p5.6.m6.1.1.2" xref="S4.SS4.p5.6.m6.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.6.m6.1.1.1" xref="S4.SS4.p5.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.6.m6.1.1.3" xref="S4.SS4.p5.6.m6.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.6.m6.1b"><apply id="S4.SS4.p5.6.m6.1.1.cmml" xref="S4.SS4.p5.6.m6.1.1"><eq id="S4.SS4.p5.6.m6.1.1.1.cmml" xref="S4.SS4.p5.6.m6.1.1.1"></eq><ci id="S4.SS4.p5.6.m6.1.1.2.cmml" xref="S4.SS4.p5.6.m6.1.1.2">𝑛</ci><cn id="S4.SS4.p5.6.m6.1.1.3.cmml" type="integer" xref="S4.SS4.p5.6.m6.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.6.m6.1c">n=3</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.6.m6.1d">italic_n = 3</annotation></semantics></math> for InfoSeek) with the best performing oracle-based version (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p5.10.2">i.e.</em>, <math alttext="k=1" class="ltx_Math" display="inline" id="S4.SS4.p5.7.m7.1"><semantics id="S4.SS4.p5.7.m7.1a"><mrow id="S4.SS4.p5.7.m7.1.1" xref="S4.SS4.p5.7.m7.1.1.cmml"><mi id="S4.SS4.p5.7.m7.1.1.2" xref="S4.SS4.p5.7.m7.1.1.2.cmml">k</mi><mo id="S4.SS4.p5.7.m7.1.1.1" xref="S4.SS4.p5.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.7.m7.1.1.3" xref="S4.SS4.p5.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.7.m7.1b"><apply id="S4.SS4.p5.7.m7.1.1.cmml" xref="S4.SS4.p5.7.m7.1.1"><eq id="S4.SS4.p5.7.m7.1.1.1.cmml" xref="S4.SS4.p5.7.m7.1.1.1"></eq><ci id="S4.SS4.p5.7.m7.1.1.2.cmml" xref="S4.SS4.p5.7.m7.1.1.2">𝑘</ci><cn id="S4.SS4.p5.7.m7.1.1.3.cmml" type="integer" xref="S4.SS4.p5.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.7.m7.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.7.m7.1d">italic_k = 1</annotation></semantics></math> and <math alttext="n=2" class="ltx_Math" display="inline" id="S4.SS4.p5.8.m8.1"><semantics id="S4.SS4.p5.8.m8.1a"><mrow id="S4.SS4.p5.8.m8.1.1" xref="S4.SS4.p5.8.m8.1.1.cmml"><mi id="S4.SS4.p5.8.m8.1.1.2" xref="S4.SS4.p5.8.m8.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.8.m8.1.1.1" xref="S4.SS4.p5.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.8.m8.1.1.3" xref="S4.SS4.p5.8.m8.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.8.m8.1b"><apply id="S4.SS4.p5.8.m8.1.1.cmml" xref="S4.SS4.p5.8.m8.1.1"><eq id="S4.SS4.p5.8.m8.1.1.1.cmml" xref="S4.SS4.p5.8.m8.1.1.1"></eq><ci id="S4.SS4.p5.8.m8.1.1.2.cmml" xref="S4.SS4.p5.8.m8.1.1.2">𝑛</ci><cn id="S4.SS4.p5.8.m8.1.1.3.cmml" type="integer" xref="S4.SS4.p5.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.8.m8.1c">n=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.8.m8.1d">italic_n = 2</annotation></semantics></math> for Encyclopedic-VQA and <math alttext="k=1" class="ltx_Math" display="inline" id="S4.SS4.p5.9.m9.1"><semantics id="S4.SS4.p5.9.m9.1a"><mrow id="S4.SS4.p5.9.m9.1.1" xref="S4.SS4.p5.9.m9.1.1.cmml"><mi id="S4.SS4.p5.9.m9.1.1.2" xref="S4.SS4.p5.9.m9.1.1.2.cmml">k</mi><mo id="S4.SS4.p5.9.m9.1.1.1" xref="S4.SS4.p5.9.m9.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.9.m9.1.1.3" xref="S4.SS4.p5.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.9.m9.1b"><apply id="S4.SS4.p5.9.m9.1.1.cmml" xref="S4.SS4.p5.9.m9.1.1"><eq id="S4.SS4.p5.9.m9.1.1.1.cmml" xref="S4.SS4.p5.9.m9.1.1.1"></eq><ci id="S4.SS4.p5.9.m9.1.1.2.cmml" xref="S4.SS4.p5.9.m9.1.1.2">𝑘</ci><cn id="S4.SS4.p5.9.m9.1.1.3.cmml" type="integer" xref="S4.SS4.p5.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.9.m9.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.9.m9.1d">italic_k = 1</annotation></semantics></math> and <math alttext="n=3" class="ltx_Math" display="inline" id="S4.SS4.p5.10.m10.1"><semantics id="S4.SS4.p5.10.m10.1a"><mrow id="S4.SS4.p5.10.m10.1.1" xref="S4.SS4.p5.10.m10.1.1.cmml"><mi id="S4.SS4.p5.10.m10.1.1.2" xref="S4.SS4.p5.10.m10.1.1.2.cmml">n</mi><mo id="S4.SS4.p5.10.m10.1.1.1" xref="S4.SS4.p5.10.m10.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.10.m10.1.1.3" xref="S4.SS4.p5.10.m10.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.10.m10.1b"><apply id="S4.SS4.p5.10.m10.1.1.cmml" xref="S4.SS4.p5.10.m10.1.1"><eq id="S4.SS4.p5.10.m10.1.1.1.cmml" xref="S4.SS4.p5.10.m10.1.1.1"></eq><ci id="S4.SS4.p5.10.m10.1.1.2.cmml" xref="S4.SS4.p5.10.m10.1.1.2">𝑛</ci><cn id="S4.SS4.p5.10.m10.1.1.3.cmml" type="integer" xref="S4.SS4.p5.10.m10.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.10.m10.1c">n=3</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.10.m10.1d">italic_n = 3</annotation></semantics></math> for InfoSeek). These results confirm the effectiveness of directly employing retrieved passages to augment a pre-trained MLLM and further highlight the importance of having a good entity retrieval model to limit the possibility of feeding the MLLM with irrelevant content.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:433.6pt;height:124.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.7pt,-17.1pt) scale(1.38023343261579,1.38023343261579) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T3.2.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.2.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.2.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.3.1">Enc-VQA</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.2.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.2.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.5.1">InfoSeek</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.2.2">
<td class="ltx_td ltx_align_left" id="S4.T3.2.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.2.2.1.1">Fine-tuning</span></td>
<td class="ltx_td" id="S4.T3.2.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">Single-Hop</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">All</td>
<td class="ltx_td" id="S4.T3.2.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">Unseen-Q</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">Unseen-E</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.2.8" style="padding-left:3.0pt;padding-right:3.0pt;">All</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">✗</td>
<td class="ltx_td ltx_border_t" id="S4.T3.2.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">16.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">16.9</td>
<td class="ltx_td ltx_border_t" id="S4.T3.2.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">9.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">9.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">9.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.1.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="S4.T3.2.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">23.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">29.0</td>
<td class="ltx_td ltx_border_t" id="S4.T3.2.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">17.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.4.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">15.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.4.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">16.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.2.1.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">✓ + LLaVA-Instruct</td>
<td class="ltx_td ltx_border_bb" id="S4.T3.2.1.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">23.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">28.5</td>
<td class="ltx_td ltx_border_bb" id="S4.T3.2.1.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">19.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">16.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">17.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Performance analysis when using the LLaVA-Instruct dataset during fine-tuning. All results are obtained without external knowledge retrieval.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">Some qualitative results on sample image-question pairs from Encyclopedic-VQA (first row) and InfoSeek (second row) are reported in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.F3" title="Figure 3 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">3</span></a>, comparing the answers given by Wiki-LLaVA with those coming from the original LLaVA-1.5 model. For completeness, we also report some failure cases (third row) in which both models are not able to correctly answer the given question.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.1" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.1.g1" src="extracted/2404.15406v2/images/qualitatives/resized_encyclopedic_27c590f5dfe2d909.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.11" style="width:69.4pt;"><span class="ltx_text" id="S4.F3.11.1" style="font-size:70%;">In what state is this building located? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.11.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>California <span class="ltx_text" id="S4.F3.11.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.11.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>Washington <span class="ltx_text" id="S4.F3.11.1.4" style="color:#00B050;">✓</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.2" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.2.g1" src="extracted/2404.15406v2/images/qualitatives/resized_encyclopedic_27e011f811e7b62e.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.12" style="width:65.0pt;"><span class="ltx_text" id="S4.F3.12.1" style="font-size:70%;">When was this building constructed? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.12.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>1970 <span class="ltx_text" id="S4.F3.12.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.12.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>1927 <span class="ltx_text" id="S4.F3.12.1.4" style="color:#00B050;">✓</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.3" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.3.g1" src="extracted/2404.15406v2/images/qualitatives/resized_encyclopedic_41b81647e136ee90.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.13" style="width:65.0pt;"><span class="ltx_text" id="S4.F3.13.1" style="font-size:70%;">What’s the height of the tallest minaret from this mosque? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.13.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>100 feet <span class="ltx_text" id="S4.F3.13.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.13.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>49mt <span class="ltx_text" id="S4.F3.13.1.4" style="color:#00B050;">✓</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.4" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.4.g1" src="extracted/2404.15406v2/images/qualitatives/resized_infoseek_val_00000016.jpeg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.14" style="width:69.4pt;"><span class="ltx_text" id="S4.F3.14.1" style="font-size:70%;">Which geographic area is this fish found? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.14.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>Gulf of Mexico <span class="ltx_text" id="S4.F3.14.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.14.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>Brazil <span class="ltx_text" id="S4.F3.14.1.4" style="color:#00B050;">✓</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.5" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.5.g1" src="extracted/2404.15406v2/images/qualitatives/resized_infoseek_val_00000048.jpeg" width="580"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.15" style="width:65.0pt;">
<p class="ltx_p" id="S4.F3.15.1"><span class="ltx_text" id="S4.F3.15.1.1" style="font-size:70%;">What is the oldest age of this animal? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.15.1.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>10 years <span class="ltx_text" id="S4.F3.15.1.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.15.1.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>24.9 <span class="ltx_text" id="S4.F3.15.1.1.4" style="color:#00B050;">✓</span></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.6" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.6.g1" src="extracted/2404.15406v2/images/qualitatives/resized_infoseek_val_00000131.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.16" style="width:65.0pt;"><span class="ltx_text" id="S4.F3.16.1" style="font-size:70%;">Who designed this building? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.16.1.1">LLaVA-1.5:
<br class="ltx_break"/></span>Architect <span class="ltx_text" id="S4.F3.16.1.2" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.16.1.3">Wiki-LLaVA:
<br class="ltx_break"/></span>James of Saint George <span class="ltx_text" id="S4.F3.16.1.4" style="color:#00B050;">✓</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.7" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.7.g1" src="extracted/2404.15406v2/images/qualitatives/fail/resized_infoseek_val_00000210.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.17" style="width:69.4pt;"><span class="ltx_text" id="S4.F3.17.1" style="font-size:70%;">Which culture is associated with this place?
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S4.F3.17.1.1">Ancient Greek</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.17.1.2">LLaVA-1.5:
<br class="ltx_break"/></span>Roman <span class="ltx_text" id="S4.F3.17.1.3" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.17.1.4">Wiki-LLaVA:
<br class="ltx_break"/></span>Nuragic Civilization <span class="ltx_text" id="S4.F3.17.1.5" style="color:#FF0000;">✗</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.8" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.8.g1" src="extracted/2404.15406v2/images/qualitatives/resized_encyclopedic_fa07f04eea2fdcf0.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.18" style="width:65.0pt;"><span class="ltx_text" id="S4.F3.18.1" style="font-size:70%;">What is the name of the main club of this stadium? 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S4.F3.18.1.1">FC Rotor
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="S4.F3.18.1.2">LLaVA-1.5:
<br class="ltx_break"/></span>Real Madrid <span class="ltx_text" id="S4.F3.18.1.3" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.18.1.4">Wiki-LLaVA:
<br class="ltx_break"/></span>FC Dynamo Kyiv <span class="ltx_text" id="S4.F3.18.1.5" style="color:#FF0000;">✗</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.9" style="width:71.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="435" id="S4.F3.9.g1" src="extracted/2404.15406v2/images/qualitatives/fail/resized_infoseek_val_00000169.jpg" width="580"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.19" style="width:65.0pt;"><span class="ltx_text" id="S4.F3.19.1" style="font-size:70%;">Which mountain range is this mountain belong to?
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S4.F3.19.1.1">Snowdonia
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="S4.F3.19.1.2">LLaVA-1.5:
<br class="ltx_break"/></span>Rocky mountains <span class="ltx_text" id="S4.F3.19.1.3" style="color:#FF0000;">✗</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.F3.19.1.4">Wiki-LLaVA:
<br class="ltx_break"/></span>Lake District <span class="ltx_text" id="S4.F3.19.1.5" style="color:#FF0000;">✗</span></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.20.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.21.2" style="font-size:90%;">Qualitative results on sample image-question pairs from Encyclopedic-VQA (first row) and InfoSeek (second row) comparing the proposed approach with the original LLaVA-1.5 model. Some failure cases are shown in the third row with the corresponding ground-truth.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p7">
<p class="ltx_p" id="S4.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p7.1.1">Evaluating the importance of the fine-tuning datasets.</span>
As described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S3.SS2" title="3.2 Training ‣ 3 Proposed Method ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">3.2</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">4.2</span></a>, the MLLM fine-tuning is done with a mixture of data containing image-question-answer triples from the Encyclopedic-VQA or InfoSeek training set and visual instruction tuning data from LLaVA-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, which has been used to originally fine-tune the LLaVA model. In Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.T3" title="Table 3 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">3</span></a>, we evaluate the effect of mixing fine-tuning data for the knowledge-based VQA task. In this setting, we only report the results of the fine-tuned models without external knowledge retrieval. Notably, using visual instruction tuning data can help to regularize the fine-tuning phase on the InfoSeek dataset, leading to an overall improvement of 1.9 accuracy points compared to the model fine-tuned only on image-question-answer triplets from the training set of the dataset. On Encyclopedic-VQA, instead, training with instruction tuning data does not lead to performance improvement although without degrading the original results.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.2" style="width:433.6pt;height:160pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(46.1pt,-17.0pt) scale(1.26985213615248,1.26985213615248) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T4.2.1.1.1.1" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.2.1.1.1.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.1.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.3.1">MME</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.2.1.1.1.4" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.1.1.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.5.1">MMMU</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.2.1.1.1.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.1.1.1.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.7.1">MMB</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.2.1.1.1.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.1.1.9" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.9.1">POPE</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.2.2">
<td class="ltx_td ltx_align_left" id="S4.T4.2.1.2.2.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.2.2.1.1">Fine-tuning</span></td>
<td class="ltx_td" id="S4.T4.2.1.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;">Cogn</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.4" style="padding-left:2.5pt;padding-right:2.5pt;">Perc</td>
<td class="ltx_td" id="S4.T4.2.1.2.2.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.6" style="padding-left:2.5pt;padding-right:2.5pt;">Acc</td>
<td class="ltx_td" id="S4.T4.2.1.2.2.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.8" style="padding-left:2.5pt;padding-right:2.5pt;">Acc</td>
<td class="ltx_td" id="S4.T4.2.1.2.2.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.10" style="padding-left:2.5pt;padding-right:2.5pt;">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.11" style="padding-left:2.5pt;padding-right:2.5pt;">F1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.1.3.3.1" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.3.3.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.3" style="padding-left:2.5pt;padding-right:2.5pt;">355.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.4" style="padding-left:2.5pt;padding-right:2.5pt;">1513.3</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.3.3.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.6" style="padding-left:2.5pt;padding-right:2.5pt;">35.1</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.3.3.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.8" style="padding-left:2.5pt;padding-right:2.5pt;">71.6</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.3.3.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.10" style="padding-left:2.5pt;padding-right:2.5pt;">86.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.11" style="padding-left:2.5pt;padding-right:2.5pt;">85.8</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.1.4.4.1" style="padding-left:2.5pt;padding-right:2.5pt;">Enc-VQA</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.4.4.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.3" style="padding-left:2.5pt;padding-right:2.5pt;">200.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.4" style="padding-left:2.5pt;padding-right:2.5pt;">802.8</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.4.4.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.6" style="padding-left:2.5pt;padding-right:2.5pt;">36.6</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.4.4.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.8" style="padding-left:2.5pt;padding-right:2.5pt;">67.7</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.4.4.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.10" style="padding-left:2.5pt;padding-right:2.5pt;">72.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.4.4.11" style="padding-left:2.5pt;padding-right:2.5pt;">63.4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T4.2.1.5.5.1" style="padding-left:2.5pt;padding-right:2.5pt;">Enc-VQA + LLaVA-Instruct</td>
<td class="ltx_td" id="S4.T4.2.1.5.5.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.3" style="padding-left:2.5pt;padding-right:2.5pt;">290.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.4" style="padding-left:2.5pt;padding-right:2.5pt;">1170.1</td>
<td class="ltx_td" id="S4.T4.2.1.5.5.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.6" style="padding-left:2.5pt;padding-right:2.5pt;">36.6</td>
<td class="ltx_td" id="S4.T4.2.1.5.5.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.8" style="padding-left:2.5pt;padding-right:2.5pt;">70.4</td>
<td class="ltx_td" id="S4.T4.2.1.5.5.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.10" style="padding-left:2.5pt;padding-right:2.5pt;">87.2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.5.11" style="padding-left:2.5pt;padding-right:2.5pt;">86.6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.1.6.6.1" style="padding-left:2.5pt;padding-right:2.5pt;">InfoSeek</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.6.6.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.3" style="padding-left:2.5pt;padding-right:2.5pt;">296.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.4" style="padding-left:2.5pt;padding-right:2.5pt;">1377.2</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.6.6.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.6" style="padding-left:2.5pt;padding-right:2.5pt;">35.2</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.6.6.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.8" style="padding-left:2.5pt;padding-right:2.5pt;">71.7</td>
<td class="ltx_td ltx_border_t" id="S4.T4.2.1.6.6.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.10" style="padding-left:2.5pt;padding-right:2.5pt;">82.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.6.6.11" style="padding-left:2.5pt;padding-right:2.5pt;">79.6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.2.1.7.7.1" style="padding-left:2.5pt;padding-right:2.5pt;">InfoSeek + LLaVA-Instruct</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.2.1.7.7.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.3" style="padding-left:2.5pt;padding-right:2.5pt;">341.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.4" style="padding-left:2.5pt;padding-right:2.5pt;">1438.9</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.2.1.7.7.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.6" style="padding-left:2.5pt;padding-right:2.5pt;">35.6</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.2.1.7.7.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.8" style="padding-left:2.5pt;padding-right:2.5pt;">71.1</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.2.1.7.7.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.10" style="padding-left:2.5pt;padding-right:2.5pt;">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.7.7.11" style="padding-left:2.5pt;padding-right:2.5pt;">84.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Performance preservation analysis with respect to the original LLaVA-1.5 model (first row) on diverse benchmarks for MLLM evaluation.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p8.1.1">Preservation of LLaVA performance.</span>
Finally, we analyze the impact of LLaVA fine-tuning on knowledge-based VQA datasets when evaluating the model on common MLLM evaluation benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. In particular, we include results on MME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> which contains image-question pairs covering 14 different tasks grouped in two macro-categories (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p8.1.2">i.e.</em>, cognition and perception), MMMU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> that is composed of multiple-choice and open-ended questions possibly interleaved with one or more images and extracted from diverse university textbooks and online courses, MMBench (MMB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> that includes multiple-choice questions across 20 different domains, and POPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> that is focused on evaluating object hallucinations and comprises binary classification entries, each related to an image. More details about the evaluation metrics and number of samples can be found in the original paper of each dataset.</p>
</div>
<div class="ltx_para" id="S4.SS4.p9">
<p class="ltx_p" id="S4.SS4.p9.1">Results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15406v2#S4.T4" title="Table 4 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> comparing the original LLaVA model with the two fine-tuned versions on Encyclopedic-VQA and InfoSeek, with and without the use of visual instruction tuning data. Overall, employing samples from the LLaVA-Instruct dataset can better preserve the results of the original model, only partially degrading the performance on the considered benchmarks compared to the original model. While the most significant deterioration is achieved on the MME dataset, in the other settings the original performances are better preserved, also leading to a slight improvement on MMMU and POPE benchmarks compared to the LLaVA-1.5 results.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Limitations and Future Works</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">While our work provides an initial step towards MLLM which can properly exploit external multimodal data, it is worthwhile mentioning that significant research is needed in two directions. The fist is defining proper embedding spaces in which documents can be retrieved from questions and input images, so as to improve the performance of the higher level of our hierarchical retrieval. The second is modeling an efficient and sustainable paradigm to select from one or more documents. Here, the challenge is to
increase the capability of the MLLM of distinguishing the appropriateness of retrieved items. This point might also require novel architectural design, which might go beyond the pure inclusion of retrieved items in the context.
Regardless of its current limitations, our research testifies the potential of adding multimodal external knowledge to a MLLM and inherits all the advantages of retrieval-augmented approaches, such as the adaptability to different domains and the loosely-coupled relationship between pre-trained information and retrievable data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We have presented Wiki-LLaVA, an architecture for augmenting an existing MLLM with external knowledge. Our proposal leverages an external knowledge source of documents to improve the effectiveness of an MLLM when tasked with questions and dialogues. In particular, we devise a hierarchical architecture for retrieving documents and eliciting selected parts to be included in the MLLM input context. Extensive experiments demonstrate the effectiveness of the proposed solution, and its capability to maintain the proficiency of the MLLM across different tasks.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support. This work has been conducted under two research grants, one co-funded by Leonardo S.p.A. and the other co-funded by Altilia s.r.l., and supported by the PNRRM4C2 project “FAIR - Future Artificial Intelligence Research”, funded by the European Commission, and by the PNRR project “Italian Strengthening of Esfri RI Resilience” (ITSERR) funded by the European Union - NextGenerationEU (CUP B53C22001770006).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Alayrac et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Flamingo: a Visual Language Model for Few-Shot Learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib1.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Barraco et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Manuele Barraco, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Caffagni et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">The (R)Evolution of Multimodal Large Language Models: A Survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.12451</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Chen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib4.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">EMNLP</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Chiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Chung et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Scaling Instruction-Finetuned Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">arXiv preprint arXiv:2210.11416</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Dai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.06500</em><span class="ltx_text" id="bib.bib8.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Fu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.13394</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Guu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Retrieval Augmented Language Model Pre-Training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Hu et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">LoRA: Low-Rank Adaptation of Large Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.9.1" style="font-size:90%;">arXiv preprint arXiv:2106.09685</em><span class="ltx_text" id="bib.bib11.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Hu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib12.11.3" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Hu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Huang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Language Is Not All You Need: Aligning Perception with Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">arXiv preprint arXiv:2302.14045</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Izacard et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Unsupervised Dense Information Retrieval with Contrastive Learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.9.1" style="font-size:90%;">arXiv preprint arXiv:2112.09118</em><span class="ltx_text" id="bib.bib15.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Jain et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">SIGIR</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Jiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Active Retrieval Augmented Generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.06983</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Johnson et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Jeff Johnson, Matthijs Douze, and Hervé Jégou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Billion-Scale Similarity Search with GPUs.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.9.1" style="font-size:90%;">IEEE Trans. on Big Data</em><span class="ltx_text" id="bib.bib18.10.2" style="font-size:90%;">, 7(3):535–547, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Koh et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Grounding Language Models to Images for Multimodal Inputs and Outputs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Lerner et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Paul Lerner, Olivier Ferret, and Camille Guinaudeau.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Cross-modal Retrieval for Knowledge-based Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">arXiv preprint arXiv:2401.05736</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">arXiv preprint arXiv:2301.12597</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Evaluating Object Hallucination in Large Vision-Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.10355</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Liu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Improved Baselines with Visual Instruction Tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.03744</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Liu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Visual Instruction Tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Liu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Liu et al. [2023c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">MMBench: Is Your Multi-modal Model an All-around Player?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.06281</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Marino et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Ok-VQA: A Visual Question Answering Benchmark Requiring External Knowledge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib27.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Mensink et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Nakano et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">WebGPT: Browser-assisted question-answering with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.1" style="font-size:90%;">arXiv preprint arXiv:2112.09332</em><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Ouyang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Training Language Models to Follow Instructions with Human Feedback.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Poppi et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Samuele Poppi, Tobia Poppi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.16254</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Qiu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A Crook, Yifan Ethan Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, and Seungwhan Moon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.04735</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Learning Transferable Visual Models from Natural Language Supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib33.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Raffel et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">JMLR</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 21(140):1–67, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Ramos et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Rita Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Sarto et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Retrieval-Augmented Transformer for Image Captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">CBMI</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Sarto et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Schwenk et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Stiennon et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Learning to Summarize with Human Feedback.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib39.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Taori et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Stanford Alpaca: An Instruction-Following LLaMA Model, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Touvron et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">LLaMA: Open and Efficient Foundation Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">arXiv preprint arXiv:2302.13971</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Tsimpoukelli et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Multimodal Few-Shot Learning with Frozen Language Models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Van Horn et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Benchmarking Representation Learning for Natural World Image Collections.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Wei et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Uniir: Training and Benchmarking Universal Multimodal Information Retrievers.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.17136</em><span class="ltx_text" id="bib.bib44.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Weyand et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib45.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Xiao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, and Vicente Ordonez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Grounding Language Models for Visual Entity Recognition.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.18695</em><span class="ltx_text" id="bib.bib46.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Yue et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.16502</em><span class="ltx_text" id="bib.bib47.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.10592</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 16:55:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
