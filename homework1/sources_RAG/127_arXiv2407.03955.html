<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Meta-prompting Optimized Retrieval-augmented Generation</title>
<!--Generated on Thu Jul  4 14:08:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="RAG Retrieval-Augmented Generation Prompt Optimization Large Language Models Meta-prompting Multi-hop QA" lang="en" name="keywords"/>
<base href="/html/2407.03955v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S1" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S1.SS1" title="In 1 Introduction ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Retrieval-augmented generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S1.SS2" title="In 1 Introduction ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Prompt optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S2" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S3" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Dataset and models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4.SS1" title="In 4 Dataset and models ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task and dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4.SS2" title="In 4 Dataset and models ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4.SS3" title="In 4 Dataset and models ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation procedure and metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.SS1" title="In 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.SS2" title="In 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Examining the tentative instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.SS3" title="In 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Examining the responses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S6" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S7" title="In Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S7.SS0.SSS1" title="In 7 Conclusion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.0.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Lisbon
<br class="ltx_break"/>NLX—Natural Language and Speech Group, Dept of Informatics
<br class="ltx_break"/>Faculdade de Ciências (FCUL), Campo Grande, 1749-016 Lisboa, Portugal
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{jarodrigues,antonio.branco}@fc.ul.pt</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Meta-prompting Optimized Retrieval-augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">João Rodrigues
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">António Branco
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Retrieval-augmented generation resorts to content retrieved from external sources in order to leverage the performance of large language models in downstream tasks.
The excessive volume of retrieved content, the possible dispersion of its parts, or their out of focus range may happen nevertheless to eventually have a detrimental rather than an incremental effect.
To mitigate this issue and improve retrieval-augmented generation, we propose a method to refine the retrieved content before it is included in the prompt by resorting to meta-prompting optimization.
Put to empirical test with the demanding multi-hop question answering task from the StrategyQA dataset, the evaluation results indicate that this method outperforms a similar retrieval-augmented system but without this method by over 30 %.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>RAG Retrieval-Augmented Generation Prompt Optimization Large Language Models Meta-prompting Multi-hop QA
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pre-trained Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib22" title="">22</a>]</cite> are known for their hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib12" title="">12</a>]</cite> and for their further limitations regarding truthfulness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib17" title="">17</a>]</cite>. To tackle these issues, remediation techniques have been explored such as, for instance, fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib10" title="">10</a>]</cite>, prompt-engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib18" title="">18</a>]</cite> or Retrieval-Augmented Generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib15" title="">15</a>]</cite>, initiated by Houlsby et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib9" title="">9</a>]</cite> among several others.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Retrieval-augmented generation</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Focusing on Retrieval-Augmented Generation (RAG), this approach seeks to enhance truthfulness and curb hallucinations by expanding the initial prompt, which contains the initial query, with additional content retrieved from sources that are external to the LLM. Such additional content is obtained with the help of an auxiliary Retrieval Model
where the retrieval model may be a simple Jacquard model or a vector database that extracts relevant content from external sources and pass it on to a Large Language Model that generates an appropriate response given the original query and the extracted content.
If this external content is unstructured text, it may be of different lengths, such as sentences, paragraphs or full documents, among others.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">By feeding LLM’s knowledge, and curbing its whim, with further knowledge from external sources, more accurate answers are likely to be provided.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">Compared with other techniques, such as fine-tuning or prompt-engineering, RAG key advantage is the ease with which newer, up-to-date content is taken advantage of, as this does not require the costly compute of re-training neural networks (as in fine-tuning) or the costly human labour for the creation of further manually designed prompts (as in prompt-engineering).
To be sure, all these techniques can nevertheless be mixed and function together.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Prompt optimization</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Usually, the pieces of content retrieved may be from heterogeneous sources and they tend to lack a connecting thread.
They may also be redundant or may be of very high volume.
These, among other aspects, may end up having a detrimental effect and eventually jeopardizing the generation task, rather than enhancing it.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">To mitigate this problem, we present a method that consists of adding an intermediate step between the retrieval of the external content and the entering of the expanded prompt into the LLM to finally obtain the response to the initial query. Aiming at improving the performance of this generation-LLM, this intermediate step seeks to obtain a refined version of the external knowledge.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">This refinement is accomplished by means of an auxiliary transformation-LLM that is entered with a prompt containing the pieces of retrieved contents, preceded by an instruction with the request for the sought refinement.</p>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p" id="S1.SS2.p4.1">For example, if several pages of Wikipedia are retrieved as possible relevant content, the transformation-LLM processes this content and may generate a summary or remove unnecessary information from that original content.</p>
</div>
<div class="ltx_para" id="S1.SS2.p5">
<p class="ltx_p" id="S1.SS2.p5.1">Turning to this refinement instruction, this is obtained by an automatic procedure that is preliminary to running the RAG system made of transformation- and generation-LLMs, and it is undertaken by yet a third LLM.</p>
</div>
<div class="ltx_para" id="S1.SS2.p6">
<p class="ltx_p" id="S1.SS2.p6.1">Inspired in Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib35" title="">35</a>]</cite>, in this procedure a meta-prompt is used as input to this third, optimizer-LLM for this to iteratively generate new tentative instructions, score them, and retain, in the meta-prompt itself, a list with the top k ones that induce better performance for the RAG system. By the end of this optimization process, the best scoring instruction in this list is the one retained to be used in the refinement step of the retrieved contents with the transformation-LLM.</p>
</div>
<div class="ltx_para" id="S1.SS2.p7">
<p class="ltx_p" id="S1.SS2.p7.1">This meta-prompt contains a meta-instruction and a list of tentative instructions that is aimed at being updated during this process with new instructions that induce better RAG performance.
After a new tentative instruction is generated, its contribution to approximate the gold output to the initial query is scored, and the list of tentative instructions in the meta-prompt is possibly updated so that it retains the top-performing ones so far.
This is iterated, and an optimization trajectory is hence accomplished to eventually find the new refinement instruction that maximizes the success of the RAG system.</p>
</div>
<div class="ltx_para" id="S1.SS2.p8">
<p class="ltx_p ltx_align_center" id="S1.SS2.p8.1">***</p>
</div>
<div class="ltx_para" id="S1.SS2.p9">
<p class="ltx_p" id="S1.SS2.p9.1">In this paper, we propose a method for RAG to be enhanced with the refinement of the retrieved content, a refinement that is optimized by resorting to iterative meta-prompting. This is a novel method that can be combined with previous approaches aimed at enhancing RAG.</p>
</div>
<div class="ltx_para" id="S1.SS2.p10">
<p class="ltx_p" id="S1.SS2.p10.1">We report on the experiments performed to put this method to the test. This approach is extrinsically evaluated by being embedded in a demanding question-answering downstream task. Its performance demonstrates that it is an effective method to enhance RAG by improving by 30% the performance of a baseline RAG without this method, and that it can be combined with other previous state of the art methods for RAG enhancement proposed in the literature.</p>
</div>
<div class="ltx_para" id="S1.SS2.p11">
<p class="ltx_p" id="S1.SS2.p11.1">The remainder of this paper is structured as follows:
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S2" title="2 Related work ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> discusses related work;
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S3" title="3 Method ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> describes the method proposed in this study;
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4" title="4 Dataset and models ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> reports on the models and dataset resorted to;
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5" title="5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a>
presents the
experiments undertaken and their evaluation, and discusses the results obtained;
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S6" title="6 Future work ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">6</span></a> addresses future research paths;
and finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S7" title="7 Conclusion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">7</span></a> closes this paper with concluding remarks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Prompt optimization has gained traction as an effective mechanism for enhancing LLMs in several downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib1" title="">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The earliest approaches in prompt optimization sought to directly optimize the prompt embedding space, such as prefix-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib16" title="">16</a>]</cite> or OptiPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib37" title="">37</a>]</cite>. These aimed at optimizing a sequence of continuous task-specific vectors applied to the prompt to leverage downstream tasks.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">More recent studies have introduced further techniques to enhance prompts, such as chain-of-thought <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib34" title="">34</a>]</cite> and tree-of-thoughts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib36" title="">36</a>]</cite>.
The former involves extending prompts with a few manually written chain of thought demonstrations as examples, which results in improved performance across various tasks, including arithmetic, commonsense and symbolic reasoning.
The latter builds upon the chain-of-thought by considering multiple reasoning paths, self-evaluating choices, and by making global decisions by looking ahead or backtracking when necessary.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Other methods for optimizing prompts include searching through a pool of prompt candidates generated by an LLM, employing principled planning algorithms based on Monte Carlo tree search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib33" title="">33</a>]</cite>, or applying iterative local edit operations at a syntactic phrase-level split within the prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Further proposals encompass EvoPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib7" title="">7</a>]</cite>, which uses evolutionary operators over a prompt population for optimization, while Sabbatella et al. employs Bayesian Optimization within a prompt search space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib26" title="">26</a>]</cite>, reinforcement learning to rewrite prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib13" title="">13</a>]</cite> or a prompt optimization that integrates human-design feedback rules to suggest improvements automatically <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Recently, Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib35" title="">35</a>]</cite> introduced OPRO, leveraging LLMs as optimizers through meta-prompts, which are natural language descriptions that guide prompt optimization.
It was applied to optimize prompts by retrieving and re-ranking top-K relevant instructions with respect to an initial instruction, and by appending them to the global task description.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">In contrast, to enhance RAG, we propose a method to optimize the prompt that differs from the previous proposals in the literature.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">A prompt for RAG includes a query and the content retrieved from external sources on the basis of that query. It may contain also an instruction about how to handle the query or how the retrieved content should be used by the generation-LLM to answer it. Related work for RAG enhanced with prompt optimization has concentrated on optimizing the instruction and/or the query. Differently from previous approaches, our method focuses instead on optimizing the version of the retrieved content that is included in the prompt entered into the generation-LLM. Hence, rather than being an approach alternative to previous ones, it is a new one that is complementary to them and may be combined.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The objective of our method is to enhance the RAG performance of a generation-LLM by means of the improvement of its input prompt, which is made of a query introduced by the user and of pieces of content retrieved from external sources on the basis of that query. Before it is entered into the generation-LLM, this prompt is improved by means of a refinement of the retrieved content, performed by a transformation-LLM.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_l ltx_border_t" id="S3.T1.1.1.1.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S3.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S3.T1.1.1.1.2.1.1.1" style="font-size:70%;">I have some prompts along with their corresponding scores. The prompts are arranged in ascending order based on their scores, where higher scores indicate better quality. Together with relevant information extracted from a database, these prompts are given as input to a large language model in order to optimize the provided relevant information. Several techniques may help the optimization, such as re-ranking paragraphs, cleaning, filtering and summarization. Write your new prompt taking into account the previous ones and aiming to achieve a higher score.
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text" id="S3.T1.1.1.1.2.1.1.2" style="font-size:70%;color:#009901;">
prompt:
<br class="ltx_break"/>Summarize the main idea of the previous text.
<br class="ltx_break"/>score:
<br class="ltx_break"/>3.0
<br class="ltx_break"/>
<br class="ltx_break"/>prompt:
<br class="ltx_break"/>Summarize the main points in 30 words or less.
<br class="ltx_break"/>score:
<br class="ltx_break"/>3.0
<br class="ltx_break"/></span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.1.3"></td>
</tr>
</tbody>
</table>
<br class="ltx_break"/>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S3.T1.5.1">Meta-prompt</span> - An example of a meta-prompt: in black, the top paragraph with the meta-instruction actually used in the experiments; below, in green, the list of top performing instructions so far, and the respective scores.</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">And before a first query is accepted to put the RAG system to use, the prompt to be used with the transformation-LLM for refinement purposes is optimized. This prompt includes a refinement instruction and the pieces of retrieved content to be refined. It is optimized by means of the optimization of this instruction through iterative meta-prompting.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">This meta-prompting optimization is undertaken by an optimizer-LLM that is entered with a (meta-)prompt that includes a (meta-)instruction and a list of tentative refinement instructions and respective performance scores. These scores are obtained by running the RAG with the tentative refinement instruction through a sample of training examples and evaluating the output against the respective gold responses.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Focusing on the optimization phase, a meta-prompt is used that contains both the description of the optimization problem and the history with previous best solutions for the instruction. Such meta-prompt is iteratively entered into the optimizer-LLM, and at each iteration that history is possibly updated with generated instructions if these support better performance for the task at stake in the generation phase. The instruction selected out of this optimization process is the best scoring one in the history obtained as this iteration is over.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">An example of a meta-prompt is in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S3.T1" title="Table 1 ‣ 3 Method ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, and a detailed description of this optimization via iterative meta-prompting is presented in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#alg1" title="Algorithm 1 ‣ 3 Method ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Optimization with meta-prompting</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l1.2">Input:</span> Dataset <math alttext="D" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_D</annotation></semantics></math> with <math alttext="n" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_n</annotation></semantics></math> examples, each containing a query <math alttext="q" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">italic_q</annotation></semantics></math>, retrieved contents <math alttext="c" class="ltx_Math" display="inline" id="alg1.l1.m4.1"><semantics id="alg1.l1.m4.1a"><mi id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m4.1d">italic_c</annotation></semantics></math> and the answer <math alttext="a" class="ltx_Math" display="inline" id="alg1.l1.m5.1"><semantics id="alg1.l1.m5.1a"><mi id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><ci id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">a</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m5.1d">italic_a</annotation></semantics></math>; meta-prompt <math alttext="metaP" class="ltx_Math" display="inline" id="alg1.l1.m6.1"><semantics id="alg1.l1.m6.1a"><mrow id="alg1.l1.m6.1.1" xref="alg1.l1.m6.1.1.cmml"><mi id="alg1.l1.m6.1.1.2" xref="alg1.l1.m6.1.1.2.cmml">m</mi><mo id="alg1.l1.m6.1.1.1" xref="alg1.l1.m6.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m6.1.1.3" xref="alg1.l1.m6.1.1.3.cmml">e</mi><mo id="alg1.l1.m6.1.1.1a" xref="alg1.l1.m6.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m6.1.1.4" xref="alg1.l1.m6.1.1.4.cmml">t</mi><mo id="alg1.l1.m6.1.1.1b" xref="alg1.l1.m6.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m6.1.1.5" xref="alg1.l1.m6.1.1.5.cmml">a</mi><mo id="alg1.l1.m6.1.1.1c" xref="alg1.l1.m6.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m6.1.1.6" xref="alg1.l1.m6.1.1.6.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m6.1b"><apply id="alg1.l1.m6.1.1.cmml" xref="alg1.l1.m6.1.1"><times id="alg1.l1.m6.1.1.1.cmml" xref="alg1.l1.m6.1.1.1"></times><ci id="alg1.l1.m6.1.1.2.cmml" xref="alg1.l1.m6.1.1.2">𝑚</ci><ci id="alg1.l1.m6.1.1.3.cmml" xref="alg1.l1.m6.1.1.3">𝑒</ci><ci id="alg1.l1.m6.1.1.4.cmml" xref="alg1.l1.m6.1.1.4">𝑡</ci><ci id="alg1.l1.m6.1.1.5.cmml" xref="alg1.l1.m6.1.1.5">𝑎</ci><ci id="alg1.l1.m6.1.1.6.cmml" xref="alg1.l1.m6.1.1.6">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m6.1c">metaP</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m6.1d">italic_m italic_e italic_t italic_a italic_P</annotation></semantics></math> with the description of the optimization task and with a list of instructions and respective scores

</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l2.2">Output:</span> List of scored instructions and the best scoring instruction

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l3.2">while</span> optimizing prompt <span class="ltx_text ltx_font_bold" id="alg1.l3.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span>     Enter meta-prompt to optimizer-LLM

</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span>     Generate new instructions <math alttext="I" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">italic_I</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span>     Select a random subset <math alttext="E" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mi id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_E</annotation></semantics></math> of examples <math alttext="e" class="ltx_Math" display="inline" id="alg1.l6.m2.1"><semantics id="alg1.l6.m2.1a"><mi id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">e</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.1d">italic_e</annotation></semantics></math> from <math alttext="D" class="ltx_Math" display="inline" id="alg1.l6.m3.1"><semantics id="alg1.l6.m3.1a"><mi id="alg1.l6.m3.1.1" xref="alg1.l6.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m3.1b"><ci id="alg1.l6.m3.1.1.cmml" xref="alg1.l6.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m3.1d">italic_D</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l7.2">for</span> each instruction <math alttext="I_{j}" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><msub id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">I</mi><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">𝐼</ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">I_{j}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l7.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span>         <span class="ltx_text ltx_font_bold" id="alg1.l8.2">for</span> each example <math alttext="e_{k}" class="ltx_Math" display="inline" id="alg1.l8.m1.1"><semantics id="alg1.l8.m1.1a"><msub id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">e</mi><mi id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1">subscript</csymbol><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">𝑒</ci><ci id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">e_{k}</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.m1.1d">italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="E" class="ltx_Math" display="inline" id="alg1.l8.m2.1"><semantics id="alg1.l8.m2.1a"><mi id="alg1.l8.m2.1.1" xref="alg1.l8.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m2.1b"><ci id="alg1.l8.m2.1.1.cmml" xref="alg1.l8.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m2.1c">E</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.m2.1d">italic_E</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l8.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span>              Assemble prompt <math alttext="TransP" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml">T</mi><mo id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml">r</mi><mo id="alg1.l9.m1.1.1.1a" xref="alg1.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.4" xref="alg1.l9.m1.1.1.4.cmml">a</mi><mo id="alg1.l9.m1.1.1.1b" xref="alg1.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.5" xref="alg1.l9.m1.1.1.5.cmml">n</mi><mo id="alg1.l9.m1.1.1.1c" xref="alg1.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.6" xref="alg1.l9.m1.1.1.6.cmml">s</mi><mo id="alg1.l9.m1.1.1.1d" xref="alg1.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.7" xref="alg1.l9.m1.1.1.7.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><times id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1"></times><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑇</ci><ci id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3">𝑟</ci><ci id="alg1.l9.m1.1.1.4.cmml" xref="alg1.l9.m1.1.1.4">𝑎</ci><ci id="alg1.l9.m1.1.1.5.cmml" xref="alg1.l9.m1.1.1.5">𝑛</ci><ci id="alg1.l9.m1.1.1.6.cmml" xref="alg1.l9.m1.1.1.6">𝑠</ci><ci id="alg1.l9.m1.1.1.7.cmml" xref="alg1.l9.m1.1.1.7">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">TransP</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_T italic_r italic_a italic_n italic_s italic_P</annotation></semantics></math> from <math alttext="I_{j}" class="ltx_Math" display="inline" id="alg1.l9.m2.1"><semantics id="alg1.l9.m2.1a"><msub id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml"><mi id="alg1.l9.m2.1.1.2" xref="alg1.l9.m2.1.1.2.cmml">I</mi><mi id="alg1.l9.m2.1.1.3" xref="alg1.l9.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><apply id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.1.cmml" xref="alg1.l9.m2.1.1">subscript</csymbol><ci id="alg1.l9.m2.1.1.2.cmml" xref="alg1.l9.m2.1.1.2">𝐼</ci><ci id="alg1.l9.m2.1.1.3.cmml" xref="alg1.l9.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">I_{j}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m2.1d">italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and contents <math alttext="c_{k}" class="ltx_Math" display="inline" id="alg1.l9.m3.1"><semantics id="alg1.l9.m3.1a"><msub id="alg1.l9.m3.1.1" xref="alg1.l9.m3.1.1.cmml"><mi id="alg1.l9.m3.1.1.2" xref="alg1.l9.m3.1.1.2.cmml">c</mi><mi id="alg1.l9.m3.1.1.3" xref="alg1.l9.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m3.1b"><apply id="alg1.l9.m3.1.1.cmml" xref="alg1.l9.m3.1.1"><csymbol cd="ambiguous" id="alg1.l9.m3.1.1.1.cmml" xref="alg1.l9.m3.1.1">subscript</csymbol><ci id="alg1.l9.m3.1.1.2.cmml" xref="alg1.l9.m3.1.1.2">𝑐</ci><ci id="alg1.l9.m3.1.1.3.cmml" xref="alg1.l9.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m3.1c">c_{k}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m3.1d">italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span>              Enter <math alttext="TransP" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">T</mi><mo id="alg1.l10.m1.1.1.1" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3.cmml">r</mi><mo id="alg1.l10.m1.1.1.1a" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.4" xref="alg1.l10.m1.1.1.4.cmml">a</mi><mo id="alg1.l10.m1.1.1.1b" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.5" xref="alg1.l10.m1.1.1.5.cmml">n</mi><mo id="alg1.l10.m1.1.1.1c" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.6" xref="alg1.l10.m1.1.1.6.cmml">s</mi><mo id="alg1.l10.m1.1.1.1d" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.7" xref="alg1.l10.m1.1.1.7.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><times id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1"></times><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝑇</ci><ci id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">𝑟</ci><ci id="alg1.l10.m1.1.1.4.cmml" xref="alg1.l10.m1.1.1.4">𝑎</ci><ci id="alg1.l10.m1.1.1.5.cmml" xref="alg1.l10.m1.1.1.5">𝑛</ci><ci id="alg1.l10.m1.1.1.6.cmml" xref="alg1.l10.m1.1.1.6">𝑠</ci><ci id="alg1.l10.m1.1.1.7.cmml" xref="alg1.l10.m1.1.1.7">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">TransP</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_T italic_r italic_a italic_n italic_s italic_P</annotation></semantics></math> to transformation-LLM

</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span>              Generate transformed contents <math alttext="tc" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml"><mi id="alg1.l11.m1.1.1.2" xref="alg1.l11.m1.1.1.2.cmml">t</mi><mo id="alg1.l11.m1.1.1.1" xref="alg1.l11.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3" xref="alg1.l11.m1.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"><times id="alg1.l11.m1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1"></times><ci id="alg1.l11.m1.1.1.2.cmml" xref="alg1.l11.m1.1.1.2">𝑡</ci><ci id="alg1.l11.m1.1.1.3.cmml" xref="alg1.l11.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">tc</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">italic_t italic_c</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span>              Assemble prompt <math alttext="TaskP" class="ltx_Math" display="inline" id="alg1.l12.m1.1"><semantics id="alg1.l12.m1.1a"><mrow id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">T</mi><mo id="alg1.l12.m1.1.1.1" xref="alg1.l12.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3.cmml">a</mi><mo id="alg1.l12.m1.1.1.1a" xref="alg1.l12.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.4" xref="alg1.l12.m1.1.1.4.cmml">s</mi><mo id="alg1.l12.m1.1.1.1b" xref="alg1.l12.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.5" xref="alg1.l12.m1.1.1.5.cmml">k</mi><mo id="alg1.l12.m1.1.1.1c" xref="alg1.l12.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.6" xref="alg1.l12.m1.1.1.6.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><times id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1.1"></times><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">𝑇</ci><ci id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3">𝑎</ci><ci id="alg1.l12.m1.1.1.4.cmml" xref="alg1.l12.m1.1.1.4">𝑠</ci><ci id="alg1.l12.m1.1.1.5.cmml" xref="alg1.l12.m1.1.1.5">𝑘</ci><ci id="alg1.l12.m1.1.1.6.cmml" xref="alg1.l12.m1.1.1.6">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">TaskP</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m1.1d">italic_T italic_a italic_s italic_k italic_P</annotation></semantics></math> from query <math alttext="q_{k}" class="ltx_Math" display="inline" id="alg1.l12.m2.1"><semantics id="alg1.l12.m2.1a"><msub id="alg1.l12.m2.1.1" xref="alg1.l12.m2.1.1.cmml"><mi id="alg1.l12.m2.1.1.2" xref="alg1.l12.m2.1.1.2.cmml">q</mi><mi id="alg1.l12.m2.1.1.3" xref="alg1.l12.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l12.m2.1b"><apply id="alg1.l12.m2.1.1.cmml" xref="alg1.l12.m2.1.1"><csymbol cd="ambiguous" id="alg1.l12.m2.1.1.1.cmml" xref="alg1.l12.m2.1.1">subscript</csymbol><ci id="alg1.l12.m2.1.1.2.cmml" xref="alg1.l12.m2.1.1.2">𝑞</ci><ci id="alg1.l12.m2.1.1.3.cmml" xref="alg1.l12.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m2.1c">q_{k}</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m2.1d">italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="tc" class="ltx_Math" display="inline" id="alg1.l12.m3.1"><semantics id="alg1.l12.m3.1a"><mrow id="alg1.l12.m3.1.1" xref="alg1.l12.m3.1.1.cmml"><mi id="alg1.l12.m3.1.1.2" xref="alg1.l12.m3.1.1.2.cmml">t</mi><mo id="alg1.l12.m3.1.1.1" xref="alg1.l12.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l12.m3.1.1.3" xref="alg1.l12.m3.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m3.1b"><apply id="alg1.l12.m3.1.1.cmml" xref="alg1.l12.m3.1.1"><times id="alg1.l12.m3.1.1.1.cmml" xref="alg1.l12.m3.1.1.1"></times><ci id="alg1.l12.m3.1.1.2.cmml" xref="alg1.l12.m3.1.1.2">𝑡</ci><ci id="alg1.l12.m3.1.1.3.cmml" xref="alg1.l12.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m3.1c">tc</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m3.1d">italic_t italic_c</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span>              Input <math alttext="TaskP" class="ltx_Math" display="inline" id="alg1.l13.m1.1"><semantics id="alg1.l13.m1.1a"><mrow id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml"><mi id="alg1.l13.m1.1.1.2" xref="alg1.l13.m1.1.1.2.cmml">T</mi><mo id="alg1.l13.m1.1.1.1" xref="alg1.l13.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l13.m1.1.1.3" xref="alg1.l13.m1.1.1.3.cmml">a</mi><mo id="alg1.l13.m1.1.1.1a" xref="alg1.l13.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l13.m1.1.1.4" xref="alg1.l13.m1.1.1.4.cmml">s</mi><mo id="alg1.l13.m1.1.1.1b" xref="alg1.l13.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l13.m1.1.1.5" xref="alg1.l13.m1.1.1.5.cmml">k</mi><mo id="alg1.l13.m1.1.1.1c" xref="alg1.l13.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l13.m1.1.1.6" xref="alg1.l13.m1.1.1.6.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><apply id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"><times id="alg1.l13.m1.1.1.1.cmml" xref="alg1.l13.m1.1.1.1"></times><ci id="alg1.l13.m1.1.1.2.cmml" xref="alg1.l13.m1.1.1.2">𝑇</ci><ci id="alg1.l13.m1.1.1.3.cmml" xref="alg1.l13.m1.1.1.3">𝑎</ci><ci id="alg1.l13.m1.1.1.4.cmml" xref="alg1.l13.m1.1.1.4">𝑠</ci><ci id="alg1.l13.m1.1.1.5.cmml" xref="alg1.l13.m1.1.1.5">𝑘</ci><ci id="alg1.l13.m1.1.1.6.cmml" xref="alg1.l13.m1.1.1.6">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">TaskP</annotation><annotation encoding="application/x-llamapun" id="alg1.l13.m1.1d">italic_T italic_a italic_s italic_k italic_P</annotation></semantics></math> to generation-LLM

</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span>              Generate answer and evaluate it against gold <math alttext="a_{k}" class="ltx_Math" display="inline" id="alg1.l14.m1.1"><semantics id="alg1.l14.m1.1a"><msub id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml"><mi id="alg1.l14.m1.1.1.2" xref="alg1.l14.m1.1.1.2.cmml">a</mi><mi id="alg1.l14.m1.1.1.3" xref="alg1.l14.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><apply id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1"><csymbol cd="ambiguous" id="alg1.l14.m1.1.1.1.cmml" xref="alg1.l14.m1.1.1">subscript</csymbol><ci id="alg1.l14.m1.1.1.2.cmml" xref="alg1.l14.m1.1.1.2">𝑎</ci><ci id="alg1.l14.m1.1.1.3.cmml" xref="alg1.l14.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">a_{k}</annotation><annotation encoding="application/x-llamapun" id="alg1.l14.m1.1d">italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l15.1.1.1" style="font-size:80%;">15:</span></span>         <span class="ltx_text ltx_font_bold" id="alg1.l15.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l15.3">for</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l16.1.1.1" style="font-size:80%;">16:</span></span>         Compute <math alttext="I_{j}score" class="ltx_Math" display="inline" id="alg1.l16.m1.1"><semantics id="alg1.l16.m1.1a"><mrow id="alg1.l16.m1.1.1" xref="alg1.l16.m1.1.1.cmml"><msub id="alg1.l16.m1.1.1.2" xref="alg1.l16.m1.1.1.2.cmml"><mi id="alg1.l16.m1.1.1.2.2" xref="alg1.l16.m1.1.1.2.2.cmml">I</mi><mi id="alg1.l16.m1.1.1.2.3" xref="alg1.l16.m1.1.1.2.3.cmml">j</mi></msub><mo id="alg1.l16.m1.1.1.1" xref="alg1.l16.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l16.m1.1.1.3" xref="alg1.l16.m1.1.1.3.cmml">s</mi><mo id="alg1.l16.m1.1.1.1a" xref="alg1.l16.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l16.m1.1.1.4" xref="alg1.l16.m1.1.1.4.cmml">c</mi><mo id="alg1.l16.m1.1.1.1b" xref="alg1.l16.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l16.m1.1.1.5" xref="alg1.l16.m1.1.1.5.cmml">o</mi><mo id="alg1.l16.m1.1.1.1c" xref="alg1.l16.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l16.m1.1.1.6" xref="alg1.l16.m1.1.1.6.cmml">r</mi><mo id="alg1.l16.m1.1.1.1d" xref="alg1.l16.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l16.m1.1.1.7" xref="alg1.l16.m1.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l16.m1.1b"><apply id="alg1.l16.m1.1.1.cmml" xref="alg1.l16.m1.1.1"><times id="alg1.l16.m1.1.1.1.cmml" xref="alg1.l16.m1.1.1.1"></times><apply id="alg1.l16.m1.1.1.2.cmml" xref="alg1.l16.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l16.m1.1.1.2.1.cmml" xref="alg1.l16.m1.1.1.2">subscript</csymbol><ci id="alg1.l16.m1.1.1.2.2.cmml" xref="alg1.l16.m1.1.1.2.2">𝐼</ci><ci id="alg1.l16.m1.1.1.2.3.cmml" xref="alg1.l16.m1.1.1.2.3">𝑗</ci></apply><ci id="alg1.l16.m1.1.1.3.cmml" xref="alg1.l16.m1.1.1.3">𝑠</ci><ci id="alg1.l16.m1.1.1.4.cmml" xref="alg1.l16.m1.1.1.4">𝑐</ci><ci id="alg1.l16.m1.1.1.5.cmml" xref="alg1.l16.m1.1.1.5">𝑜</ci><ci id="alg1.l16.m1.1.1.6.cmml" xref="alg1.l16.m1.1.1.6">𝑟</ci><ci id="alg1.l16.m1.1.1.7.cmml" xref="alg1.l16.m1.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l16.m1.1c">I_{j}score</annotation><annotation encoding="application/x-llamapun" id="alg1.l16.m1.1d">italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l17.1.1.1" style="font-size:80%;">17:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l17.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l17.3">for</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l18.1.1.1" style="font-size:80%;">18:</span></span>     Update <math alttext="metaP" class="ltx_Math" display="inline" id="alg1.l18.m1.1"><semantics id="alg1.l18.m1.1a"><mrow id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml"><mi id="alg1.l18.m1.1.1.2" xref="alg1.l18.m1.1.1.2.cmml">m</mi><mo id="alg1.l18.m1.1.1.1" xref="alg1.l18.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m1.1.1.3" xref="alg1.l18.m1.1.1.3.cmml">e</mi><mo id="alg1.l18.m1.1.1.1a" xref="alg1.l18.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m1.1.1.4" xref="alg1.l18.m1.1.1.4.cmml">t</mi><mo id="alg1.l18.m1.1.1.1b" xref="alg1.l18.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m1.1.1.5" xref="alg1.l18.m1.1.1.5.cmml">a</mi><mo id="alg1.l18.m1.1.1.1c" xref="alg1.l18.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m1.1.1.6" xref="alg1.l18.m1.1.1.6.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><apply id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1"><times id="alg1.l18.m1.1.1.1.cmml" xref="alg1.l18.m1.1.1.1"></times><ci id="alg1.l18.m1.1.1.2.cmml" xref="alg1.l18.m1.1.1.2">𝑚</ci><ci id="alg1.l18.m1.1.1.3.cmml" xref="alg1.l18.m1.1.1.3">𝑒</ci><ci id="alg1.l18.m1.1.1.4.cmml" xref="alg1.l18.m1.1.1.4">𝑡</ci><ci id="alg1.l18.m1.1.1.5.cmml" xref="alg1.l18.m1.1.1.5">𝑎</ci><ci id="alg1.l18.m1.1.1.6.cmml" xref="alg1.l18.m1.1.1.6">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">metaP</annotation><annotation encoding="application/x-llamapun" id="alg1.l18.m1.1d">italic_m italic_e italic_t italic_a italic_P</annotation></semantics></math> by replacing its worst scoring instruction by <math alttext="I_{j}" class="ltx_Math" display="inline" id="alg1.l18.m2.1"><semantics id="alg1.l18.m2.1a"><msub id="alg1.l18.m2.1.1" xref="alg1.l18.m2.1.1.cmml"><mi id="alg1.l18.m2.1.1.2" xref="alg1.l18.m2.1.1.2.cmml">I</mi><mi id="alg1.l18.m2.1.1.3" xref="alg1.l18.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l18.m2.1b"><apply id="alg1.l18.m2.1.1.cmml" xref="alg1.l18.m2.1.1"><csymbol cd="ambiguous" id="alg1.l18.m2.1.1.1.cmml" xref="alg1.l18.m2.1.1">subscript</csymbol><ci id="alg1.l18.m2.1.1.2.cmml" xref="alg1.l18.m2.1.1.2">𝐼</ci><ci id="alg1.l18.m2.1.1.3.cmml" xref="alg1.l18.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m2.1c">I_{j}</annotation><annotation encoding="application/x-llamapun" id="alg1.l18.m2.1d">italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="I_{j}score" class="ltx_Math" display="inline" id="alg1.l18.m3.1"><semantics id="alg1.l18.m3.1a"><mrow id="alg1.l18.m3.1.1" xref="alg1.l18.m3.1.1.cmml"><msub id="alg1.l18.m3.1.1.2" xref="alg1.l18.m3.1.1.2.cmml"><mi id="alg1.l18.m3.1.1.2.2" xref="alg1.l18.m3.1.1.2.2.cmml">I</mi><mi id="alg1.l18.m3.1.1.2.3" xref="alg1.l18.m3.1.1.2.3.cmml">j</mi></msub><mo id="alg1.l18.m3.1.1.1" xref="alg1.l18.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m3.1.1.3" xref="alg1.l18.m3.1.1.3.cmml">s</mi><mo id="alg1.l18.m3.1.1.1a" xref="alg1.l18.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m3.1.1.4" xref="alg1.l18.m3.1.1.4.cmml">c</mi><mo id="alg1.l18.m3.1.1.1b" xref="alg1.l18.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m3.1.1.5" xref="alg1.l18.m3.1.1.5.cmml">o</mi><mo id="alg1.l18.m3.1.1.1c" xref="alg1.l18.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m3.1.1.6" xref="alg1.l18.m3.1.1.6.cmml">r</mi><mo id="alg1.l18.m3.1.1.1d" xref="alg1.l18.m3.1.1.1.cmml">⁢</mo><mi id="alg1.l18.m3.1.1.7" xref="alg1.l18.m3.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l18.m3.1b"><apply id="alg1.l18.m3.1.1.cmml" xref="alg1.l18.m3.1.1"><times id="alg1.l18.m3.1.1.1.cmml" xref="alg1.l18.m3.1.1.1"></times><apply id="alg1.l18.m3.1.1.2.cmml" xref="alg1.l18.m3.1.1.2"><csymbol cd="ambiguous" id="alg1.l18.m3.1.1.2.1.cmml" xref="alg1.l18.m3.1.1.2">subscript</csymbol><ci id="alg1.l18.m3.1.1.2.2.cmml" xref="alg1.l18.m3.1.1.2.2">𝐼</ci><ci id="alg1.l18.m3.1.1.2.3.cmml" xref="alg1.l18.m3.1.1.2.3">𝑗</ci></apply><ci id="alg1.l18.m3.1.1.3.cmml" xref="alg1.l18.m3.1.1.3">𝑠</ci><ci id="alg1.l18.m3.1.1.4.cmml" xref="alg1.l18.m3.1.1.4">𝑐</ci><ci id="alg1.l18.m3.1.1.5.cmml" xref="alg1.l18.m3.1.1.5">𝑜</ci><ci id="alg1.l18.m3.1.1.6.cmml" xref="alg1.l18.m3.1.1.6">𝑟</ci><ci id="alg1.l18.m3.1.1.7.cmml" xref="alg1.l18.m3.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m3.1c">I_{j}score</annotation><annotation encoding="application/x-llamapun" id="alg1.l18.m3.1d">italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e</annotation></semantics></math> if this is better scored

</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l19.1.1.1" style="font-size:80%;">19:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l19.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l19.3">while</span>
</div>
</div>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset and models</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To empirically assess the performance gains of the proposed method, it was integrated into an RAG for question-answering whose performance provides for its extrinsic evaluation.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task and dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Multi-hop question answering requires taking into account disparate pieces of content to get at the answer for a query, which constitutes a most demanding scenario for the task of question answering.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We resorted to a most complex benchmark for multi-hop question-answering available in the literature, the StrategyQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib8" title="">8</a>]</cite>, which contains 2,780 queries, each associated with related content made of paragraphs and the respective <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">yes</span> or <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">no</span> answer.
Based on Wikipedia content, this dataset covers a range of diverse topics and
the task consists in, given a query, to provide an accurate answer to it together with the passages retrieved from Wikipedia with the most relevant content to get at that answer — Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S4.T2" title="Table 2 ‣ 4.1 Task and dataset ‣ 4 Dataset and models ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> displays an example.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.1.1">
<span class="ltx_p" id="S4.T2.1.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1.1" style="font-size:70%;">Query</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.2.1">
<span class="ltx_p" id="S4.T2.1.1.1.2.1.1" style="width:256.1pt;"><span class="ltx_text" id="S4.T2.1.1.1.2.1.1.1" style="font-size:70%;">Could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.1.1">
<span class="ltx_p" id="S4.T2.1.2.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1.1.1" style="font-size:70%;">Content #1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.2.1">
<span class="ltx_p" id="S4.T2.1.2.1.2.1.1" style="width:256.1pt;"><span class="ltx_text" id="S4.T2.1.2.1.2.1.1.1" style="font-size:70%;">It set out to tell the news through people, and for many decades through the late 1960s, the magazine’s cover depicted a single person. […]
Raymond Fielding also noted that Larsen was "originally circulation manager and then general manager of Time, later publisher of Life, for many years president of Time Inc., and in the long history of the corporation the most influential and important figure after Luce"</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.1.1">
<span class="ltx_p" id="S4.T2.1.3.2.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1.1.1.1" style="font-size:70%;">Content #2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.1.3.2.2.1.1" style="width:256.1pt;"><span class="ltx_text" id="S4.T2.1.3.2.2.1.1.1" style="font-size:70%;">Total eclipses are rare because the timing of the new moon within the eclipse season needs to be more exact for an alignment between the observer (on Earth) and the centers of the Sun and Moon. […]
because totality exists only along a narrow path on the Earth’s surface traced by the Moon’s full shadow or umbra.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.1.1">
<span class="ltx_p" id="S4.T2.1.4.3.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.1.1.1.1" style="font-size:70%;">Content #3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.2.1">
<span class="ltx_p" id="S4.T2.1.4.3.2.1.1" style="width:256.1pt;"><span class="ltx_text" id="S4.T2.1.4.3.2.1.1.1" style="font-size:70%;">At least two lunar eclipses and as many as five occur every year, although total lunar eclipses are significantly less common. If the date and time of an eclipse is known, the occurrences of upcoming eclipses are predictable using an eclipse cycle, like the saros.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.1.1">
<span class="ltx_p" id="S4.T2.1.5.4.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.1.1.1.1" style="font-size:70%;">Answer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.2.1">
<span class="ltx_p" id="S4.T2.1.5.4.2.1.1" style="width:256.1pt;"><span class="ltx_text" id="S4.T2.1.5.4.2.1.1.1" style="font-size:70%;">Yes</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.5.1">StrategyQA</span> - An example from the StrategyQA, with a query, three of the most relevant pieces of content, and the respective answer. </figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">To provide for the evaluation of the proposed method, and isolate the accrued performance induced by it, thus disregarding possible fluctuation or loss of performance due to the retrieval process, only the gold pieces of content from a test set should be taken into account.
Since the answers are not provided in the original test set of StrategyQA, a new test set for the present evaluation exercise had to be built.
Accordingly, we divided the original training set into two parts: a new test set with 490 of the original training examples, which matches the size of the original test set, and a new training set containing a subset with 1800 such examples. The resulting train and test sets have an average query length of 9.6 words, and 2.33 contents (paragraphs) per query and are almost balanced.
The training set contains 834 yes answers and 966 no answers (46.32 % / 53.68 %).
The test set contains 237 yes answers and 253 no answers (48.40 % / 51.60 %).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Two Transformer-based language models with 70 Billion parameters were used, a pre-trained Llama-2-70b and an instruct model Llama-2-70b-chat fine-tuned for dialogue use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib31" title="">31</a>]</cite>.
These models were trained and fine-tuned with a context length of 4k tokens over 2 Trillion tokens on a mix of publicly available data.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In general, the default Llama2 model hyper-parameters were applied and no hyper-parameters search bound was performed.
All language models use a temperature value of 1.0, a maximum of 64 generation tokens for the new instructions, a maximum of 128 generation tokens for the refined content, and a maximum of 64 generation tokens for the response to the task.
The optimization run was performed for two days on two NVIDIA A100 40GB GPUs.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">All software and versioning along with hyper-parameters are fully described in the source code of these experiments.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For the sake of reproducibility, data and code are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nlx-group/rag-meta-prompt" title="">https://github.com/nlx-group/rag-meta-prompt</a></span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation procedure and metrics</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Based on empirical experimentation, we arrived at a meta-prompt, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S3.T1" title="Table 1 ‣ 3 Method ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, that indicates the aim of the optimization problem and includes a starting example instruction.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The starting instruction is ”Clean and organize the previous text.”</span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The instruction optimization was iterated over 100 steps. At each step, 3 instructions were generated, each such instruction was evaluated on a random sample of 6 training examples, and the meta-prompt was eventually updated to containing the 8 top scoring queries so far. When this iteration was concluded, the best scoring instruction was retained as the optimized instruction.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We compare against the same generation-LLM using test queries and associated pieces of content, that is without the later being refined by the transformation-LLM under the instruction that was optimized by the optimizer-LLM.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">For the StrategyQA task, a Boolean answer is expected. Accuracy is thus the metric used for evaluating the match between the answer output by the system and the gold answer in the data set.
Accuracy score is given by the proportion of correct answers, and a generated response was counted as correct if the gold answer was found in the exact beginning of it.
The response underwent minimal normalization, with just lowercasing.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">As for the instrumental process of instruction optimization, it is worth recalling that the evaluation is performed over sample examples from the training set. For a tentatively generated instruction, a correct answer to it counted 1 point; an incorrect answer, in turn, counted 0.5 points if it was nevertheless in a Boolean format, or counted 0 points otherwise.
The maximum possible score was thus 6 points, given each tentative instruction was evaluated against 6 sampled queries as indicated above.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we report on the evaluation exercise undertaken to assess the proposed method and discuss its results, summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.T3" title="Table 3 ‣ 5.1 Experiments ‣ 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experiments</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">All in all, six experiments were undertaken, two resorting to the model Llama-2-70b, developed with a pre-training regime only, and four resorting to the model Llama-2-70b-chat, which resulted from further fine-tuning it with dialogue data.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.2.1.1.1">
<span class="ltx_p" id="S5.T3.1.2.1.1.1.1">Llama-2-70b</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.2.1.2.1">
<span class="ltx_p" id="S5.T3.1.2.1.2.1.1">query</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.3">17 (3.46 %)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.3.2.1.1">
<span class="ltx_p" id="S5.T3.1.3.2.1.1.1">Llama-2-70b</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.3.2.2.1">
<span class="ltx_p" id="S5.T3.1.3.2.2.1.1">query+contents</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.3">33 (6.73 %)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.4.3.1.1">
<span class="ltx_p" id="S5.T3.1.4.3.1.1.1">Llama-2-70b-chat</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.4.3.2.1">
<span class="ltx_p" id="S5.T3.1.4.3.2.1.1">query</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.3.3">81 (16.53 %)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.5.4.1.1">
<span class="ltx_p" id="S5.T3.1.5.4.1.1.1">Llama-2-70b-chat</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.5.4.2.1">
<span class="ltx_p" id="S5.T3.1.5.4.2.1.1">query+contents (plain RAG)</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.4.3">128 (26.12 %)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.6.5.1.1">
<span class="ltx_p" id="S5.T3.1.6.5.1.1.1">Llama-2-70b-chat</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.6.5.2.1">
<span class="ltx_p" id="S5.T3.1.6.5.2.1.1">refined query+contents (ours)</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.6.5.3.1">170 (34.69 %)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.7.6.1.1">
<span class="ltx_p" id="S5.T3.1.7.6.1.1.1">Llama-2-70b-chat</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.7.6.2.1">
<span class="ltx_p" id="S5.T3.1.7.6.2.1.1">ref. query+contents no iteration</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.7.6.3">127 (25.92 %)</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S5.T3.3.1">Evaluation</span> - From the total 490 test set examples, the number of correct answers is presented and the respective accuracy.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Both these models were used in two evaluation scenarios. In one of these scenarios—noted as <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">query</span> in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.T3" title="Table 3 ‣ 5.1 Experiments ‣ 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>—, the response to the query entered was provided by the LLM alone, with no further content from external sources being entered. In the other scenario, in turn,—noted as <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.2">query+contents</span>—, further content from external sources was included in the prompt as well. The performance scores for these two scenarios with the two models are displayed in the top four rows <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.T3" title="Table 3 ‣ 5.1 Experiments ‣ 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">External, non-parametric content improves generation</span> As expected, and in line with results in the literature, the retrieval-augmented generation (26.12%) outperforms the plain generation based solely on the query (16.53%).</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Fine-tuning improves generation</span> Also as expected, and by a very large margin, better performance scores were obtained with Llama-2-70b-chat, which had been fine-tuned on dialogue tasks, namely 16.53% against 3.46<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1"><semantics id="S5.SS1.p4.1.m1.1a"><mo id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.1d">%</annotation></semantics></math>, with the query only, and 26.12% against 6.73%, with the query and external content.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">Retrieved content refinement via meta-prompting optimization improves RAG — the proposed method is effective</span>
The model Llama-2-70b-chat was thus retained and two further evaluation scenarios were considered.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">A scenario with the application of the proposed method—noted as <span class="ltx_text ltx_font_bold" id="S5.SS1.p6.1.1">refined query+contents</span>—, where the external content was refined with the help of an instruction optimized with meta-prompting.</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1">The performance scores indicate that, with 34.69% accuracy, our proposed method of enhanced RAG outperforms plain RAG, with 26.12%, thus contributing for a large improvement of over 8.5 percentage points, that represents here an improvement rate of almost 33%.</p>
</div>
<div class="ltx_para" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p8.1.1">Retrieved content refinement via "brute force" optimization does not improve RAG</span>
A sixth scenario—noted as <span class="ltx_text ltx_font_bold" id="S5.SS1.p8.1.2">refined query+contents no iter</span>—was also considered. Here the external content was refined as in the proposed method, but the instruction was refined under an alternative way that dispensed with iterative meta-prompting.</p>
</div>
<div class="ltx_para" id="S5.SS1.p9">
<p class="ltx_p" id="S5.SS1.p9.1">All in all, 300 tentative instructions are generated during all optimization steps —recall that we had 100 iteration steps with 3 tentative instructions generated per step with meta-prompting optimization. To dispense with this iterative meta-prompting, the same number of 300 new tentative instructions were generated at once, in a "brute force" fashion.
By the end of this process, all tentative instructions were scored with the same scoring function as in the proposed method, and the top instruction was evaluated on the test set.</p>
</div>
<div class="ltx_para" id="S5.SS1.p10">
<p class="ltx_p" id="S5.SS1.p10.1">This "brute force" optimization approach, scoring 25.92%, is outperformed not only by the proposed method of meta-prompting optimization, with 34.69%, but even also by the baseline, plain RAG, with 26.12%.</p>
</div>
<div class="ltx_para" id="S5.SS1.p11">
<p class="ltx_p" id="S5.SS1.p11.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p11.1.1">Statistical significance</span>
To assess the statistical significance of the improvements by our method, we employed the unpaired t-test.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The unpaired t-test evaluates if there exists a statistically significant distinction between the means of two independent samples by comparing them.</span></span></span>
We evaluated the baseline system with three seeds and did the same for the meta-prompting optimized system.
Both samples are independent and one may assume the samples are normally distributed.
Applying the unpaired t-test, a two-tailed P value equal to 0.0004 was obtained, which is considered statistically significant.
</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Examining the tentative instructions</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#S5.T4" title="Table 4 ‣ 5.2 Examining the tentative instructions ‣ 5 Results and discussion ‣ Meta-prompting Optimized Retrieval-augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> presents the top generated prompts. The best scoring prompt (last row), with 5.5 (out of a maximum of 6), was obtained at iteration step 46 (out of 300 steps in total), and a good prompt (first row) can be obtained with only 28 steps.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">When reading the best prompt (last row), one realizes that it aims to improve the task through the summarization of the retrieved contents, considering their broader context, and identifying the main theme or message.
It appears thus like a reasonable prompt a human might have thoughtfully arrived at if aiming at improving the performance of the task.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1" style="font-size:70%;">Generated instruction</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1" style="font-size:70%;">Score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1" style="font-size:70%;">Iter. step</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T4.1.2.1.1.1.1" style="width:241.8pt;"><span class="ltx_text" id="S5.T4.1.2.1.1.1.1.1" style="font-size:70%;">Summarize the previous text in 2-3 sentences, while also considering the broader context, the author’s intent, the potential implications of the information, and also identify the main theme or message and its significance, and also analyze the impact of the information on the reader.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.2.1">
<span class="ltx_p" id="S5.T4.1.2.1.2.1.1"><span class="ltx_text" id="S5.T4.1.2.1.2.1.1.1" style="font-size:70%;">5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.3.1">
<span class="ltx_p" id="S5.T4.1.2.1.3.1.1"><span class="ltx_text" id="S5.T4.1.2.1.3.1.1.1" style="font-size:70%;">65</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T4.1.3.2.1.1.1" style="width:241.8pt;"><span class="ltx_text" id="S5.T4.1.3.2.1.1.1.1" style="font-size:70%;">Summarize the previous text in 1 sentence, while also considering the broader context, the author’s intent, the potential implications of the information, and also identify the main theme or message and its significance, and also analyze the impact of the information on the reader, and also provide recommendations for further</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.2.1">
<span class="ltx_p" id="S5.T4.1.3.2.2.1.1"><span class="ltx_text" id="S5.T4.1.3.2.2.1.1.1" style="font-size:70%;">5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.3.1">
<span class="ltx_p" id="S5.T4.1.3.2.3.1.1"><span class="ltx_text" id="S5.T4.1.3.2.3.1.1.1" style="font-size:70%;">72</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T4.1.4.3.1.1.1" style="width:241.8pt;"><span class="ltx_text" id="S5.T4.1.4.3.1.1.1.1" style="font-size:70%;">Summarize the previous text in 2-3 sentences, while also considering the broader context, the author’s intent and the potential implications of the information, and also identify the main theme or message.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.2.1">
<span class="ltx_p" id="S5.T4.1.4.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.3.2.1.1.1" style="font-size:70%;">5.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.3.1">
<span class="ltx_p" id="S5.T4.1.4.3.3.1.1"><span class="ltx_text" id="S5.T4.1.4.3.3.1.1.1" style="font-size:70%;">46</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="S5.T4.5.1">Top meta-prompting optimized instructions</span> scoring 5 or higher, with respective scores and iteration steps at which they were obtained, ordered top to bottom, with the top-scoring, retained instruction in the last row.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">It is reasonable to assume that the meta-prompt iteration in subsequent steps used this query and its score in its search for further tentative instructions, with the generated instructions in three subsequent steps (58, 65 and 72) being some derivation of it (second, fourth and fifth rows).</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">When taking a look at the entire set of generated queries, a high fluctuation of the evaluation scores can be observed along the iteration steps. This is likely due to some interim, generated instructions happening to perform poorly.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Examining the responses</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To gain insight about where our method outperformed the plain RAG baseline, we examined the first 10 instances where our method correctly provided the answer while the baseline failed.
Among these, in six cases, the baseline provided a verbose response and might have failed the exact-match evaluation criterion used.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>An example of a verbose response: [query] Was Superhero fiction invented in the digital format? [response] The answer is no; superhero fiction did not originate in digital format. Superheroes have their roots in pulp magazines, comic strips, and comic books, which were all print media formats before the advent of digital technology.</span></span></span>
In the remaining four cases, the baseline either answered incorrectly, responded with a query, or failed to provide an answer.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Conversely, we reviewed the first 10 instances where our method failed to provide the correct answer while the baseline succeeded.
We observed that our method exhibited a verbose response behavior in five cases that eventually arrived at the correct answer but failed the exact-match evaluation criterion.
In two other cases, our method gave a verbose response without providing an answer, while in two remainder cases, it provided an incorrect response.
Finally, in one instance, our method did not provide any response.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Both methods seem thus to be similarly penalized by the evaluation criterion for not providing straight answers when the correct answer may happen to be included in the verbose response. </p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">While providing a method that effectively enhances RAG, our proposal paves the way for future research, such as the exploration of optimal hyper-parameters, refining content retrieved without gold content, scaling up with larger models, exploring further evaluation functions, or tackling other downstream tasks.
A significant number of hyper-parameters remain unexplored, which is an opportunity to further enhance the efficacy of this method.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">It is worth noting that the evaluation with exact matching is a binary task, and achieving an exact match with a task demanding a more complex string match still needs to be studied, questioning the need for additional training, a different meta-prompt, or a different approach.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">It will be interesting also to study the interaction of our proposed method with the Portuguese language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib2" title="">2</a>]</cite> with the existing family of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib29" title="">29</a>]</cite> and multi-modal LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib27" title="">27</a>]</cite> as also with other tasks such as argument mining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib25" title="">25</a>]</cite>, exploring data spuriousness and others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03955v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper introduces a novel method that enhances RAG and that can be combined with previous approaches for RAG enhancement. It consists in refining the retrieved content included in the prompt entered into the generation model with the help of a refinement instruction that was obtained by means of meta-prompting optimization.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">It reports also on the empirical assessment of this proposal by means of it being embedded in a most demanding multi-hop question answering task. The evaluation results indicate that it is highly effective in as much as it outperforms RAG without this method by over 30%.</p>
</div>
<div class="ltx_para" id="S7.p3">
<span class="ltx_ERROR undefined" id="S7.p3.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S7.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S7.SS0.SSS1.p1">
<p class="ltx_p" id="S7.SS0.SSS1.p1.1">This research was partially supported by:
PORTULAN CLARIN — Research Infrastructure for the Science and Technology of Language, funded by Lisboa 2020, Alentejo 2020 and FCT (PINFRA/22117/2016); ACCELERAT.AI - Multilingual Intelligent Contact Centers, funded by IAPMEI (C625734525-00462629);</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aarohi Srivastava, A.R., et al.: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Branco, A., Mendes, A., Quaresma, P., Gomes, L., Silva, J., Teixeira, A.: Infrastructure for the science and technology of language PORTULAN CLARIN. In: Rehm, G., Bontcheva, K., Choukri, K., Hajič, J., Piperidis, S., Vasiļjevs, A. (eds.) Proceedings of the 1st International Workshop on Language Technology Platforms. pp. 1–7. European Language Resources Association, Marseille, France (May 2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.iwltp-1.1" title="">https://aclanthology.org/2020.iwltp-1.1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Branco, A., Rodrigues, J., Salawa, M., Branco, R., Saedi, C.: Comparative probing of lexical semantics theories for cognitive plausibility and technological usefulness. In: Proceedings of the 28th International Conference on Computational Linguistics. pp. 4004–4019 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Branco, R., Branco, A., António Rodrigues, J., Silva, J.R.: Shortcutted commonsense: Data spuriousness in deep learning of commonsense reasoning. In: Moens, M.F., Huang, X., Specia, L., Yih, S.W.t. (eds.) Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 1504–1521. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.113, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.113" title="">https://aclanthology.org/2021.emnlp-main.113</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, Y., Arkin, J., Hao, Y., Zhang, Y., Roy, N., Fan, C.: Prompt optimization in multi-step tasks (promst): Integrating human feedback and preference alignment. arXiv preprint arXiv:2402.08702 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., Berant, J.: Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">9</span>, 346–361 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., Yang, Y.: Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ho, X., Duong Nguyen, A.K., Sugawara, S., Aizawa, A.: Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In: Proceedings of the 28th International Conference on Computational Linguistics. pp. 6609–6625. International Committee on Computational Linguistics (Dec 2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp. In: International conference on machine learning. pp. 2790–2799. PMLR (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 328–339 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Inoue, N., Stenetorp, P., Inui, K.: R4C: A benchmark for evaluating RC systems to get the right answer for the right reason. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6740–6750 (Jul 2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of hallucination in natural language generation. ACM Computing Surveys <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">55</span>(12), 1–38 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kong, W., Amba Hombaiah, S., Zhang, M., Mei, Q., Bendersky, M.: Prewrite: Prompt rewriting with reinforcement learning. arXiv e-prints pp. arXiv–2401 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 3045–3059 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">33</span>, 9459–9474 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for generation. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics. pp. 4582–4597 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lin, S., Hilton, J., Evans, O.: Truthfulqa: Measuring how models mimic human falsehoods. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. pp. 3214–3252 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">55</span>(9), 1–35 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Min, S., Wallace, E., Singh, S., Gardner, M., Hajishirzi, H., Zettlemoyer, L.: Compositional questions do not necessitate multi-hop reasoning. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Jul 2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Osório, T.F., Leite, B., Lopes Cardoso, H., Gomes, L., Rodrigues, J., Santos, R., Branco, A.: PORTULAN ExtraGLUE datasets and models: Kick-starting a benchmark for the neural processing of Portuguese. In: Zweigenbaum, P., Rapp, R., Sharoff, S. (eds.) Proceedings of the 17th Workshop on Building and Using Comparable Corpora (BUCC) @ LREC-COLING 2024. pp. 24–34. ELRA and ICCL, Torino, Italia (May 2024), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.bucc-1.3" title="">https://aclanthology.org/2024.bucc-1.3</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Prasad, A., Hase, P., Zhou, X., Bansal, M.: Grips: Gradient-free, edit-based instruction search for prompting large language models. In: Proceedings of the 17th Conference of the EACL. pp. 3845–3864 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer: Exploring the limits of transfer learning with a unified text-to-text transformer. Machine Learning Research <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">21</span>, 5485–5551 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Rodrigues, J., Branco, R., Silva, J., Saedi, C., Branco, A.: Predicting brain activation with wordnet embeddings. In: Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing. pp. 1–5 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H.L., Osório, T.: Advancing neural encoding of portuguese with transformer albertina pt. In: EPIA Conference on Artificial Intelligence. pp. 441–453. Springer (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Rodrigues, J.A., Branco, A.: Transferring confluent knowledge to argument mining. In: Calzolari, N., Huang, C.R., Kim, H., Pustejovsky, J., Wanner, L., Choi, K.S., Ryu, P.M., Chen, H.H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He, Z., Lee, T.K., Santus, E., Bond, F., Na, S.H. (eds.) Proceedings of the 29th International Conference on Computational Linguistics. pp. 6859–6874. International Committee on Computational Linguistics, Gyeongju, Republic of Korea (Oct 2022), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.coling-1.597" title="">https://aclanthology.org/2022.coling-1.597</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Sabbatella, A., Ponti, A., Giordani, I., Candelieri, A., Archetti, F.: Prompt optimization in large language models. Mathematics <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">12</span>(6) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Santos, R., Branco, A., Silva, J.R.: Cost-effective language driven image editing with LX-DRIM. In: Proceedings of the First Workshop on Performance and Interpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models. pp. 31–43. International Conference on Computational Linguistics, Virtual (Oct 2022), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.mmmpie-1.5" title="">https://aclanthology.org/2022.mmmpie-1.5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Santos, R., Rodrigues, J., Branco, A., Vaz, R.: Neural text categorization with transformers for learning portuguese as a second language. In: Progress in Artificial Intelligence: 20th EPIA Conference on Artificial Intelligence, EPIA 2021, Virtual Event, September 7–9, 2021, Proceedings 20. pp. 715–726. Springer (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Santos, R., Rodrigues, J., Gomes, L., Silva, J.R., Branco, A., Lopes Cardoso, H., Osório, T.F., Leite, B.: Fostering the ecosystem of open neural encoders for Portuguese with albertina PT* family. In: Melero, M., Sakti, S., Soria, C. (eds.) Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-resourced Languages @ LREC-COLING 2024. pp. 105–114. ELRA and ICCL, Torino, Italia (May 2024), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.sigul-1.14" title="">https://aclanthology.org/2024.sigul-1.14</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Santos, R., Silva, J.R., Gomes, L., Rodrigues, J., Branco, A.: Advancing generative AI for Portuguese with open decoder gervásio PT*. In: Melero, M., Sakti, S., Soria, C. (eds.) Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-resourced Languages @ LREC-COLING 2024. pp. 16–26. ELRA and ICCL, Torino, Italia (May 2024), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.sigul-1.3" title="">https://aclanthology.org/2024.sigul-1.3</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib32.1.1">30</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Wang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic, N., Xing, E.P., Hu, Z.: Promptagent: Strategic planning with language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in NIPS <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">35</span> (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large language models as optimizers. arXiv preprint arXiv:2309.03409 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Yao, S., Yu, D., Zhao, J., et al.: Tree of thoughts: Deliberate problem solving with large language models. Advances in NIPS <span class="ltx_text ltx_font_bold" id="bib.bib36.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhong, Z., Friedman, D., Chen, D.: Factual probing is [mask]: Learning vs. learning to recall. In: Proceedings of the 2021 Conference of the NACL (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul  4 14:08:48 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
