<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</title>
<!--Generated on Wed Oct  2 14:30:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13385v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S1" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1" title="In 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Semantic Compression</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS1" title="In 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Context Distillation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS2" title="In 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS3" title="In 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Efficient Attention Operations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS4" title="In 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Extrapolation and Interpolation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS1.SSS5" title="In 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.5 </span>Context Window Extension</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2" title="In 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pre-Trained Language Models (PLMs)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS1" title="In 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>AutoCompressors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS2" title="In 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>LongNET</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS3" title="In 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>In-Context Auto-Encoders</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS2.SSS4" title="In 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>RECOMP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3" title="In 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Retrievers</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS1" title="In 2.3 Retrievers ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>LLMChainExtractor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS2" title="In 2.3 Retrievers ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>EmbeddingsFilter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.SS3.SSS3" title="In 2.3 Retrievers ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>DocumentCompressorPipeline</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Metrics and Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1" title="In 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS1" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Compression Ratio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS2" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Inference Time</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS3" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Context Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS4" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>Groundedness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS5" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.5 </span>Answer Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS1.SSS6" title="In 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.6 </span>Others</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS2" title="In 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Benchmarks and Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.SS2.SSS1" title="In 3.2 Benchmarks and Datasets ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Common Benchmarks and Datasets</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Challenges and Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS1" title="In 4 Challenges and Future Directions ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>More advanced Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS2" title="In 4 Challenges and Future Directions ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance-Size Trade-offs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS3" title="In 4 Challenges and Future Directions ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Dynamic Contextual Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S4.SS4" title="In 4 Challenges and Future Directions ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Explainability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S5" title="In Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sourav Verma 
<br class="ltx_break"/>IBM Watsonx Client Engineering, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">sourav.verma@ibm.com</span> | <span class="ltx_text ltx_font_typewriter" id="id2.2.id2">souravv.vermaa@gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. By combining the intrinsic knowledge of LLMs with the vast, dynamic repositories of external databases, RAG achieves a synergistic effect.
However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Resources are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SrGrace/Contextual-Compression" title="">https://github.com/SrGrace/Contextual-Compression</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Sourav Verma</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">IBM Watsonx Client Engineering, India</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">sourav.verma@ibm.com</span> | <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.2">souravv.vermaa@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The pioneering accomplishments of large language models (LLMs) have galvanized research initiatives across both industrial and academic spheres. These LLMs showcase their capacity to converse with humans in a natural and articulate manner, excelling across various tasks such as document summarization, Q&amp;A systems, conversational AI, and coding assistants. Despite their advancements, LLMs continue to struggle with tasks that require specialized knowledge or domain-specific expertise. <cite class="ltx_cite ltx_citemacro_cite">Kandpal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib29" title="">2023</a>)</cite>. Notably, they may produce “hallucinations” <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib64" title="">2023</a>)</cite> when confronted with out-of-scope queries or requests that necessitate up-to-date knowledge. To address these challenges, Retrieval-Augmented Generation (RAG) leverages external knowledge bases to retrieve relevant document snippets, utilizing semantic similarity metrics to identify the most pertinent information. By tapping into external knowledge sources, RAG successfully alleviates the issue of generating inaccurate content, thereby increasing the reliability of LLMs and paving the way for their widespread adoption in real-world applications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, RAG also has its challenges. One issue is that when retrieving relevant documents, the important information may be buried in a large amount of irrelevant text, leading to inefficient and poor responses. Another challenge is that current language models have a limited input length, which causes their performance to decline when processing lengthy documents, such as academic articles, research papers, or literary works. This constraint has fueled research into developing methods to increase the input length while maintaining the model’s accuracy and efficiency.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper aims to shed light on the latest advancements in contextual compression methods, with a focus on their application in retrieval-based systems. Our research involves a comprehensive review of methodologies, metrics, and benchmarks, which we systematically categorize into a novel taxonomy. Our taxonomy, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, presents a structured and comprehensive framework for categorizing and analyzing Contextual Compression techniques for LLMs. Our investigation involves a comprehensive analysis of established techniques, such as semantic compression, in-context auto-encoder compressors, and auto-compressors, among others. Furthermore, our research highlights the ongoing challenges in this field and provides a roadmap for future investigations. We emphasize the need for collective efforts to create a sustainable and environmentally responsible future for LLMs.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F1.1" style="width:433.6pt;height:273.2pt;vertical-align:-269.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.5pt,1.4pt) scale(0.601691617452058,0.601691617452058) ;"><span class="ltx_ERROR undefined" id="S1.F1.1.1">{forest}</span>
<p class="ltx_p" id="S1.F1.1.2">forked edges,
for tree=
grow=east,reversed=true,anchor=base west,parent anchor=east,
child anchor=west,base=left,font=,rectangle,
draw=hidden-draw,rounded corners,align=left,minimum width=4em,
edge+=darkgray, line width=1pt,s sep=3pt,inner xsep=2pt,
inner ysep=3pt,ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=1text width=6em,font=,
where level=2text width=9em,font=,
where level=3text width=6.6em,font=,
[Contextual Compression for Large Language Models, ver
[Semantic 
<br class="ltx_break"/>Compression
[
Context Distillation
[
Learning by distilling context <cite class="ltx_cite ltx_citemacro_cite">Snell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite>, Gisting <cite class="ltx_cite ltx_citemacro_cite">Mu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite>, leaf, text width=30em
]
]
[
Concept Distillation
[
Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib51" title="">2024</a>)</cite>, leaf, text width=30em
]
]
[
Prompting
[
Soft Prompts
[
The Power of Scale for PEPT <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, 
<br class="ltx_break"/>OptiPrompt <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib65" title="">2021</a>)</cite>, Recurrentgpt <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib67" title="">2023</a>)</cite>, 
<br class="ltx_break"/>P-Tuning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib36" title="">2022</a>)</cite>, leaf, text width=21.8em
]
]
[
Prompt Compression
[
Prompt compression and contrastive conditioning <cite class="ltx_cite ltx_citemacro_cite">Wingate et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib61" title="">2022</a>)</cite>, leaf, text width=21.8em
]
]
[
Task-Agnostic 
<br class="ltx_break"/>Prompt Compression
[
LLMLingua <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib28" title="">2023b</a>)</cite>, LongLLMLingua <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib27" title="">2023a</a>)</cite>, 
<br class="ltx_break"/>LLMLingua-2 <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite>, leaf, text width=21.8em
]
]
]
[
Efficient Attention 
<br class="ltx_break"/>Operations
[
Transformer-XL <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib14" title="">2019</a>)</cite>, Longformer <cite class="ltx_cite ltx_citemacro_cite">Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite>, FlashAttention <cite class="ltx_cite ltx_citemacro_cite">Dao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib15" title="">2022</a>)</cite>, 
<br class="ltx_break"/>LongLoRA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib12" title="">2023b</a>)</cite>, leaf, text width=30em
]
]
[
Extrapolation and 
<br class="ltx_break"/>Interpolation
[
Exploring length generalization in LLMs <cite class="ltx_cite ltx_citemacro_cite">Anil et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib1" title="">2022</a>)</cite>, Positional Interpolation(PI) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib10" title="">2023a</a>)</cite>, 
<br class="ltx_break"/>YaRN <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib44" title="">2023</a>)</cite>, leaf, text width=30em ]
]
[
Context Window 
<br class="ltx_break"/>Extension
[
Extending context window of LLMs via semantic compression <cite class="ltx_cite ltx_citemacro_cite">Fei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite>, leaf, text width=30em
]
]
]
[
Pre-Trained 
<br class="ltx_break"/>Language Models
[
AutoCompressors
[
Adapting LMs to compress contexts <cite class="ltx_cite ltx_citemacro_cite">Chevalier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
LongNET
[
LongNET: Scaling transformers to 1B tokens <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
In-Context Auto-Encoders
[
In-context autoencoder for context compression in a LLM <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite>, leaf, text width=30em
]
]
[
RECOMP
[
Retrieve-Compress-Prepend <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite>, leaf, text width=30em
]
]
]
[
Retrievers
[
LLMChainExtractor
[
LangChain’s Method <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
[
EmbeddingsFilter
[
LangChain’s Method <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
[
DocumentCompressorPipeline
[
LangChain’s Method <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite>, leaf, text width=30em
]
]
]
]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taxonomy of Contextual Compression Methods for Large Language Models.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic Compression</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Semantic compression is a technique that helps identify common patterns of thought in a specific context by generalizing terms. It uses a "domain frequency dictionary" to establish the context and disambiguate multiple possible meanings of words. This approach, based on semantic networks, offers improvements over existing natural language processing techniques.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Semantic compression reduces the number of terms in a text document by replacing less frequent terms with more general terms (their hypernyms) using a semantic network and term frequency data. This compression minimizes information loss and enables efficient processing, especially in tasks involving vector space models <cite class="ltx_cite ltx_citemacro_cite">Baeza-Yates et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib4" title="">1999</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Erk and Padó (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib18" title="">2008</a>)</cite>. It also helps address linguistic <cite class="ltx_cite ltx_citemacro_cite">Sinha and Mihalcea (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib52" title="">2007</a>)</cite> challenges like polysemy and synonymy <cite class="ltx_cite ltx_citemacro_cite">Krovetz and Croft (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib30" title="">1992</a>)</cite> by replacing multiple rare terms with a single, more general concept. By using statistical analysis and frequency dictionaries, semantic compression can handle polysemic concepts more effectively and with lower error rates than other techniques.
These efforts can be summarized into five approaches: <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">Context Distillation</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">Prompting</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">Efficient Attention Operations</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.4">Extrapolation and Interpolation</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.5">Context Window Extension</span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Context Distillation</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Recent studies have demonstrated that augmenting language models (LMs) with contextual information, such as task descriptions, illustrative examples, and explanatory notes <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib11" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Scheurer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib50" title="">2022</a>)</cite>, can substantially enhance their performance capabilities. This approach can even facilitate zero-shot learning <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib58" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Victor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib56" title="">2022</a>)</cite> and enable models to tackle complex tasks by generating sequential reasoning steps <cite class="ltx_cite ltx_citemacro_cite">Nye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib42" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib59" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib66" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">While LMs perform better with context tokens, this advantage disappears when the tokens are removed. Additionally, processing context tokens requires extra computation, which can be a drawback. The context tokens can also be very long, and it’s unclear how to handle them when they exceed the context window size. These limitations are similar to human cognitive limitations <cite class="ltx_cite ltx_citemacro_cite">Wason and Evans (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib57" title="">1974</a>)</cite>, such as struggling with complex tasks and having limited working memory <cite class="ltx_cite ltx_citemacro_cite">Baddeley (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib3" title="">1992</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">Humans overcome challenges through practice, which allows them to "distill" knowledge into habits and muscle memory. For example, learning to type a phone number becomes automatic with repetition, freeing up conscious reasoning for more complex tasks <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>procedural learning vs. declarative learning - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Procedural_knowledge" title="">https://en.wikipedia.org/wiki/Procedural_knowledge</a></span></span></span>. This process is essential for building skills and knowledge, enabling us to tackle increasingly intricate challenges.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">Researchers in NLP <cite class="ltx_cite ltx_citemacro_cite">Askell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib2" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Snell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite> are exploring techniques to fine-tune language models, such as context distillation and "Gisting". Context distillation involves generating "practice" questions, having the model reason step-by-step, and fine-tuning it to predict answers from simpler prompts. This helps the model internalize skills, like step-by-step addition (ref Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F2" title="Figure 2 ‣ 2.1.1 Context Distillation ‣ 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>). "Gisting" <cite class="ltx_cite ltx_citemacro_cite">Mu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite> compresses instructions into concise key-value attention prefixes, saving computational resources and generalizing well to new tasks. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F3" title="Figure 3 ‣ 2.1.1 Context Distillation ‣ 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>, the approach involves learning a gist model by incorporating gist tokens during instruction tuning, enabling the model to handle prompt compression and instruction following simultaneously.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="268" id="S2.F2.g1" src="extracted/5896160/figures/context_distill.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Internalization of step-by-step reasoning via context distillation <cite class="ltx_cite ltx_citemacro_cite">Snell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib54" title="">2022</a>)</cite></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S2.F3.g1" src="extracted/5896160/figures/gisting.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Gisting - Each vertical rectangle here represents a stack of Transformer activations <cite class="ltx_cite ltx_citemacro_cite">Mu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib41" title="">2024</a>)</cite>
</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Prompting</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p1.1.1">Soft Prompts -</span> As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F4" title="Figure 4 ‣ 2.1.2 Prompting ‣ 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a>, soft prompt tuning enables the adaptation of pre-trained Transformers without modifying their underlying parameters, as demonstrated in recent studies <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib65" title="">2021</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib36" title="">2022</a>)</cite>. It entails adding novel embeddings to the input sequence and fine-tuning only these new parameters while keeping the remainder of the model’s architecture frozen. This approach is categorized as a parameter-efficient fine-tuning method (PEFT) <cite class="ltx_cite ltx_citemacro_cite">Lialin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib33" title="">2023</a>)</cite>, and bears resemblance to prefix tuning, which prepends task-specific vectors to the attention states instead of the input sequence <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib32" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S2.F4.g1" src="extracted/5896160/figures/soft_prompting.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>From 11 billion for a tuned model to just 20,480 for a tuned prompt, a reduction of over 5 orders of magnitude <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.8"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.8.1">Prompt Compression -</span> In their work, <cite class="ltx_cite ltx_citemacro_cite">Wingate et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib61" title="">2022</a>)</cite> hypothesize using a soft prompt <math alttext="sp" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.1.m1.1"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mrow id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p2.1.m1.1.1.2" xref="S2.SS1.SSS2.p2.1.m1.1.1.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.1.m1.1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.1.m1.1.1.3" xref="S2.SS1.SSS2.p2.1.m1.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><apply id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1"><times id="S2.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.1"></times><ci id="S2.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.2">𝑠</ci><ci id="S2.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">sp</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.1.m1.1d">italic_s italic_p</annotation></semantics></math> to compress information from a context <math alttext="ctx" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.2.m2.1"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mrow id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p2.2.m2.1.1.2" xref="S2.SS1.SSS2.p2.2.m2.1.1.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.2.m2.1.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.2.m2.1.1.3" xref="S2.SS1.SSS2.p2.2.m2.1.1.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.2.m2.1.1.1a" xref="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.2.m2.1.1.4" xref="S2.SS1.SSS2.p2.2.m2.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><apply id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1"><times id="S2.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.1"></times><ci id="S2.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.2">𝑐</ci><ci id="S2.SS1.SSS2.p2.2.m2.1.1.3.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.3">𝑡</ci><ci id="S2.SS1.SSS2.p2.2.m2.1.1.4.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1.4">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">ctx</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.2.m2.1d">italic_c italic_t italic_x</annotation></semantics></math>. They use a pre-trained LM <math alttext="p_{\text{LM}}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.3.m3.1"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><msub id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml"><mi id="S2.SS1.SSS2.p2.3.m3.1.1.2" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.3.m3.1.1.3" xref="S2.SS1.SSS2.p2.3.m3.1.1.3a.cmml">LM</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><apply id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.2">𝑝</ci><ci id="S2.SS1.SSS2.p2.3.m3.1.1.3a.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.3"><mtext id="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.3.m3.1.1.3">LM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">p_{\text{LM}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.3.m3.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT</annotation></semantics></math> to generate continuations <math alttext="cty\sim p_{\text{LM}}(\cdot\mid ctx)" class="ltx_math_unparsed" display="inline" id="S2.SS1.SSS2.p2.4.m4.1"><semantics id="S2.SS1.SSS2.p2.4.m4.1a"><mrow id="S2.SS1.SSS2.p2.4.m4.1b"><mi id="S2.SS1.SSS2.p2.4.m4.1.1">c</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.2">t</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.3">y</mi><mo id="S2.SS1.SSS2.p2.4.m4.1.4">∼</mo><msub id="S2.SS1.SSS2.p2.4.m4.1.5"><mi id="S2.SS1.SSS2.p2.4.m4.1.5.2">p</mi><mtext id="S2.SS1.SSS2.p2.4.m4.1.5.3">LM</mtext></msub><mrow id="S2.SS1.SSS2.p2.4.m4.1.6"><mo id="S2.SS1.SSS2.p2.4.m4.1.6.1" stretchy="false">(</mo><mo id="S2.SS1.SSS2.p2.4.m4.1.6.2" lspace="0em" rspace="0em">⋅</mo><mo id="S2.SS1.SSS2.p2.4.m4.1.6.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S2.SS1.SSS2.p2.4.m4.1.6.4">c</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.6.5">t</mi><mi id="S2.SS1.SSS2.p2.4.m4.1.6.6">x</mi><mo id="S2.SS1.SSS2.p2.4.m4.1.6.7" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.4.m4.1c">cty\sim p_{\text{LM}}(\cdot\mid ctx)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.4.m4.1d">italic_c italic_t italic_y ∼ italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( ⋅ ∣ italic_c italic_t italic_x )</annotation></semantics></math> based on the context, and then calibrate the model’s outputs with the soft prompt <math alttext="sf" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.5.m5.1"><semantics id="S2.SS1.SSS2.p2.5.m5.1a"><mrow id="S2.SS1.SSS2.p2.5.m5.1.1" xref="S2.SS1.SSS2.p2.5.m5.1.1.cmml"><mi id="S2.SS1.SSS2.p2.5.m5.1.1.2" xref="S2.SS1.SSS2.p2.5.m5.1.1.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.5.m5.1.1.1" xref="S2.SS1.SSS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.5.m5.1.1.3" xref="S2.SS1.SSS2.p2.5.m5.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.5.m5.1b"><apply id="S2.SS1.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1"><times id="S2.SS1.SSS2.p2.5.m5.1.1.1.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.1"></times><ci id="S2.SS1.SSS2.p2.5.m5.1.1.2.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.2">𝑠</ci><ci id="S2.SS1.SSS2.p2.5.m5.1.1.3.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.5.m5.1c">sf</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.5.m5.1d">italic_s italic_f</annotation></semantics></math>, <math alttext="p_{\text{LM}}(cty\mid sf)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.6.m6.1"><semantics id="S2.SS1.SSS2.p2.6.m6.1a"><mrow id="S2.SS1.SSS2.p2.6.m6.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.cmml"><msub id="S2.SS1.SSS2.p2.6.m6.1.1.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.3.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.6.m6.1.1.3.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3a.cmml">LM</mtext></msub><mo id="S2.SS1.SSS2.p2.6.m6.1.1.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.2.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1a" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4.cmml">y</mi></mrow><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml">∣</mo><mrow id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2.cmml">s</mi><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3.cmml">f</mi></mrow></mrow><mo id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.6.m6.1b"><apply id="S2.SS1.SSS2.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1"><times id="S2.SS1.SSS2.p2.6.m6.1.1.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.2"></times><apply id="S2.SS1.SSS2.p2.6.m6.1.1.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.6.m6.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p2.6.m6.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.2">𝑝</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.3.3a.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3"><mtext id="S2.SS1.SSS2.p2.6.m6.1.1.3.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.6.m6.1.1.3.3">LM</mtext></ci></apply><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2"><times id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.2">𝑐</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.3">𝑡</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.4">𝑦</ci></apply><apply id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3"><times id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.2">𝑠</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.6.m6.1c">p_{\text{LM}}(cty\mid sf)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.6.m6.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_c italic_t italic_y ∣ italic_s italic_f )</annotation></semantics></math> to the outputs based on the context <math alttext="ctx" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.7.m7.1"><semantics id="S2.SS1.SSS2.p2.7.m7.1a"><mrow id="S2.SS1.SSS2.p2.7.m7.1.1" xref="S2.SS1.SSS2.p2.7.m7.1.1.cmml"><mi id="S2.SS1.SSS2.p2.7.m7.1.1.2" xref="S2.SS1.SSS2.p2.7.m7.1.1.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.7.m7.1.1.1" xref="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.7.m7.1.1.3" xref="S2.SS1.SSS2.p2.7.m7.1.1.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.7.m7.1.1.1a" xref="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.7.m7.1.1.4" xref="S2.SS1.SSS2.p2.7.m7.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.7.m7.1b"><apply id="S2.SS1.SSS2.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1"><times id="S2.SS1.SSS2.p2.7.m7.1.1.1.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.1"></times><ci id="S2.SS1.SSS2.p2.7.m7.1.1.2.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.2">𝑐</ci><ci id="S2.SS1.SSS2.p2.7.m7.1.1.3.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.3">𝑡</ci><ci id="S2.SS1.SSS2.p2.7.m7.1.1.4.cmml" xref="S2.SS1.SSS2.p2.7.m7.1.1.4">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.7.m7.1c">ctx</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.7.m7.1d">italic_c italic_t italic_x</annotation></semantics></math>, <math alttext="p_{\text{LM}}(cty\mid ctx)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.8.m8.1"><semantics id="S2.SS1.SSS2.p2.8.m8.1a"><mrow id="S2.SS1.SSS2.p2.8.m8.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.cmml"><msub id="S2.SS1.SSS2.p2.8.m8.1.1.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.3.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.2.cmml">p</mi><mtext id="S2.SS1.SSS2.p2.8.m8.1.1.3.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3a.cmml">LM</mtext></msub><mo id="S2.SS1.SSS2.p2.8.m8.1.1.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.2.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml"><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1a" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4.cmml">y</mi></mrow><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1.cmml">∣</mo><mrow id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2.cmml">c</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3.cmml">t</mi><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1a" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4.cmml">x</mi></mrow></mrow><mo id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.8.m8.1b"><apply id="S2.SS1.SSS2.p2.8.m8.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1"><times id="S2.SS1.SSS2.p2.8.m8.1.1.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.2"></times><apply id="S2.SS1.SSS2.p2.8.m8.1.1.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.8.m8.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS2.p2.8.m8.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.2">𝑝</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.3.3a.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3"><mtext id="S2.SS1.SSS2.p2.8.m8.1.1.3.3.cmml" mathsize="70%" xref="S2.SS1.SSS2.p2.8.m8.1.1.3.3">LM</mtext></ci></apply><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2"><times id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.1"></times><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.2">𝑐</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.3">𝑡</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.2.4">𝑦</ci></apply><apply id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3"><times id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.2">𝑐</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.3">𝑡</ci><ci id="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4.cmml" xref="S2.SS1.SSS2.p2.8.m8.1.1.1.1.1.3.4">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.8.m8.1c">p_{\text{LM}}(cty\mid ctx)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.8.m8.1d">italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_c italic_t italic_y ∣ italic_c italic_t italic_x )</annotation></semantics></math>. They find that soft prompts effectively preserve abstract knowledge and improve guided output. Nevertheless, this method necessitates distinct optimization for each novel context, lacking the ability to leverage knowledge across analogous contexts.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.8.2">Task-Agnostic Prompt Compression -</span> Current methods for compressing natural language prompts remove tokens or lexical units based on information entropy from a language model like LlaMa-7B. However, using information entropy as a compression metric has two limitations: 1) it only considers unidirectional context, which may miss important information, and 2) it doesn’t perfectly align with the goal of prompt compression.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1">To address these issues, <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite> propose a data distillation approach to compress prompts while retaining essential information. They introduce an extractive text compression dataset and frame prompt compression as a token classification problem (preserve or discard) (Refer to Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F5" title="Figure 5 ‣ 2.1.2 Prompting ‣ 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>). The key benefits are as follows:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.1">Comprehensive Information Capture:</span> By leveraging a Transformer encoder, the method captures essential details from the full bidirectional context.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.1">Reduced Latency:</span> Smaller models explicitly learn the compression objective, leading to lower latency.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i3.p1.1.1">Faithfulness:</span> The compressed prompt remains faithful to the original content.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S2.F5.g1" src="extracted/5896160/figures/llmlingua-2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of LLMLingua-2 <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib43" title="">2024</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Efficient Attention Operations</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The self-attention mechanism in LLMs leads to an inference cost that scales quadratically with sequence length, prompting the development of various methods to alleviate this complexity. For example:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i1.p1.1.1">Transformer-XL <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib14" title="">2019</a>)</cite></span> - employs a recurrent architecture that operates on segments, paired with a novel positional encoding technique.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i2.p1.1.1">Longformer <cite class="ltx_cite ltx_citemacro_cite">Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite></span> - introduces sparse attention, scaling linearly with sequence length.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.1">FlashAttention <cite class="ltx_cite ltx_citemacro_cite">Dao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib15" title="">2022</a>)</cite></span> - uses chunking and re-computation to avoid quadratic attention complexity.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS1.SSS3.p1.2">However, these methods can be expensive to train and struggle with out-of-distribution content lengths <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>. To address this, <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p1.2.1">LongLoRA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib12" title="">2023b</a>)</cite></span> provides a computationally efficient fine-tuning method with minimal resource requirements. For further insights, refer to the study by <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib25" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Extrapolation and Interpolation</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">In the field of NLP, researchers are investigating methods to extend the capabilities of existing language models, initially trained on brief texts, to process longer sequences during inference <cite class="ltx_cite ltx_citemacro_cite">Anil et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib1" title="">2022</a>)</cite>. One approach is to alter positional embeddings, which are typically designed for shorter contexts. The Rotary Position Embeddings (RoPE) from LLaMA is a key foundation for several studies in this area. For example:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i1.p1.1.1">Position Interpolation (PI) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib11" title="">2021</a>)</cite></span> applies a linear transformation to input positional indices.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.1">YaRN <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib44" title="">2023</a>)</cite></span> leverages neural tangent kernel-inspired mechanisms to scale up the context window to 64,000 and 128,000 tokens.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Context Window Extension</h4>
<div class="ltx_para" id="S2.SS1.SSS5.p1">
<p class="ltx_p" id="S2.SS1.SSS5.p1.1">Researchers <cite class="ltx_cite ltx_citemacro_cite">Fei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite> propose a semantic compression method that distills long texts into concise forms, retaining their meaning and broadening the context window (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F6" title="Figure 6 ‣ 2.1.5 Context Window Extension ‣ 2.1 Semantic Compression ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a>). This method occurs before inputting tokens into pre-trained language models and is customizable and optimized for specific tasks. It outperforms existing methods in various tasks, including question answering, summarization, and few-shot learning, without requiring additional parameter updates or memory consumption, making it computationally efficient.</p>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S2.F6.g1" src="extracted/5896160/figures/semantic_compression.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>1) clustering the input text into thematic groups, represented as a graph, to facilitate topic-based analysis, 2) tuning the thematic segments using pre-trained models to preserve crucial details, and 3) reassembling the refined chunks in their original order - reducing the text length by approximately 6-8 times. Additionally, other techniques like extrapolation and interpolation can be used to further extend the length <cite class="ltx_cite ltx_citemacro_cite">Fei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib20" title="">2023</a>)</cite></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pre-Trained Language Models (PLMs)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The development of PLMs has revolutionized the field of NLP. The first generation of PLMs, such as Skip-Gram <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib40" title="">2013b</a>)</cite>, word2vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib39" title="">2013a</a>)</cite>, and GloVe <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib45" title="">2014</a>)</cite>, used shallow neural networks <cite class="ltx_cite ltx_citemacro_cite">Qiu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib48" title="">2020</a>)</cite> to obtain word embeddings. The second generation, including CoVe <cite class="ltx_cite ltx_citemacro_cite">McCann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib38" title="">2017</a>)</cite>, ELMo <cite class="ltx_cite ltx_citemacro_cite">Peters et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib46" title="">2018</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib16" title="">2018</a>)</cite>, and GPT <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib49" title="">2018</a>)</cite>, focused on learning dynamic word embeddings using transformers. The pre-training and fine-tuning approach has achieved remarkable success in various NLP tasks. Moreover, recent breakthroughs in prompt learning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib35" title="">2023a</a>)</cite> have empowered PLMs to accomplish few-shot or zero-shot learning with minimal labeled data. Notable examples of successful PLMs include ChatGPT, GPT-4, Gemini, Claude, LlaMA-3, Mixtral, etc.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>AutoCompressors</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The authors of <cite class="ltx_cite ltx_citemacro_cite">Chevalier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite> propose teaching PLMs to compress text into summary vectors <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib31" title="">2021</a>)</cite>, which are significantly shorter than the original text (often 1-2 orders of magnitude shorter). These vectors have a two-pronged function: 1) they allow the LM to handle long documents by extending its context window with minimal computational overhead, and 2) they accelerate inference for pre-computed and cached text.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">AutoCompressors, proposed by <cite class="ltx_cite ltx_citemacro_cite">Chevalier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite>, are trained To distill key information into summary vectors, generated sequentially from extended documents (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F7" title="Figure 7 ‣ 2.2.1 AutoCompressors ‣ 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a>). The approach builds upon the Recurrent Memory Transformers (RMT) architecture <cite class="ltx_cite ltx_citemacro_cite">Bulatov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib7" title="">2022</a>)</cite>, introducing summary accumulation and training with randomly segmented inputs. This enhances long-range information retention and facilitates reasoning across multiple passages. AutoCompressors can be seeded with PLMs and fine-tuned on long sequences. They improve perplexity for long documents and demonstrate robust compression capabilities across different domains, making them valuable for various downstream applications.</p>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S2.F7.g1" src="extracted/5896160/figures/AutoCompressors.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>AutoCompressors recursively generate summary vectors from long documents, using them as soft prompts for subsequent segments <cite class="ltx_cite ltx_citemacro_cite">Chevalier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib13" title="">2023</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>LongNET</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Overcoming sequence length limitations in language models has several advantages, including improved interactions with human language, better capture of complex causality and reasoning, and reduced catastrophic forgetting. However, scaling up sequence length poses a challenge in balancing computational complexity and model expressivity. RNN-style models and state space models <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib24" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Smith et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib53" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Fu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib21" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Poli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib47" title="">2023</a>)</cite> have been proposed, but they have limitations from the perspective of parallelization and model adaptability <cite class="ltx_cite ltx_citemacro_cite">Fathi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib19" title="">2023</a>)</cite>. An alternative approach is to reduce the complexity of Transformers <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib55" title="">2017</a>)</cite>, such as using sliding windows or convolution modules for attention, or sparse attention. LongNet <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite>, a novel approach, replaces the attention mechanism with "dilated attention", which achieves linear computational complexity and logarithmic dependency between tokens. This allows LongNet to efficiently scale sequence lengths to 1 billion tokens, overcoming the constraints of computation and memory.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>In-Context Auto-Encoders</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Modeling long-range dependencies is a hurdle for Transformer-based LMs <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib55" title="">2017</a>)</cite> due to their self-attention mechanism. Previous research by <cite class="ltx_cite ltx_citemacro_cite">Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib5" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Bulatov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib7" title="">2022</a>)</cite>, and Ding <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib17" title="">2023</a>)</cite> has attempted to cope with this issue through architectural innovations, but these approaches often struggle to maintain performance in long contexts, as underscored by <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib34" title="">2024</a>)</cite>. A novel approach, "context compression", is proposed by <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite>, which recognizes that an LLM can represent the same information in varying lengths. They introduce the In-context Autoencoder (ICAE), which compresses lengthy contexts into a fixed number of memory buffers using a learnable encoder and a fixed decoder (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F8" title="Figure 8 ‣ 2.2.3 In-Context Auto-Encoders ‣ 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">8</span></a>). The ICAE is pre-trained using auto-encoding and language modeling objectives and fine-tuned using instruction data. The approach achieves 4x context compression while maintaining effective conditioning for the target LLM, enabling faster and more memory-efficient inference.</p>
</div>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S2.F8.g1" src="extracted/5896160/figures/ICAE.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Condensing an extended context into a compact memory representation, which can be leveraged by the target LLM to respond to diverse prompts. <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib23" title="">2023</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>RECOMP</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">In their work, <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite> introduce RECOMP, an intermediary step for Retrieval-augmented Language Models (RALMs) <cite class="ltx_cite ltx_citemacro_cite">Izacard et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib26" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Borgeaud et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib6" title="">2022</a>)</cite>. RECOMP compresses retrieved documents into concise textual summaries before integrating them during inference, reducing computational costs and alleviating the burden on LMs to process lengthy documents. The aim is to produce summaries that balance brevity and fidelity to the original evidence documents, guiding the RALM to produce targeted outputs when the summary is used as a prefix to the input (illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S2.F9" title="Figure 9 ‣ 2.2.4 RECOMP ‣ 2.2 Pre-Trained Language Models (PLMs) ‣ 2 Methods ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">9</span></a>). To achieve this, the authors train two types of compressors:</p>
<ol class="ltx_enumerate" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I4.i1.p1.1.1">Extractive Compressor:</span> This compressor filters out irrelevant sentences, retaining only the most pertinent ones from the retrieved document set.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I4.i2.p1.1.1">Abstractive Compressor:</span> This compressor produces a summary by fusing information from multiple retrieved documents.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS2.SSS4.p1.2">Both compressors employ a multi-document query-based summarization approach <cite class="ltx_cite ltx_citemacro_cite">Xu and Lapata (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib63" title="">2020</a>)</cite>, summarizing evidence documents concerning the input query. The authors develop training strategies that maximize performance on the target task to guarantee accurate output. Contrastive learning is employed to train the extractive compressor enabling it to select key sentences effectively, while the abstractive compressor is distilled <cite class="ltx_cite ltx_citemacro_cite">West et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib60" title="">2021</a>)</cite> from a large language model (like GPT-3 or GPT-4), achieving strong summarization performance. This approach holds promise for enhancing the efficiency and efficacy of RALMs.</p>
</div>
<figure class="ltx_figure" id="S2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S2.F9.g1" src="extracted/5896160/figures/recomp.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>RECOMP’s document compression technique generates a summary that serves as input to a language model, facilitating correct answer generation while minimizing encoding costs. <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib62" title="">2024</a>)</cite></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Retrievers</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The retriever <cite class="ltx_cite ltx_citemacro_cite">Chase (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib8" title="">2017-</a>)</cite> is an interface that processes an unstructured query and returns a curated list of documents in response. Contextual compression aims to address the challenges of retrieval by compressing the retrieved context to only include relevant information. In this context, "compressing" encompasses both condensing the content of individual documents and eliminating irrelevant documents altogether. The Contextual Compression Retriever uses a <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">base retriever</span> and a <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">Document Compressor</span> to process queries. The base retriever retrieves the initial documents, which are then passed through the Document Compressor to shorten the list of documents by either reducing the contents of individual documents or excluding entire documents altogether.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>LLMChainExtractor</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">In this approach, the base retriever is wrapped with a <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.1">ContextualCompressionRetriever</span>. Additionally, an <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.2">LLMChainExtractor</span> serves as the base compressor. The <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.3">LLMChainExtractor</span> iterates over the initially retrieved documents and extracts only the relevant content for the given query. It achieves this by making an additional LLM call for each retrieved document and summarizing the relevant information</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>EmbeddingsFilter</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Making an additional LLM call for each retrieved document can be both costly and slow. However, the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">EmbeddingsFilter</span> offers a more economical and faster alternative. By embedding both the documents and the query, it selectively returns only those documents that exhibit sufficiently similar embeddings to the query. This approach optimizes retrieval efficiency while maintaining relevance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>DocumentCompressorPipeline</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">The DocumentCompressorPipeline allows a seamless combination of multiple compressors in a sequence. Alongside these compressors, we can incorporate <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.1">BaseDocumentTransformers</span> into our pipeline. Unlike contextual compressors, these transformers don’t alter the content significantly but perform specific transformations on a set of documents. For instance, <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.2">TextSplitters</span> can divide documents into smaller segments, while the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.3">EmbeddingsRedundantFilter</span> identifies and filters out redundant documents based on embedding similarity. This modular approach enhances flexibility and adaptability in document processing. e.g.</p>
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i1.p1.1.1">Splitter:</span> create small chunks</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i2.p1.1.1">Redundant filter:</span> remove similar docs — embedded</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i3.p1.1.1">Relevant filter:</span> relevant to query</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Metrics and Benchmarks</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Metrics</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Evaluating language model inference efficiency involves considering various metrics that capture different performance aspects, including accuracy, zero-shot capabilities, compression ratio, and inference time. Within the framework of RAG-based solutions, the "Triad of Metrics" <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>RAG Triad (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#S3.F10" title="Figure 10 ‣ 3.1 Metrics ‣ 3 Metrics and Benchmarks ‣ Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">10</span></a>): <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/" title="">https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/</a></span></span></span> - Groundedness, Context Relevance, and Answer Relevance - are also employed for evaluation. Achieving satisfactory performance across these metrics helps ensure that the language model application is reliable and free from hallucinations.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S3.F10.g1" src="extracted/5896160/figures/rag_triad.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>RAG-Triad</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Compression Ratio</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The compression ratio measures the reduction in size from the original uncompressed context to the compressed context. A higher compression ratio means that the compression is more efficient, as it achieves a greater reduction in size while preserving the context’s coherence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Inference Time</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Inference time, also known as latency, measures how long it takes for a Large Language Model (LLM) to process input data and generate responses. This metric is crucial for real-world applications that require quick handling of user queries or processing of large data volumes in real-time.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Context Relevance</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">In RAG applications, the first step is retrieval, and it’s crucial to ensure that the retrieved context chunks are relevant to the input query. Irrelevant information in the context can lead to hallucinations in the LLM’s answer. To evaluate context relevance, the structure of the serialized record can be analyzed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Groundedness</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">After retrieving the context, an LLM transforms it into an answer. However, LLMs can sometimes stray from the facts and generate responses that are not entirely accurate. To ensure the groundedness of the application, the response can be broken down into individual claims and verified by searching for supporting evidence within the retrieved context.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>Answer Relevance</h4>
<div class="ltx_para" id="S3.SS1.SSS5.p1">
<p class="ltx_p" id="S3.SS1.SSS5.p1.1">Furthermore, our response must still effectively address the original question. We can assess this by evaluating the relevance of the final response to the user’s input.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.6 </span>Others</h4>
<div class="ltx_para" id="S3.SS1.SSS6.p1">
<p class="ltx_p" id="S3.SS1.SSS6.p1.1">RAG evaluation also encompasses four key abilities that reflect the model’s adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib9" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib37" title="">2023b</a>)</cite>. The model’s quality scores are heavily influenced by its ability to leverage these capabilities in diverse challenges and complex scenarios:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Noise Robustness:</span> This metric gauges a model’s capacity to distinguish between relevant and irrelevant documents, even when the latter are tangentially related to the question.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Negative Rejection:</span> The metric measures a model’s capacity to recognize when the retrieved documents are insufficient to answer a question, and to withhold a response accordingly.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">Information Integration:</span> Information integration tests a model’s proficiency in combining relevant information from multiple documents to provide well-informed answers to challenging questions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i4.p1.1.1">Counterfactual Robustness:</span> Counterfactual robustness measures a model’s skill in identifying and ignoring flawed or misleading information in documents, regardless of its awareness of potential errors.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.SSS6.p1.2">In brief, context relevance and noise robustness are crucial for evaluating the retrieval process, while answer groundedness, answer relevance, negative rejection, information integration, and counterfactual robustness are vital for assessing the quality of generated text.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Benchmarks and Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The primary objective of these benchmarks and datasets is to assess the trade-offs between compressed and uncompressed contexts in terms of effectiveness, efficiency, and accuracy, covering a broad range of NLP tasks and applications.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Common Benchmarks and Datasets</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">RAG’s primary function revolves around answering questions, encompassing various formats such as single-hop and multi-hop queries, multiple-choice options, and domain-specific inquiries, as well as lengthy scenarios that leverage RAG’s capabilities. Moreover, RAG is constantly evolving to tackle additional tasks, including extracting relevant information, generating conversational dialogue, and searching for code snippets, documentations and even interpreting them. For more details, refer to the study by <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13385v2#bib.bib22" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Future Directions</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>More advanced Methods</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Research on contextual compression for LLMs is still in its early stages. While previous studies have shown compressed contexts, they still lag behind uncompressed contexts in terms of performance. By exploring more advanced compression methods tailored for LLMs, we can potentially bridge this performance gap and enhance the performance of uncompressed contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance-Size Trade-offs</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Previous research highlights the importance of balancing LLM performance with context size, considering hardware limitations and practical constraints. Despite its significance, the theoretical and empirical foundations of this trade-off remain poorly understood. Future investigations should focus on conducting exhaustive examinations to drive the creation of sophisticated compression techniques that can meet the demands of increasingly complex data sets, enabling researchers to create tailored methods that effectively navigate the design space and optimize performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Dynamic Contextual Compression</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Contemporary compression approaches still utilize manual compressors, such as retrievers, which often require an empirical methodology driven by input data or task specifications. This can be a practical hindrance to adoption, especially in scenarios like context distillation, where finding suitable student templates within computational constraints can be time-consuming and require multiple trials.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Explainability</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Compressing pre-trained language models can make them hard to understand (lacking explainability). To fix this, using explainable compression methods can help make models more interpretable, easier to evaluate, and more reliable in real-life scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This in-depth analysis explores the domain of contextual compression techniques, with a focus on their application to LLMs. Our study encompasses a broad range of compression methods, evaluation metrics, and benchmark datasets, providing a comprehensive understanding of the field. By examining the complexities of contextual compression, we identify the key challenges and opportunities that arise in this area. As research in this field continues to advance, the development of specialized methodologies tailored to the needs of LLMs is crucial for unlocking their full potential across various domains. This survey aims to serve as a valuable resource, providing a detailed overview of the current landscape and encouraging further investigation into this vital topic.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While this survey provides a comprehensive overview of contextual compression techniques for large language models, there are several limitations to acknowledge. Firstly, the field of contextual compression is rapidly evolving, and this survey may not capture the very latest advancements in the area. Additionally, the focus on large language models may not be representative of other types of language models or AI systems, which may have different compression requirements. Furthermore, the survey’s reliance on existing evaluation metrics and benchmark datasets may not fully capture the complexities and nuances of contextual compression. Moreover, the need for advanced methodologies specifically designed for LLMs highlights the potential limitations of current approaches, which may not be scalable or effective for future LLM architectures. Finally, the survey’s scope is limited to contextual compression, and future research may uncover new challenges and opportunities at the intersection of compression and other aspects of LLMs.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">As research in contextual compression for large language models continues to advance, it is essential to consider the ethical implications of these developments. One key concern is the potential for biased or unfair compression methods, which could perpetuate existing social inequalities or create new ones. For instance, compression techniques that prioritize certain types of data or language styles over others may disadvantage certain groups or communities. Furthermore, the focus on large language models may exacerbate existing power imbalances, where only those with access to significant computational resources and data can develop and deploy these models.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Additionally, the reliance on existing evaluation metrics and benchmark datasets may perpetuate biases and limitations in the development of compression techniques. It is crucial to ensure that these metrics and datasets are diverse, representative, and regularly updated to reflect the complexities of real-world language use.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">The need for advanced methodologies specifically designed for LLMs also raises ethical concerns around the responsible development and deployment of these models. As LLMs become increasingly ubiquitous, it is essential to consider their potential impact on individuals, communities, and society as a whole. This includes ensuring that these models are transparent, explainable, and accountable, and that their development and deployment are guided by ethical principles and values.</p>
</div>
<div class="ltx_para" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1">Finally, the survey’s limited scope to contextual compression highlights the need for a more comprehensive consideration of the ethical implications of LLMs and their applications. Future research should prioritize ethical considerations and ensure that the development of compression techniques and LLMs is guided by a commitment to social responsibility, fairness, and transparency.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al. (2022)</span>
<span class="ltx_bibblock">
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.

</span>
<span class="ltx_bibblock">Exploring length generalization in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>, 35:38546–38556.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et al. (2021)</span>
<span class="ltx_bibblock">
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2112.00861</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baddeley (1992)</span>
<span class="ltx_bibblock">
Alan Baddeley. 1992.

</span>
<span class="ltx_bibblock">Working memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Science</em>, 255(5044):556–559.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baeza-Yates et al. (1999)</span>
<span class="ltx_bibblock">
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Modern information retrieval</em>, volume 463.

</span>
<span class="ltx_bibblock">ACM press New York.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2004.05150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International conference on machine learning</em>, pages 2206–2240. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulatov et al. (2022)</span>
<span class="ltx_bibblock">
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.

</span>
<span class="ltx_bibblock">Recurrent memory transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 35:11079–11091.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase (2017-)</span>
<span class="ltx_bibblock">
Harrison Chase. 2017-.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/langchain-ai/langchain" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1.1">LangChain</em></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 17754–17762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2306.15595</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2021.

</span>
<span class="ltx_bibblock">Meta-learning via language model in-context tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2110.07814</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b.

</span>
<span class="ltx_bibblock">Longlora: Efficient fine-tuning of long-context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2309.12307</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chevalier et al. (2023)</span>
<span class="ltx_bibblock">
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023.

</span>
<span class="ltx_bibblock">Adapting language models to compress contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2305.14788</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.

</span>
<span class="ltx_bibblock">Transformer-xl: Attentive language models beyond a fixed-length context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1901.02860</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, 35:16344–16359.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2023)</span>
<span class="ltx_bibblock">
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Longnet: Scaling transformers to 1,000,000,000 tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2307.02486</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erk and Padó (2008)</span>
<span class="ltx_bibblock">
Katrin Erk and Sebastian Padó. 2008.

</span>
<span class="ltx_bibblock">A structured vector space model for word meaning in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2008 conference on empirical methods in natural language processing</em>, pages 897–906.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fathi et al. (2023)</span>
<span class="ltx_bibblock">
Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. 2023.

</span>
<span class="ltx_bibblock">Block-state transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2306.09539</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2023)</span>
<span class="ltx_bibblock">
Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, and Wei Han. 2023.

</span>
<span class="ltx_bibblock">Extending context window of large language models via semantic compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2312.09571</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2022)</span>
<span class="ltx_bibblock">
Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. 2022.

</span>
<span class="ltx_bibblock">Hungry hungry hippos: Towards language modeling with state space models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2212.14052</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2023)</span>
<span class="ltx_bibblock">
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">In-context autoencoder for context compression in a large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2307.06945</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2021)</span>
<span class="ltx_bibblock">
Albert Gu, Karan Goel, and Christopher Ré. 2021.

</span>
<span class="ltx_bibblock">Efficiently modeling long sequences with structured state spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2111.00396</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.

</span>
<span class="ltx_bibblock">Advancing transformer architecture in long-context large language models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2311.12351</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2208.03299</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.06839" title="">LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ArXiv preprint</em>, abs/2310.06839.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.825" title="">LLMLingua: Compressing prompts for accelerated inference of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 13358–13376. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et al. (2023)</span>
<span class="ltx_bibblock">
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock">Large language models struggle to learn long-tail knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">International Conference on Machine Learning</em>, pages 15696–15707. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krovetz and Croft (1992)</span>
<span class="ltx_bibblock">
Robert Krovetz and W Bruce Croft. 1992.

</span>
<span class="ltx_bibblock">Lexical ambiguity and information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ACM Transactions on Information Systems (TOIS)</em>, 10(2):115–141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2101.00190</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lialin et al. (2023)</span>
<span class="ltx_bibblock">
Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023.

</span>
<span class="ltx_bibblock">Scaling down to scale up: A guide to parameter-efficient fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2303.15647</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Transactions of the Association for Computational Linguistics</em>, 12:157–173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ACM Computing Surveys</em>, 55(9):1–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-short.8" title="">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 61–68, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023b.

</span>
<span class="ltx_bibblock">Recall: A benchmark for llms robustness against external counterfactual knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2311.08147</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCann et al. (2017)</span>
<span class="ltx_bibblock">
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Learned in translation: Contextualized word vectors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013a)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1301.3781</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013b)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their compositionality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in neural information processing systems</em>, 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al. (2024)</span>
<span class="ltx_bibblock">
Jesse Mu, Xiang Li, and Noah Goodman. 2024.

</span>
<span class="ltx_bibblock">Learning to compress prompts with gist tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nye et al. (2021)</span>
<span class="ltx_bibblock">
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021.

</span>
<span class="ltx_bibblock">Show your work: Scratchpads for intermediate computation with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2112.00114</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2024)</span>
<span class="ltx_bibblock">
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.12968" title="">LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ArXiv preprint</em>, abs/2403.12968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.00071</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pages 1532–1543.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al. (2018)</span>
<span class="ltx_bibblock">
Matthew E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018.

</span>
<span class="ltx_bibblock">Dissecting contextual word embeddings: Architecture and representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:1808.08949</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poli et al. (2023)</span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International Conference on Machine Learning</em>, pages 28043–28078. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2020)</span>
<span class="ltx_bibblock">
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.

</span>
<span class="ltx_bibblock">Pre-trained models for natural language processing: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Science China Technological Sciences</em>, 63(10):1872–1897.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">OpenAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheurer et al. (2022)</span>
<span class="ltx_bibblock">
Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022.

</span>
<span class="ltx_bibblock">Learning from natural language feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACL Workshop on Learning with Natural Language Supervision</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
Kaize Shi, Xueyao Sun, Qing Li, and Guandong Xu. 2024.

</span>
<span class="ltx_bibblock">Compressing long context for enhancing rag with amr-based concept distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2405.03085</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha and Mihalcea (2007)</span>
<span class="ltx_bibblock">
Ravi Sinha and Rada Mihalcea. 2007.

</span>
<span class="ltx_bibblock">Unsupervised graph-basedword sense disambiguation using measures of word semantic similarity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">International conference on semantic computing (ICSC 2007)</em>, pages 363–369. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. (2022)</span>
<span class="ltx_bibblock">
Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. 2022.

</span>
<span class="ltx_bibblock">Simplified state space layers for sequence modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2208.04933</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et al. (2022)</span>
<span class="ltx_bibblock">
Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022.

</span>
<span class="ltx_bibblock">Learning by distilling context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2209.15189</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Victor et al. (2022)</span>
<span class="ltx_bibblock">
Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et al. 2022.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wason and Evans (1974)</span>
<span class="ltx_bibblock">
Peter C Wason and J St BT Evans. 1974.

</span>
<span class="ltx_bibblock">Dual processes in reasoning?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Cognition</em>, 3(2):141–154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in neural information processing systems</em>, 35:24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">West et al. (2021)</span>
<span class="ltx_bibblock">
Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">Symbolic knowledge distillation: from general language models to commonsense models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2110.07178</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wingate et al. (2022)</span>
<span class="ltx_bibblock">
David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022.

</span>
<span class="ltx_bibblock">Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2210.03162</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=mlJLVigNHp" title="">RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Lapata (2020)</span>
<span class="ltx_bibblock">
Yumo Xu and Mirella Lapata. 2020.

</span>
<span class="ltx_bibblock">Coarse-to-fine query focused multi-document summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 2020 Conference on empirical methods in natural language processing (EMNLP)</em>, pages 3632–3645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: a survey on hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2309.01219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2021)</span>
<span class="ltx_bibblock">
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.398" title="">Factual probing is [MASK]: Learning vs. learning to recall</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5017–5033, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022.

</span>
<span class="ltx_bibblock">Least-to-most prompting enables complex reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2205.10625</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock">Recurrentgpt: Interactive generation of (arbitrarily) long text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2305.13304</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 14:30:45 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
