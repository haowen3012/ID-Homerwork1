<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards</title>
<!--Generated on Wed Aug 21 17:00:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
6G networks,  AGI,  LLM,  LoRA,  RAG,  retrieval
" lang="en" name="keywords"/>
<base href="/html/2408.11775v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S1" title="In Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S2" title="In Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3" title="In Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS1" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">General Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS2" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Semantic Chunking Strategy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS3" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Embedding Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS4" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Retrieval with Re-ranking</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS5" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Extending the Context Window with SelfExtend</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS6" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">The Generator: Fine-tuned Phi-2 with Multiple Contexts</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.SS7" title="In III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span> </span><span class="ltx_text ltx_font_italic">Prompt Engineering</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4" title="In Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4.SS1" title="In IV Experimental results ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Settings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4.SS2" title="In IV Experimental results ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Results and Analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S5" title="In Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusions And Future Works</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<section class="ltx_glossary ltx_acronym ltx_list_acronym" id="id18">
<dl class="ltx_glossarylist" id="id18.18">
<dt class="ltx_glossaryentry" id="id1.1.id1">LM</dt>
<dd>language model</dd>
<dt class="ltx_glossaryentry" id="id2.2.id2">LLM</dt>
<dd>large language model</dd>
<dt class="ltx_glossaryentry" id="id3.3.id3">SLM</dt>
<dd>small language model</dd>
<dt class="ltx_glossaryentry" id="id4.4.id4">RAG</dt>
<dd>retrieval-augmented generation</dd>
<dt class="ltx_glossaryentry" id="id5.5.id5">LM</dt>
<dd>language model</dd>
<dt class="ltx_glossaryentry" id="id6.6.id6">LoRA</dt>
<dd>Low-Rank Adaptation</dd>
<dt class="ltx_glossaryentry" id="id7.7.id7">QLoRA</dt>
<dd>Quantized Low-Rank Adaptation</dd>
<dt class="ltx_glossaryentry" id="id8.8.id8">SBERT</dt>
<dd>Sentence BERT</dd>
<dt class="ltx_glossaryentry" id="id9.9.id9">ML</dt>
<dd>machine learning</dd>
<dt class="ltx_glossaryentry" id="id10.10.id10">TF-IDF</dt>
<dd>term frequency and inverse document frequency</dd>
<dt class="ltx_glossaryentry" id="id11.11.id11">OOD</dt>
<dd>out-of-distribution</dd>
<dt class="ltx_glossaryentry" id="id12.12.id12">QnA</dt>
<dd>questions and answering</dd>
<dt class="ltx_glossaryentry" id="id13.13.id13">AI</dt>
<dd>artificial intelligence</dd>
<dt class="ltx_glossaryentry" id="id14.14.id14">TeleQuAD</dt>
<dd>telecom question answering dataset</dd>
<dt class="ltx_glossaryentry" id="id15.15.id15">3GPP</dt>
<dd>Third Generation Partnership Project</dd>
<dt class="ltx_glossaryentry" id="id16.16.id16">O-RAN</dt>
<dd>open radio access networks</dd>
<dt class="ltx_glossaryentry" id="id17.17.id17">MCQ</dt>
<dd>multiple choice question</dd>
<dt class="ltx_glossaryentry" id="id18.18.id18">RoPE</dt>
<dd>rotary position embedding</dd>
</dl>
</section>
<h1 class="ltx_title ltx_title_document">Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: 
<br class="ltx_break"/>For 3GPP Standards</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Omar Erak<sup class="ltx_sup" id="id29.11.id1"><span class="ltx_text ltx_font_italic" id="id29.11.id1.1">1</span></sup>, Nouf Alabbasi<sup class="ltx_sup" id="id30.12.id2"><span class="ltx_text ltx_font_italic" id="id30.12.id2.1">1</span></sup>, Omar Alhussein<sup class="ltx_sup" id="id31.13.id3"><span class="ltx_text ltx_font_italic" id="id31.13.id3.1">1</span></sup>, Ismail Lotfi<sup class="ltx_sup" id="id32.14.id4"><span class="ltx_text ltx_font_italic" id="id32.14.id4.1">1</span></sup>, Amr Hussein<sup class="ltx_sup" id="id33.15.id5"><span class="ltx_text ltx_font_italic" id="id33.15.id5.1">1</span></sup>, 
<br class="ltx_break"/>Sami Muhaidat<sup class="ltx_sup" id="id34.16.id6"><span class="ltx_text ltx_font_italic" id="id34.16.id6.1">2,3</span></sup>, Merouane Debbah<sup class="ltx_sup" id="id35.17.id7"><span class="ltx_text ltx_font_italic" id="id35.17.id7.1">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup" id="id36.18.id1"><span class="ltx_text ltx_font_italic" id="id36.18.id1.1">1</span></sup>KU 6G Research Centre, Department of Computer Science, Khalifa University, Abu Dhabi, UAE
<br class="ltx_break"/><sup class="ltx_sup" id="id37.19.id2"><span class="ltx_text ltx_font_italic" id="id37.19.id2.1">2</span></sup>KU 6G Research Centre, Department of Computer and Information Engineering, Khalifa University, Abu Dhabi, UAE
<br class="ltx_break"/><sup class="ltx_sup" id="id38.20.id3"><span class="ltx_text ltx_font_italic" id="id38.20.id3.1">3</span></sup>Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada
<br class="ltx_break"/>Emails: omarerak@ieee.org, 100064507@ku.ac.ae, omar.alhussein@ku.ac.ae, ismail.lotfi@ku.ac.ae, 
<br class="ltx_break"/>100059484@ku.ac.ae, muhaidat@ieee.org, merouane.debbah@ku.ac.ae
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id39.id1">Recent studies show that large language models (LLMs) struggle with technical standards in telecommunications. We propose a fine-tuned retrieval-augmented generation (RAG) system based on the Phi-2 small language model (SLM) to serve as an oracle for communication networks.
Our developed system leverages forward-looking semantic chunking to adaptively determine parsing breakpoints based on embedding similarity, enabling effective processing of diverse document formats. To handle the challenge of multiple similar contexts in technical standards, we employ a re-ranking algorithm to prioritize the most relevant retrieved chunks.
Recognizing the limitations of Phi-2’s small context window,
we implement a recent technique, namely SelfExtend, to expand the context window during inference, which not only boosts the performance but also can accommodate a wider range of user queries and design requirements from customers to specialized technicians.
For fine-tuning, we utilize the low-rank adaptation (LoRA) technique to enhance computational efficiency during training and enable effective fine-tuning on small datasets. Our comprehensive experiments demonstrate substantial improvements over existing question-answering approaches in the telecom domain, achieving performance that exceeds larger language models such as GPT-4 (which is about 880 times larger in size).
This work presents a novel approach to leveraging SLMs for communication networks, offering a balance of efficiency and performance. This work can serve as a foundation towards agentic language models for networks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
6G networks, AGI, LLM, LoRA, RAG, retrieval

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">LLMs have demonstrated impressive capabilities, from basic automation to complex decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib1" title="">1</a>]</cite>. They have proven their efficiency in a variety of tasks, including  <a href="https://arxiv.org/html/2408.11775v1#id12.12.id12"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id12.12.id12" title="questions and answering"><span class="ltx_text ltx_glossary_long">questions and answering</span></span></a> (<a href="https://arxiv.org/html/2408.11775v1#id12.12.id12"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id12.12.id12" title="questions and answering"><span class="ltx_text ltx_glossary_short">QnA</span></abbr></a>), code generation, and other problems.
Their ability to process natural language and generate human-like responses makes them powerful tools in many fields, including telecommunications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As telecom networks grow more complex and data-driven, <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_long-plural">large language models</span></span></a> offer significant potential to enhance automation, optimize network management, and improve customer experiences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib4" title="">4</a>]</cite>. However, to fully leverage agentic LLM-based models in telecommunications, we need to develop models that deeply understand the nuances of telecom systems and possess comprehensive knowledge of telecom models. Building such specialized models is crucial for adapting <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> to agentic roles where they can autonomously handle involved tasks, such as dynamic network optimization and predictive maintenance.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">LLMs can be adapted to various tasks through fine-tuning or  <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_long">retrieval-augmented generation</span></span></a> (<a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a>).
Fine-tuning enhances the performance of <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> by adjusting the model’s internal knowledge through iterative training on specialized datasets. However, the telecom industry is rapidly evolving, rendering fine-tuning an expensive and inefficient approach that cannot easily keep up with such a fast-paced industry. Furthermore, once a model is fine-tuned, editing or forgetting a specific piece of information becomes challenging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<span class="ltx_ERROR undefined" id="S1.p4.1">\Ac</span>
<p class="ltx_p" id="S1.p4.2">RAG, on the other hand, augments text generation with information retrieval, enabling models to produce more accurate and contextually aware responses. This approach allows flexible model adaptation and rapid integration of new information. <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> also grounds the model’s response in the relevant retrieved context, reducing (yet not entirely eliminating) the risk of hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib6" title="">6</a>]</cite>. Telecom applications can benefit from real-time data to produce more accurate and up-to-date responses.
Therefore, we believe LLM-based RAG systems can better enable emerging applications, such as dynamic network management, customer support, and predictive maintenance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Integrating <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> into telecommunication systems involves deploying <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr></a> frameworks on user equipment and edge devices, a process which presents a significant challenge due to the involved computational intensity of <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a>, both in terms of training and inference costs.
This challenge highlights the appeal of SLMs. These <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_long-plural">language models</span></span></a> offer computational and storage efficiency while maintaining adequate performance, suggesting their suitability for deployment on edge devices and possible enablement of on-device artificial intelligence (AI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib7" title="">7</a>]</cite>.
Several <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_long-plural">small language models</span></span></a> have been proposed in the literature, including Microsoft’s Phi-2 with 2.7B parameters<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib8" title="">8</a>]</cite> and <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.1">Gemini Nano 2</span> with 3.2B parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib9" title="">9</a>]</cite>. The Phi-2 model is currently considered as the state-of-the-art <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short">SLM</span></abbr></a> as it is able to match or outperform models up to 25x larger such as <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.2">Llama-2-70b</span> which has 70B parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Recent studies indicate that while state-of-the-art <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> perform well on general telecommunications queries, they struggle with questions related to technical standards in the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib12" title="">12</a>]</cite>. We believe this is mainly due to the fact that standard-type knowledge and system specifications do not exist in common research papers and other publications, which serve as the main learning sources for <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a>.
For instance, with respect to telecom systems, the polysemy of abbreviations (e.g., SAP: service access sample vs. system application and protocol) can hinder the model from inferring a correct answer. Additionally, the <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_short-plural">LMs</span></abbr></a> training on generalized knowledge can interfere with its performance in specialized domain tasks. For instance, certain protocols and methods in telecom-specific domains do not necessarily follow the generally followed-upon practices in broader contexts. A model trained on generalized knowledge may not adequately capture these domain-specific nuances and practices.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We propose a carefully developed Phi-2 based fine-tuned RAG system to serve as an oracle for communication networks. To the best of our knowledge, this is the first work to present a fine-tuned RAG system for communication networks. Previous works focus on presenting a frozen RAG framework or fine-tuning an <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_short">LM</span></abbr></a>. Our RAG system leverages a forward-looking semantic chunking (or parsing) strategy that adaptively determines breakpoints between sentences based on embedding similarity. This approach enables the system to effectively process documents with diverse formatting. In the  <a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_long">Third Generation Partnership Project</span></span></a> (<a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_short">3GPP</span></abbr></a>) documents, a query can often relate to multiple similar contexts, as discussions and paragraphs on related topics may appear in various sections or be phrased similarly. Therefore, we utilize a re-ranking algorithm to further rank the retrieved chunks based on their relevance to the input query.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Since Phi-2 is an <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short">SLM</span></abbr></a>, its performance is limited by its small context window, rendering it inefficient for certain tasks such as responding to open-ended or under-specified queries. This limitation is particularly relevant in telecom applications, where users can range from customers to specialized technicians. Therefore, we utilize a new technique, namely SelfExtend <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib13" title="">13</a>]</cite>, to significantly extend the context window during inference.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Finally, we use  <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_long">Low-Rank Adaptation</span></span></a> (<a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a>) not only to enhance computational efficiency during training but also because it allows users to effectively fine-tune on small datasets. Our in-depth experiments demonstrate considerable improvements over existing QnA approaches in telecom, contributing to the ongoing advancement of the field.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">The remainder of the paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S2" title="II Related Works ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">II</span></a> provides an overview of related works. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3" title="III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">III</span></a> discusses the proposed fine-tuned Phi-2 RAG system. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4" title="IV Experimental results ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">IV</span></a> provides our experimental results, and Section <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S5" title="V Conclusions And Future Works ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">V</span></a> concludes the paper and discusses some insightful future research directions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Several benchmarking datasets have been proposed to enable <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr></a>-based systems in the telecommunication domain, with mainly three different tasks: text classification, text summarization, and <a href="https://arxiv.org/html/2408.11775v1#id17.17.id17"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id17.17.id17" title="multiple choice question"><span class="ltx_text ltx_glossary_long-plural">multiple choice questions</span></span></a>.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib14" title="">14</a>]</cite>, the SPEC5G dataset was introduced with the objective of performing text classification and text summarization in telecom domain. Ericsson’s team introduced TeleQuAD, a private 4,000 entry <a href="https://arxiv.org/html/2408.11775v1#id12.12.id12"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id12.12.id12" title="questions and answering"><span class="ltx_text ltx_glossary_short">QnA</span></abbr></a> dataset, and developed a proprietary TeleRoBERTa, a 124M bidirectional encoder representation from transformers (BERT)-based RAG system<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib11" title="">11</a>]</cite>, the introduction of the TeleQnA dataset marks a significant advancement in evaluating <a href="https://arxiv.org/html/2408.11775v1#id12.12.id12"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id12.12.id12" title="questions and answering"><span class="ltx_text ltx_glossary_short">QnA</span></abbr></a> tasks for telecommunications.
The TeleQnA dataset contains 10,000 <a href="https://arxiv.org/html/2408.11775v1#id17.17.id17"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id17.17.id17" title="multiple choice question"><span class="ltx_text ltx_glossary_short-plural">MCQs</span></abbr></a> about telecommunication systems, curated from the various sources such as 3GPP and research papers. The dataset is verified by telecom human-in-the-loop experts.
Another dataset, namely TSpec-LLM, is recently released <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib16" title="">16</a>]</cite>. The authors develop an automated framework to generate QnA pairs from 3GPP specifications, then test a naive RAG architecture to assess the quality of their dataset.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib17" title="">17</a>]</cite>, Gajjar et al. introduce ORAN-Bench-13K, a dataset dedicated for the evaluation of  <a href="https://arxiv.org/html/2408.11775v1#id16.16.id16"><span class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id16.16.id16" title="open radio access networks"><span class="ltx_text ltx_glossary_long">open radio access networks</span></span></a> (<a href="https://arxiv.org/html/2408.11775v1#id16.16.id16"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id16.16.id16" title="open radio access networks"><span class="ltx_text ltx_glossary_short">O-RAN</span></abbr></a>) tasks. The dataset is based on 116 <a href="https://arxiv.org/html/2408.11775v1#id16.16.id16"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id16.16.id16" title="open radio access networks"><span class="ltx_text ltx_glossary_short">O-RAN</span></abbr></a> specification documents and contains 13,000 pairs of <a href="https://arxiv.org/html/2408.11775v1#id17.17.id17"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id17.17.id17" title="multiple choice question"><span class="ltx_text ltx_glossary_short-plural">MCQs</span></abbr></a>, based on which ORANSight is developed, a <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a>-based framework.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib18" title="">18</a>]</cite> evaluates the performance of various zero shot <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> in a few tasks in the telecommunications domain, including a <a href="https://arxiv.org/html/2408.11775v1#id12.12.id12"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id12.12.id12" title="questions and answering"><span class="ltx_text ltx_glossary_short">QnA</span></abbr></a> task. Among their findings, they note that though <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short">LLM</span></abbr></a> models such as Zephyr, and Mistral perform outstandingly in the tasks, their performance still comes strikingly short when compared to GPT-3.5 or GPT-4.
The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib19" title="">19</a>]</cite> demonstrate the effectiveness of fine-tuning different <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> for the telecom domain.
The work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib12" title="">12</a>]</cite> proposes Telco-RAG, a framework specialized for <a href="https://arxiv.org/html/2408.11775v1#id17.17.id17"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id17.17.id17" title="multiple choice question"><span class="ltx_text ltx_glossary_short">MCQ</span></abbr></a> answering for telecom applications, tailored to the specific requirements of telecom standards, particularly <a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_short">3GPP</span></abbr></a> documents. Their contribution focuses on modifying the <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> framework by employing a router,
using generated candidate answers to enhance retrieval quality, and appending the definitions of acronyms and technical terms to the user’s query.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">To the best of the authors’ knowledge, this is the first work to present a <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> architecture with fine-tuned SLM generator. Additionally, critical components such as SelfExtend for handling long contexts, re-ranking for enhancing retrieval accuracy, and semantic chunking to preserve contextual coherence have not been proposed in the relevant literature.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed architecture</span>
</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S3.F1.g1" src="extracted/5805716/figures/_arch_v4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Overview of proposed RAG architecture with semantic chunking, extended context support, and fine-tuned Phi-2 SLM integration for <a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_short">3GPP</span></abbr></a> document processing.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">General Overview</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> integrates four key components: a chunking mechanism to segment information, an embedding model to encode the information in a latent space, a retriever to fetch relevant context, and a generator to produce responses.
In this work, we present an advanced <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> framework, illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.F1" title="Figure 1 ‣ III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">1</span></a>, that optimizes each component to enhance performance and adapt the model to the telecom domain.
In our architecture, 3GPP documents are first chunked using a semantic chunker then embedded and stored in a vector database (DB). During inference, the user’s query is embedded and then passed to the vector retriever.
The retriever performs a vector similarity search in embedding space to return the nearest relevant neighbors from the indexed corpus.
The retrieved chunks are then passed to a re-ranking algorithm that returns an ordered sublist containing the most relevant chunks.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">The Phi-2 generator model is then given the chunks, the query, and a set of instructions to provide an answer. Here, we efficiently fine-tune the generator using <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a>. The model initially is fine-tuned with <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_n</annotation></semantics></math> contexts based on retrieved information and the prompt. During inference, we employ the self-extend technique to dynamically expand the context window, accommodating retrieved contexts beyond the initial <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_n</annotation></semantics></math> limit.
In what follows, we highlight the key components of the proposed framework.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Semantic Chunking Strategy</span>
</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="881" id="S3.F2.g1" src="extracted/5805716/figures/sem_v5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Comparison of fixed-size chunking and semantic chunking applied to an excerpt from a 3GPP document.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="559" id="S3.F3.g1" src="extracted/5805716/figures/prompt_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Prompt structure that includes retrieved context and instructions.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The method of splitting or chunking is critical as improper chunks can lead to inaccurate representations in the embedding space.
Chunking can be implemented through either fixed-size segments or adaptable segments based on specific criteria. While fixed-size chunking can yield reasonable results and is computationally efficient, it often creates blocks of text that do not consider the content or context. To mitigate this issue, we leverage semantic chunking to split the 3GPP documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib20" title="">20</a>]</cite>.
Semantic chunking adaptively determines breakpoints between sentences based on embedding similarity.
This way the meaning of the text is preserved through the logical breaks where sentences are semantically connected, rather than arbitrarily cutting the text at fixed intervals. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.F2" title="Figure 2 ‣ III-B Semantic Chunking Strategy ‣ III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">2</span></a> shows an illustrative example from a 3GPP document.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Given that telecom documents don’t follow a strict format, leveraging semantic chunking preserves essential context while minimizing information fragmentation and irrelevant grouping.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Embedding Model</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We utilize <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">bge-small-en-v1.5</span>, an open-source embedding model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib21" title="">21</a>]</cite>. It is optimized for balancing efficiency and accuracy in text embedding tasks. The model is trained using contrastive learning on a large-scale dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib22" title="">22</a>]</cite> and it creates vector representations that capture semantic relationships between elements. This method effectively reduces the distance between similar pairs while increasing it between dissimilar ones which helps refine the model’s ability to discriminate between relevant features.
To ensure faster performance at runtime, these embeddings are calculated and stored in Chroma dB,
an AI-native vector database designed to efficiently handle high-dimensional embeddings which allows for faster similarity search compared to traditional databases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Retrieval with Re-ranking</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The performance of <a href="https://arxiv.org/html/2408.11775v1#id4.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id4.4.id4" title="retrieval-augmented generation"><span class="ltx_text ltx_glossary_short">RAG</span></abbr></a> is highly dependent on the relevance and quality of the retrieved context.
This work employs a cross-encoder re-ranker, a widely adopted semantic re-ranking method to ensure that the most relevant contexts are ranked first <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib24" title="">24</a>]</cite>.
Unlike bi-encoders, which embed each chunk independently, cross-encoders process pairs of text to calculate the similarity between them. This approach, allows it to fully capture the interactions and relationships between the query and each chunk of context. We specifically use the <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">ms-marco-MiniLM-L-6-v2</span> model
due to its balance between efficiency and performance, both essential for telecom tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib24" title="">24</a>]</cite></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.4.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.5.2">Extending the Context Window with SelfExtend</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">However, <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> typically struggle to generalize effectively to input sequences longer than those encountered during training. This presents as a challenge during inference with long contexts.
The context window of <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> are often short. For example, Phi-2 has a context window of 2048 tokens. Semantic chunking does not guarantee a fixed chunk size, and therefore, might exceed the context window of the <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">Furthermore, supplementing the context with tables from telecom documents is crucial given the significance of the information they hold. Therefore, in order to support enhanced performance, and to allow for future expansions, we utilize SelfExtend<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">This method leverages the inherent capabilities of LLMs to handle extended contexts without the need for fine-tuning. SelfExtend achieves this by implementing a bi-level attention mechanism: grouped attention for capturing dependencies between distant tokens, and neighbor attention for focusing on adjacent tokens. These attentions are computed using the model’s existing self-attention during inference. By making use of SelfExtend, we are able to extend Phi-2’s context window to 8192 tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.4.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.5.2">The Generator: Fine-tuned Phi-2 with Multiple Contexts</span>
</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">A substantial portion of telecom key terms and special language is confined to specification documents and white papers, neither of which <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_short-plural">LMs</span></abbr></a> are heavily trained on. We fine-tune Phi-2 to overcome this and to adapt the model to recognize telecom terminology.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">It should also be emphasized that fine-tuning the model is not intended to expand its knowledge base, but rather to enhance its ability to discern important details within the context and respond in the correct format.
To accommodate limited resources, gradient accumulation and <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a> are utilized for fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib25" title="">25</a>]</cite>.
Gradient accumulation is a technique that allows the model to effectively handle large batch sizes by minimizing the memory needed for storing gradients. It does this by processing several small batches and accumulating the gradients from each batch before updating the weights rather than calculating and updating the weights after each batch.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="388" id="S3.F4.g1" src="extracted/5805716/figures/LoRA_arch_v4.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Schematic illustration of the Low-Rank Adaptation (LoRA) technique for efficient fine-tuning of neural networks with low-rank matrices (e.g., <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_short-plural">LMs</span></abbr></a>).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">Fine-tuning all parameters in a model is impractical in our domain due to resource constraints. Moreover, for smaller or specialized datasets, this approach risks overfitting and poor generalization, potentially yielding diminishing returns.</p>
</div>
<div class="ltx_para" id="S3.SS6.p4">
<p class="ltx_p" id="S3.SS6.p4.1">The concept underlying <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a> is that pre-trained <a href="https://arxiv.org/html/2408.11775v1#id5.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id5.5.id5" title="language model"><span class="ltx_text ltx_glossary_short-plural">LMs</span></abbr></a> possess a low ‘intrinsic dimension’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib26" title="">26</a>]</cite>. That is, the model’s essential information is concentrated in a smaller subspace, even if the overall parameter space is high-dimensional. <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a> harnesses this observation and focus the model updates on a smaller only a subset of the learning parameters.
During fine-tuning, weight updates <math alttext="W_{new}" class="ltx_Math" display="inline" id="S3.SS6.p4.1.m1.1"><semantics id="S3.SS6.p4.1.m1.1a"><msub id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml"><mi id="S3.SS6.p4.1.m1.1.1.2" xref="S3.SS6.p4.1.m1.1.1.2.cmml">W</mi><mrow id="S3.SS6.p4.1.m1.1.1.3" xref="S3.SS6.p4.1.m1.1.1.3.cmml"><mi id="S3.SS6.p4.1.m1.1.1.3.2" xref="S3.SS6.p4.1.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS6.p4.1.m1.1.1.3.1" xref="S3.SS6.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS6.p4.1.m1.1.1.3.3" xref="S3.SS6.p4.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS6.p4.1.m1.1.1.3.1a" xref="S3.SS6.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS6.p4.1.m1.1.1.3.4" xref="S3.SS6.p4.1.m1.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><apply id="S3.SS6.p4.1.m1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p4.1.m1.1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p4.1.m1.1.1.2.cmml" xref="S3.SS6.p4.1.m1.1.1.2">𝑊</ci><apply id="S3.SS6.p4.1.m1.1.1.3.cmml" xref="S3.SS6.p4.1.m1.1.1.3"><times id="S3.SS6.p4.1.m1.1.1.3.1.cmml" xref="S3.SS6.p4.1.m1.1.1.3.1"></times><ci id="S3.SS6.p4.1.m1.1.1.3.2.cmml" xref="S3.SS6.p4.1.m1.1.1.3.2">𝑛</ci><ci id="S3.SS6.p4.1.m1.1.1.3.3.cmml" xref="S3.SS6.p4.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS6.p4.1.m1.1.1.3.4.cmml" xref="S3.SS6.p4.1.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">W_{new}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> are represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="W_{new}=W_{0}+\Delta W," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">W</mi><mrow id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.2.3.2.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.2.3.1a" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.2.3.4" xref="S3.E1.m1.1.1.1.1.2.3.4.cmml">w</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">W</mi><mn id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" mathvariant="normal" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">Δ</mi><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">W</mi></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">𝑊</ci><apply id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><times id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.3.2">𝑛</ci><ci id="S3.E1.m1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.2.3.4">𝑤</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝑊</ci><cn id="S3.E1.m1.1.1.1.1.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.2.3">0</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">Δ</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">W_{new}=W_{0}+\Delta W,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_W start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + roman_Δ italic_W ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.p4.10">where <math alttext="W_{0}\in\mathcal{R}^{d\times k}" class="ltx_Math" display="inline" id="S3.SS6.p4.2.m1.1"><semantics id="S3.SS6.p4.2.m1.1a"><mrow id="S3.SS6.p4.2.m1.1.1" xref="S3.SS6.p4.2.m1.1.1.cmml"><msub id="S3.SS6.p4.2.m1.1.1.2" xref="S3.SS6.p4.2.m1.1.1.2.cmml"><mi id="S3.SS6.p4.2.m1.1.1.2.2" xref="S3.SS6.p4.2.m1.1.1.2.2.cmml">W</mi><mn id="S3.SS6.p4.2.m1.1.1.2.3" xref="S3.SS6.p4.2.m1.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS6.p4.2.m1.1.1.1" xref="S3.SS6.p4.2.m1.1.1.1.cmml">∈</mo><msup id="S3.SS6.p4.2.m1.1.1.3" xref="S3.SS6.p4.2.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p4.2.m1.1.1.3.2" xref="S3.SS6.p4.2.m1.1.1.3.2.cmml">ℛ</mi><mrow id="S3.SS6.p4.2.m1.1.1.3.3" xref="S3.SS6.p4.2.m1.1.1.3.3.cmml"><mi id="S3.SS6.p4.2.m1.1.1.3.3.2" xref="S3.SS6.p4.2.m1.1.1.3.3.2.cmml">d</mi><mo id="S3.SS6.p4.2.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.2.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS6.p4.2.m1.1.1.3.3.3" xref="S3.SS6.p4.2.m1.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.2.m1.1b"><apply id="S3.SS6.p4.2.m1.1.1.cmml" xref="S3.SS6.p4.2.m1.1.1"><in id="S3.SS6.p4.2.m1.1.1.1.cmml" xref="S3.SS6.p4.2.m1.1.1.1"></in><apply id="S3.SS6.p4.2.m1.1.1.2.cmml" xref="S3.SS6.p4.2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p4.2.m1.1.1.2.1.cmml" xref="S3.SS6.p4.2.m1.1.1.2">subscript</csymbol><ci id="S3.SS6.p4.2.m1.1.1.2.2.cmml" xref="S3.SS6.p4.2.m1.1.1.2.2">𝑊</ci><cn id="S3.SS6.p4.2.m1.1.1.2.3.cmml" type="integer" xref="S3.SS6.p4.2.m1.1.1.2.3">0</cn></apply><apply id="S3.SS6.p4.2.m1.1.1.3.cmml" xref="S3.SS6.p4.2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p4.2.m1.1.1.3.1.cmml" xref="S3.SS6.p4.2.m1.1.1.3">superscript</csymbol><ci id="S3.SS6.p4.2.m1.1.1.3.2.cmml" xref="S3.SS6.p4.2.m1.1.1.3.2">ℛ</ci><apply id="S3.SS6.p4.2.m1.1.1.3.3.cmml" xref="S3.SS6.p4.2.m1.1.1.3.3"><times id="S3.SS6.p4.2.m1.1.1.3.3.1.cmml" xref="S3.SS6.p4.2.m1.1.1.3.3.1"></times><ci id="S3.SS6.p4.2.m1.1.1.3.3.2.cmml" xref="S3.SS6.p4.2.m1.1.1.3.3.2">𝑑</ci><ci id="S3.SS6.p4.2.m1.1.1.3.3.3.cmml" xref="S3.SS6.p4.2.m1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.2.m1.1c">W_{0}\in\mathcal{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.2.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> are the initial pre-trained weights, and <math alttext="\Delta W\in\mathcal{R}^{d\times k}" class="ltx_Math" display="inline" id="S3.SS6.p4.3.m2.1"><semantics id="S3.SS6.p4.3.m2.1a"><mrow id="S3.SS6.p4.3.m2.1.1" xref="S3.SS6.p4.3.m2.1.1.cmml"><mrow id="S3.SS6.p4.3.m2.1.1.2" xref="S3.SS6.p4.3.m2.1.1.2.cmml"><mi id="S3.SS6.p4.3.m2.1.1.2.2" mathvariant="normal" xref="S3.SS6.p4.3.m2.1.1.2.2.cmml">Δ</mi><mo id="S3.SS6.p4.3.m2.1.1.2.1" xref="S3.SS6.p4.3.m2.1.1.2.1.cmml">⁢</mo><mi id="S3.SS6.p4.3.m2.1.1.2.3" xref="S3.SS6.p4.3.m2.1.1.2.3.cmml">W</mi></mrow><mo id="S3.SS6.p4.3.m2.1.1.1" xref="S3.SS6.p4.3.m2.1.1.1.cmml">∈</mo><msup id="S3.SS6.p4.3.m2.1.1.3" xref="S3.SS6.p4.3.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p4.3.m2.1.1.3.2" xref="S3.SS6.p4.3.m2.1.1.3.2.cmml">ℛ</mi><mrow id="S3.SS6.p4.3.m2.1.1.3.3" xref="S3.SS6.p4.3.m2.1.1.3.3.cmml"><mi id="S3.SS6.p4.3.m2.1.1.3.3.2" xref="S3.SS6.p4.3.m2.1.1.3.3.2.cmml">d</mi><mo id="S3.SS6.p4.3.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.3.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS6.p4.3.m2.1.1.3.3.3" xref="S3.SS6.p4.3.m2.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.3.m2.1b"><apply id="S3.SS6.p4.3.m2.1.1.cmml" xref="S3.SS6.p4.3.m2.1.1"><in id="S3.SS6.p4.3.m2.1.1.1.cmml" xref="S3.SS6.p4.3.m2.1.1.1"></in><apply id="S3.SS6.p4.3.m2.1.1.2.cmml" xref="S3.SS6.p4.3.m2.1.1.2"><times id="S3.SS6.p4.3.m2.1.1.2.1.cmml" xref="S3.SS6.p4.3.m2.1.1.2.1"></times><ci id="S3.SS6.p4.3.m2.1.1.2.2.cmml" xref="S3.SS6.p4.3.m2.1.1.2.2">Δ</ci><ci id="S3.SS6.p4.3.m2.1.1.2.3.cmml" xref="S3.SS6.p4.3.m2.1.1.2.3">𝑊</ci></apply><apply id="S3.SS6.p4.3.m2.1.1.3.cmml" xref="S3.SS6.p4.3.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p4.3.m2.1.1.3.1.cmml" xref="S3.SS6.p4.3.m2.1.1.3">superscript</csymbol><ci id="S3.SS6.p4.3.m2.1.1.3.2.cmml" xref="S3.SS6.p4.3.m2.1.1.3.2">ℛ</ci><apply id="S3.SS6.p4.3.m2.1.1.3.3.cmml" xref="S3.SS6.p4.3.m2.1.1.3.3"><times id="S3.SS6.p4.3.m2.1.1.3.3.1.cmml" xref="S3.SS6.p4.3.m2.1.1.3.3.1"></times><ci id="S3.SS6.p4.3.m2.1.1.3.3.2.cmml" xref="S3.SS6.p4.3.m2.1.1.3.3.2">𝑑</ci><ci id="S3.SS6.p4.3.m2.1.1.3.3.3.cmml" xref="S3.SS6.p4.3.m2.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.3.m2.1c">\Delta W\in\mathcal{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.3.m2.1d">roman_Δ italic_W ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> represents the change in weights. Computing <math alttext="\Delta W" class="ltx_Math" display="inline" id="S3.SS6.p4.4.m3.1"><semantics id="S3.SS6.p4.4.m3.1a"><mrow id="S3.SS6.p4.4.m3.1.1" xref="S3.SS6.p4.4.m3.1.1.cmml"><mi id="S3.SS6.p4.4.m3.1.1.2" mathvariant="normal" xref="S3.SS6.p4.4.m3.1.1.2.cmml">Δ</mi><mo id="S3.SS6.p4.4.m3.1.1.1" xref="S3.SS6.p4.4.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS6.p4.4.m3.1.1.3" xref="S3.SS6.p4.4.m3.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.4.m3.1b"><apply id="S3.SS6.p4.4.m3.1.1.cmml" xref="S3.SS6.p4.4.m3.1.1"><times id="S3.SS6.p4.4.m3.1.1.1.cmml" xref="S3.SS6.p4.4.m3.1.1.1"></times><ci id="S3.SS6.p4.4.m3.1.1.2.cmml" xref="S3.SS6.p4.4.m3.1.1.2">Δ</ci><ci id="S3.SS6.p4.4.m3.1.1.3.cmml" xref="S3.SS6.p4.4.m3.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.4.m3.1c">\Delta W</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.4.m3.1d">roman_Δ italic_W</annotation></semantics></math> is computationally expensive. In <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a>, trainable parameters <math alttext="\Delta W" class="ltx_Math" display="inline" id="S3.SS6.p4.5.m4.1"><semantics id="S3.SS6.p4.5.m4.1a"><mrow id="S3.SS6.p4.5.m4.1.1" xref="S3.SS6.p4.5.m4.1.1.cmml"><mi id="S3.SS6.p4.5.m4.1.1.2" mathvariant="normal" xref="S3.SS6.p4.5.m4.1.1.2.cmml">Δ</mi><mo id="S3.SS6.p4.5.m4.1.1.1" xref="S3.SS6.p4.5.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS6.p4.5.m4.1.1.3" xref="S3.SS6.p4.5.m4.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.5.m4.1b"><apply id="S3.SS6.p4.5.m4.1.1.cmml" xref="S3.SS6.p4.5.m4.1.1"><times id="S3.SS6.p4.5.m4.1.1.1.cmml" xref="S3.SS6.p4.5.m4.1.1.1"></times><ci id="S3.SS6.p4.5.m4.1.1.2.cmml" xref="S3.SS6.p4.5.m4.1.1.2">Δ</ci><ci id="S3.SS6.p4.5.m4.1.1.3.cmml" xref="S3.SS6.p4.5.m4.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.5.m4.1c">\Delta W</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.5.m4.1d">roman_Δ italic_W</annotation></semantics></math> are expressed as a product of two low-rank matrices, <math alttext="B\times A" class="ltx_Math" display="inline" id="S3.SS6.p4.6.m5.1"><semantics id="S3.SS6.p4.6.m5.1a"><mrow id="S3.SS6.p4.6.m5.1.1" xref="S3.SS6.p4.6.m5.1.1.cmml"><mi id="S3.SS6.p4.6.m5.1.1.2" xref="S3.SS6.p4.6.m5.1.1.2.cmml">B</mi><mo id="S3.SS6.p4.6.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.6.m5.1.1.1.cmml">×</mo><mi id="S3.SS6.p4.6.m5.1.1.3" xref="S3.SS6.p4.6.m5.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.6.m5.1b"><apply id="S3.SS6.p4.6.m5.1.1.cmml" xref="S3.SS6.p4.6.m5.1.1"><times id="S3.SS6.p4.6.m5.1.1.1.cmml" xref="S3.SS6.p4.6.m5.1.1.1"></times><ci id="S3.SS6.p4.6.m5.1.1.2.cmml" xref="S3.SS6.p4.6.m5.1.1.2">𝐵</ci><ci id="S3.SS6.p4.6.m5.1.1.3.cmml" xref="S3.SS6.p4.6.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.6.m5.1c">B\times A</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.6.m5.1d">italic_B × italic_A</annotation></semantics></math>, where <math alttext="B\in\mathcal{R}^{d\times r}" class="ltx_Math" display="inline" id="S3.SS6.p4.7.m6.1"><semantics id="S3.SS6.p4.7.m6.1a"><mrow id="S3.SS6.p4.7.m6.1.1" xref="S3.SS6.p4.7.m6.1.1.cmml"><mi id="S3.SS6.p4.7.m6.1.1.2" xref="S3.SS6.p4.7.m6.1.1.2.cmml">B</mi><mo id="S3.SS6.p4.7.m6.1.1.1" xref="S3.SS6.p4.7.m6.1.1.1.cmml">∈</mo><msup id="S3.SS6.p4.7.m6.1.1.3" xref="S3.SS6.p4.7.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p4.7.m6.1.1.3.2" xref="S3.SS6.p4.7.m6.1.1.3.2.cmml">ℛ</mi><mrow id="S3.SS6.p4.7.m6.1.1.3.3" xref="S3.SS6.p4.7.m6.1.1.3.3.cmml"><mi id="S3.SS6.p4.7.m6.1.1.3.3.2" xref="S3.SS6.p4.7.m6.1.1.3.3.2.cmml">d</mi><mo id="S3.SS6.p4.7.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.7.m6.1.1.3.3.1.cmml">×</mo><mi id="S3.SS6.p4.7.m6.1.1.3.3.3" xref="S3.SS6.p4.7.m6.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.7.m6.1b"><apply id="S3.SS6.p4.7.m6.1.1.cmml" xref="S3.SS6.p4.7.m6.1.1"><in id="S3.SS6.p4.7.m6.1.1.1.cmml" xref="S3.SS6.p4.7.m6.1.1.1"></in><ci id="S3.SS6.p4.7.m6.1.1.2.cmml" xref="S3.SS6.p4.7.m6.1.1.2">𝐵</ci><apply id="S3.SS6.p4.7.m6.1.1.3.cmml" xref="S3.SS6.p4.7.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p4.7.m6.1.1.3.1.cmml" xref="S3.SS6.p4.7.m6.1.1.3">superscript</csymbol><ci id="S3.SS6.p4.7.m6.1.1.3.2.cmml" xref="S3.SS6.p4.7.m6.1.1.3.2">ℛ</ci><apply id="S3.SS6.p4.7.m6.1.1.3.3.cmml" xref="S3.SS6.p4.7.m6.1.1.3.3"><times id="S3.SS6.p4.7.m6.1.1.3.3.1.cmml" xref="S3.SS6.p4.7.m6.1.1.3.3.1"></times><ci id="S3.SS6.p4.7.m6.1.1.3.3.2.cmml" xref="S3.SS6.p4.7.m6.1.1.3.3.2">𝑑</ci><ci id="S3.SS6.p4.7.m6.1.1.3.3.3.cmml" xref="S3.SS6.p4.7.m6.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.7.m6.1c">B\in\mathcal{R}^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.7.m6.1d">italic_B ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="A\in\mathcal{R}^{r\times k}" class="ltx_Math" display="inline" id="S3.SS6.p4.8.m7.1"><semantics id="S3.SS6.p4.8.m7.1a"><mrow id="S3.SS6.p4.8.m7.1.1" xref="S3.SS6.p4.8.m7.1.1.cmml"><mi id="S3.SS6.p4.8.m7.1.1.2" xref="S3.SS6.p4.8.m7.1.1.2.cmml">A</mi><mo id="S3.SS6.p4.8.m7.1.1.1" xref="S3.SS6.p4.8.m7.1.1.1.cmml">∈</mo><msup id="S3.SS6.p4.8.m7.1.1.3" xref="S3.SS6.p4.8.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p4.8.m7.1.1.3.2" xref="S3.SS6.p4.8.m7.1.1.3.2.cmml">ℛ</mi><mrow id="S3.SS6.p4.8.m7.1.1.3.3" xref="S3.SS6.p4.8.m7.1.1.3.3.cmml"><mi id="S3.SS6.p4.8.m7.1.1.3.3.2" xref="S3.SS6.p4.8.m7.1.1.3.3.2.cmml">r</mi><mo id="S3.SS6.p4.8.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.8.m7.1.1.3.3.1.cmml">×</mo><mi id="S3.SS6.p4.8.m7.1.1.3.3.3" xref="S3.SS6.p4.8.m7.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.8.m7.1b"><apply id="S3.SS6.p4.8.m7.1.1.cmml" xref="S3.SS6.p4.8.m7.1.1"><in id="S3.SS6.p4.8.m7.1.1.1.cmml" xref="S3.SS6.p4.8.m7.1.1.1"></in><ci id="S3.SS6.p4.8.m7.1.1.2.cmml" xref="S3.SS6.p4.8.m7.1.1.2">𝐴</ci><apply id="S3.SS6.p4.8.m7.1.1.3.cmml" xref="S3.SS6.p4.8.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p4.8.m7.1.1.3.1.cmml" xref="S3.SS6.p4.8.m7.1.1.3">superscript</csymbol><ci id="S3.SS6.p4.8.m7.1.1.3.2.cmml" xref="S3.SS6.p4.8.m7.1.1.3.2">ℛ</ci><apply id="S3.SS6.p4.8.m7.1.1.3.3.cmml" xref="S3.SS6.p4.8.m7.1.1.3.3"><times id="S3.SS6.p4.8.m7.1.1.3.3.1.cmml" xref="S3.SS6.p4.8.m7.1.1.3.3.1"></times><ci id="S3.SS6.p4.8.m7.1.1.3.3.2.cmml" xref="S3.SS6.p4.8.m7.1.1.3.3.2">𝑟</ci><ci id="S3.SS6.p4.8.m7.1.1.3.3.3.cmml" xref="S3.SS6.p4.8.m7.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.8.m7.1c">A\in\mathcal{R}^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.8.m7.1d">italic_A ∈ caligraphic_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, with rank <math alttext="r\ll\min(d,k)" class="ltx_Math" display="inline" id="S3.SS6.p4.9.m8.3"><semantics id="S3.SS6.p4.9.m8.3a"><mrow id="S3.SS6.p4.9.m8.3.4" xref="S3.SS6.p4.9.m8.3.4.cmml"><mi id="S3.SS6.p4.9.m8.3.4.2" xref="S3.SS6.p4.9.m8.3.4.2.cmml">r</mi><mo id="S3.SS6.p4.9.m8.3.4.1" xref="S3.SS6.p4.9.m8.3.4.1.cmml">≪</mo><mrow id="S3.SS6.p4.9.m8.3.4.3.2" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml"><mi id="S3.SS6.p4.9.m8.1.1" xref="S3.SS6.p4.9.m8.1.1.cmml">min</mi><mo id="S3.SS6.p4.9.m8.3.4.3.2a" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml">⁡</mo><mrow id="S3.SS6.p4.9.m8.3.4.3.2.1" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml"><mo id="S3.SS6.p4.9.m8.3.4.3.2.1.1" stretchy="false" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml">(</mo><mi id="S3.SS6.p4.9.m8.2.2" xref="S3.SS6.p4.9.m8.2.2.cmml">d</mi><mo id="S3.SS6.p4.9.m8.3.4.3.2.1.2" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml">,</mo><mi id="S3.SS6.p4.9.m8.3.3" xref="S3.SS6.p4.9.m8.3.3.cmml">k</mi><mo id="S3.SS6.p4.9.m8.3.4.3.2.1.3" stretchy="false" xref="S3.SS6.p4.9.m8.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.9.m8.3b"><apply id="S3.SS6.p4.9.m8.3.4.cmml" xref="S3.SS6.p4.9.m8.3.4"><csymbol cd="latexml" id="S3.SS6.p4.9.m8.3.4.1.cmml" xref="S3.SS6.p4.9.m8.3.4.1">much-less-than</csymbol><ci id="S3.SS6.p4.9.m8.3.4.2.cmml" xref="S3.SS6.p4.9.m8.3.4.2">𝑟</ci><apply id="S3.SS6.p4.9.m8.3.4.3.1.cmml" xref="S3.SS6.p4.9.m8.3.4.3.2"><min id="S3.SS6.p4.9.m8.1.1.cmml" xref="S3.SS6.p4.9.m8.1.1"></min><ci id="S3.SS6.p4.9.m8.2.2.cmml" xref="S3.SS6.p4.9.m8.2.2">𝑑</ci><ci id="S3.SS6.p4.9.m8.3.3.cmml" xref="S3.SS6.p4.9.m8.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.9.m8.3c">r\ll\min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.9.m8.3d">italic_r ≪ roman_min ( italic_d , italic_k )</annotation></semantics></math>. Thus, with <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a>, weight updates <math alttext="W_{new}" class="ltx_Math" display="inline" id="S3.SS6.p4.10.m9.1"><semantics id="S3.SS6.p4.10.m9.1a"><msub id="S3.SS6.p4.10.m9.1.1" xref="S3.SS6.p4.10.m9.1.1.cmml"><mi id="S3.SS6.p4.10.m9.1.1.2" xref="S3.SS6.p4.10.m9.1.1.2.cmml">W</mi><mrow id="S3.SS6.p4.10.m9.1.1.3" xref="S3.SS6.p4.10.m9.1.1.3.cmml"><mi id="S3.SS6.p4.10.m9.1.1.3.2" xref="S3.SS6.p4.10.m9.1.1.3.2.cmml">n</mi><mo id="S3.SS6.p4.10.m9.1.1.3.1" xref="S3.SS6.p4.10.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS6.p4.10.m9.1.1.3.3" xref="S3.SS6.p4.10.m9.1.1.3.3.cmml">e</mi><mo id="S3.SS6.p4.10.m9.1.1.3.1a" xref="S3.SS6.p4.10.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS6.p4.10.m9.1.1.3.4" xref="S3.SS6.p4.10.m9.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.10.m9.1b"><apply id="S3.SS6.p4.10.m9.1.1.cmml" xref="S3.SS6.p4.10.m9.1.1"><csymbol cd="ambiguous" id="S3.SS6.p4.10.m9.1.1.1.cmml" xref="S3.SS6.p4.10.m9.1.1">subscript</csymbol><ci id="S3.SS6.p4.10.m9.1.1.2.cmml" xref="S3.SS6.p4.10.m9.1.1.2">𝑊</ci><apply id="S3.SS6.p4.10.m9.1.1.3.cmml" xref="S3.SS6.p4.10.m9.1.1.3"><times id="S3.SS6.p4.10.m9.1.1.3.1.cmml" xref="S3.SS6.p4.10.m9.1.1.3.1"></times><ci id="S3.SS6.p4.10.m9.1.1.3.2.cmml" xref="S3.SS6.p4.10.m9.1.1.3.2">𝑛</ci><ci id="S3.SS6.p4.10.m9.1.1.3.3.cmml" xref="S3.SS6.p4.10.m9.1.1.3.3">𝑒</ci><ci id="S3.SS6.p4.10.m9.1.1.3.4.cmml" xref="S3.SS6.p4.10.m9.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.10.m9.1c">W_{new}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.10.m9.1d">italic_W start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> are computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="W_{new}=W_{0}+(\frac{\alpha}{r})B\times A," class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.cmml">W</mi><mrow id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.3.2.cmml">n</mi><mo id="S3.E2.m1.2.2.1.1.2.3.1" xref="S3.E2.m1.2.2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.2.3.3" xref="S3.E2.m1.2.2.1.1.2.3.3.cmml">e</mi><mo id="S3.E2.m1.2.2.1.1.2.3.1a" xref="S3.E2.m1.2.2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.2.3.4" xref="S3.E2.m1.2.2.1.1.2.3.4.cmml">w</mi></mrow></msub><mo id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.cmml">W</mi><mn id="S3.E2.m1.2.2.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.cmml">0</mn></msub><mo id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><mrow id="S3.E2.m1.2.2.1.1.3.3.2.2.2" xref="S3.E2.m1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.3.3.2.2.2.1" stretchy="false" xref="S3.E2.m1.1.1.cmml">(</mo><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">α</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">r</mi></mfrac><mo id="S3.E2.m1.2.2.1.1.3.3.2.2.2.2" stretchy="false" xref="S3.E2.m1.1.1.cmml">)</mo></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.2.1" xref="S3.E2.m1.2.2.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.cmml">B</mi></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.2.2.1.1.3.3.1.cmml">×</mo><mi id="S3.E2.m1.2.2.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.cmml">A</mi></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"></eq><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2">𝑊</ci><apply id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3"><times id="S3.E2.m1.2.2.1.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.2.3.1"></times><ci id="S3.E2.m1.2.2.1.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.2.3.2">𝑛</ci><ci id="S3.E2.m1.2.2.1.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3.3">𝑒</ci><ci id="S3.E2.m1.2.2.1.1.2.3.4.cmml" xref="S3.E2.m1.2.2.1.1.2.3.4">𝑤</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><plus id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></plus><apply id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2">𝑊</ci><cn id="S3.E2.m1.2.2.1.1.3.2.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.3.2.3">0</cn></apply><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3"><times id="S3.E2.m1.2.2.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1"></times><apply id="S3.E2.m1.2.2.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><times id="S3.E2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.1"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2"><divide id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2"></divide><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝛼</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝑟</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3">𝐵</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">W_{new}=W_{0}+(\frac{\alpha}{r})B\times A,</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">italic_W start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + ( divide start_ARG italic_α end_ARG start_ARG italic_r end_ARG ) italic_B × italic_A ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.p4.12">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS6.p4.11.m1.1"><semantics id="S3.SS6.p4.11.m1.1a"><mi id="S3.SS6.p4.11.m1.1.1" xref="S3.SS6.p4.11.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.11.m1.1b"><ci id="S3.SS6.p4.11.m1.1.1.cmml" xref="S3.SS6.p4.11.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.11.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.11.m1.1d">italic_α</annotation></semantics></math> is a scaling factor, reflecting how important the weight updates are to the initial pre-trained weights.
As a result of this matrix decomposition, only <math alttext="d\times r+r\times k" class="ltx_Math" display="inline" id="S3.SS6.p4.12.m2.1"><semantics id="S3.SS6.p4.12.m2.1a"><mrow id="S3.SS6.p4.12.m2.1.1" xref="S3.SS6.p4.12.m2.1.1.cmml"><mrow id="S3.SS6.p4.12.m2.1.1.2" xref="S3.SS6.p4.12.m2.1.1.2.cmml"><mi id="S3.SS6.p4.12.m2.1.1.2.2" xref="S3.SS6.p4.12.m2.1.1.2.2.cmml">d</mi><mo id="S3.SS6.p4.12.m2.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.12.m2.1.1.2.1.cmml">×</mo><mi id="S3.SS6.p4.12.m2.1.1.2.3" xref="S3.SS6.p4.12.m2.1.1.2.3.cmml">r</mi></mrow><mo id="S3.SS6.p4.12.m2.1.1.1" xref="S3.SS6.p4.12.m2.1.1.1.cmml">+</mo><mrow id="S3.SS6.p4.12.m2.1.1.3" xref="S3.SS6.p4.12.m2.1.1.3.cmml"><mi id="S3.SS6.p4.12.m2.1.1.3.2" xref="S3.SS6.p4.12.m2.1.1.3.2.cmml">r</mi><mo id="S3.SS6.p4.12.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS6.p4.12.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS6.p4.12.m2.1.1.3.3" xref="S3.SS6.p4.12.m2.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.12.m2.1b"><apply id="S3.SS6.p4.12.m2.1.1.cmml" xref="S3.SS6.p4.12.m2.1.1"><plus id="S3.SS6.p4.12.m2.1.1.1.cmml" xref="S3.SS6.p4.12.m2.1.1.1"></plus><apply id="S3.SS6.p4.12.m2.1.1.2.cmml" xref="S3.SS6.p4.12.m2.1.1.2"><times id="S3.SS6.p4.12.m2.1.1.2.1.cmml" xref="S3.SS6.p4.12.m2.1.1.2.1"></times><ci id="S3.SS6.p4.12.m2.1.1.2.2.cmml" xref="S3.SS6.p4.12.m2.1.1.2.2">𝑑</ci><ci id="S3.SS6.p4.12.m2.1.1.2.3.cmml" xref="S3.SS6.p4.12.m2.1.1.2.3">𝑟</ci></apply><apply id="S3.SS6.p4.12.m2.1.1.3.cmml" xref="S3.SS6.p4.12.m2.1.1.3"><times id="S3.SS6.p4.12.m2.1.1.3.1.cmml" xref="S3.SS6.p4.12.m2.1.1.3.1"></times><ci id="S3.SS6.p4.12.m2.1.1.3.2.cmml" xref="S3.SS6.p4.12.m2.1.1.3.2">𝑟</ci><ci id="S3.SS6.p4.12.m2.1.1.3.3.cmml" xref="S3.SS6.p4.12.m2.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.12.m2.1c">d\times r+r\times k</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p4.12.m2.1d">italic_d × italic_r + italic_r × italic_k</annotation></semantics></math> parameters need to be updated.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS7.4.1.1">III-G</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS7.5.2">Prompt Engineering</span>
</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">Prompt engineering is crucial for optimizing the performance of LLMs like Phi-2 in telecom applications.
In this work, we supplement the inputted question with chunks of relevant context, and a set of instructions for the <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> to follow, forming the prompt. n exemplary prompt is outlined in <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.F3" title="Figure 3 ‣ III-B Semantic Chunking Strategy ‣ III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">3</span></a>. This approach helps unify the format of the <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short">SLM</span></abbr></a>’s output, focus its attention on the appearance of critical terms, and encourage it to reply on the context rather than its prior knowledge.
This prompt results in focused outputs, ensuring accurate and relevant responses for complex telecom inquiries.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental results</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Settings</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The RAG framework relies on around 550 <a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_short">3GPP</span></abbr></a> documents up to Release 18. For fine-tuning, we utilize the TeleQnA dataset which contains 10,000 multiple-choice questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib11" title="">11</a>]</cite>. The testing set features another 2,000 multiple-choice questions that focus on <a href="https://arxiv.org/html/2408.11775v1#id15.15.id15"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id15.15.id15" title="Third Generation Partnership Project"><span class="ltx_text ltx_glossary_short">3GPP</span></abbr></a> standards. For fine-tuning, we set the following parameters: weight decay of 0.01, batch size of 32, dropout rate of 0.05, and learning rate of <math alttext="10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msup id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mo id="S4.SS1.p1.1.m1.1.1.3a" xref="S4.SS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">superscript</csymbol><cn id="S4.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.2">10</cn><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><minus id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3"></minus><cn id="S4.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>. For <a href="https://arxiv.org/html/2408.11775v1#id6.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id6.6.id6" title="Low-Rank Adaptation"><span class="ltx_text ltx_glossary_short">LoRA</span></abbr></a>, we set the rank to 32 and the alpha to 64.
For semantic chunking, there are two hyper-parameters, namely the breakpoint percentile threshold and the buffer size. The former represents the percentile of cosine dissimilarity that must be exceeded between a group of sentences and the next to form a node. The latter determines the number of sentences to group together when evaluating semantic similarity. We set the breakpoint percentile threshold to 90, and the buffer size to 3.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For the vector retriever, for each query, we first retrieve 150 chunks, from which the re-ranker would return the top 15 most relevant chunks. The aforementioned hyper-parameters are not necessarily optimized, but rather set based on qualitative judgment and limited exploration.
Fine-tuning the Phi-2 generator (with multiple contexts) relies on three retrieved-context chunks along with the set of instructions and the respective query. For reproducibility and reuse, our source code is made publicly available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Results and Analysis</span>
</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Accuracy comparison of our fine-tuned Phi-2 model against baseline models, both with and without retrieved context.</span></figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S4.T1.g1" src="x1.png" width="358"/>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">Performance comparison of various configurations of the fine-tuned Phi-2 model with RAG and additional components; the table uses the following acronyms: SE for SelfExtend, RR for Rerank, SC for Semantic Chunking, and MC for Multiple Context.</span></figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="113" id="S4.T2.g1" src="x2.png" width="356"/>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4.T1" title="TABLE I ‣ IV-B Results and Analysis ‣ IV Experimental results ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">I</span></a>, we benchmark our developed framework against three other solutions: base Phi-2 (2.7B), GPT-4o mini (8B), and GPT4o (1.76T). As expected, the base Phi-2 model shows poor performance of about 49.95% accuracy when tested on the dataset when no context is provided. Although the performance is notably improved to 71.35% when the base model is supplemented by retrieved context, it is still outperformed by our proposed model. Our fine-tuned RAG model is better aligned with the required task, enabling it to leverage the retrieved context better and produce more accurate recommendations.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Notably, our developed system also outperforms GPT-4o with and without context. Since GPT-4o is a significantly more generalized model, it is not well-aligned to the specialized task at hand. Qualitative analysis reveals that GPT-4o’s <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">a priori</span> knowledge from other domains sometimes interferes with our domain-specific task, albeit when the relevant context is present in the prompt.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S4.T2" title="TABLE II ‣ IV-B Results and Analysis ‣ IV Experimental results ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">II</span></a> represents a brief ablation study, where we analyze the impact of each added component on the predictive accuracy.
The re-ranking algorithm has significant positive effects, adding 5% in accuracy. This underscores the importance of prioritizing retrieved-context chunks. When implemented on its own, semantic chunking degrades the performance. However, when paired with SelfExtend, the accuracy is significantly increased to 80.30% which is an increase of around 8% compared to the base model. Although semantic chunking produces more semantically coherent chunks (as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.11775v1#S3.F2" title="Figure 2 ‣ III-B Semantic Chunking Strategy ‣ III Proposed architecture ‣ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards"><span class="ltx_text ltx_ref_tag">2</span></a>), it often results in longer chunks, and SelfExtend helps incorporate them into complete semantic units.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusions And Future Works</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The significance of <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> has not gone unrecognized in the industry, as a shift from <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> to <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> can be evidenced by the recent directions of several big firms. They are especially pertinent in the telecom industry given the constraints often encountered in AI-driven telecom systems, such as the need for efficient deployment on edge devices with limited computational power. Coupled with RAG, <a href="https://arxiv.org/html/2408.11775v1#id3.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id3.3.id3" title="small language model"><span class="ltx_text ltx_glossary_short-plural">SLMs</span></abbr></a> have the potential to be a dominant tool in the industry. In this paper, we develop an fine-tuned Phi-2-based RAG system to serve as an oracle for telecommunication networks. The proposed system integrates semantic chunking
and re-ranking to improve the relevance and accuracy of retrieved contexts. Moreover, we fine-tune the generator with a carefully designed prompt and retrieved contexts to adapt it to the problem domain. Additionally, we utilize SelfExtend to significantly extend the model’s context window, enabling it to process longer sequences without fine-tuning. Experimental results show that our approach competes with larger state-of-the-art LLMs and also offers significant efficiency advantages, making it suitable for deployment on edge devices. Our approach prioritizes transparency and explainability, offering a framework that allows for scrutiny, understanding, and further development in this field. Maintaining transparency through open-source models is particularly important in telecom, where it fosters trust and facilitates community contributions.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We believe the developed system can serve as a foundation for other downstream telecommunication tasks. Future work can also explore optimizing the embedding model, better integration of structured data such as tables and graphs, and further enhancements to the RAG framework to continue advancing the capabilities of <a href="https://arxiv.org/html/2408.11775v1#id2.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2408.11775v1#id2.2.id2" title="large language model"><span class="ltx_text ltx_glossary_short-plural">LLMs</span></abbr></a> in telecom applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Bubeck <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Sparks of artificial general intelligence: Early experiments with GPT-4,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">CoRR</em>, vol. abs/2303.12712, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Maatouk, N. Piovesan, F. Ayed, A. D. Domenico, and M. Debbah, “Large language models for telecom: Forthcoming impact on the industry,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Commun. Mag</em>, pp. 1–7, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Zhang, H. Du, Y. Liu, D. Niyato, J. Kang, S. Sun, X. Shen, and H. V. Poor, “Interactive AI with retrieval-augmented generation for next generation networking,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Netw</em>, pp. 1–1, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
O. Erak, O. Alhussein, S. Naser, N. Alabbasi, D. Mi, and S. Muhaidat, “Large language model-driven curriculum design for mobile networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">CoRR</em>, vol. abs/2405.18039, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Yao, X. Xu, and Y. Liu, “Large language model unlearning,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">CoRR</em>, vol. abs/2310.10683, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, “A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, vol. abs/2311.05232, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N. Piovesan, A. D. Domenico, and F. Ayed, “Telecom language models: Must they be large?” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, vol. abs/2403.04666, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Javaheripi and S. Bubeck, “Phi-2: The surprising power of small language models,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</span>, Dec. 2023, accessed: 2024-8-19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
G. Team <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">et al.</em>, “Gemini: A family of highly capable multimodal models,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">CoRR</em>, vol. abs/2312.11805, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H. Touvron <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">et al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2">CoRR</em>, vol. abs/2307.09288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Maatouk, F. Ayed, N. Piovesan, A. D. Domenico, M. Debbah, and Z.-Q. Luo, “TeleQnA: A benchmark dataset to assess large language models telecommunications knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, vol. abs/2310.15051, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.-L. Bornea, F. Ayed, A. D. Domenico, N. Piovesan, and A. Maatouk, “Telco-RAG: Navigating the challenges of retrieval-augmented language models for telecommunications,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, vol. abs/2404.15939, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Jin, X. Han, J. Yang, Z. Jiang, Z. Liu, C.-Y. Chang, H. Chen, and X. Hu, “LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, vol. abs/2401.01325, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I. Karim, K. S. Mubasshir, M. M. Rahman, and E. Bertino, “SPEC5G: A dataset for 5G cellular network protocol analysis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proc. IJCNLP-AACL</em>, 2023, pp. 20–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Karapantelakis, M. Thakur, A. Nikou, F. Moradi, C. Orlog, F. Gaim, H. Holm, D. D. Nimara, and V. Huang, “Using large language models to understand telecom standards,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, vol. abs/2404.02929, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Nikbakht, M. Benzaghta, and G. Geraci, “TSpec-LLM: An open-source dataset for LLM understanding of 3GPP specifications,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CoRR</em>, vol. abs/2406.01768, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
P. Gajjar and V. K. Shah, “ORAN-Bench-13K: An open source benchmark for assessing LLMs in open radio access networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, vol. abs/2407.06245, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. Ahmed, N. Piovesan, A. D. Domenico, and S. Choudhury, “Linguistic intelligence in large language models for telecommunications,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, vol. abs/2402.15818, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L. Bariah, H. Zou, Q. Zhao, B. Mouhouche, F. Bader, and M. Debbah, “Understanding telecom language through large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proc. IEEE Globecom</em>, 2023, pp. 6542–6547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Liu, “LlamaIndex,” 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/jerryjliu/llama_index</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Xiao, Z. Liu, P. Zhang, N. Muennighoff, D. Lian, and J.-Y. Nie, “C-Pack: Packaged resources to advance general chinese embedding,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">CoRR</em>, vol. abs/2309.07597, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proc. ICML</em>, 2020, pp. 1597–1607.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Chroma, “ChromaDB,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.trychroma.com/</span>, accessed: 08/05/2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Inferless, “Ms marco: ms-marco-minilm-l-6-v2,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2</span>, 2023, accessed: 2024-08-18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “LoRA: Low-rank adaptation of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proc. ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Aghajanyan, S. Gupta, and L. Zettlemoyer, “Intrinsic dimensionality explains the effectiveness of language model fine-tuning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proc. ACL-IJCNLP</em>, 2021, pp. 7319–7328.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
N. Alabbasi and O. Erak, “Specializing large language models for telecom networks,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/Nouf-Alabbasi/oKUmura_AI_Telecom_challenge</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Aug 21 17:00:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
