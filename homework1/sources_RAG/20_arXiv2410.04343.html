<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Inference Scaling for Long-Context Retrieval Augmented Generation</title>
<!--Generated on Sun Oct  6 03:38:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04343v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S2" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S2.SS1" title="In 2 Related Work ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Long-Context LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S2.SS2" title="In 2 Related Work ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>In-Context Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S2.SS3" title="In 2 Related Work ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Inference Scaling Strategies for RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3.SS1" title="In 3 Inference Scaling Strategies for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3.SS2" title="In 3 Inference Scaling Strategies for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Demonstration-Based RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3.SS3" title="In 3 Inference Scaling Strategies for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Iterative Demonstration-Based RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>RAG Performance and Inference Computation Scale</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.SS1" title="In 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Fixed Budget Optimal Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.SS2" title="In 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Overall Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.SS3" title="In 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Inference Scaling Laws for RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.SS4" title="In 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Parameter-Specific Scaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Inference Computation Allocation for Long-Context RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.SS1" title="In 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Formulation and Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.SS2" title="In 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Validating the Computation Allocation Model for RAG</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.SS2.SSS0.Px1" title="In 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Ablation Study.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.SS2.SSS0.Px2" title="In 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Domain Generalization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.SS2.SSS0.Px3" title="In 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Length Extrapolation.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S6" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S6.SS0.SSS0.Px1" title="In 6 Discussion ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S6.SS0.SSS0.Px2" title="In 6 Discussion ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Error Analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S6.SS0.SSS0.Px3" title="In 6 Discussion ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Long-Context Modeling.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S7" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Retrieval Quality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A2" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Chain-of-Thought vs. IterDRAG.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional RAG Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A4" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Results on Inference Scaling Laws for RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Additional Results on Computation Allocation Model for RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A6" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Error Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A7" title="In Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Implementation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\correspondingauthor</span>
<p class="ltx_p" id="p1.2">zhenrui3@illinois.edu, hlz@google.com

<span class="ltx_ERROR undefined" id="p1.2.1">\reportnumber</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Inference Scaling for Long-Context Retrieval Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenrui Yue
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">University of Illinois Urbana-Champaign
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Honglei Zhuang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Equal contribution
</span>
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aijun Bai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kai Hui
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rolf Jagerman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hansi Zeng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Work done while at Google DeepMind
</span>
<span class="ltx_contact ltx_role_affiliation">University of Massachusetts Amherst
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhen Qin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dong Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Illinois Urbana-Champaign
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuanhui Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Bendersky
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs’ ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the <em class="ltx_emph ltx_font_italic" id="id1.id1.1">scaling of inference computation</em> when optimally configured? (2) Can we predict the optimal <em class="ltx_emph ltx_font_italic" id="id1.id1.2">test-time compute allocation</em> for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the <em class="ltx_emph ltx_font_italic" id="id1.id1.3">inference scaling laws for RAG</em>. Building on this, we further develop the <em class="ltx_emph ltx_font_italic" id="id1.id1.4">computation allocation model</em> to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Inference scaling, Retrieval augmented generation, Long-context LLMs
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Long-context large language models (LLMs) are designed to handle extended input sequences, enabling them to process and understand longer context (e.g., Gemini 1.5 Pro with up to 2M tokens) <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib1" title="">2023</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib57" title="">2023</a>; Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib49" title="">2024</a>)</cite>. Combined with increased inference computation, long-context LLMs demonstrate improved performance across various downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib2" title="">Agarwal et al., </a>; Snell et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib54" title="">2024</a>)</cite>. For example, many-shot in-context learning (ICL) can match the performance of supervised fine-tuning by providing extensive in-context examples <cite class="ltx_cite ltx_citemacro_citep">(Bertsch et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib5" title="">2024</a>)</cite>. Particularly for knowledge-intensive tasks that leverage retrieval augmented generation (RAG), increasing the quantity or size of retrieved documents up to a certain threshold consistently enhances the performance <cite class="ltx_cite ltx_citemacro_citep">(Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib48" title="">2023</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib65" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S1.F1.g1" src="extracted/5904574/figures/perf_vs_length_musique.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.2.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.2.1" style="font-size:90%;">Normalized performance vs. effective context lengths on MuSiQue. Each line represents a fixed configuration, scaled by adjusting the number of documents. Red dots and dash lines represent the optimal configurations and their fitting results. Standard RAG plateaus early at <math alttext="10^{4}" class="ltx_Math" display="inline" id="S1.F1.2.1.m1.1"><semantics id="S1.F1.2.1.m1.1b"><msup id="S1.F1.2.1.m1.1.1" xref="S1.F1.2.1.m1.1.1.cmml"><mn id="S1.F1.2.1.m1.1.1.2" xref="S1.F1.2.1.m1.1.1.2.cmml">10</mn><mn id="S1.F1.2.1.m1.1.1.3" xref="S1.F1.2.1.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S1.F1.2.1.m1.1c"><apply id="S1.F1.2.1.m1.1.1.cmml" xref="S1.F1.2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.2.1.m1.1.1.1.cmml" xref="S1.F1.2.1.m1.1.1">superscript</csymbol><cn id="S1.F1.2.1.m1.1.1.2.cmml" type="integer" xref="S1.F1.2.1.m1.1.1.2">10</cn><cn id="S1.F1.2.1.m1.1.1.3.cmml" type="integer" xref="S1.F1.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.1.m1.1d">10^{4}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.1.m1.1e">10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> tokens, in contrast, DRAG and IterDRAG show near-linear improvement as the effective context length grows.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Previous studies on inference scaling for RAG focus on expanding the retrieved knowledge by increasing the number or lengths of retrieved documents <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib65" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib22" title="">2024</a>; Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib52" title="">2024</a>)</cite>. However, only emphasizing on the knowledge quantity without providing further guidance presents certain limitations. On one hand, current long-context LLMs still have limited ability to effectively locate relevant information in ultra-long sequences upon challenging tasks <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib34" title="">2024</a>; Kuratov et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib28" title="">2024</a>)</cite>. For instance, the optimal performance of long-context LLMs is often achieved without fully utilizing the maximum length <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib2" title="">Agarwal et al., </a>)</cite>. On the other hand, numerous studies show that retrieving over soft thresholds (e.g., top-10 documents) leads to a performance plateau and may even cause declines <cite class="ltx_cite ltx_citemacro_citep">(Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib48" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib30" title="">2024a</a>; Kuratov et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib28" title="">2024</a>)</cite>. Such performance drops may be traced back to the increased noise within context, which causes distraction and adversely affects generation <cite class="ltx_cite ltx_citemacro_citep">(Yoran et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib68" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib71" title="">2024</a>)</cite>. As a result, inference scaling of long-context RAG remains challenging for existing methods.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we leverage a broader range of strategies to comprehensively explore how RAG benefits from the scaling of inference computation. A straightforward strategy is <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">demonstration-based RAG</em> (DRAG), where multiple RAG examples are provided as demonstrations to utilize the long-context capabilities of LLMs <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib7" title="">2020</a>)</cite>. DRAG allows models to learn (in-context) how to locate relevant information and apply it to response generation<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Different from in-context RAG that prepends documents / QA examples <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib47" title="">2023</a>; Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib48" title="">2023</a>)</cite>, we leverage multiple examples comprising of documents, questions and answers to demonstrate the task.</span></span></span>. Nevertheless, the quality of one-step retrieval varies across tasks and often fails to provide sufficient information. Inspired by iterative methods <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib59" title="">2023</a>; Yoran et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib68" title="">2024</a>)</cite>, we develop <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">iterative demonstration-based RAG</em> (IterDRAG). IterDRAG learns to decompose input queries into simpler sub-queries and answer them using interleaved retrieval. By iteratively retrieving and generating upon sub-queries, LLMs construct reasoning chains that bridge the compositionality gap for multi-hop queries. Together, these strategies provide additional flexibility in scaling inference computation for RAG, allowing long-context LLMs to more effectively address complex knowledge-intensive queries.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Building on these strategies, we investigate multiple ways to scale up inference computation. Here, we measure computation by considering the total number of input tokens across all iterations, referred to as the <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">effective context length</em>. In DRAG, scaling the effective context length can be done by increasing two inference parameters: the number of retrieved documents and in-context examples. In IterDRAG, test-time compute can be further extended by introducing additional generation steps. Since different combinations of inference parameters result in varied allocations of computational resources, our goal is to establish the relationship between RAG <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">performance</em>, different <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">scales</em> and <em class="ltx_emph ltx_font_italic" id="S1.p4.1.4">allocations</em> of inference computation. Through extensive experiments on benchmark QA datasets, we demonstrate an almost linear relationship between RAG performance and the scale of effective context length by combining both RAG strategies, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F1" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> (right). Moreover, our RAG strategies exhibit improved performance than merely scaling the number of documents, achieving state-of-the-art performance with the compact Gemini 1.5 Flash (See evaluation in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F2" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Drawing from our observations, we examine the relationship between RAG performance and inference computation, which we quantify as the <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">inference scaling laws for RAG</em>. These observed inference scaling laws reveal that RAG performance consistently improves with the expansion of the effective context length under optimal configurations. Consequently, we take a deeper dive into modeling RAG performance with respect to various inference computation <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">allocations</em>. Our goal is to predict the optimal set of inference parameters that maximize the performance across different RAG tasks. To achieve this, we quantitatively model the relationship between RAG performance and varying inference configurations with the <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">computation allocation model for RAG</em>. Using the estimated computation allocation model, the optimal configurations can be empirically determined and generalize well for various scenarios, thereby maximizing the utilization of the computation budget. We summarize our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We systematically investigate inference scaling for long-context RAG, for which we introduce two scaling strategies, DRAG and IterDRAG, to effectively scale inference computation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We comprehensively evaluate DRAG and IterDRAG, where they not only achieve state-of-the-art performance, but also exhibit superior scaling properties compared to solely increasing the quantity of documents.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Through extensive experiments on benchmark QA datasets, we demonstrate that when test-time compute is optimally allocated, long-context RAG performance can scale almost linearly with the increasing order of magnitude of the computation budget.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We quantitatively model the relationship between RAG performance and different inference parameters, deriving the computation allocation model. This model aligns closely with our experimental results and generalize well across scenarios, providing practical guidance for optimal computation allocation in long-context RAG.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S1.F2.g1" src="extracted/5904574/figures/intro_barchart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">Evaluation accuracy of Gemini 1.5 Flash using different methods: zero-shot QA, many-shot QA, RAG (with an optimal number of documents), DRAG and IterDRAG on benchmark QA datasets. By scaling up inference compute (up to 5M tokens), DRAG consistently outperforms baselines, while IterDRAG improves upon DRAG through interleaving retrieval and iterative generation.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Long-Context LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Long-context large language models (LLMs) are designed to utilize extensive context and thereby improve their generative capabilities. Early works in extending context lengths involve sparse / low-rank kernels to reduce memory requirements <cite class="ltx_cite ltx_citemacro_citep">(Kitaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib26" title="">2019</a>; Beltagy et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib4" title="">2020</a>; Zaheer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib70" title="">2020</a>; Choromanski et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib9" title="">2020</a>)</cite>. In addition, recurrent and state space models (SSMs) are proposed as efficient substitutes for transformer-based models <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib13" title="">2021</a>; Gu and Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib12" title="">2023</a>; Peng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib43" title="">2023a</a>; Beck et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib3" title="">2024</a>)</cite>. For causal LLMs, extrapolation and interpolation methods have proven effective in expanding context window lengths <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib46" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib8" title="">2023</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib56" title="">2023</a>; Peng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib44" title="">2023b</a>)</cite>. Recent advancements in efficient attention methods <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib10" title="">2022</a>; Jacobs et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib20" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib36" title="">2023</a>)</cite> further enable LLMs to train and infer upon input sequences comprising millions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib1" title="">2023</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib57" title="">2023</a>; Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib49" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>In-Context Learning</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In-context learning (ICL) offers a computationally efficient approach to enhance model performance at inference time by conditioning on a few demonstrations of the task <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib7" title="">2020</a>)</cite>. To further improve ICL performance, existing works focuses on pretraining strategies that optimize the language models to learn in-context <cite class="ltx_cite ltx_citemacro_citep">(Min et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib42" title="">2022</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib62" title="">2023</a>; Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib14" title="">2023</a>)</cite>. In addition, selective usage of few-shot examples are shown to be helpful for enhancing downstream task performance <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib37" title="">2022</a>; Rubin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib50" title="">2022</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib60" title="">2024</a>)</cite>. Notably, reformatting or finding optimal ordering of in-context examples also improves ICL performance effectiveness <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib40" title="">2022</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib64" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib38" title="">2024a</a>)</cite>. With the emergence of long-context LLMs <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib1" title="">2023</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib57" title="">2023</a>; Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib49" title="">2024</a>)</cite>, scaling the number of examples becomes possible in ICL <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib33" title="">2023</a>); Bertsch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib5" title="">2024</a>); <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib2" title="">Agarwal et al. </a></cite>. For instance, <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib2" title="">Agarwal et al. </a></cite> show that many-shot ICL can mitigate pretraining biases within LLMs and thus improves ICL performance across various tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Retrieval Augmented Generation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Retrieval augmented generation (RAG) improves language model performance by incorporating relevant knowledge from external sources <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib32" title="">2020</a>; Guu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib16" title="">2020</a>; Karpukhin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib24" title="">2020</a>)</cite>. In contrast to naïve RAG, optimizing the retrieval stage can effectively enhance context relevance and improve generation performance <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib41" title="">2023</a>; Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib59" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib21" title="">2023</a>; Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib53" title="">2024</a>; Sarthi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib51" title="">2024</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib35" title="">2024</a>)</cite>. An example is REPLUG, in which <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib53" title="">2024</a>)</cite> leverage LLM as supervision to learn a dense retriever model. In addition, encoding documents can increase knowledge retrieval and improve generation capabilities <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib25" title="">2019</a>; Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib18" title="">2021</a>; Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib6" title="">2022</a>; Izacard et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib19" title="">2023</a>)</cite>. For instance, <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib18" title="">2021</a>)</cite> leverages fusion-in-decoder architecture to encode multiple question-passage pairs while maintaining the model efficiency. Alternatively, selectively utilizing knowledge from the documents improves the robustness of LLMs against irrelevant context <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib69" title="">2023</a>; Yoran et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib68" title="">2024</a>; Yan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib66" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib71" title="">2024</a>)</cite>. For example, RAFT proposes to train language models with negative documents to improve generation quality and relevance <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib71" title="">2024</a>)</cite>. Concurrent to our work, long-document retrieval and datastore scaling are proposed to optimize RAG performance <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib22" title="">2024</a>; Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib52" title="">2024</a>)</cite>. Despite such progress, inference scaling remains under-explored for long-context RAG methods in knowledge-intensive settings. To bridge this gap, we investigate how variations in inference computation impact RAG performance, with the goal of optimizing test-time compute allocation in downstream tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Inference Scaling Strategies for RAG</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We measure inference computation with <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">effective context length</span>, defined as the total number of input tokens across all iterations before the LLM outputs the final answer. For most methods that only call the LLM once, the effective context length is equivalent to the number of input tokens in the prompt and is limited by the context window limit of the LLM. For methods that iteratively call the LLM, the effective context length can be extended indefinitely depending on the strategy. We exclude output tokens and retrieval costs from our analysis, as LLMs typically generate significantly fewer tokens (fewer than 10) in knowledge-intensive tasks. Additionally, retrieval is generally much less computationally expensive than LLM inference, especially with scalable matching methods <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib55" title="">2024</a>)</cite>. Our objective is to understand how RAG performance changes as we scale up inference computation. In demonstration-based RAG (DRAG), we achieve such scaling by incorporating both extensive documents and in-context examples. For further scaling, we increase generation steps through iterative demonstration-based RAG (IterDRAG). We introduce both strategies below.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Demonstration-Based RAG</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Demonstration-based RAG (DRAG) leverages in-context learning to exploit the capabilities of long-context LLMs by directly generating answers from an extended input context. DRAG builds upon naïve RAG and integrates both documents and in-context examples into the input prompt. This expanded context allows the model to generate answers to the input query within a single inference request (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3.F3" title="In 3.2 Demonstration-Based RAG ‣ 3 Inference Scaling Strategies for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> left). For both in-context examples and the test-time query, we employ a retrieval model to select the top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved documents from a large corpus (e.g., Wikipedia). We reverse the order of the retrieved documents, placing higher-ranked documents closer to the query <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib39" title="">2024b</a>)</cite>. As we use instruction-tuned LLMs, we design a similar prompt template following <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib2" title="">Agarwal et al. </a></cite> and align the formatting with prefixes for retrieved documents, input and output (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A7" title="Appendix G Implementation ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">G</span></a>). Unlike previous works <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib47" title="">2023</a>; Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib59" title="">2023</a>)</cite>, DRAG incorporates extensive retrieved documents within the demonstrations, enabling long-context LLMs to learn to extract relevant information and answer questions using a rich input context.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S3.F3.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">DRAG vs. IterDRAG. IterDRAG breaks down the input query into sub-queries and answer them to improve the accuracy of the final answer. In test-time, IterDRAG scales the computation through multiple inference steps to decompose complex queries and retrieve documents.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Iterative Demonstration-Based RAG</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Despite access to external knowledge, complex multi-hop queries remain challenging due to the compositionality gap. To tackle this issue, we introduce iterative demonstration-based RAG (IterDRAG), which handles complex queries by decomposing the query into simpler sub-queries. For each sub-query, retrieval is performed to gather additional contextual information, which is then used to generate intermediate answers. After all sub-queries are resolved, the retrieved context, sub-queries, and their answers are combined to synthesize the final answer (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S3.F3" title="In 3.2 Demonstration-Based RAG ‣ 3 Inference Scaling Strategies for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> right).</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">While multiple existing datasets provide training data with queries and corresponding answers, sub-queries and intermediate answers are often absent. To generate in-context examples with sub-queries and intermediate answers, we prompt LLMs with constrained decoding to follow the Self-Ask format <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib47" title="">2023</a>; Koo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib27" title="">2024</a>)</cite>. In each iteration, LLMs generate either a sub-query, an intermediate answer, or the final answer. If a sub-query is generated, additional documents are retrieved and interleaved into the prompt before producing the intermediate answer. IterDRAG continues until the final answer is generated or the number of maximum iterations is reached, at which point LLM is forced to generate the final answer. We retain examples with intermediate steps and correct final answers to construct in-context demonstrations. Each example should include the retrieved documents, sub-query and answer pairs, as well as the final answer.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">During inference, in-context examples are prepended to the initial documents retrieved for the input query. Similarly, each inference request yields a sub-query, an intermediate answer, or the final answer. Upon sub-queries, additional documents are retrieved and merged with the initial ones to generate intermediate answers. In our implementation, we allow up to five iterations of query decomposition before generating the final answer. This iterative process effectively scales test-time computation, with the input tokens from all iterations summed to calculate the effective context length. IterDRAG facilitates a more granular approach by learning to: (1) decompose query into simple and manageable sub-queries; and (2) retrieve and locate relevant information to answer (sub)-queries. As such, the iterative retrieval and generation strategy helps narrowing the compositionality gap and improves knowledge extraction, thereby enhancing overall RAG performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>RAG Performance and Inference Computation Scale</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fixed Budget Optimal Performance</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">For a given budget on inference computation, i.e., a maximum effective context length <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">L</mi><mtext id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝐿</ci><ci id="S4.SS1.p1.1.m1.1.1.3a.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><mtext id="S4.SS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p1.1.m1.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>, there are multiple ways to optimize the use of computation resources through inference parameters. For example, in DRAG, we can adjust both the number of retrieved documents and in-context examples, while in the IterDRAG strategy, we additionally introduce the number of iterations for retrieval and generation. Henceforth, we use <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_θ</annotation></semantics></math> to denote all these inference parameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.11">For each input query and its ground-truth answer <math alttext="(x_{i},y_{i})\in\mathcal{X}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.2"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml"><mrow id="S4.SS1.p2.1.m1.2.2.2.2" xref="S4.SS1.p2.1.m1.2.2.2.3.cmml"><mo id="S4.SS1.p2.1.m1.2.2.2.2.3" stretchy="false" xref="S4.SS1.p2.1.m1.2.2.2.3.cmml">(</mo><msub id="S4.SS1.p2.1.m1.1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS1.p2.1.m1.1.1.1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.1.m1.2.2.2.2.4" xref="S4.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><msub id="S4.SS1.p2.1.m1.2.2.2.2.2" xref="S4.SS1.p2.1.m1.2.2.2.2.2.cmml"><mi id="S4.SS1.p2.1.m1.2.2.2.2.2.2" xref="S4.SS1.p2.1.m1.2.2.2.2.2.2.cmml">y</mi><mi id="S4.SS1.p2.1.m1.2.2.2.2.2.3" xref="S4.SS1.p2.1.m1.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.p2.1.m1.2.2.2.2.5" stretchy="false" xref="S4.SS1.p2.1.m1.2.2.2.3.cmml">)</mo></mrow><mo id="S4.SS1.p2.1.m1.2.2.3" xref="S4.SS1.p2.1.m1.2.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.2.2.4" xref="S4.SS1.p2.1.m1.2.2.4.cmml">𝒳</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><apply id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2"><in id="S4.SS1.p2.1.m1.2.2.3.cmml" xref="S4.SS1.p2.1.m1.2.2.3"></in><interval closure="open" id="S4.SS1.p2.1.m1.2.2.2.3.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2"><apply id="S4.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS1.p2.1.m1.2.2.2.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.2.2">𝑦</ci><ci id="S4.SS1.p2.1.m1.2.2.2.2.2.3.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.2.3">𝑖</ci></apply></interval><ci id="S4.SS1.p2.1.m1.2.2.4.cmml" xref="S4.SS1.p2.1.m1.2.2.4">𝒳</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">(x_{i},y_{i})\in\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.2d">( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_X</annotation></semantics></math>, we can apply the RAG inference strategy <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_f</annotation></semantics></math> parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_θ</annotation></semantics></math>. We denote the effective input context length to the LLM as <math alttext="l(x_{i};\theta)" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.2"><semantics id="S4.SS1.p2.4.m4.2a"><mrow id="S4.SS1.p2.4.m4.2.2" xref="S4.SS1.p2.4.m4.2.2.cmml"><mi id="S4.SS1.p2.4.m4.2.2.3" xref="S4.SS1.p2.4.m4.2.2.3.cmml">l</mi><mo id="S4.SS1.p2.4.m4.2.2.2" xref="S4.SS1.p2.4.m4.2.2.2.cmml">⁢</mo><mrow id="S4.SS1.p2.4.m4.2.2.1.1" xref="S4.SS1.p2.4.m4.2.2.1.2.cmml"><mo id="S4.SS1.p2.4.m4.2.2.1.1.2" stretchy="false" xref="S4.SS1.p2.4.m4.2.2.1.2.cmml">(</mo><msub id="S4.SS1.p2.4.m4.2.2.1.1.1" xref="S4.SS1.p2.4.m4.2.2.1.1.1.cmml"><mi id="S4.SS1.p2.4.m4.2.2.1.1.1.2" xref="S4.SS1.p2.4.m4.2.2.1.1.1.2.cmml">x</mi><mi id="S4.SS1.p2.4.m4.2.2.1.1.1.3" xref="S4.SS1.p2.4.m4.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.4.m4.2.2.1.1.3" xref="S4.SS1.p2.4.m4.2.2.1.2.cmml">;</mo><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">θ</mi><mo id="S4.SS1.p2.4.m4.2.2.1.1.4" stretchy="false" xref="S4.SS1.p2.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.2b"><apply id="S4.SS1.p2.4.m4.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2"><times id="S4.SS1.p2.4.m4.2.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2.2"></times><ci id="S4.SS1.p2.4.m4.2.2.3.cmml" xref="S4.SS1.p2.4.m4.2.2.3">𝑙</ci><list id="S4.SS1.p2.4.m4.2.2.1.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1"><apply id="S4.SS1.p2.4.m4.2.2.1.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.4.m4.2.2.1.1.1.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.1.2">𝑥</ci><ci id="S4.SS1.p2.4.m4.2.2.1.1.1.3.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.1.3">𝑖</ci></apply><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝜃</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.2c">l(x_{i};\theta)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.2d">italic_l ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math> and the obtained prediction as <math alttext="\hat{y}_{i}=f(x_{i};\theta)" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.2"><semantics id="S4.SS1.p2.5.m5.2a"><mrow id="S4.SS1.p2.5.m5.2.2" xref="S4.SS1.p2.5.m5.2.2.cmml"><msub id="S4.SS1.p2.5.m5.2.2.3" xref="S4.SS1.p2.5.m5.2.2.3.cmml"><mover accent="true" id="S4.SS1.p2.5.m5.2.2.3.2" xref="S4.SS1.p2.5.m5.2.2.3.2.cmml"><mi id="S4.SS1.p2.5.m5.2.2.3.2.2" xref="S4.SS1.p2.5.m5.2.2.3.2.2.cmml">y</mi><mo id="S4.SS1.p2.5.m5.2.2.3.2.1" xref="S4.SS1.p2.5.m5.2.2.3.2.1.cmml">^</mo></mover><mi id="S4.SS1.p2.5.m5.2.2.3.3" xref="S4.SS1.p2.5.m5.2.2.3.3.cmml">i</mi></msub><mo id="S4.SS1.p2.5.m5.2.2.2" xref="S4.SS1.p2.5.m5.2.2.2.cmml">=</mo><mrow id="S4.SS1.p2.5.m5.2.2.1" xref="S4.SS1.p2.5.m5.2.2.1.cmml"><mi id="S4.SS1.p2.5.m5.2.2.1.3" xref="S4.SS1.p2.5.m5.2.2.1.3.cmml">f</mi><mo id="S4.SS1.p2.5.m5.2.2.1.2" xref="S4.SS1.p2.5.m5.2.2.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.5.m5.2.2.1.1.1" xref="S4.SS1.p2.5.m5.2.2.1.1.2.cmml"><mo id="S4.SS1.p2.5.m5.2.2.1.1.1.2" stretchy="false" xref="S4.SS1.p2.5.m5.2.2.1.1.2.cmml">(</mo><msub id="S4.SS1.p2.5.m5.2.2.1.1.1.1" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1.cmml"><mi id="S4.SS1.p2.5.m5.2.2.1.1.1.1.2" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1.2.cmml">x</mi><mi id="S4.SS1.p2.5.m5.2.2.1.1.1.1.3" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.5.m5.2.2.1.1.1.3" xref="S4.SS1.p2.5.m5.2.2.1.1.2.cmml">;</mo><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">θ</mi><mo id="S4.SS1.p2.5.m5.2.2.1.1.1.4" stretchy="false" xref="S4.SS1.p2.5.m5.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.2b"><apply id="S4.SS1.p2.5.m5.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2"><eq id="S4.SS1.p2.5.m5.2.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2.2"></eq><apply id="S4.SS1.p2.5.m5.2.2.3.cmml" xref="S4.SS1.p2.5.m5.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.2.2.3.1.cmml" xref="S4.SS1.p2.5.m5.2.2.3">subscript</csymbol><apply id="S4.SS1.p2.5.m5.2.2.3.2.cmml" xref="S4.SS1.p2.5.m5.2.2.3.2"><ci id="S4.SS1.p2.5.m5.2.2.3.2.1.cmml" xref="S4.SS1.p2.5.m5.2.2.3.2.1">^</ci><ci id="S4.SS1.p2.5.m5.2.2.3.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2.3.2.2">𝑦</ci></apply><ci id="S4.SS1.p2.5.m5.2.2.3.3.cmml" xref="S4.SS1.p2.5.m5.2.2.3.3">𝑖</ci></apply><apply id="S4.SS1.p2.5.m5.2.2.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1"><times id="S4.SS1.p2.5.m5.2.2.1.2.cmml" xref="S4.SS1.p2.5.m5.2.2.1.2"></times><ci id="S4.SS1.p2.5.m5.2.2.1.3.cmml" xref="S4.SS1.p2.5.m5.2.2.1.3">𝑓</ci><list id="S4.SS1.p2.5.m5.2.2.1.1.2.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.1"><apply id="S4.SS1.p2.5.m5.2.2.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.2.2.1.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.5.m5.2.2.1.1.1.1.2.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1.2">𝑥</ci><ci id="S4.SS1.p2.5.m5.2.2.1.1.1.1.3.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.1.1.3">𝑖</ci></apply><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">𝜃</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.2c">\hat{y}_{i}=f(x_{i};\theta)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.2d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math>. A metric <math alttext="P(y_{i},\hat{y}_{i})" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.2"><semantics id="S4.SS1.p2.6.m6.2a"><mrow id="S4.SS1.p2.6.m6.2.2" xref="S4.SS1.p2.6.m6.2.2.cmml"><mi id="S4.SS1.p2.6.m6.2.2.4" xref="S4.SS1.p2.6.m6.2.2.4.cmml">P</mi><mo id="S4.SS1.p2.6.m6.2.2.3" xref="S4.SS1.p2.6.m6.2.2.3.cmml">⁢</mo><mrow id="S4.SS1.p2.6.m6.2.2.2.2" xref="S4.SS1.p2.6.m6.2.2.2.3.cmml"><mo id="S4.SS1.p2.6.m6.2.2.2.2.3" stretchy="false" xref="S4.SS1.p2.6.m6.2.2.2.3.cmml">(</mo><msub id="S4.SS1.p2.6.m6.1.1.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.1.1.1.2" xref="S4.SS1.p2.6.m6.1.1.1.1.1.2.cmml">y</mi><mi id="S4.SS1.p2.6.m6.1.1.1.1.1.3" xref="S4.SS1.p2.6.m6.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.6.m6.2.2.2.2.4" xref="S4.SS1.p2.6.m6.2.2.2.3.cmml">,</mo><msub id="S4.SS1.p2.6.m6.2.2.2.2.2" xref="S4.SS1.p2.6.m6.2.2.2.2.2.cmml"><mover accent="true" id="S4.SS1.p2.6.m6.2.2.2.2.2.2" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2.cmml"><mi id="S4.SS1.p2.6.m6.2.2.2.2.2.2.2" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2.2.cmml">y</mi><mo id="S4.SS1.p2.6.m6.2.2.2.2.2.2.1" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p2.6.m6.2.2.2.2.2.3" xref="S4.SS1.p2.6.m6.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.p2.6.m6.2.2.2.2.5" stretchy="false" xref="S4.SS1.p2.6.m6.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.2b"><apply id="S4.SS1.p2.6.m6.2.2.cmml" xref="S4.SS1.p2.6.m6.2.2"><times id="S4.SS1.p2.6.m6.2.2.3.cmml" xref="S4.SS1.p2.6.m6.2.2.3"></times><ci id="S4.SS1.p2.6.m6.2.2.4.cmml" xref="S4.SS1.p2.6.m6.2.2.4">𝑃</ci><interval closure="open" id="S4.SS1.p2.6.m6.2.2.2.3.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2"><apply id="S4.SS1.p2.6.m6.1.1.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.6.m6.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.2">𝑦</ci><ci id="S4.SS1.p2.6.m6.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS1.p2.6.m6.2.2.2.2.2.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2">subscript</csymbol><apply id="S4.SS1.p2.6.m6.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2"><ci id="S4.SS1.p2.6.m6.2.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2.1">^</ci><ci id="S4.SS1.p2.6.m6.2.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2.2.2">𝑦</ci></apply><ci id="S4.SS1.p2.6.m6.2.2.2.2.2.3.cmml" xref="S4.SS1.p2.6.m6.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.2c">P(y_{i},\hat{y}_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.2d">italic_P ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> can then be calculated based on <math alttext="y_{i}" class="ltx_Math" display="inline" id="S4.SS1.p2.7.m7.1"><semantics id="S4.SS1.p2.7.m7.1a"><msub id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">y</mi><mi id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">𝑦</ci><ci id="S4.SS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.7.m7.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\hat{y}_{i}" class="ltx_Math" display="inline" id="S4.SS1.p2.8.m8.1"><semantics id="S4.SS1.p2.8.m8.1a"><msub id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mover accent="true" id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2.2" xref="S4.SS1.p2.8.m8.1.1.2.2.cmml">y</mi><mo id="S4.SS1.p2.8.m8.1.1.2.1" xref="S4.SS1.p2.8.m8.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1">subscript</csymbol><apply id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2"><ci id="S4.SS1.p2.8.m8.1.1.2.1.cmml" xref="S4.SS1.p2.8.m8.1.1.2.1">^</ci><ci id="S4.SS1.p2.8.m8.1.1.2.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2.2">𝑦</ci></apply><ci id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">\hat{y}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.8.m8.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. To understand the relationship between RAG performance and inference computation, we sample a few different inference computation budgets. For each budget <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS1.p2.9.m9.1"><semantics id="S4.SS1.p2.9.m9.1a"><msub id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml"><mi id="S4.SS1.p2.9.m9.1.1.2" xref="S4.SS1.p2.9.m9.1.1.2.cmml">L</mi><mtext id="S4.SS1.p2.9.m9.1.1.3" xref="S4.SS1.p2.9.m9.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><apply id="S4.SS1.p2.9.m9.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.1.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.p2.9.m9.1.1.2.cmml" xref="S4.SS1.p2.9.m9.1.1.2">𝐿</ci><ci id="S4.SS1.p2.9.m9.1.1.3a.cmml" xref="S4.SS1.p2.9.m9.1.1.3"><mtext id="S4.SS1.p2.9.m9.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.9.m9.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.9.m9.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>, we find the optimal average metric <math alttext="P^{*}(L_{\text{max}})" class="ltx_Math" display="inline" id="S4.SS1.p2.10.m10.1"><semantics id="S4.SS1.p2.10.m10.1a"><mrow id="S4.SS1.p2.10.m10.1.1" xref="S4.SS1.p2.10.m10.1.1.cmml"><msup id="S4.SS1.p2.10.m10.1.1.3" xref="S4.SS1.p2.10.m10.1.1.3.cmml"><mi id="S4.SS1.p2.10.m10.1.1.3.2" xref="S4.SS1.p2.10.m10.1.1.3.2.cmml">P</mi><mo id="S4.SS1.p2.10.m10.1.1.3.3" xref="S4.SS1.p2.10.m10.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS1.p2.10.m10.1.1.2" xref="S4.SS1.p2.10.m10.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.10.m10.1.1.1.1" xref="S4.SS1.p2.10.m10.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.10.m10.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.10.m10.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.10.m10.1.1.1.1.1" xref="S4.SS1.p2.10.m10.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.10.m10.1.1.1.1.1.2" xref="S4.SS1.p2.10.m10.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.SS1.p2.10.m10.1.1.1.1.1.3" xref="S4.SS1.p2.10.m10.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS1.p2.10.m10.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.10.m10.1b"><apply id="S4.SS1.p2.10.m10.1.1.cmml" xref="S4.SS1.p2.10.m10.1.1"><times id="S4.SS1.p2.10.m10.1.1.2.cmml" xref="S4.SS1.p2.10.m10.1.1.2"></times><apply id="S4.SS1.p2.10.m10.1.1.3.cmml" xref="S4.SS1.p2.10.m10.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.10.m10.1.1.3.1.cmml" xref="S4.SS1.p2.10.m10.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.10.m10.1.1.3.2.cmml" xref="S4.SS1.p2.10.m10.1.1.3.2">𝑃</ci><times id="S4.SS1.p2.10.m10.1.1.3.3.cmml" xref="S4.SS1.p2.10.m10.1.1.3.3"></times></apply><apply id="S4.SS1.p2.10.m10.1.1.1.1.1.cmml" xref="S4.SS1.p2.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.10.m10.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.10.m10.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.10.m10.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.10.m10.1.1.1.1.1.2">𝐿</ci><ci id="S4.SS1.p2.10.m10.1.1.1.1.1.3a.cmml" xref="S4.SS1.p2.10.m10.1.1.1.1.1.3"><mtext id="S4.SS1.p2.10.m10.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.10.m10.1.1.1.1.1.3">max</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.10.m10.1c">P^{*}(L_{\text{max}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.10.m10.1d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )</annotation></semantics></math> achievable within this budget by enumerating different <math alttext="\theta\in\Theta" class="ltx_Math" display="inline" id="S4.SS1.p2.11.m11.1"><semantics id="S4.SS1.p2.11.m11.1a"><mrow id="S4.SS1.p2.11.m11.1.1" xref="S4.SS1.p2.11.m11.1.1.cmml"><mi id="S4.SS1.p2.11.m11.1.1.2" xref="S4.SS1.p2.11.m11.1.1.2.cmml">θ</mi><mo id="S4.SS1.p2.11.m11.1.1.1" xref="S4.SS1.p2.11.m11.1.1.1.cmml">∈</mo><mi id="S4.SS1.p2.11.m11.1.1.3" mathvariant="normal" xref="S4.SS1.p2.11.m11.1.1.3.cmml">Θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.11.m11.1b"><apply id="S4.SS1.p2.11.m11.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1"><in id="S4.SS1.p2.11.m11.1.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1.1"></in><ci id="S4.SS1.p2.11.m11.1.1.2.cmml" xref="S4.SS1.p2.11.m11.1.1.2">𝜃</ci><ci id="S4.SS1.p2.11.m11.1.1.3.cmml" xref="S4.SS1.p2.11.m11.1.1.3">Θ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.11.m11.1c">\theta\in\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.11.m11.1d">italic_θ ∈ roman_Θ</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P^{*}(L_{\text{max}}):=\max_{\theta\in\Theta}\Big{\{}\frac{1}{|\mathcal{X}|}%
\sum_{i}P\big{(}y_{i},f(x_{i};\theta)\big{)}\Big{|}\forall i,l(x_{i};\theta)%
\leq L_{\text{max}}\Big{\}}." class="ltx_Math" display="block" id="S4.E1.m1.4"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.cmml"><msup id="S4.E1.m1.4.4.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.3.cmml"><mi id="S4.E1.m1.4.4.1.1.1.3.2" xref="S4.E1.m1.4.4.1.1.1.3.2.cmml">P</mi><mo id="S4.E1.m1.4.4.1.1.1.3.3" xref="S4.E1.m1.4.4.1.1.1.3.3.cmml">∗</mo></msup><mo id="S4.E1.m1.4.4.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E1.m1.4.4.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.E1.m1.4.4.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.E1.m1.4.4.1.1.1.1.1.3" rspace="0.278em" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.4" rspace="0.278em" xref="S4.E1.m1.4.4.1.1.4.cmml">:=</mo><mrow id="S4.E1.m1.4.4.1.1.3.2" xref="S4.E1.m1.4.4.1.1.3.3.cmml"><munder id="S4.E1.m1.4.4.1.1.2.1.1" xref="S4.E1.m1.4.4.1.1.2.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.2.1.1.2" xref="S4.E1.m1.4.4.1.1.2.1.1.2.cmml">max</mi><mrow id="S4.E1.m1.4.4.1.1.2.1.1.3" xref="S4.E1.m1.4.4.1.1.2.1.1.3.cmml"><mi id="S4.E1.m1.4.4.1.1.2.1.1.3.2" xref="S4.E1.m1.4.4.1.1.2.1.1.3.2.cmml">θ</mi><mo id="S4.E1.m1.4.4.1.1.2.1.1.3.1" xref="S4.E1.m1.4.4.1.1.2.1.1.3.1.cmml">∈</mo><mi id="S4.E1.m1.4.4.1.1.2.1.1.3.3" mathvariant="normal" xref="S4.E1.m1.4.4.1.1.2.1.1.3.3.cmml">Θ</mi></mrow></munder><mo id="S4.E1.m1.4.4.1.1.3.2a" xref="S4.E1.m1.4.4.1.1.3.3.cmml">⁡</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2" xref="S4.E1.m1.4.4.1.1.3.3.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.2" maxsize="160%" minsize="160%" xref="S4.E1.m1.4.4.1.1.3.3.cmml">{</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.4" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.cmml"><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.cmml"><mfrac id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mn id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml">1</mn><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.1.3.1" stretchy="false" xref="S4.E1.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">𝒳</mi><mo id="S4.E1.m1.1.1.1.3.2" stretchy="false" xref="S4.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.3.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.cmml"><munder id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.2" movablelimits="false" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.2.cmml">∑</mo><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.3.cmml">i</mi></munder><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.4" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.4.cmml">P</mi><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.3.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.3.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.3" maxsize="120%" minsize="120%" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.3.cmml">(</mo><msub id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.4" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.3.cmml">,</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.3.cmml">f</mi><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.2.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.2.cmml">(</mo><msub id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.2.cmml">;</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">θ</mi><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.4" stretchy="false" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.5" maxsize="120%" minsize="120%" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo fence="false" id="S4.E1.m1.4.4.1.1.3.2.2.1.4.5" mathsize="160%" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.5.cmml">|</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.3.cmml"><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.1" rspace="0.167em" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.1.cmml">∀</mo><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.2.cmml">i</mi></mrow><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.3.cmml">,</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.3.cmml">l</mi><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.2.cmml"><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.2.cmml">(</mo><msub id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.2.cmml">;</mo><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">θ</mi><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.4" stretchy="false" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.3.2.2.1.5" xref="S4.E1.m1.4.4.1.1.3.2.2.1.5.cmml">≤</mo><msub id="S4.E1.m1.4.4.1.1.3.2.2.1.6" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2.2.1.6.2" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.2.cmml">L</mi><mtext id="S4.E1.m1.4.4.1.1.3.2.2.1.6.3" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.3a.cmml">max</mtext></msub></mrow><mo id="S4.E1.m1.4.4.1.1.3.2.2.3" maxsize="160%" minsize="160%" xref="S4.E1.m1.4.4.1.1.3.3.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.2" lspace="0em" xref="S4.E1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.1.1.cmml" xref="S4.E1.m1.4.4.1"><csymbol cd="latexml" id="S4.E1.m1.4.4.1.1.4.cmml" xref="S4.E1.m1.4.4.1.1.4">assign</csymbol><apply id="S4.E1.m1.4.4.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"><times id="S4.E1.m1.4.4.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.2"></times><apply id="S4.E1.m1.4.4.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.3">superscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.3.2">𝑃</ci><times id="S4.E1.m1.4.4.1.1.1.3.3.cmml" xref="S4.E1.m1.4.4.1.1.1.3.3"></times></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2">𝐿</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.3a.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3"><mtext id="S4.E1.m1.4.4.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3">max</mtext></ci></apply></apply><apply id="S4.E1.m1.4.4.1.1.3.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2"><apply id="S4.E1.m1.4.4.1.1.2.1.1.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.2.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1">subscript</csymbol><max id="S4.E1.m1.4.4.1.1.2.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1.2"></max><apply id="S4.E1.m1.4.4.1.1.2.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1.3"><in id="S4.E1.m1.4.4.1.1.2.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1.3.1"></in><ci id="S4.E1.m1.4.4.1.1.2.1.1.3.2.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1.3.2">𝜃</ci><ci id="S4.E1.m1.4.4.1.1.2.1.1.3.3.cmml" xref="S4.E1.m1.4.4.1.1.2.1.1.3.3">Θ</ci></apply></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1"><leq id="S4.E1.m1.4.4.1.1.3.2.2.1.5.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.5"></leq><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.4.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4"><csymbol cd="latexml" id="S4.E1.m1.4.4.1.1.3.2.2.1.4.5.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.5">conditional</csymbol><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2"><times id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.3"></times><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><divide id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1"></divide><cn id="S4.E1.m1.1.1.3.cmml" type="integer" xref="S4.E1.m1.1.1.3">1</cn><apply id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.3"><abs id="S4.E1.m1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.3.1"></abs><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝒳</ci></apply></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2"><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3">subscript</csymbol><sum id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.2"></sum><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.3.3">𝑖</ci></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2"><times id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.3"></times><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.4.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.4">𝑃</ci><interval closure="open" id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2"><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2"><times id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.2"></times><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.3">𝑓</ci><list id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1"><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.2.2.2.2.2.2.2.1.1.1.3">𝑖</ci></apply><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝜃</ci></list></apply></interval></apply></apply></apply><list id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2"><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1"><csymbol cd="latexml" id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.1">for-all</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.3.3.1.1.2">𝑖</ci></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2"><times id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.2"></times><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.3">𝑙</ci><list id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1"><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.4.4.2.2.1.1.1.3">𝑖</ci></apply><ci id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">𝜃</ci></list></apply></list></apply><apply id="S4.E1.m1.4.4.1.1.3.2.2.1.6.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.2.2.1.6.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.6.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.2">𝐿</ci><ci id="S4.E1.m1.4.4.1.1.3.2.2.1.6.3a.cmml" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.3"><mtext id="S4.E1.m1.4.4.1.1.3.2.2.1.6.3.cmml" mathsize="70%" xref="S4.E1.m1.4.4.1.1.3.2.2.1.6.3">max</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">P^{*}(L_{\text{max}}):=\max_{\theta\in\Theta}\Big{\{}\frac{1}{|\mathcal{X}|}%
\sum_{i}P\big{(}y_{i},f(x_{i};\theta)\big{)}\Big{|}\forall i,l(x_{i};\theta)%
\leq L_{\text{max}}\Big{\}}.</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.4d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ) := roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT { divide start_ARG 1 end_ARG start_ARG | caligraphic_X | end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_P ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ ) ) | ∀ italic_i , italic_l ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ ) ≤ italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p2.20">Our goal is to establish the relationship between the inference computation budget <math alttext="L_{max}" class="ltx_Math" display="inline" id="S4.SS1.p2.12.m1.1"><semantics id="S4.SS1.p2.12.m1.1a"><msub id="S4.SS1.p2.12.m1.1.1" xref="S4.SS1.p2.12.m1.1.1.cmml"><mi id="S4.SS1.p2.12.m1.1.1.2" xref="S4.SS1.p2.12.m1.1.1.2.cmml">L</mi><mrow id="S4.SS1.p2.12.m1.1.1.3" xref="S4.SS1.p2.12.m1.1.1.3.cmml"><mi id="S4.SS1.p2.12.m1.1.1.3.2" xref="S4.SS1.p2.12.m1.1.1.3.2.cmml">m</mi><mo id="S4.SS1.p2.12.m1.1.1.3.1" xref="S4.SS1.p2.12.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.12.m1.1.1.3.3" xref="S4.SS1.p2.12.m1.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p2.12.m1.1.1.3.1a" xref="S4.SS1.p2.12.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.12.m1.1.1.3.4" xref="S4.SS1.p2.12.m1.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.12.m1.1b"><apply id="S4.SS1.p2.12.m1.1.1.cmml" xref="S4.SS1.p2.12.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m1.1.1.1.cmml" xref="S4.SS1.p2.12.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.12.m1.1.1.2.cmml" xref="S4.SS1.p2.12.m1.1.1.2">𝐿</ci><apply id="S4.SS1.p2.12.m1.1.1.3.cmml" xref="S4.SS1.p2.12.m1.1.1.3"><times id="S4.SS1.p2.12.m1.1.1.3.1.cmml" xref="S4.SS1.p2.12.m1.1.1.3.1"></times><ci id="S4.SS1.p2.12.m1.1.1.3.2.cmml" xref="S4.SS1.p2.12.m1.1.1.3.2">𝑚</ci><ci id="S4.SS1.p2.12.m1.1.1.3.3.cmml" xref="S4.SS1.p2.12.m1.1.1.3.3">𝑎</ci><ci id="S4.SS1.p2.12.m1.1.1.3.4.cmml" xref="S4.SS1.p2.12.m1.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.12.m1.1c">L_{max}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.12.m1.1d">italic_L start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math> and the best possible performance within this budget <math alttext="P^{*}(L_{\text{max}})" class="ltx_Math" display="inline" id="S4.SS1.p2.13.m2.1"><semantics id="S4.SS1.p2.13.m2.1a"><mrow id="S4.SS1.p2.13.m2.1.1" xref="S4.SS1.p2.13.m2.1.1.cmml"><msup id="S4.SS1.p2.13.m2.1.1.3" xref="S4.SS1.p2.13.m2.1.1.3.cmml"><mi id="S4.SS1.p2.13.m2.1.1.3.2" xref="S4.SS1.p2.13.m2.1.1.3.2.cmml">P</mi><mo id="S4.SS1.p2.13.m2.1.1.3.3" xref="S4.SS1.p2.13.m2.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS1.p2.13.m2.1.1.2" xref="S4.SS1.p2.13.m2.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.13.m2.1.1.1.1" xref="S4.SS1.p2.13.m2.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.13.m2.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.13.m2.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.13.m2.1.1.1.1.1" xref="S4.SS1.p2.13.m2.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.13.m2.1.1.1.1.1.2" xref="S4.SS1.p2.13.m2.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.SS1.p2.13.m2.1.1.1.1.1.3" xref="S4.SS1.p2.13.m2.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS1.p2.13.m2.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.13.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.13.m2.1b"><apply id="S4.SS1.p2.13.m2.1.1.cmml" xref="S4.SS1.p2.13.m2.1.1"><times id="S4.SS1.p2.13.m2.1.1.2.cmml" xref="S4.SS1.p2.13.m2.1.1.2"></times><apply id="S4.SS1.p2.13.m2.1.1.3.cmml" xref="S4.SS1.p2.13.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m2.1.1.3.1.cmml" xref="S4.SS1.p2.13.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.13.m2.1.1.3.2.cmml" xref="S4.SS1.p2.13.m2.1.1.3.2">𝑃</ci><times id="S4.SS1.p2.13.m2.1.1.3.3.cmml" xref="S4.SS1.p2.13.m2.1.1.3.3"></times></apply><apply id="S4.SS1.p2.13.m2.1.1.1.1.1.cmml" xref="S4.SS1.p2.13.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.13.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.13.m2.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.13.m2.1.1.1.1.1.2">𝐿</ci><ci id="S4.SS1.p2.13.m2.1.1.1.1.1.3a.cmml" xref="S4.SS1.p2.13.m2.1.1.1.1.1.3"><mtext id="S4.SS1.p2.13.m2.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.13.m2.1.1.1.1.1.3">max</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.13.m2.1c">P^{*}(L_{\text{max}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.13.m2.1d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )</annotation></semantics></math>, using any possible strategies and parameter configurations to allocate the inference computation resources. For simplicity, we also refer to <math alttext="P^{*}(L_{\text{max}})" class="ltx_Math" display="inline" id="S4.SS1.p2.14.m3.1"><semantics id="S4.SS1.p2.14.m3.1a"><mrow id="S4.SS1.p2.14.m3.1.1" xref="S4.SS1.p2.14.m3.1.1.cmml"><msup id="S4.SS1.p2.14.m3.1.1.3" xref="S4.SS1.p2.14.m3.1.1.3.cmml"><mi id="S4.SS1.p2.14.m3.1.1.3.2" xref="S4.SS1.p2.14.m3.1.1.3.2.cmml">P</mi><mo id="S4.SS1.p2.14.m3.1.1.3.3" xref="S4.SS1.p2.14.m3.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS1.p2.14.m3.1.1.2" xref="S4.SS1.p2.14.m3.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p2.14.m3.1.1.1.1" xref="S4.SS1.p2.14.m3.1.1.1.1.1.cmml"><mo id="S4.SS1.p2.14.m3.1.1.1.1.2" stretchy="false" xref="S4.SS1.p2.14.m3.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.14.m3.1.1.1.1.1" xref="S4.SS1.p2.14.m3.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.14.m3.1.1.1.1.1.2" xref="S4.SS1.p2.14.m3.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.SS1.p2.14.m3.1.1.1.1.1.3" xref="S4.SS1.p2.14.m3.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS1.p2.14.m3.1.1.1.1.3" stretchy="false" xref="S4.SS1.p2.14.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.14.m3.1b"><apply id="S4.SS1.p2.14.m3.1.1.cmml" xref="S4.SS1.p2.14.m3.1.1"><times id="S4.SS1.p2.14.m3.1.1.2.cmml" xref="S4.SS1.p2.14.m3.1.1.2"></times><apply id="S4.SS1.p2.14.m3.1.1.3.cmml" xref="S4.SS1.p2.14.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.14.m3.1.1.3.1.cmml" xref="S4.SS1.p2.14.m3.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.14.m3.1.1.3.2.cmml" xref="S4.SS1.p2.14.m3.1.1.3.2">𝑃</ci><times id="S4.SS1.p2.14.m3.1.1.3.3.cmml" xref="S4.SS1.p2.14.m3.1.1.3.3"></times></apply><apply id="S4.SS1.p2.14.m3.1.1.1.1.1.cmml" xref="S4.SS1.p2.14.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.14.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.14.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.14.m3.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.14.m3.1.1.1.1.1.2">𝐿</ci><ci id="S4.SS1.p2.14.m3.1.1.1.1.1.3a.cmml" xref="S4.SS1.p2.14.m3.1.1.1.1.1.3"><mtext id="S4.SS1.p2.14.m3.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p2.14.m3.1.1.1.1.1.3">max</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.14.m3.1c">P^{*}(L_{\text{max}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.14.m3.1d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )</annotation></semantics></math> as the <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.20.1">optimal performance</span>. We investigate the following factors within the inference parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS1.p2.15.m4.1"><semantics id="S4.SS1.p2.15.m4.1a"><mi id="S4.SS1.p2.15.m4.1.1" xref="S4.SS1.p2.15.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.15.m4.1b"><ci id="S4.SS1.p2.15.m4.1.1.cmml" xref="S4.SS1.p2.15.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.15.m4.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.15.m4.1d">italic_θ</annotation></semantics></math>: (1) the number of documents <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p2.16.m5.1"><semantics id="S4.SS1.p2.16.m5.1a"><mi id="S4.SS1.p2.16.m5.1.1" xref="S4.SS1.p2.16.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.16.m5.1b"><ci id="S4.SS1.p2.16.m5.1.1.cmml" xref="S4.SS1.p2.16.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.16.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.16.m5.1d">italic_k</annotation></semantics></math>, which are retrieved from a large corpus (e.g., Wikipedia) based on the input query; (2) the number of in-context examples <math alttext="m" class="ltx_Math" display="inline" id="S4.SS1.p2.17.m6.1"><semantics id="S4.SS1.p2.17.m6.1a"><mi id="S4.SS1.p2.17.m6.1.1" xref="S4.SS1.p2.17.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.17.m6.1b"><ci id="S4.SS1.p2.17.m6.1.1.cmml" xref="S4.SS1.p2.17.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.17.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.17.m6.1d">italic_m</annotation></semantics></math>, where each of the examples consists of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p2.18.m7.1"><semantics id="S4.SS1.p2.18.m7.1a"><mi id="S4.SS1.p2.18.m7.1.1" xref="S4.SS1.p2.18.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.18.m7.1b"><ci id="S4.SS1.p2.18.m7.1.1.cmml" xref="S4.SS1.p2.18.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.18.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.18.m7.1d">italic_k</annotation></semantics></math> documents, an input query and its label; and (3) the number of generation iterations <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p2.19.m8.1"><semantics id="S4.SS1.p2.19.m8.1a"><mi id="S4.SS1.p2.19.m8.1.1" xref="S4.SS1.p2.19.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.19.m8.1b"><ci id="S4.SS1.p2.19.m8.1.1.cmml" xref="S4.SS1.p2.19.m8.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.19.m8.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.19.m8.1d">italic_n</annotation></semantics></math>. In DRAG, an answer can be directly generated upon input context, so <math alttext="n=1" class="ltx_Math" display="inline" id="S4.SS1.p2.20.m9.1"><semantics id="S4.SS1.p2.20.m9.1a"><mrow id="S4.SS1.p2.20.m9.1.1" xref="S4.SS1.p2.20.m9.1.1.cmml"><mi id="S4.SS1.p2.20.m9.1.1.2" xref="S4.SS1.p2.20.m9.1.1.2.cmml">n</mi><mo id="S4.SS1.p2.20.m9.1.1.1" xref="S4.SS1.p2.20.m9.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.20.m9.1.1.3" xref="S4.SS1.p2.20.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.20.m9.1b"><apply id="S4.SS1.p2.20.m9.1.1.cmml" xref="S4.SS1.p2.20.m9.1.1"><eq id="S4.SS1.p2.20.m9.1.1.1.cmml" xref="S4.SS1.p2.20.m9.1.1.1"></eq><ci id="S4.SS1.p2.20.m9.1.1.2.cmml" xref="S4.SS1.p2.20.m9.1.1.2">𝑛</ci><cn id="S4.SS1.p2.20.m9.1.1.3.cmml" type="integer" xref="S4.SS1.p2.20.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.20.m9.1c">n=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.20.m9.1d">italic_n = 1</annotation></semantics></math>. In contrast, IterDRAG involves multiple steps of interleaved retrieval and generation, expanding both the effective context length and inference compute without needing longer context windows.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.10.4.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.6.3" style="font-size:90%;">Optimal performance of different methods with varying maximum effective context lengths <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.T1.4.1.m1.1"><semantics id="S4.T1.4.1.m1.1b"><msub id="S4.T1.4.1.m1.1.1" xref="S4.T1.4.1.m1.1.1.cmml"><mi id="S4.T1.4.1.m1.1.1.2" xref="S4.T1.4.1.m1.1.1.2.cmml">L</mi><mtext id="S4.T1.4.1.m1.1.1.3" xref="S4.T1.4.1.m1.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.1.m1.1c"><apply id="S4.T1.4.1.m1.1.1.cmml" xref="S4.T1.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.1.m1.1.1.1.cmml" xref="S4.T1.4.1.m1.1.1">subscript</csymbol><ci id="S4.T1.4.1.m1.1.1.2.cmml" xref="S4.T1.4.1.m1.1.1.2">𝐿</ci><ci id="S4.T1.4.1.m1.1.1.3a.cmml" xref="S4.T1.4.1.m1.1.1.3"><mtext id="S4.T1.4.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.T1.4.1.m1.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.1.m1.1d">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.1.m1.1e">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> (i.e., the total number of input tokens <span class="ltx_text ltx_font_italic" id="S4.T1.6.3.1">across all iterations</span>). ZS QA and MS QA refers to zero-shot QA and many-shot QA respectively. Partial results are omitted for methods that do not further scale with increasing <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.T1.5.2.m2.1"><semantics id="S4.T1.5.2.m2.1b"><msub id="S4.T1.5.2.m2.1.1" xref="S4.T1.5.2.m2.1.1.cmml"><mi id="S4.T1.5.2.m2.1.1.2" xref="S4.T1.5.2.m2.1.1.2.cmml">L</mi><mtext id="S4.T1.5.2.m2.1.1.3" xref="S4.T1.5.2.m2.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.5.2.m2.1c"><apply id="S4.T1.5.2.m2.1.1.cmml" xref="S4.T1.5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.5.2.m2.1.1.1.cmml" xref="S4.T1.5.2.m2.1.1">subscript</csymbol><ci id="S4.T1.5.2.m2.1.1.2.cmml" xref="S4.T1.5.2.m2.1.1.2">𝐿</ci><ci id="S4.T1.5.2.m2.1.1.3a.cmml" xref="S4.T1.5.2.m2.1.1.3"><mtext id="S4.T1.5.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.T1.5.2.m2.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.2.m2.1d">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.2.m2.1e">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>. For clarity, we mark the best results for each <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.T1.6.3.m3.1"><semantics id="S4.T1.6.3.m3.1b"><msub id="S4.T1.6.3.m3.1.1" xref="S4.T1.6.3.m3.1.1.cmml"><mi id="S4.T1.6.3.m3.1.1.2" xref="S4.T1.6.3.m3.1.1.2.cmml">L</mi><mtext id="S4.T1.6.3.m3.1.1.3" xref="S4.T1.6.3.m3.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.6.3.m3.1c"><apply id="S4.T1.6.3.m3.1.1.cmml" xref="S4.T1.6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.T1.6.3.m3.1.1.1.cmml" xref="S4.T1.6.3.m3.1.1">subscript</csymbol><ci id="S4.T1.6.3.m3.1.1.2.cmml" xref="S4.T1.6.3.m3.1.1.2">𝐿</ci><ci id="S4.T1.6.3.m3.1.1.3a.cmml" xref="S4.T1.6.3.m3.1.1.3"><mtext id="S4.T1.6.3.m3.1.1.3.cmml" mathsize="70%" xref="S4.T1.6.3.m3.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.3.m3.1d">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.3.m3.1e">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> in bold.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.7" style="width:433.6pt;height:281.7pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.6pt,3.6pt) scale(0.974699076101843,0.974699076101843) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.7.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.7.1.1.1.1"><math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.T1.7.1.1.1.1.m1.1"><semantics id="S4.T1.7.1.1.1.1.m1.1a"><msub id="S4.T1.7.1.1.1.1.m1.1.1" xref="S4.T1.7.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.7.1.1.1.1.m1.1.1.2" xref="S4.T1.7.1.1.1.1.m1.1.1.2.cmml">L</mi><mtext id="S4.T1.7.1.1.1.1.m1.1.1.3" xref="S4.T1.7.1.1.1.1.m1.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.7.1.1.1.1.m1.1b"><apply id="S4.T1.7.1.1.1.1.m1.1.1.cmml" xref="S4.T1.7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.7.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.7.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.7.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.7.1.1.1.1.m1.1.1.2">𝐿</ci><ci id="S4.T1.7.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T1.7.1.1.1.1.m1.1.1.3"><mtext id="S4.T1.7.1.1.1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.T1.7.1.1.1.1.m1.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.1.1.1.1.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.1.1.1.1.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.7.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T1.7.1.1.2.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.7.1.1.3">Bamboogle</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.7.1.1.4">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.7.1.1.5">MuSiQue</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.7.1.1.6">2WikiMultiHopQA</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.1">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.2">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.3">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.4">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.5">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.6">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.7">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.8">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.9">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.10">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.11">F1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.7.1.2.1.12">Acc</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.3.2.1" rowspan="5"><span class="ltx_text" id="S4.T1.7.1.3.2.1.1">16k</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.3.2.2">ZS QA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.3">16.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.4">25.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.5">19.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.6">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.7">32.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.8">25.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.9">5.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.10">13.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.11">6.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.12">28.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.13">33.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.7.1.3.2.14">30.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.4.3.1">MS QA</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.2">24.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.3">30.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.4">24.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.5">24.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.6">34.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.7">26.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.8">7.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.9">16.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.10">8.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.11">33.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.3.12">37.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.4.3.13">34.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.5.4.1">RAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.2">44.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.3">54.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.4">45.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.5">44.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.6">57.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.7">49.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.8">12.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.9">21.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.10">15.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.11">42.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.4.12">49.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.5.4.13">46.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.6.5.1">DRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.2">44.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.3">55.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.4">45.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.5.1">45.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.6.1">58.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.7.1">50.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.8.1">14.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.9"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.9.1">24.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.10"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.10.1">16.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.11"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.11.1">45.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.5.12"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.12.1">53.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.6.5.13"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.6.5.13.1">50.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.7.6.1">IterDRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.7.6.2.1">46.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.7.6.3.1">56.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.7.6.4.1">51.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.5">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.6">47.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.7">44.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.8">8.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.9">17.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.10">12.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.11">33.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.6.12">38.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.7.6.13">43.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.8.7.1" rowspan="3"><span class="ltx_text" id="S4.T1.7.1.8.7.1.1">32k</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.8.7.2">RAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.3">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.4">56.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.5">49.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.6">44.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.7">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.8">49.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.9">12.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.10">21.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.11">15.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.12">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.13">50.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.7.1.8.7.14">48.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.9.8.1">DRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.2.1">48.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.3.1">59.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.4">50.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.5.1">46.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.6.1">60.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.7.1">52.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.8.1">15.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.9"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.9.1">26.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.10">17.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.11"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.11.1">45.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.8.12"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.12.1">53.7</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.9.8.13"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.9.8.13.1">51.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.10.9.1">IterDRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.2">46.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.3">56.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.10.9.4.1">52.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.5">38.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.6">49.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.7">44.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.8">12.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.9">23.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.10"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.10.9.10.1">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.11">44.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.9.12">54.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.10.9.13">56.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.11.10.1" rowspan="3"><span class="ltx_text" id="S4.T1.7.1.11.10.1.1">128k</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.11.10.2">RAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.3">51.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.4">60.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.5">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.6">45.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.7">59.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.8">50.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.9">14.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.10">23.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.11">16.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.12">43.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.13">50.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.7.1.11.10.14">48.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.12.11.1">DRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.2">52.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.3">62.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.4">54.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.12.11.5.1">47.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.12.11.6.1">61.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.7">52.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.8">15.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.9">26.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.10">17.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.11">47.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.12.11.12">55.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.12.11.13">53.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.13.12.1">IterDRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.2.1">63.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.3.1">74.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.4.1">68.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.5">44.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.6">59.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.7.1">52.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.8.1">17.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.9"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.9.1">28.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.10"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.10.1">24.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.11"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.11.1">62.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.13.12.12"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.12.1">73.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.13.12.13"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.13.12.13.1">74.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.14.13.1" rowspan="2"><span class="ltx_text" id="S4.T1.7.1.14.13.1.1">1M</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.14.13.2">DRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.3">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.4">62.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.5">57.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.6">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.7">61.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.8">52.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.9">15.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.10">26.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.11">18.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.12">48.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.13">55.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.7.1.14.13.14">53.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.15.14.1">IterDRAG</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.2.1">65.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.3.1">75.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.4.1">68.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.5.1">48.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.6.1">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.7.1">55.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.8.1">22.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.9"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.9.1">34.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.10"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.10.1">30.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.11"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.11.1">65.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.15.14.12"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.12.1">75.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.1.15.14.13"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.15.14.13.1">76.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.1">5M</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.2">IterDRAG</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.3.1">65.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.4.1">75.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.5.1">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.6.1">51.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.7.1">64.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.8.1">56.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.9"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.9.1">22.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.10"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.10.1">35.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.11"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.11.1">30.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.12"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.12.1">67.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.13"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.13.1">75.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.1.16.15.14"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.16.15.14.1">76.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.11">We evaluate the performance of Gemini 1.5 Flash with context length window up to 1M tokens on knowledge-intensive question answering, including multi-hop datasets Bamboogle, HotpotQA, MuSiQue and 2WikiMultiHopQA <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib47" title="">2023</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib67" title="">2018</a>; Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib58" title="">2022</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib17" title="">2020</a>)</cite>. Additional results are provided in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A2" title="Appendix B Chain-of-Thought vs. IterDRAG. ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3" title="Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>. To manage the computational costs of extensive experiments, we follow <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib63" title="">2024</a>); Gutiérrez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib15" title="">2024</a>)</cite> and sample 1.2k examples from each dataset for evaluation. The evaluation metrics include exact match (EM), F1 score (F1) and accuracy (Acc), in which the accuracy metric assesses whether the ground truth is located within the prediction. We sample the inference computation budget <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><msub id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">L</mi><mtext id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝐿</ci><ci id="S4.SS1.p3.1.m1.1.1.3a.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><mtext id="S4.SS1.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p3.1.m1.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> as 16k, 32k, 128k, 1M and 5M tokens. For the parameter space <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" mathvariant="normal" xref="S4.SS1.p3.2.m2.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">roman_Θ</annotation></semantics></math> of DRAG, we consider the number of documents <math alttext="k\in\{0,1,2,5,10,20,50,100,200,500,1000\}" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.11"><semantics id="S4.SS1.p3.3.m3.11a"><mrow id="S4.SS1.p3.3.m3.11.12" xref="S4.SS1.p3.3.m3.11.12.cmml"><mi id="S4.SS1.p3.3.m3.11.12.2" xref="S4.SS1.p3.3.m3.11.12.2.cmml">k</mi><mo id="S4.SS1.p3.3.m3.11.12.1" xref="S4.SS1.p3.3.m3.11.12.1.cmml">∈</mo><mrow id="S4.SS1.p3.3.m3.11.12.3.2" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml"><mo id="S4.SS1.p3.3.m3.11.12.3.2.1" stretchy="false" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">{</mo><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">0</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.2" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.2.2" xref="S4.SS1.p3.3.m3.2.2.cmml">1</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.3" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.3.3" xref="S4.SS1.p3.3.m3.3.3.cmml">2</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.4" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.4.4" xref="S4.SS1.p3.3.m3.4.4.cmml">5</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.5" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.5.5" xref="S4.SS1.p3.3.m3.5.5.cmml">10</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.6" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.6.6" xref="S4.SS1.p3.3.m3.6.6.cmml">20</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.7" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.7.7" xref="S4.SS1.p3.3.m3.7.7.cmml">50</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.8" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.8.8" xref="S4.SS1.p3.3.m3.8.8.cmml">100</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.9" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.9.9" xref="S4.SS1.p3.3.m3.9.9.cmml">200</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.10" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.10.10" xref="S4.SS1.p3.3.m3.10.10.cmml">500</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.11" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.11.11" xref="S4.SS1.p3.3.m3.11.11.cmml">1000</mn><mo id="S4.SS1.p3.3.m3.11.12.3.2.12" stretchy="false" xref="S4.SS1.p3.3.m3.11.12.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.11b"><apply id="S4.SS1.p3.3.m3.11.12.cmml" xref="S4.SS1.p3.3.m3.11.12"><in id="S4.SS1.p3.3.m3.11.12.1.cmml" xref="S4.SS1.p3.3.m3.11.12.1"></in><ci id="S4.SS1.p3.3.m3.11.12.2.cmml" xref="S4.SS1.p3.3.m3.11.12.2">𝑘</ci><set id="S4.SS1.p3.3.m3.11.12.3.1.cmml" xref="S4.SS1.p3.3.m3.11.12.3.2"><cn id="S4.SS1.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p3.3.m3.1.1">0</cn><cn id="S4.SS1.p3.3.m3.2.2.cmml" type="integer" xref="S4.SS1.p3.3.m3.2.2">1</cn><cn id="S4.SS1.p3.3.m3.3.3.cmml" type="integer" xref="S4.SS1.p3.3.m3.3.3">2</cn><cn id="S4.SS1.p3.3.m3.4.4.cmml" type="integer" xref="S4.SS1.p3.3.m3.4.4">5</cn><cn id="S4.SS1.p3.3.m3.5.5.cmml" type="integer" xref="S4.SS1.p3.3.m3.5.5">10</cn><cn id="S4.SS1.p3.3.m3.6.6.cmml" type="integer" xref="S4.SS1.p3.3.m3.6.6">20</cn><cn id="S4.SS1.p3.3.m3.7.7.cmml" type="integer" xref="S4.SS1.p3.3.m3.7.7">50</cn><cn id="S4.SS1.p3.3.m3.8.8.cmml" type="integer" xref="S4.SS1.p3.3.m3.8.8">100</cn><cn id="S4.SS1.p3.3.m3.9.9.cmml" type="integer" xref="S4.SS1.p3.3.m3.9.9">200</cn><cn id="S4.SS1.p3.3.m3.10.10.cmml" type="integer" xref="S4.SS1.p3.3.m3.10.10">500</cn><cn id="S4.SS1.p3.3.m3.11.11.cmml" type="integer" xref="S4.SS1.p3.3.m3.11.11">1000</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.11c">k\in\{0,1,2,5,10,20,50,100,200,500,1000\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.11d">italic_k ∈ { 0 , 1 , 2 , 5 , 10 , 20 , 50 , 100 , 200 , 500 , 1000 }</annotation></semantics></math>, and the number in-context examples <math alttext="m" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.1"><semantics id="S4.SS1.p3.4.m4.1a"><mi id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.1d">italic_m</annotation></semantics></math> ranging from <math alttext="0" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m5.1"><semantics id="S4.SS1.p3.5.m5.1a"><mn id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><cn id="S4.SS1.p3.5.m5.1.1.cmml" type="integer" xref="S4.SS1.p3.5.m5.1.1">0</cn></annotation-xml></semantics></math>, <math alttext="2^{0}" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m6.1"><semantics id="S4.SS1.p3.6.m6.1a"><msup id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml"><mn id="S4.SS1.p3.6.m6.1.1.2" xref="S4.SS1.p3.6.m6.1.1.2.cmml">2</mn><mn id="S4.SS1.p3.6.m6.1.1.3" xref="S4.SS1.p3.6.m6.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><apply id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.6.m6.1.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">superscript</csymbol><cn id="S4.SS1.p3.6.m6.1.1.2.cmml" type="integer" xref="S4.SS1.p3.6.m6.1.1.2">2</cn><cn id="S4.SS1.p3.6.m6.1.1.3.cmml" type="integer" xref="S4.SS1.p3.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">2^{0}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.6.m6.1d">2 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="2^{1}" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m7.1"><semantics id="S4.SS1.p3.7.m7.1a"><msup id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mn id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml">2</mn><mn id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1">superscript</csymbol><cn id="S4.SS1.p3.7.m7.1.1.2.cmml" type="integer" xref="S4.SS1.p3.7.m7.1.1.2">2</cn><cn id="S4.SS1.p3.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.p3.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">2^{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.7.m7.1d">2 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, …, to <math alttext="2^{8}" class="ltx_Math" display="inline" id="S4.SS1.p3.8.m8.1"><semantics id="S4.SS1.p3.8.m8.1a"><msup id="S4.SS1.p3.8.m8.1.1" xref="S4.SS1.p3.8.m8.1.1.cmml"><mn id="S4.SS1.p3.8.m8.1.1.2" xref="S4.SS1.p3.8.m8.1.1.2.cmml">2</mn><mn id="S4.SS1.p3.8.m8.1.1.3" xref="S4.SS1.p3.8.m8.1.1.3.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m8.1b"><apply id="S4.SS1.p3.8.m8.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m8.1.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1">superscript</csymbol><cn id="S4.SS1.p3.8.m8.1.1.2.cmml" type="integer" xref="S4.SS1.p3.8.m8.1.1.2">2</cn><cn id="S4.SS1.p3.8.m8.1.1.3.cmml" type="integer" xref="S4.SS1.p3.8.m8.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m8.1c">2^{8}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.8.m8.1d">2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT</annotation></semantics></math>. For IterDRAG we further experiment with number of iterations <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p3.9.m9.1"><semantics id="S4.SS1.p3.9.m9.1a"><mi id="S4.SS1.p3.9.m9.1.1" xref="S4.SS1.p3.9.m9.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.9.m9.1b"><ci id="S4.SS1.p3.9.m9.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.9.m9.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.9.m9.1d">italic_n</annotation></semantics></math> up to 5. We compare to the following baselines: (1) zero-shot QA (QA), where the model does not leverage any retrieved documents or demonstrations; (2) many-shot QA (MS QA), where the model only uses varying number of demonstrations <math alttext="m" class="ltx_Math" display="inline" id="S4.SS1.p3.10.m10.1"><semantics id="S4.SS1.p3.10.m10.1a"><mi id="S4.SS1.p3.10.m10.1.1" xref="S4.SS1.p3.10.m10.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.10.m10.1b"><ci id="S4.SS1.p3.10.m10.1.1.cmml" xref="S4.SS1.p3.10.m10.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.10.m10.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.10.m10.1d">italic_m</annotation></semantics></math> without any retrieved document; and (3) retrieval augmented generation (RAG), where the model only uses <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p3.11.m11.1"><semantics id="S4.SS1.p3.11.m11.1a"><mi id="S4.SS1.p3.11.m11.1.1" xref="S4.SS1.p3.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.11.m11.1b"><ci id="S4.SS1.p3.11.m11.1.1.cmml" xref="S4.SS1.p3.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.11.m11.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.11.m11.1d">italic_k</annotation></semantics></math> retrieved documents without demonstrations. We report the optimal performance of each method with different maximum effective context length budgets by examining their performance with different inference parameter configurations.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S4.F4.g1" src="extracted/5904574/figures/perf_vs_length.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Normalized performance vs. effective context lengths across datasets. Each line represents a fixed configuration, scaled by varying the number of documents. Red dots indicate the optimal configurations, with the dashed line showing the fitting results. The observed optimal performance can be approximated by a linear relationship with the effective context lengths.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Overall Performance</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.6">We report the optimal performance <math alttext="P^{*}(L_{\text{max}})" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><msup id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">P</mi><mo id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS2.p1.1.m1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.SS2.p1.1.m1.1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"></times><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">𝑃</ci><times id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"></times></apply><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2">𝐿</ci><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3"><mtext id="S4.SS2.p1.1.m1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3">max</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">P^{*}(L_{\text{max}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )</annotation></semantics></math> for different inference strategies in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.T1" title="In 4.1 Fixed Budget Optimal Performance ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, where we identify the optimal inference parameters for each computation budget <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">L</mi><mtext id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝐿</ci><ci id="S4.SS2.p1.2.m2.1.1.3a.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><mtext id="S4.SS2.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p1.2.m2.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>. Some variants are omitted for certain <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">L</mi><mtext id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">𝐿</ci><ci id="S4.SS2.p1.3.m3.1.1.3a.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><mtext id="S4.SS2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p1.3.m3.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> because they do not scale to the corresponding context length. For example, the prompt for zero-shot QA cannot be increased, while the number of in-context examples for many-shot QA is capped at <math alttext="2^{8}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><msup id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">2</mn><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">superscript</csymbol><cn id="S4.SS2.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.2">2</cn><cn id="S4.SS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">2^{8}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT</annotation></semantics></math>, so neither scales to <math alttext="L_{\text{max}}=" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><msub id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2.2" xref="S4.SS2.p1.5.m5.1.1.2.2.cmml">L</mi><mtext id="S4.SS2.p1.5.m5.1.1.2.3" xref="S4.SS2.p1.5.m5.1.1.2.3a.cmml">max</mtext></msub><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">=</mo><mi id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><eq id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></eq><apply id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.2.1.cmml" xref="S4.SS2.p1.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2.2">𝐿</ci><ci id="S4.SS2.p1.5.m5.1.1.2.3a.cmml" xref="S4.SS2.p1.5.m5.1.1.2.3"><mtext id="S4.SS2.p1.5.m5.1.1.2.3.cmml" mathsize="70%" xref="S4.SS2.p1.5.m5.1.1.2.3">max</mtext></ci></apply><csymbol cd="latexml" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">L_{\text{max}}=</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT =</annotation></semantics></math> 32k. Similarly, RAG does not scale to <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><msub id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">L</mi><mtext id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">𝐿</ci><ci id="S4.SS2.p1.6.m6.1.1.3a.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><mtext id="S4.SS2.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p1.6.m6.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> larger than 128k, and DRAG is limited by the LLM’s context window limit of 1M.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Unlike QA and RAG baselines, the performance of DRAG and IterDRAG consistently increase as we expand the maximum effective context length. More specifically, we observe: (1) <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.1">DRAG and IterDRAG scale better than baselines.</em> Baselines like many-shot QA peak at 16k tokens, while RAG improves until 128k, after which performance plateaus. In comparison, DRAG and IterDRAG can find optimal configurations to more effectively utilize test-time compute, exhibiting superior performance and scaling properties. Performance of DRAG consistently improves until 1M tokens, while IterDRAG further enhances RAG performance with 5M tokens of computation budget by iteratively calling LLMs. (2) <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.2">DRAG excels with shorter maximum lengths, while IterDRAG scales more effectively with longer effective context length.</em> At 16k and 32k, DRAG typically delivers the best performance, while at 128k and beyond, IterDRAG achieves superior results overall, highlighting the effectiveness of inference scaling with iterative retrieval and generation. These results suggest that increasing <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">L</mi><mtext id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝐿</ci><ci id="S4.SS2.p2.1.m1.1.1.3a.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><mtext id="S4.SS2.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p2.1.m1.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> is beneficial for RAG performance, with DRAG and IterDRAG strategies each excelling at different scales.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Inference Scaling Laws for RAG</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">To analyze the performance changes with different effective context lengths, we plot the performance of all configurations across datasets in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F4" title="In 4.1 Fixed Budget Optimal Performance ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Similar to <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F1" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we visualize DRAG and IterDRAG and highlight the optimal performance <math alttext="P^{*}(L_{\text{max}})" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><msup id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">P</mi><mo id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS3.p1.1.m1.1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS3.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS3.p1.1.m1.1.1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.1.1.1.2.cmml">L</mi><mtext id="S4.SS3.p1.1.m1.1.1.1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS3.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"></times><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">𝑃</ci><times id="S4.SS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"></times></apply><apply id="S4.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1.1.2">𝐿</ci><ci id="S4.SS3.p1.1.m1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1.1.3"><mtext id="S4.SS3.p1.1.m1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p1.1.m1.1.1.1.1.1.3">max</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">P^{*}(L_{\text{max}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_P start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )</annotation></semantics></math> for different selections of <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">L</mi><mtext id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝐿</ci><ci id="S4.SS3.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><mtext id="S4.SS3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p1.2.m2.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>. The fitting results are shown as grey dashed lines. We provide additional dataset-specific results in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A4" title="Appendix D Additional Results on Inference Scaling Laws for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">The optimal performance exhibits consistent gains as the effective context length expands, demonstrating a strong linear correlation, which we term the <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.2.3">inference scaling laws for RAG</em>. Combined with dataset-specific results, our key observations are: (1) <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.2.4">The optimal performance scales nearly linearly with the order of magnitude of the inference compute.</em> Such linear relationship suggests that RAG performance can be improved by increasing computation, allowing for more accurate predictions of performance given available compute resources. (2) <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.2.2">For <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.1.m1.1"><semantics id="S4.SS3.p2.1.1.m1.1a"><msub id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.1.m1.1.1.2" xref="S4.SS3.p2.1.1.m1.1.1.2.cmml">L</mi><mtext class="ltx_mathvariant_italic" id="S4.SS3.p2.1.1.m1.1.1.3" xref="S4.SS3.p2.1.1.m1.1.1.3b.cmml"><em class="ltx_emph" id="S4.SS3.p2.1.1.m1.1.1.3.1nest" style="font-size:70%;">max</em></mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><apply id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.1.m1.1.1.2">𝐿</ci><ci id="S4.SS3.p2.1.1.m1.1.1.3b.cmml" xref="S4.SS3.p2.1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS3.p2.1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p2.1.1.m1.1.1.3"><em class="ltx_emph" id="S4.SS3.p2.1.1.m1.1.1.3.1anest" style="font-size:70%;">max</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.1.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> above <math alttext="10^{5}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.2.m2.1"><semantics id="S4.SS3.p2.2.2.m2.1a"><msup id="S4.SS3.p2.2.2.m2.1.1" xref="S4.SS3.p2.2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.2.m2.1.1.2" xref="S4.SS3.p2.2.2.m2.1.1.2.cmml">10</mn><mn id="S4.SS3.p2.2.2.m2.1.1.3" xref="S4.SS3.p2.2.2.m2.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.2.m2.1b"><apply id="S4.SS3.p2.2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1">superscript</csymbol><cn id="S4.SS3.p2.2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p2.2.2.m2.1.1.2">10</cn><cn id="S4.SS3.p2.2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.p2.2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.2.m2.1c">10^{5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.2.m2.1d">10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, IterDRAG continues to scale effectively with interleaving retrieval and iterative generation.</em> This aligns with our results in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.T1" title="In 4.1 Fixed Budget Optimal Performance ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, where IterDRAG better utilizes computation budgets for effective context lengths exceeding 128k. (3) <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.2.5">Gains on optimal performance gradually diminish beyond an effective context length of 1M.</em> Despite dataset variations, the performance follows similar trends up to 1M tokens. Beyond that, improvements from 1M to 5M are less substantial or plateau, potentially due to limitations in long-context modeling. In summary, while gains are smaller beyond 1M tokens, optimal RAG performance scales almost linearly with increasing inference compute through DRAG and IterDRAG.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S4.F5.sf1.g1" src="x2.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F5.sf1.3.2" style="font-size:90%;">Averaged DRAG performance heatmap for different metrics.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="710" id="S4.F5.sf2.g1" src="x3.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F5.sf2.3.2" style="font-size:90%;">Performance vs. number of documents.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="710" id="S4.F5.sf3.g1" src="x4.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F5.sf3.3.2" style="font-size:90%;">Performance vs. number of shots.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">RAG performance changes with varying number of documents and in-context examples. <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf1" title="Figure 5(a) ‣ Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5(a)</span></a> reports the averaged metric values across datasets, whereas in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf2" title="Figure 5(b) ‣ Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5(b)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf3" title="Figure 5(c) ‣ Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5(c)</span></a>, each line represents the normalized performance of a consistent configuration with progressively increasing documents / shots.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Parameter-Specific Scaling</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.13">To gain further insights into the dynamics of DRAG and IterDRAG, we grid search over different combinations of <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_θ</annotation></semantics></math> and evaluate the performance. The results are presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5" title="In 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>, where we visualize DRAG performance using heatmaps (See IterDRAG heatmap in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3" title="Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>). Additionally, we provide further results with varying numbers of documents (<math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">italic_k</annotation></semantics></math>) and shots (<math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mi id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><ci id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">italic_m</annotation></semantics></math>). In summary, scaling retrieval, demonstrations and more generation steps leads to performance gains in most cases, yet such gains vary by effective context length and method. In particular, we note: (1) <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.13.4">Documents and in-context examples are not equally helpful.</em> For a fixed configuration, increasing the number of retrieved documents <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><mi id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><ci id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">italic_k</annotation></semantics></math> usually leads to more substantial performance gains, as evidenced by the differing slopes in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5" title="In 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. (2) <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.5.1">Increasing shots <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.5.1.m1.1"><semantics id="S4.SS4.p1.5.1.m1.1a"><mi id="S4.SS4.p1.5.1.m1.1.1" xref="S4.SS4.p1.5.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.1.m1.1b"><ci id="S4.SS4.p1.5.1.m1.1.1.cmml" xref="S4.SS4.p1.5.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.5.1.m1.1d">italic_m</annotation></semantics></math> is more helpful for IterDRAG.</em> For example, increase <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.6.m5.1"><semantics id="S4.SS4.p1.6.m5.1a"><mi id="S4.SS4.p1.6.m5.1.1" xref="S4.SS4.p1.6.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m5.1b"><ci id="S4.SS4.p1.6.m5.1.1.cmml" xref="S4.SS4.p1.6.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.6.m5.1d">italic_m</annotation></semantics></math> from 0 to 1 (rather than <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p1.7.m6.1"><semantics id="S4.SS4.p1.7.m6.1a"><mi id="S4.SS4.p1.7.m6.1.1" xref="S4.SS4.p1.7.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m6.1b"><ci id="S4.SS4.p1.7.m6.1.1.cmml" xref="S4.SS4.p1.7.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.7.m6.1d">italic_k</annotation></semantics></math>) is more helpful for IterDRAG, possibly due to demonstrations that leads to improved in-context query decomposition and knowledge extraction. (3) <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.13.5">Scaling saturates differently for DRAG and IterDRAG.</em> An example can be found in the increase of <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.8.m7.1"><semantics id="S4.SS4.p1.8.m7.1a"><mi id="S4.SS4.p1.8.m7.1.1" xref="S4.SS4.p1.8.m7.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m7.1b"><ci id="S4.SS4.p1.8.m7.1.1.cmml" xref="S4.SS4.p1.8.m7.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m7.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.8.m7.1d">italic_m</annotation></semantics></math> from 0 to 1, which results in significant improvements for IterDRAG but shows little impact on DRAG. Beyond the soft thresholds, further increases in <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p1.9.m8.1"><semantics id="S4.SS4.p1.9.m8.1a"><mi id="S4.SS4.p1.9.m8.1.1" xref="S4.SS4.p1.9.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.9.m8.1b"><ci id="S4.SS4.p1.9.m8.1.1.cmml" xref="S4.SS4.p1.9.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.9.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.9.m8.1d">italic_k</annotation></semantics></math> or <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.10.m9.1"><semantics id="S4.SS4.p1.10.m9.1a"><mi id="S4.SS4.p1.10.m9.1.1" xref="S4.SS4.p1.10.m9.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.10.m9.1b"><ci id="S4.SS4.p1.10.m9.1.1.cmml" xref="S4.SS4.p1.10.m9.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.10.m9.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.10.m9.1d">italic_m</annotation></semantics></math> yield marginal gains or even results in performance declines. (4) <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.12.3">For a given <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S4.SS4.p1.11.2.m1.1"><semantics id="S4.SS4.p1.11.2.m1.1a"><msub id="S4.SS4.p1.11.2.m1.1.1" xref="S4.SS4.p1.11.2.m1.1.1.cmml"><mi id="S4.SS4.p1.11.2.m1.1.1.2" xref="S4.SS4.p1.11.2.m1.1.1.2.cmml">L</mi><mtext class="ltx_mathvariant_italic" id="S4.SS4.p1.11.2.m1.1.1.3" xref="S4.SS4.p1.11.2.m1.1.1.3b.cmml"><em class="ltx_emph" id="S4.SS4.p1.11.2.m1.1.1.3.1nest" style="font-size:70%;">max</em></mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.11.2.m1.1b"><apply id="S4.SS4.p1.11.2.m1.1.1.cmml" xref="S4.SS4.p1.11.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.11.2.m1.1.1.1.cmml" xref="S4.SS4.p1.11.2.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.11.2.m1.1.1.2.cmml" xref="S4.SS4.p1.11.2.m1.1.1.2">𝐿</ci><ci id="S4.SS4.p1.11.2.m1.1.1.3b.cmml" xref="S4.SS4.p1.11.2.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS4.p1.11.2.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p1.11.2.m1.1.1.3"><em class="ltx_emph" id="S4.SS4.p1.11.2.m1.1.1.3.1anest" style="font-size:70%;">max</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.11.2.m1.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.11.2.m1.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>, the optimal <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS4.p1.12.3.m2.1"><semantics id="S4.SS4.p1.12.3.m2.1a"><mi id="S4.SS4.p1.12.3.m2.1.1" xref="S4.SS4.p1.12.3.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.12.3.m2.1b"><ci id="S4.SS4.p1.12.3.m2.1.1.cmml" xref="S4.SS4.p1.12.3.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.12.3.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.12.3.m2.1d">italic_θ</annotation></semantics></math> depends on the method, metric and dataset.</em> As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf1" title="In Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3.F8" title="In Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, the optimal combinations are sensitive to the metrics and located differently, posing challenges for performance modeling w.r.t. <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS4.p1.13.m10.1"><semantics id="S4.SS4.p1.13.m10.1a"><mi id="S4.SS4.p1.13.m10.1.1" xref="S4.SS4.p1.13.m10.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.13.m10.1b"><ci id="S4.SS4.p1.13.m10.1.1.cmml" xref="S4.SS4.p1.13.m10.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.13.m10.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.13.m10.1d">italic_θ</annotation></semantics></math>. In conclusion, increasing documents, demonstrations and iterations can enhance RAG performance, but each contributes differently to the overall results. As such, identifying the optimal combination of hyperparameters remains challenging.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Inference Computation Allocation for Long-Context RAG</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.3">After examining the overall performance of different RAG strategies and the varying impacts of different inference parameters, we now quantify the relationship between performance and the hyperparameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_θ</annotation></semantics></math>. We hypothesize that for long-context RAG, we can model such test-time scaling properties and term it <em class="ltx_emph ltx_font_italic" id="S5.p1.3.1">computation allocation model for RAG</em>. This model, in turn, can be used to guide the selection of <math alttext="\theta" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_θ</annotation></semantics></math> based on the maximum effective context length <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><msub id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">L</mi><mtext id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1">subscript</csymbol><ci id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">𝐿</ci><ci id="S5.p1.3.m3.1.1.3a.cmml" xref="S5.p1.3.m3.1.1.3"><mtext id="S5.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S5.p1.3.m3.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Formulation and Estimation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.13">With a slight abuse of notation, we redefine the average performance metric <math alttext="P" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_P</annotation></semantics></math> (e.g., accuracy) on dataset <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">caligraphic_X</annotation></semantics></math> as a function of <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">italic_θ</annotation></semantics></math>. We consider the number of documents <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1"><semantics id="S5.SS1.p1.4.m4.1a"><mi id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><ci id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">italic_k</annotation></semantics></math>, demonstrations <math alttext="m" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1"><semantics id="S5.SS1.p1.5.m5.1a"><mi id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><ci id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.1d">italic_m</annotation></semantics></math> and maximum iterations <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.p1.6.m6.1"><semantics id="S5.SS1.p1.6.m6.1a"><mi id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><ci id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.6.m6.1d">italic_n</annotation></semantics></math> within <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS1.p1.7.m7.1"><semantics id="S5.SS1.p1.7.m7.1a"><mi id="S5.SS1.p1.7.m7.1.1" xref="S5.SS1.p1.7.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.7.m7.1b"><ci id="S5.SS1.p1.7.m7.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.7.m7.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.7.m7.1d">italic_θ</annotation></semantics></math>, namely <math alttext="\theta:=(k,m,n)^{T}" class="ltx_Math" display="inline" id="S5.SS1.p1.8.m8.3"><semantics id="S5.SS1.p1.8.m8.3a"><mrow id="S5.SS1.p1.8.m8.3.4" xref="S5.SS1.p1.8.m8.3.4.cmml"><mi id="S5.SS1.p1.8.m8.3.4.2" xref="S5.SS1.p1.8.m8.3.4.2.cmml">θ</mi><mo id="S5.SS1.p1.8.m8.3.4.1" lspace="0.278em" rspace="0.278em" xref="S5.SS1.p1.8.m8.3.4.1.cmml">:=</mo><msup id="S5.SS1.p1.8.m8.3.4.3" xref="S5.SS1.p1.8.m8.3.4.3.cmml"><mrow id="S5.SS1.p1.8.m8.3.4.3.2.2" xref="S5.SS1.p1.8.m8.3.4.3.2.1.cmml"><mo id="S5.SS1.p1.8.m8.3.4.3.2.2.1" stretchy="false" xref="S5.SS1.p1.8.m8.3.4.3.2.1.cmml">(</mo><mi id="S5.SS1.p1.8.m8.1.1" xref="S5.SS1.p1.8.m8.1.1.cmml">k</mi><mo id="S5.SS1.p1.8.m8.3.4.3.2.2.2" xref="S5.SS1.p1.8.m8.3.4.3.2.1.cmml">,</mo><mi id="S5.SS1.p1.8.m8.2.2" xref="S5.SS1.p1.8.m8.2.2.cmml">m</mi><mo id="S5.SS1.p1.8.m8.3.4.3.2.2.3" xref="S5.SS1.p1.8.m8.3.4.3.2.1.cmml">,</mo><mi id="S5.SS1.p1.8.m8.3.3" xref="S5.SS1.p1.8.m8.3.3.cmml">n</mi><mo id="S5.SS1.p1.8.m8.3.4.3.2.2.4" stretchy="false" xref="S5.SS1.p1.8.m8.3.4.3.2.1.cmml">)</mo></mrow><mi id="S5.SS1.p1.8.m8.3.4.3.3" xref="S5.SS1.p1.8.m8.3.4.3.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.8.m8.3b"><apply id="S5.SS1.p1.8.m8.3.4.cmml" xref="S5.SS1.p1.8.m8.3.4"><csymbol cd="latexml" id="S5.SS1.p1.8.m8.3.4.1.cmml" xref="S5.SS1.p1.8.m8.3.4.1">assign</csymbol><ci id="S5.SS1.p1.8.m8.3.4.2.cmml" xref="S5.SS1.p1.8.m8.3.4.2">𝜃</ci><apply id="S5.SS1.p1.8.m8.3.4.3.cmml" xref="S5.SS1.p1.8.m8.3.4.3"><csymbol cd="ambiguous" id="S5.SS1.p1.8.m8.3.4.3.1.cmml" xref="S5.SS1.p1.8.m8.3.4.3">superscript</csymbol><vector id="S5.SS1.p1.8.m8.3.4.3.2.1.cmml" xref="S5.SS1.p1.8.m8.3.4.3.2.2"><ci id="S5.SS1.p1.8.m8.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1">𝑘</ci><ci id="S5.SS1.p1.8.m8.2.2.cmml" xref="S5.SS1.p1.8.m8.2.2">𝑚</ci><ci id="S5.SS1.p1.8.m8.3.3.cmml" xref="S5.SS1.p1.8.m8.3.3">𝑛</ci></vector><ci id="S5.SS1.p1.8.m8.3.4.3.3.cmml" xref="S5.SS1.p1.8.m8.3.4.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.8.m8.3c">\theta:=(k,m,n)^{T}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.8.m8.3d">italic_θ := ( italic_k , italic_m , italic_n ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. To account for the variance across methods and tasks, we introduce <math alttext="i:=(i_{\text{doc}},i_{\text{shot}},0)^{T}" class="ltx_Math" display="inline" id="S5.SS1.p1.9.m9.3"><semantics id="S5.SS1.p1.9.m9.3a"><mrow id="S5.SS1.p1.9.m9.3.3" xref="S5.SS1.p1.9.m9.3.3.cmml"><mi id="S5.SS1.p1.9.m9.3.3.4" xref="S5.SS1.p1.9.m9.3.3.4.cmml">i</mi><mo id="S5.SS1.p1.9.m9.3.3.3" lspace="0.278em" rspace="0.278em" xref="S5.SS1.p1.9.m9.3.3.3.cmml">:=</mo><msup id="S5.SS1.p1.9.m9.3.3.2" xref="S5.SS1.p1.9.m9.3.3.2.cmml"><mrow id="S5.SS1.p1.9.m9.3.3.2.2.2" xref="S5.SS1.p1.9.m9.3.3.2.2.3.cmml"><mo id="S5.SS1.p1.9.m9.3.3.2.2.2.3" stretchy="false" xref="S5.SS1.p1.9.m9.3.3.2.2.3.cmml">(</mo><msub id="S5.SS1.p1.9.m9.2.2.1.1.1.1" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.cmml"><mi id="S5.SS1.p1.9.m9.2.2.1.1.1.1.2" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.9.m9.2.2.1.1.1.1.3" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.3a.cmml">doc</mtext></msub><mo id="S5.SS1.p1.9.m9.3.3.2.2.2.4" xref="S5.SS1.p1.9.m9.3.3.2.2.3.cmml">,</mo><msub id="S5.SS1.p1.9.m9.3.3.2.2.2.2" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.cmml"><mi id="S5.SS1.p1.9.m9.3.3.2.2.2.2.2" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.2.cmml">i</mi><mtext id="S5.SS1.p1.9.m9.3.3.2.2.2.2.3" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.3a.cmml">shot</mtext></msub><mo id="S5.SS1.p1.9.m9.3.3.2.2.2.5" xref="S5.SS1.p1.9.m9.3.3.2.2.3.cmml">,</mo><mn id="S5.SS1.p1.9.m9.1.1" xref="S5.SS1.p1.9.m9.1.1.cmml">0</mn><mo id="S5.SS1.p1.9.m9.3.3.2.2.2.6" stretchy="false" xref="S5.SS1.p1.9.m9.3.3.2.2.3.cmml">)</mo></mrow><mi id="S5.SS1.p1.9.m9.3.3.2.4" xref="S5.SS1.p1.9.m9.3.3.2.4.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.9.m9.3b"><apply id="S5.SS1.p1.9.m9.3.3.cmml" xref="S5.SS1.p1.9.m9.3.3"><csymbol cd="latexml" id="S5.SS1.p1.9.m9.3.3.3.cmml" xref="S5.SS1.p1.9.m9.3.3.3">assign</csymbol><ci id="S5.SS1.p1.9.m9.3.3.4.cmml" xref="S5.SS1.p1.9.m9.3.3.4">𝑖</ci><apply id="S5.SS1.p1.9.m9.3.3.2.cmml" xref="S5.SS1.p1.9.m9.3.3.2"><csymbol cd="ambiguous" id="S5.SS1.p1.9.m9.3.3.2.3.cmml" xref="S5.SS1.p1.9.m9.3.3.2">superscript</csymbol><vector id="S5.SS1.p1.9.m9.3.3.2.2.3.cmml" xref="S5.SS1.p1.9.m9.3.3.2.2.2"><apply id="S5.SS1.p1.9.m9.2.2.1.1.1.1.cmml" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.9.m9.2.2.1.1.1.1.1.cmml" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1">subscript</csymbol><ci id="S5.SS1.p1.9.m9.2.2.1.1.1.1.2.cmml" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.2">𝑖</ci><ci id="S5.SS1.p1.9.m9.2.2.1.1.1.1.3a.cmml" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.3"><mtext id="S5.SS1.p1.9.m9.2.2.1.1.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.9.m9.2.2.1.1.1.1.3">doc</mtext></ci></apply><apply id="S5.SS1.p1.9.m9.3.3.2.2.2.2.cmml" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p1.9.m9.3.3.2.2.2.2.1.cmml" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2">subscript</csymbol><ci id="S5.SS1.p1.9.m9.3.3.2.2.2.2.2.cmml" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.2">𝑖</ci><ci id="S5.SS1.p1.9.m9.3.3.2.2.2.2.3a.cmml" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.3"><mtext id="S5.SS1.p1.9.m9.3.3.2.2.2.2.3.cmml" mathsize="70%" xref="S5.SS1.p1.9.m9.3.3.2.2.2.2.3">shot</mtext></ci></apply><cn id="S5.SS1.p1.9.m9.1.1.cmml" type="integer" xref="S5.SS1.p1.9.m9.1.1">0</cn></vector><ci id="S5.SS1.p1.9.m9.3.3.2.4.cmml" xref="S5.SS1.p1.9.m9.3.3.2.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.9.m9.3c">i:=(i_{\text{doc}},i_{\text{shot}},0)^{T}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.9.m9.3d">italic_i := ( italic_i start_POSTSUBSCRIPT doc end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT shot end_POSTSUBSCRIPT , 0 ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. <math alttext="i_{\text{doc}}" class="ltx_Math" display="inline" id="S5.SS1.p1.10.m10.1"><semantics id="S5.SS1.p1.10.m10.1a"><msub id="S5.SS1.p1.10.m10.1.1" xref="S5.SS1.p1.10.m10.1.1.cmml"><mi id="S5.SS1.p1.10.m10.1.1.2" xref="S5.SS1.p1.10.m10.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.10.m10.1.1.3" xref="S5.SS1.p1.10.m10.1.1.3a.cmml">doc</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.10.m10.1b"><apply id="S5.SS1.p1.10.m10.1.1.cmml" xref="S5.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.10.m10.1.1.1.cmml" xref="S5.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S5.SS1.p1.10.m10.1.1.2.cmml" xref="S5.SS1.p1.10.m10.1.1.2">𝑖</ci><ci id="S5.SS1.p1.10.m10.1.1.3a.cmml" xref="S5.SS1.p1.10.m10.1.1.3"><mtext id="S5.SS1.p1.10.m10.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.10.m10.1.1.3">doc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.10.m10.1c">i_{\text{doc}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.10.m10.1d">italic_i start_POSTSUBSCRIPT doc end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="i_{\text{shot}}" class="ltx_Math" display="inline" id="S5.SS1.p1.11.m11.1"><semantics id="S5.SS1.p1.11.m11.1a"><msub id="S5.SS1.p1.11.m11.1.1" xref="S5.SS1.p1.11.m11.1.1.cmml"><mi id="S5.SS1.p1.11.m11.1.1.2" xref="S5.SS1.p1.11.m11.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.11.m11.1.1.3" xref="S5.SS1.p1.11.m11.1.1.3a.cmml">shot</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.11.m11.1b"><apply id="S5.SS1.p1.11.m11.1.1.cmml" xref="S5.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.11.m11.1.1.1.cmml" xref="S5.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S5.SS1.p1.11.m11.1.1.2.cmml" xref="S5.SS1.p1.11.m11.1.1.2">𝑖</ci><ci id="S5.SS1.p1.11.m11.1.1.3a.cmml" xref="S5.SS1.p1.11.m11.1.1.3"><mtext id="S5.SS1.p1.11.m11.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.11.m11.1.1.3">shot</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.11.m11.1c">i_{\text{shot}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.11.m11.1d">italic_i start_POSTSUBSCRIPT shot end_POSTSUBSCRIPT</annotation></semantics></math> measure the informativeness of documents and in-context examples respectively. While technically we can also define an <math alttext="i_{\text{iter}}" class="ltx_Math" display="inline" id="S5.SS1.p1.12.m12.1"><semantics id="S5.SS1.p1.12.m12.1a"><msub id="S5.SS1.p1.12.m12.1.1" xref="S5.SS1.p1.12.m12.1.1.cmml"><mi id="S5.SS1.p1.12.m12.1.1.2" xref="S5.SS1.p1.12.m12.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.12.m12.1.1.3" xref="S5.SS1.p1.12.m12.1.1.3a.cmml">iter</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.12.m12.1b"><apply id="S5.SS1.p1.12.m12.1.1.cmml" xref="S5.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.12.m12.1.1.1.cmml" xref="S5.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S5.SS1.p1.12.m12.1.1.2.cmml" xref="S5.SS1.p1.12.m12.1.1.2">𝑖</ci><ci id="S5.SS1.p1.12.m12.1.1.3a.cmml" xref="S5.SS1.p1.12.m12.1.1.3"><mtext id="S5.SS1.p1.12.m12.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.12.m12.1.1.3">iter</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.12.m12.1c">i_{\text{iter}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.12.m12.1d">italic_i start_POSTSUBSCRIPT iter end_POSTSUBSCRIPT</annotation></semantics></math> to measure the informativeness of additional generation steps, applying <math alttext="i_{\text{iter}}" class="ltx_Math" display="inline" id="S5.SS1.p1.13.m13.1"><semantics id="S5.SS1.p1.13.m13.1a"><msub id="S5.SS1.p1.13.m13.1.1" xref="S5.SS1.p1.13.m13.1.1.cmml"><mi id="S5.SS1.p1.13.m13.1.1.2" xref="S5.SS1.p1.13.m13.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.13.m13.1.1.3" xref="S5.SS1.p1.13.m13.1.1.3a.cmml">iter</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.13.m13.1b"><apply id="S5.SS1.p1.13.m13.1.1.cmml" xref="S5.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.13.m13.1.1.1.cmml" xref="S5.SS1.p1.13.m13.1.1">subscript</csymbol><ci id="S5.SS1.p1.13.m13.1.1.2.cmml" xref="S5.SS1.p1.13.m13.1.1.2">𝑖</ci><ci id="S5.SS1.p1.13.m13.1.1.3a.cmml" xref="S5.SS1.p1.13.m13.1.1.3"><mtext id="S5.SS1.p1.13.m13.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.13.m13.1.1.3">iter</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.13.m13.1c">i_{\text{iter}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.13.m13.1d">italic_i start_POSTSUBSCRIPT iter end_POSTSUBSCRIPT</annotation></semantics></math> does not yield improved accuracy, so we leave it as 0 in our experiments. We formulate the computation allocation model as<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In our implementation, we shift the values within <math alttext="\theta" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\theta</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">italic_θ</annotation></semantics></math> by a small <math alttext="\epsilon" class="ltx_Math" display="inline" id="footnote2.m2.1"><semantics id="footnote2.m2.1b"><mi id="footnote2.m2.1.1" xref="footnote2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="footnote2.m2.1c"><ci id="footnote2.m2.1.1.cmml" xref="footnote2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.1d">\epsilon</annotation><annotation encoding="application/x-llamapun" id="footnote2.m2.1e">italic_ϵ</annotation></semantics></math> to prevent numerical issues with <math alttext="\log(0)" class="ltx_Math" display="inline" id="footnote2.m3.2"><semantics id="footnote2.m3.2b"><mrow id="footnote2.m3.2.3.2" xref="footnote2.m3.2.3.1.cmml"><mi id="footnote2.m3.1.1" xref="footnote2.m3.1.1.cmml">log</mi><mo id="footnote2.m3.2.3.2b" xref="footnote2.m3.2.3.1.cmml">⁡</mo><mrow id="footnote2.m3.2.3.2.1" xref="footnote2.m3.2.3.1.cmml"><mo id="footnote2.m3.2.3.2.1.1" stretchy="false" xref="footnote2.m3.2.3.1.cmml">(</mo><mn id="footnote2.m3.2.2" xref="footnote2.m3.2.2.cmml">0</mn><mo id="footnote2.m3.2.3.2.1.2" stretchy="false" xref="footnote2.m3.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m3.2c"><apply id="footnote2.m3.2.3.1.cmml" xref="footnote2.m3.2.3.2"><log id="footnote2.m3.1.1.cmml" xref="footnote2.m3.1.1"></log><cn id="footnote2.m3.2.2.cmml" type="integer" xref="footnote2.m3.2.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m3.2d">\log(0)</annotation><annotation encoding="application/x-llamapun" id="footnote2.m3.2e">roman_log ( 0 )</annotation></semantics></math>.</span></span></span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sigma^{-1}(P(\theta))\approx(a+b\odot i)^{T}\log(\theta)+c," class="ltx_Math" display="block" id="S5.E2.m1.4"><semantics id="S5.E2.m1.4a"><mrow id="S5.E2.m1.4.4.1" xref="S5.E2.m1.4.4.1.1.cmml"><mrow id="S5.E2.m1.4.4.1.1" xref="S5.E2.m1.4.4.1.1.cmml"><mrow id="S5.E2.m1.4.4.1.1.1" xref="S5.E2.m1.4.4.1.1.1.cmml"><msup id="S5.E2.m1.4.4.1.1.1.3" xref="S5.E2.m1.4.4.1.1.1.3.cmml"><mi id="S5.E2.m1.4.4.1.1.1.3.2" xref="S5.E2.m1.4.4.1.1.1.3.2.cmml">σ</mi><mrow id="S5.E2.m1.4.4.1.1.1.3.3" xref="S5.E2.m1.4.4.1.1.1.3.3.cmml"><mo id="S5.E2.m1.4.4.1.1.1.3.3a" xref="S5.E2.m1.4.4.1.1.1.3.3.cmml">−</mo><mn id="S5.E2.m1.4.4.1.1.1.3.3.2" xref="S5.E2.m1.4.4.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo id="S5.E2.m1.4.4.1.1.1.2" xref="S5.E2.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.4.4.1.1.1.1.1" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S5.E2.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E2.m1.4.4.1.1.1.1.1.1" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.4.4.1.1.1.1.1.1.2" xref="S5.E2.m1.4.4.1.1.1.1.1.1.2.cmml">P</mi><mo id="S5.E2.m1.4.4.1.1.1.1.1.1.1" xref="S5.E2.m1.4.4.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S5.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S5.E2.m1.4.4.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mi id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml">θ</mi><mo id="S5.E2.m1.4.4.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S5.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.4.4.1.1.3" xref="S5.E2.m1.4.4.1.1.3.cmml">≈</mo><mrow id="S5.E2.m1.4.4.1.1.2" xref="S5.E2.m1.4.4.1.1.2.cmml"><mrow id="S5.E2.m1.4.4.1.1.2.1" xref="S5.E2.m1.4.4.1.1.2.1.cmml"><msup id="S5.E2.m1.4.4.1.1.2.1.1" xref="S5.E2.m1.4.4.1.1.2.1.1.cmml"><mrow id="S5.E2.m1.4.4.1.1.2.1.1.1.1" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.cmml"><mo id="S5.E2.m1.4.4.1.1.2.1.1.1.1.2" stretchy="false" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.cmml">(</mo><mrow id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.cmml"><mi id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.2" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.2.cmml">a</mi><mo id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.1" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml">+</mo><mrow id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.cmml"><mi id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.2" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.2.cmml">b</mi><mo id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.1.cmml">⊙</mo><mi id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.3" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></mrow><mo id="S5.E2.m1.4.4.1.1.2.1.1.1.1.3" stretchy="false" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.E2.m1.4.4.1.1.2.1.1.3" xref="S5.E2.m1.4.4.1.1.2.1.1.3.cmml">T</mi></msup><mo id="S5.E2.m1.4.4.1.1.2.1.2" lspace="0.167em" xref="S5.E2.m1.4.4.1.1.2.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.4.4.1.1.2.1.3.2" xref="S5.E2.m1.4.4.1.1.2.1.3.1.cmml"><mi id="S5.E2.m1.2.2" xref="S5.E2.m1.2.2.cmml">log</mi><mo id="S5.E2.m1.4.4.1.1.2.1.3.2a" xref="S5.E2.m1.4.4.1.1.2.1.3.1.cmml">⁡</mo><mrow id="S5.E2.m1.4.4.1.1.2.1.3.2.1" xref="S5.E2.m1.4.4.1.1.2.1.3.1.cmml"><mo id="S5.E2.m1.4.4.1.1.2.1.3.2.1.1" stretchy="false" xref="S5.E2.m1.4.4.1.1.2.1.3.1.cmml">(</mo><mi id="S5.E2.m1.3.3" xref="S5.E2.m1.3.3.cmml">θ</mi><mo id="S5.E2.m1.4.4.1.1.2.1.3.2.1.2" stretchy="false" xref="S5.E2.m1.4.4.1.1.2.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E2.m1.4.4.1.1.2.2" xref="S5.E2.m1.4.4.1.1.2.2.cmml">+</mo><mi id="S5.E2.m1.4.4.1.1.2.3" xref="S5.E2.m1.4.4.1.1.2.3.cmml">c</mi></mrow></mrow><mo id="S5.E2.m1.4.4.1.2" xref="S5.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.4b"><apply id="S5.E2.m1.4.4.1.1.cmml" xref="S5.E2.m1.4.4.1"><approx id="S5.E2.m1.4.4.1.1.3.cmml" xref="S5.E2.m1.4.4.1.1.3"></approx><apply id="S5.E2.m1.4.4.1.1.1.cmml" xref="S5.E2.m1.4.4.1.1.1"><times id="S5.E2.m1.4.4.1.1.1.2.cmml" xref="S5.E2.m1.4.4.1.1.1.2"></times><apply id="S5.E2.m1.4.4.1.1.1.3.cmml" xref="S5.E2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.4.4.1.1.1.3.1.cmml" xref="S5.E2.m1.4.4.1.1.1.3">superscript</csymbol><ci id="S5.E2.m1.4.4.1.1.1.3.2.cmml" xref="S5.E2.m1.4.4.1.1.1.3.2">𝜎</ci><apply id="S5.E2.m1.4.4.1.1.1.3.3.cmml" xref="S5.E2.m1.4.4.1.1.1.3.3"><minus id="S5.E2.m1.4.4.1.1.1.3.3.1.cmml" xref="S5.E2.m1.4.4.1.1.1.3.3"></minus><cn id="S5.E2.m1.4.4.1.1.1.3.3.2.cmml" type="integer" xref="S5.E2.m1.4.4.1.1.1.3.3.2">1</cn></apply></apply><apply id="S5.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S5.E2.m1.4.4.1.1.1.1.1"><times id="S5.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.4.4.1.1.1.1.1.1.1"></times><ci id="S5.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.4.4.1.1.1.1.1.1.2">𝑃</ci><ci id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1">𝜃</ci></apply></apply><apply id="S5.E2.m1.4.4.1.1.2.cmml" xref="S5.E2.m1.4.4.1.1.2"><plus id="S5.E2.m1.4.4.1.1.2.2.cmml" xref="S5.E2.m1.4.4.1.1.2.2"></plus><apply id="S5.E2.m1.4.4.1.1.2.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1"><times id="S5.E2.m1.4.4.1.1.2.1.2.cmml" xref="S5.E2.m1.4.4.1.1.2.1.2"></times><apply id="S5.E2.m1.4.4.1.1.2.1.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1"><csymbol cd="ambiguous" id="S5.E2.m1.4.4.1.1.2.1.1.2.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1">superscript</csymbol><apply id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1"><plus id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.1"></plus><ci id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.2.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.2">𝑎</ci><apply id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3"><csymbol cd="latexml" id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.1">direct-product</csymbol><ci id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.2">𝑏</ci><ci id="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S5.E2.m1.4.4.1.1.2.1.1.3.cmml" xref="S5.E2.m1.4.4.1.1.2.1.1.3">𝑇</ci></apply><apply id="S5.E2.m1.4.4.1.1.2.1.3.1.cmml" xref="S5.E2.m1.4.4.1.1.2.1.3.2"><log id="S5.E2.m1.2.2.cmml" xref="S5.E2.m1.2.2"></log><ci id="S5.E2.m1.3.3.cmml" xref="S5.E2.m1.3.3">𝜃</ci></apply></apply><ci id="S5.E2.m1.4.4.1.1.2.3.cmml" xref="S5.E2.m1.4.4.1.1.2.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.4c">\sigma^{-1}(P(\theta))\approx(a+b\odot i)^{T}\log(\theta)+c,</annotation><annotation encoding="application/x-llamapun" id="S5.E2.m1.4d">italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_P ( italic_θ ) ) ≈ ( italic_a + italic_b ⊙ italic_i ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_log ( italic_θ ) + italic_c ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p1.24">where <math alttext="\odot" class="ltx_Math" display="inline" id="S5.SS1.p1.14.m1.1"><semantics id="S5.SS1.p1.14.m1.1a"><mo id="S5.SS1.p1.14.m1.1.1" xref="S5.SS1.p1.14.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.14.m1.1b"><csymbol cd="latexml" id="S5.SS1.p1.14.m1.1.1.cmml" xref="S5.SS1.p1.14.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.14.m1.1c">\odot</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.14.m1.1d">⊙</annotation></semantics></math> refers to element-wise product. <math alttext="a" class="ltx_Math" display="inline" id="S5.SS1.p1.15.m2.1"><semantics id="S5.SS1.p1.15.m2.1a"><mi id="S5.SS1.p1.15.m2.1.1" xref="S5.SS1.p1.15.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.15.m2.1b"><ci id="S5.SS1.p1.15.m2.1.1.cmml" xref="S5.SS1.p1.15.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.15.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.15.m2.1d">italic_a</annotation></semantics></math>, <math alttext="b" class="ltx_Math" display="inline" id="S5.SS1.p1.16.m3.1"><semantics id="S5.SS1.p1.16.m3.1a"><mi id="S5.SS1.p1.16.m3.1.1" xref="S5.SS1.p1.16.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.16.m3.1b"><ci id="S5.SS1.p1.16.m3.1.1.cmml" xref="S5.SS1.p1.16.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.16.m3.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.16.m3.1d">italic_b</annotation></semantics></math> and <math alttext="c" class="ltx_Math" display="inline" id="S5.SS1.p1.17.m4.1"><semantics id="S5.SS1.p1.17.m4.1a"><mi id="S5.SS1.p1.17.m4.1.1" xref="S5.SS1.p1.17.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.17.m4.1b"><ci id="S5.SS1.p1.17.m4.1.1.cmml" xref="S5.SS1.p1.17.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.17.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.17.m4.1d">italic_c</annotation></semantics></math> (scalar) are parameters to be estimated, and <math alttext="i" class="ltx_Math" display="inline" id="S5.SS1.p1.18.m5.1"><semantics id="S5.SS1.p1.18.m5.1a"><mi id="S5.SS1.p1.18.m5.1.1" xref="S5.SS1.p1.18.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.18.m5.1b"><ci id="S5.SS1.p1.18.m5.1.1.cmml" xref="S5.SS1.p1.18.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.18.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.18.m5.1d">italic_i</annotation></semantics></math> can be computed base on the specific task. There are different ways to define <math alttext="i" class="ltx_Math" display="inline" id="S5.SS1.p1.19.m6.1"><semantics id="S5.SS1.p1.19.m6.1a"><mi id="S5.SS1.p1.19.m6.1.1" xref="S5.SS1.p1.19.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.19.m6.1b"><ci id="S5.SS1.p1.19.m6.1.1.cmml" xref="S5.SS1.p1.19.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.19.m6.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.19.m6.1d">italic_i</annotation></semantics></math>; we choose a definition to compute <math alttext="i" class="ltx_Math" display="inline" id="S5.SS1.p1.20.m7.1"><semantics id="S5.SS1.p1.20.m7.1a"><mi id="S5.SS1.p1.20.m7.1.1" xref="S5.SS1.p1.20.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.20.m7.1b"><ci id="S5.SS1.p1.20.m7.1.1.cmml" xref="S5.SS1.p1.20.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.20.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.20.m7.1d">italic_i</annotation></semantics></math> based on the performance difference between selected base configurations. In particular, for each strategy on each dataset, <math alttext="i_{\text{doc}}" class="ltx_Math" display="inline" id="S5.SS1.p1.21.m8.1"><semantics id="S5.SS1.p1.21.m8.1a"><msub id="S5.SS1.p1.21.m8.1.1" xref="S5.SS1.p1.21.m8.1.1.cmml"><mi id="S5.SS1.p1.21.m8.1.1.2" xref="S5.SS1.p1.21.m8.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.21.m8.1.1.3" xref="S5.SS1.p1.21.m8.1.1.3a.cmml">doc</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.21.m8.1b"><apply id="S5.SS1.p1.21.m8.1.1.cmml" xref="S5.SS1.p1.21.m8.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.21.m8.1.1.1.cmml" xref="S5.SS1.p1.21.m8.1.1">subscript</csymbol><ci id="S5.SS1.p1.21.m8.1.1.2.cmml" xref="S5.SS1.p1.21.m8.1.1.2">𝑖</ci><ci id="S5.SS1.p1.21.m8.1.1.3a.cmml" xref="S5.SS1.p1.21.m8.1.1.3"><mtext id="S5.SS1.p1.21.m8.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.21.m8.1.1.3">doc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.21.m8.1c">i_{\text{doc}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.21.m8.1d">italic_i start_POSTSUBSCRIPT doc end_POSTSUBSCRIPT</annotation></semantics></math> is defined as the performance gain by only adding one document compared to zero-shot QA. Similarly, <math alttext="i_{\text{shot}}" class="ltx_Math" display="inline" id="S5.SS1.p1.22.m9.1"><semantics id="S5.SS1.p1.22.m9.1a"><msub id="S5.SS1.p1.22.m9.1.1" xref="S5.SS1.p1.22.m9.1.1.cmml"><mi id="S5.SS1.p1.22.m9.1.1.2" xref="S5.SS1.p1.22.m9.1.1.2.cmml">i</mi><mtext id="S5.SS1.p1.22.m9.1.1.3" xref="S5.SS1.p1.22.m9.1.1.3a.cmml">shot</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.22.m9.1b"><apply id="S5.SS1.p1.22.m9.1.1.cmml" xref="S5.SS1.p1.22.m9.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.22.m9.1.1.1.cmml" xref="S5.SS1.p1.22.m9.1.1">subscript</csymbol><ci id="S5.SS1.p1.22.m9.1.1.2.cmml" xref="S5.SS1.p1.22.m9.1.1.2">𝑖</ci><ci id="S5.SS1.p1.22.m9.1.1.3a.cmml" xref="S5.SS1.p1.22.m9.1.1.3"><mtext id="S5.SS1.p1.22.m9.1.1.3.cmml" mathsize="70%" xref="S5.SS1.p1.22.m9.1.1.3">shot</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.22.m9.1c">i_{\text{shot}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.22.m9.1d">italic_i start_POSTSUBSCRIPT shot end_POSTSUBSCRIPT</annotation></semantics></math> is defined as the performance gain by adding only one in-context example compared to zero-shot QA. To account for the sub-linearity in extremely long contexts (above 1M), we apply an inverse sigmoidal mapping <math alttext="\sigma^{-1}" class="ltx_Math" display="inline" id="S5.SS1.p1.23.m10.1"><semantics id="S5.SS1.p1.23.m10.1a"><msup id="S5.SS1.p1.23.m10.1.1" xref="S5.SS1.p1.23.m10.1.1.cmml"><mi id="S5.SS1.p1.23.m10.1.1.2" xref="S5.SS1.p1.23.m10.1.1.2.cmml">σ</mi><mrow id="S5.SS1.p1.23.m10.1.1.3" xref="S5.SS1.p1.23.m10.1.1.3.cmml"><mo id="S5.SS1.p1.23.m10.1.1.3a" xref="S5.SS1.p1.23.m10.1.1.3.cmml">−</mo><mn id="S5.SS1.p1.23.m10.1.1.3.2" xref="S5.SS1.p1.23.m10.1.1.3.2.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.23.m10.1b"><apply id="S5.SS1.p1.23.m10.1.1.cmml" xref="S5.SS1.p1.23.m10.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.23.m10.1.1.1.cmml" xref="S5.SS1.p1.23.m10.1.1">superscript</csymbol><ci id="S5.SS1.p1.23.m10.1.1.2.cmml" xref="S5.SS1.p1.23.m10.1.1.2">𝜎</ci><apply id="S5.SS1.p1.23.m10.1.1.3.cmml" xref="S5.SS1.p1.23.m10.1.1.3"><minus id="S5.SS1.p1.23.m10.1.1.3.1.cmml" xref="S5.SS1.p1.23.m10.1.1.3"></minus><cn id="S5.SS1.p1.23.m10.1.1.3.2.cmml" type="integer" xref="S5.SS1.p1.23.m10.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.23.m10.1c">\sigma^{-1}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.23.m10.1d">italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> to scale the values of the metric <math alttext="P" class="ltx_Math" display="inline" id="S5.SS1.p1.24.m11.1"><semantics id="S5.SS1.p1.24.m11.1a"><mi id="S5.SS1.p1.24.m11.1.1" xref="S5.SS1.p1.24.m11.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.24.m11.1b"><ci id="S5.SS1.p1.24.m11.1.1.cmml" xref="S5.SS1.p1.24.m11.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.24.m11.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.24.m11.1d">italic_P</annotation></semantics></math>. Further implementation details are reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A7" title="Appendix G Implementation ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S5.F6.g1" src="x5.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.3.2" style="font-size:90%;">The estimated performance using the proposed observational scaling laws vs. actual metric values in DRAG. The subplots represent different datasets, where each line corresponds to a fixed number of documents, we scale the context length by increasing the number of shots.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.8">In <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.E2" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a>, estimations on <math alttext="a" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_a</annotation></semantics></math>, <math alttext="b" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_b</annotation></semantics></math> and <math alttext="c" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mi id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><ci id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_c</annotation></semantics></math> are specific to a certain model, reflecting how LLMs improve with varying number of documents and shots (i.e., in-context learning / zero-shot capabilities). In contrast, <math alttext="i" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><mi id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><ci id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">italic_i</annotation></semantics></math> models the performance variations within the selected task (i.e., how external knowledge / demonstrations help responding to the query). Therefore, the computation allocation model can be estimated once and applied to various downstream tasks without requiring additional calibration. To estimate the parameters, varying combinations of <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS1.p2.5.m5.1"><semantics id="S5.SS1.p2.5.m5.1a"><mi id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><ci id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.5.m5.1d">italic_θ</annotation></semantics></math> are evaluated to perform ordinary least squares on <math alttext="a" class="ltx_Math" display="inline" id="S5.SS1.p2.6.m6.1"><semantics id="S5.SS1.p2.6.m6.1a"><mi id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><ci id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">a</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.6.m6.1d">italic_a</annotation></semantics></math>, <math alttext="b" class="ltx_Math" display="inline" id="S5.SS1.p2.7.m7.1"><semantics id="S5.SS1.p2.7.m7.1a"><mi id="S5.SS1.p2.7.m7.1.1" xref="S5.SS1.p2.7.m7.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.1b"><ci id="S5.SS1.p2.7.m7.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.7.m7.1d">italic_b</annotation></semantics></math> and <math alttext="c" class="ltx_Math" display="inline" id="S5.SS1.p2.8.m8.1"><semantics id="S5.SS1.p2.8.m8.1a"><mi id="S5.SS1.p2.8.m8.1.1" xref="S5.SS1.p2.8.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.8.m8.1b"><ci id="S5.SS1.p2.8.m8.1.1.cmml" xref="S5.SS1.p2.8.m8.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.8.m8.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.8.m8.1d">italic_c</annotation></semantics></math>. We report the parameters for Gemini 1.5 Flash in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5" title="Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Validating the Computation Allocation Model for RAG</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.4">We evaluate the computation allocation model for RAG by comparing the predicted metrics to the actual values, with normalized results for DRAG visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.F6" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>. Here, each subplot represents a different dataset, and each line corresponds to a document setting (<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math>), we scale the context length by adjusting in-context examples (<math alttext="m" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_m</annotation></semantics></math>). As illustrated, the performance improves with the increase of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">italic_k</annotation></semantics></math> and <math alttext="m" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">italic_m</annotation></semantics></math> across datasets, displaying highly consistent trends between the predicted and actual metric values, despite some variations. Notably, each dataset exhibits different levels of consistency: Bamboogle exhibits the highest consistency, while HotpotQA generates more variable results. Our findings demonstrate how external knowledge and in-context learning can effectively enhance RAG performance with long-context capabilities, suggesting the effectiveness of the computation allocation model for RAG and how they may be used to predict benchmark results.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study results of the computation allocation model for RAG.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.4">
<td class="ltx_td ltx_border_tt" id="S5.T2.4.4.5" rowspan="2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1" style="font-size:90%;">Exclude <math alttext="b" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">italic_b</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.1" style="font-size:90%;">Quadratic <math alttext="\theta" class="ltx_Math" display="inline" id="S5.T2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.1.m1.1a"><mi id="S5.T2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.1.m1.1d">italic_θ</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.3.3.1" style="font-size:90%;">Linear <math alttext="\sigma" class="ltx_Math" display="inline" id="S5.T2.3.3.3.1.m1.1"><semantics id="S5.T2.3.3.3.1.m1.1a"><mi id="S5.T2.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.1.m1.1d">italic_σ</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.1" style="font-size:90%;">Sigmoidal <math alttext="\sigma" class="ltx_Math" display="inline" id="S5.T2.4.4.4.1.m1.1"><semantics id="S5.T2.4.4.4.1.m1.1a"><mi id="S5.T2.4.4.4.1.m1.1.1" xref="S5.T2.4.4.4.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.m1.1b"><ci id="S5.T2.4.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.m1.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.1.m1.1d">italic_σ</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.1"><math alttext="R^{2}" class="ltx_Math" display="inline" id="S5.T2.5.5.1.m1.1"><semantics id="S5.T2.5.5.1.m1.1a"><msup id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml"><mi id="S5.T2.5.5.1.m1.1.1.2" mathsize="90%" xref="S5.T2.5.5.1.m1.1.1.2.cmml">R</mi><mn id="S5.T2.5.5.1.m1.1.1.3" mathsize="90%" xref="S5.T2.5.5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><apply id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.5.5.1.m1.1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1">superscript</csymbol><ci id="S5.T2.5.5.1.m1.1.1.2.cmml" xref="S5.T2.5.5.1.m1.1.1.2">𝑅</ci><cn id="S5.T2.5.5.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.5.5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.1.m1.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.5"><span class="ltx_text" id="S5.T2.8.8.5.1" style="font-size:90%;">MSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.2"><math alttext="R^{2}" class="ltx_Math" display="inline" id="S5.T2.6.6.2.m1.1"><semantics id="S5.T2.6.6.2.m1.1a"><msup id="S5.T2.6.6.2.m1.1.1" xref="S5.T2.6.6.2.m1.1.1.cmml"><mi id="S5.T2.6.6.2.m1.1.1.2" mathsize="90%" xref="S5.T2.6.6.2.m1.1.1.2.cmml">R</mi><mn id="S5.T2.6.6.2.m1.1.1.3" mathsize="90%" xref="S5.T2.6.6.2.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.2.m1.1b"><apply id="S5.T2.6.6.2.m1.1.1.cmml" xref="S5.T2.6.6.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.6.6.2.m1.1.1.1.cmml" xref="S5.T2.6.6.2.m1.1.1">superscript</csymbol><ci id="S5.T2.6.6.2.m1.1.1.2.cmml" xref="S5.T2.6.6.2.m1.1.1.2">𝑅</ci><cn id="S5.T2.6.6.2.m1.1.1.3.cmml" type="integer" xref="S5.T2.6.6.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.2.m1.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.2.m1.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.6"><span class="ltx_text" id="S5.T2.8.8.6.1" style="font-size:90%;">MSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.7.7.3"><math alttext="R^{2}" class="ltx_Math" display="inline" id="S5.T2.7.7.3.m1.1"><semantics id="S5.T2.7.7.3.m1.1a"><msup id="S5.T2.7.7.3.m1.1.1" xref="S5.T2.7.7.3.m1.1.1.cmml"><mi id="S5.T2.7.7.3.m1.1.1.2" mathsize="90%" xref="S5.T2.7.7.3.m1.1.1.2.cmml">R</mi><mn id="S5.T2.7.7.3.m1.1.1.3" mathsize="90%" xref="S5.T2.7.7.3.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.3.m1.1b"><apply id="S5.T2.7.7.3.m1.1.1.cmml" xref="S5.T2.7.7.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.7.7.3.m1.1.1.1.cmml" xref="S5.T2.7.7.3.m1.1.1">superscript</csymbol><ci id="S5.T2.7.7.3.m1.1.1.2.cmml" xref="S5.T2.7.7.3.m1.1.1.2">𝑅</ci><cn id="S5.T2.7.7.3.m1.1.1.3.cmml" type="integer" xref="S5.T2.7.7.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.3.m1.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.3.m1.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.7"><span class="ltx_text" id="S5.T2.8.8.7.1" style="font-size:90%;">MSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.4"><math alttext="R^{2}" class="ltx_Math" display="inline" id="S5.T2.8.8.4.m1.1"><semantics id="S5.T2.8.8.4.m1.1a"><msup id="S5.T2.8.8.4.m1.1.1" xref="S5.T2.8.8.4.m1.1.1.cmml"><mi id="S5.T2.8.8.4.m1.1.1.2" mathsize="90%" xref="S5.T2.8.8.4.m1.1.1.2.cmml">R</mi><mn id="S5.T2.8.8.4.m1.1.1.3" mathsize="90%" xref="S5.T2.8.8.4.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.4.m1.1b"><apply id="S5.T2.8.8.4.m1.1.1.cmml" xref="S5.T2.8.8.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.8.8.4.m1.1.1.1.cmml" xref="S5.T2.8.8.4.m1.1.1">superscript</csymbol><ci id="S5.T2.8.8.4.m1.1.1.2.cmml" xref="S5.T2.8.8.4.m1.1.1.2">𝑅</ci><cn id="S5.T2.8.8.4.m1.1.1.3.cmml" type="integer" xref="S5.T2.8.8.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.4.m1.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.8.4.m1.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.8.8.8"><span class="ltx_text" id="S5.T2.8.8.8.1" style="font-size:90%;">MSE</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.9.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.1"><span class="ltx_text" id="S5.T2.8.9.1.1.1" style="font-size:90%;">Values</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.2"><span class="ltx_text" id="S5.T2.8.9.1.2.1" style="font-size:90%;">0.866</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.3"><span class="ltx_text" id="S5.T2.8.9.1.3.1" style="font-size:90%;">0.116</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.4"><span class="ltx_text" id="S5.T2.8.9.1.4.1" style="font-size:90%;">0.867</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.5"><span class="ltx_text" id="S5.T2.8.9.1.5.1" style="font-size:90%;">0.117</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.6"><span class="ltx_text" id="S5.T2.8.9.1.6.1" style="font-size:90%;">0.876</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.7"><span class="ltx_text" id="S5.T2.8.9.1.7.1" style="font-size:90%;">0.109</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.8"><span class="ltx_text ltx_font_bold" id="S5.T2.8.9.1.8.1" style="font-size:90%;">0.903</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.8.9.1.9"><span class="ltx_text ltx_font_bold" id="S5.T2.8.9.1.9.1" style="font-size:90%;">0.085</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Ablation Study.</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.14">To verify the effectiveness of the computation allocation model, we perform ablation studies and evaluate the fitting performance of different variants. In particular, we assess: (1) estimation without <math alttext="b" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.1.m1.1d">italic_b</annotation></semantics></math> and <math alttext="i" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.2.m2.1d">italic_i</annotation></semantics></math> (i.e., Exclude <math alttext="b" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS2.SSS0.Px1.p1.3.m3.1a"><mi id="S5.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.3.m3.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.3.m3.1d">italic_b</annotation></semantics></math>); (2) a quadratic form of input <math alttext="\log(\theta)" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.4.m4.2"><semantics id="S5.SS2.SSS0.Px1.p1.4.m4.2a"><mrow id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mi id="S5.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S5.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">log</mi><mo id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2a" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">⁡</mo><mrow id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2.1" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mo id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2.1.1" stretchy="false" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">(</mo><mi id="S5.SS2.SSS0.Px1.p1.4.m4.2.2" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.2.cmml">θ</mi><mo id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2.1.2" stretchy="false" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.4.m4.2b"><apply id="S5.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.3.2"><log id="S5.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.4.m4.1.1"></log><ci id="S5.SS2.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S5.SS2.SSS0.Px1.p1.4.m4.2.2">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.4.m4.2c">\log(\theta)</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.4.m4.2d">roman_log ( italic_θ )</annotation></semantics></math> (Quadratic <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.5.m5.1"><semantics id="S5.SS2.SSS0.Px1.p1.5.m5.1a"><mi id="S5.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S5.SS2.SSS0.Px1.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.5.m5.1b"><ci id="S5.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.5.m5.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.5.m5.1d">italic_θ</annotation></semantics></math>); (3) linear scaling of <math alttext="P" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.6.m6.1"><semantics id="S5.SS2.SSS0.Px1.p1.6.m6.1a"><mi id="S5.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S5.SS2.SSS0.Px1.p1.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.6.m6.1b"><ci id="S5.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.6.m6.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.6.m6.1d">italic_P</annotation></semantics></math> (Linear <math alttext="\sigma" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.7.m7.1"><semantics id="S5.SS2.SSS0.Px1.p1.7.m7.1a"><mi id="S5.SS2.SSS0.Px1.p1.7.m7.1.1" xref="S5.SS2.SSS0.Px1.p1.7.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.7.m7.1b"><ci id="S5.SS2.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.7.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.7.m7.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.7.m7.1d">italic_σ</annotation></semantics></math>); and (4) sigmoid scaling of <math alttext="P" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.8.m8.1"><semantics id="S5.SS2.SSS0.Px1.p1.8.m8.1a"><mi id="S5.SS2.SSS0.Px1.p1.8.m8.1.1" xref="S5.SS2.SSS0.Px1.p1.8.m8.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.8.m8.1b"><ci id="S5.SS2.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.8.m8.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.8.m8.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.8.m8.1d">italic_P</annotation></semantics></math> (Sigmoidal <math alttext="\sigma" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.9.m9.1"><semantics id="S5.SS2.SSS0.Px1.p1.9.m9.1a"><mi id="S5.SS2.SSS0.Px1.p1.9.m9.1.1" xref="S5.SS2.SSS0.Px1.p1.9.m9.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.9.m9.1b"><ci id="S5.SS2.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.9.m9.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.9.m9.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.9.m9.1d">italic_σ</annotation></semantics></math>). The <math alttext="R^{2}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.10.m10.1"><semantics id="S5.SS2.SSS0.Px1.p1.10.m10.1a"><msup id="S5.SS2.SSS0.Px1.p1.10.m10.1.1" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1.cmml"><mi id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.2" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1.2.cmml">R</mi><mn id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.3" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.10.m10.1b"><apply id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1">superscript</csymbol><ci id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.2.cmml" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1.2">𝑅</ci><cn id="S5.SS2.SSS0.Px1.p1.10.m10.1.1.3.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.10.m10.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.10.m10.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.10.m10.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and MSE values for these variants are reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.T2" title="In 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, in which (4) represents the complete design of our computation allocation model. The results indicate that incorporating the additional <math alttext="b" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.11.m11.1"><semantics id="S5.SS2.SSS0.Px1.p1.11.m11.1a"><mi id="S5.SS2.SSS0.Px1.p1.11.m11.1.1" xref="S5.SS2.SSS0.Px1.p1.11.m11.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.11.m11.1b"><ci id="S5.SS2.SSS0.Px1.p1.11.m11.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.11.m11.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.11.m11.1c">b</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.11.m11.1d">italic_b</annotation></semantics></math> with <math alttext="i" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.12.m12.1"><semantics id="S5.SS2.SSS0.Px1.p1.12.m12.1a"><mi id="S5.SS2.SSS0.Px1.p1.12.m12.1.1" xref="S5.SS2.SSS0.Px1.p1.12.m12.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.12.m12.1b"><ci id="S5.SS2.SSS0.Px1.p1.12.m12.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.12.m12.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.12.m12.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.12.m12.1d">italic_i</annotation></semantics></math> enhances the relevance and reduces error across all tasks. Moreover, applying inverse sigmoid to <math alttext="P" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.13.m13.1"><semantics id="S5.SS2.SSS0.Px1.p1.13.m13.1a"><mi id="S5.SS2.SSS0.Px1.p1.13.m13.1.1" xref="S5.SS2.SSS0.Px1.p1.13.m13.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.13.m13.1b"><ci id="S5.SS2.SSS0.Px1.p1.13.m13.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.13.m13.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.13.m13.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.13.m13.1d">italic_P</annotation></semantics></math> significantly improves the estimation in comparison to quadratic <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.14.m14.1"><semantics id="S5.SS2.SSS0.Px1.p1.14.m14.1a"><mi id="S5.SS2.SSS0.Px1.p1.14.m14.1.1" xref="S5.SS2.SSS0.Px1.p1.14.m14.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.14.m14.1b"><ci id="S5.SS2.SSS0.Px1.p1.14.m14.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.14.m14.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.14.m14.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.14.m14.1d">italic_θ</annotation></semantics></math> or linear scaling.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Domain generalization results of the computation allocation model for RAG.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.4.1.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T3.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.2.1" style="font-size:90%;">Bamboogle</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T3.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.3.1" style="font-size:90%;">HotpotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T3.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.4.1" style="font-size:90%;">MuSiQue</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T3.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.5.1" style="font-size:90%;">2WikiMultiHopQA</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.1"><span class="ltx_text" id="S5.T3.4.2.2.1.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.2"><span class="ltx_text" id="S5.T3.4.2.2.2.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.3"><span class="ltx_text" id="S5.T3.4.2.2.3.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.4"><span class="ltx_text" id="S5.T3.4.2.2.4.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.5"><span class="ltx_text" id="S5.T3.4.2.2.5.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.6"><span class="ltx_text" id="S5.T3.4.2.2.6.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.7"><span class="ltx_text" id="S5.T3.4.2.2.7.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.8"><span class="ltx_text" id="S5.T3.4.2.2.8.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.9"><span class="ltx_text" id="S5.T3.4.2.2.9.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.10"><span class="ltx_text" id="S5.T3.4.2.2.10.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.11"><span class="ltx_text" id="S5.T3.4.2.2.11.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2.12"><span class="ltx_text" id="S5.T3.4.2.2.12.1" style="font-size:90%;">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.3.1.1"><span class="ltx_text" id="S5.T3.4.3.1.1.1" style="font-size:90%;">Baseline</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.2"><span class="ltx_text" id="S5.T3.4.3.1.2.1" style="font-size:90%;">49.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.3"><span class="ltx_text" id="S5.T3.4.3.1.3.1" style="font-size:90%;">58.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.4"><span class="ltx_text" id="S5.T3.4.3.1.4.1" style="font-size:90%;">51.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.5"><span class="ltx_text" id="S5.T3.4.3.1.5.1" style="font-size:90%;">46.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.6"><span class="ltx_text" id="S5.T3.4.3.1.6.1" style="font-size:90%;">60.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.7"><span class="ltx_text" id="S5.T3.4.3.1.7.1" style="font-size:90%;">51.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.8"><span class="ltx_text" id="S5.T3.4.3.1.8.1" style="font-size:90%;">14.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.9"><span class="ltx_text" id="S5.T3.4.3.1.9.1" style="font-size:90%;">24.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.10"><span class="ltx_text" id="S5.T3.4.3.1.10.1" style="font-size:90%;">16.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.11"><span class="ltx_text" id="S5.T3.4.3.1.11.1" style="font-size:90%;">46.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.3.1.12"><span class="ltx_text" id="S5.T3.4.3.1.12.1" style="font-size:90%;">53.7</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.4.3.1.13"><span class="ltx_text" id="S5.T3.4.3.1.13.1" style="font-size:90%;">51.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.4.2.1"><span class="ltx_text" id="S5.T3.4.4.2.1.1" style="font-size:90%;">Predict</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.2.1" style="font-size:90%;">64.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.3.1" style="font-size:90%;">75.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.4.1" style="font-size:90%;">68.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.5.1" style="font-size:90%;">47.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.6.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.7.1" style="font-size:90%;">55.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.8"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.8.1" style="font-size:90%;">19.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.9"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.9.1" style="font-size:90%;">32.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.10"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.10.1" style="font-size:90%;">29.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.11.1" style="font-size:90%;">60.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.12"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.12.1" style="font-size:90%;">72.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.4.2.13"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.2.13.1" style="font-size:90%;">74.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.1"><span class="ltx_text" id="S5.T3.4.5.3.1.1" style="font-size:90%;">Oracle</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.2"><span class="ltx_text" id="S5.T3.4.5.3.2.1" style="font-size:90%;">65.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.3"><span class="ltx_text" id="S5.T3.4.5.3.3.1" style="font-size:90%;">75.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.4"><span class="ltx_text" id="S5.T3.4.5.3.4.1" style="font-size:90%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.5"><span class="ltx_text" id="S5.T3.4.5.3.5.1" style="font-size:90%;">48.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.6"><span class="ltx_text" id="S5.T3.4.5.3.6.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.7"><span class="ltx_text" id="S5.T3.4.5.3.7.1" style="font-size:90%;">55.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.8"><span class="ltx_text" id="S5.T3.4.5.3.8.1" style="font-size:90%;">22.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.9"><span class="ltx_text" id="S5.T3.4.5.3.9.1" style="font-size:90%;">34.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.10"><span class="ltx_text" id="S5.T3.4.5.3.10.1" style="font-size:90%;">30.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.11"><span class="ltx_text" id="S5.T3.4.5.3.11.1" style="font-size:90%;">65.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.12"><span class="ltx_text" id="S5.T3.4.5.3.12.1" style="font-size:90%;">75.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.5.3.13"><span class="ltx_text" id="S5.T3.4.5.3.13.1" style="font-size:90%;">76.4</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Domain Generalization.</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">We also examine the generalization of the computation allocation model for RAG for unseen domains. In other words, the parameters of <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.E2" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a> are tested on the target domain but learnt from the remaining domains. For inference, only <math alttext="i" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.1.m1.1d">italic_i</annotation></semantics></math> is derived from the target domain. We report the results for 1M effective context length in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.T3" title="In Ablation Study. ‣ 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, where we compare to a 8-shot baseline configuration (scaled by increasing retrieved documents) and the optimum results (Oracle). In summary, the results show that computation allocation model significantly outperforms baseline and closely aligns with the oracle results (96.6% of the optimal performance). Notably, Bamboogle and HotpotQA exhibit highly similar target results, with the performance metrics varying by less than 2.5% from the oracle. These results suggest the potential of applying the computation allocation model for RAG to a wider range of knowledge-intensive tasks.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Length extrapolation results of the computation allocation model for RAG.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T4.4.4.5" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1" style="font-size:90%;">16k <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">→</annotation></semantics></math> 32k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.1" style="font-size:90%;">32k <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.1.m1.1"><semantics id="S5.T4.2.2.2.1.m1.1a"><mo id="S5.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.m1.1d">→</annotation></semantics></math> 128k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.1" style="font-size:90%;">128k <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.1.m1.1"><semantics id="S5.T4.3.3.3.1.m1.1a"><mo id="S5.T4.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T4.3.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><ci id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.1.m1.1d">→</annotation></semantics></math> 1M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.4.1" style="font-size:90%;">1M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.4.4.4.1.m1.1"><semantics id="S5.T4.4.4.4.1.m1.1a"><mo id="S5.T4.4.4.4.1.m1.1.1" stretchy="false" xref="S5.T4.4.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.1.m1.1b"><ci id="S5.T4.4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.1.m1.1d">→</annotation></semantics></math> 5M</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.1"><span class="ltx_text" id="S5.T4.4.5.1.1.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.2"><span class="ltx_text" id="S5.T4.4.5.1.2.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.3"><span class="ltx_text" id="S5.T4.4.5.1.3.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.4"><span class="ltx_text" id="S5.T4.4.5.1.4.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.5"><span class="ltx_text" id="S5.T4.4.5.1.5.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.6"><span class="ltx_text" id="S5.T4.4.5.1.6.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.7"><span class="ltx_text" id="S5.T4.4.5.1.7.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.8"><span class="ltx_text" id="S5.T4.4.5.1.8.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.9"><span class="ltx_text" id="S5.T4.4.5.1.9.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.10"><span class="ltx_text" id="S5.T4.4.5.1.10.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.11"><span class="ltx_text" id="S5.T4.4.5.1.11.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.5.1.12"><span class="ltx_text" id="S5.T4.4.5.1.12.1" style="font-size:90%;">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.4.6.1.1"><span class="ltx_text" id="S5.T4.4.6.1.1.1" style="font-size:90%;">Baseline</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.2"><span class="ltx_text" id="S5.T4.4.6.1.2.1" style="font-size:90%;">37.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.3"><span class="ltx_text" id="S5.T4.4.6.1.3.1" style="font-size:90%;">47.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.4"><span class="ltx_text" id="S5.T4.4.6.1.4.1" style="font-size:90%;">40.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.5"><span class="ltx_text" id="S5.T4.4.6.1.5.1" style="font-size:90%;">39.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.6"><span class="ltx_text" id="S5.T4.4.6.1.6.1" style="font-size:90%;">49.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.7"><span class="ltx_text" id="S5.T4.4.6.1.7.1" style="font-size:90%;">42.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.8"><span class="ltx_text" id="S5.T4.4.6.1.8.1" style="font-size:90%;">39.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.9"><span class="ltx_text" id="S5.T4.4.6.1.9.1" style="font-size:90%;">49.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.10"><span class="ltx_text" id="S5.T4.4.6.1.10.1" style="font-size:90%;">42.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.11"><span class="ltx_text" id="S5.T4.4.6.1.11.1" style="font-size:90%;">44.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.6.1.12"><span class="ltx_text" id="S5.T4.4.6.1.12.1" style="font-size:90%;">55.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.4.6.1.13"><span class="ltx_text" id="S5.T4.4.6.1.13.1" style="font-size:90%;">49.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.7.2.1"><span class="ltx_text" id="S5.T4.4.7.2.1.1" style="font-size:90%;">Predict</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.2.1" style="font-size:90%;">37.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.3.1" style="font-size:90%;">48.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.4.1" style="font-size:90%;">41.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.5"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.5.1" style="font-size:90%;">41.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.6"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.6.1" style="font-size:90%;">52.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.7"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.7.1" style="font-size:90%;">45.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.8"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.8.1" style="font-size:90%;">48.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.9"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.9.1" style="font-size:90%;">60.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.10"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.10.1" style="font-size:90%;">56.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.11"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.11.1" style="font-size:90%;">47.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.2.12"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.12.1" style="font-size:90%;">59.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.7.2.13"><span class="ltx_text ltx_font_bold" id="S5.T4.4.7.2.13.1" style="font-size:90%;">55.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.1"><span class="ltx_text" id="S5.T4.4.8.3.1.1" style="font-size:90%;">Oracle</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.2"><span class="ltx_text" id="S5.T4.4.8.3.2.1" style="font-size:90%;">39.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.3"><span class="ltx_text" id="S5.T4.4.8.3.3.1" style="font-size:90%;">49.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.4"><span class="ltx_text" id="S5.T4.4.8.3.4.1" style="font-size:90%;">42.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.5"><span class="ltx_text" id="S5.T4.4.8.3.5.1" style="font-size:90%;">46.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.6"><span class="ltx_text" id="S5.T4.4.8.3.6.1" style="font-size:90%;">59.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.7"><span class="ltx_text" id="S5.T4.4.8.3.7.1" style="font-size:90%;">55.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.8"><span class="ltx_text" id="S5.T4.4.8.3.8.1" style="font-size:90%;">50.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.9"><span class="ltx_text" id="S5.T4.4.8.3.9.1" style="font-size:90%;">62.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.10"><span class="ltx_text" id="S5.T4.4.8.3.10.1" style="font-size:90%;">57.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.11"><span class="ltx_text" id="S5.T4.4.8.3.11.1" style="font-size:90%;">51.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.12"><span class="ltx_text" id="S5.T4.4.8.3.12.1" style="font-size:90%;">62.6</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.4.8.3.13"><span class="ltx_text" id="S5.T4.4.8.3.13.1" style="font-size:90%;">58.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Length Extrapolation.</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">In addition to predictability on unseen domains, we explore the extrapolation of context length based on the computation allocation model. Here, we estimate the parameters of <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.E2" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a> using experiments with shorter context lengths and assess their predictive accuracy on longer ones. We assess different extrapolation settings and present the predicted metric values in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.T4" title="In Domain Generalization. ‣ 5.2 Validating the Computation Allocation Model for RAG ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Our observations are: (1) The predictions are accurate and consistently outperform the 8-shot baseline. For instance, the average difference between the predicted and oracle results from 128k to 1M tokens is just 2.8%. (2) Extrapolating from 32k to 128k is challenging. This is because DRAG performs best around 32k, while IterDRAG typically excels at a long context of 128k, as evidenced in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F4" title="In 4.1 Fixed Budget Optimal Performance ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Consequently, it creates a discrepancy between training and predicting performance distribution. (3) 5M context length is less predictable, with the average performance difference between predicted and oracle metrics observed at a substantial 5.6%. Overall, length extrapolation with computation allocation model is accurate and more effective for target lengths below 1M.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In our experiments, we observe consistent benefits of inference scaling using DRAG and IterDRAG. Combined with the computation allocation model for RAG, this approach enables the derivation of a (nearly) optimal solution for long-context RAG given computation constraints. In the following, we discuss additional factors that may influence the scaling of long-context RAG.</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">One critical factor in improving performance of RAG lies in the quality of the retrieved documents. To study how retrieval impacts final accuracy, we analyze retrieval performance and report the results across different document sizes in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1" title="Appendix A Retrieval Quality ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>. In all datasets, recall scores demonstrate improvements as the number of documents increases, approaching near-perfect scores with large document sets (e.g., <math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.1.m1.1d">∼</annotation></semantics></math>1k). Despite consistent gains in recall, the results show diminishing returns on discounted ranking metrics like NDCG, indicating increasing distraction within the context. This trend is also evident in in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf2" title="In Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>, where RAG performance peaks between 100 and 500 documents. Our observations suggest the necessity of refining retrieval (e.g., through re-ranking) to further optimize the document relevance, particularly in cases of complex, multi-hop queries. However, how the inference scaling behavior discovered in this paper would change in the presence of such a refining component remains unknown. Alternatively, iterative retrieval, as seen in IterDRAG, improves recall performance by using simpler, straightforward sub-queries to collect additional context for each intermediate answer. In summary, retrieving more documents improves recall but does not necessarily lead to better generation quality if the documents are not effectively ranked or filtered. This highlights the need for retrieval methods that dynamically adjust to minimize irrelevant content.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Error Analysis.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Despite overall improvements, our error analysis in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A6" title="Appendix F Error Analysis ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a> reveals that certain errors persist, particularly in cases of compositional reasoning tasks where multiple hops of reasoning are required. The common errors fall into four categories: (1) inaccurate or outdated retrieval; (2) incorrect or lack of reasoning; (3) hallucination or unfaithful reasoning; and (4) evaluation issues or refusal to answer. The first category highlights the need for enhancing retrieval methods and maintaining a reliable &amp; up-to-date knowledge base, specially for complex questions that rely on multiple supporting facts. In addition, incorrect or missing reasoning steps often result in errors or partially correct answers. In our experiments, we observe that both (1) and (2) are substantially improved with IterDRAG, suggesting the importance of interleaving retrieval and iterative generation for multi-hop queries. Moreover, developing faithful LLMs and strategies to mitigate hallucination could further enhance RAG performance. Finally, we note that existing metrics fail in certain cases (e.g., abbreviations), underscoring the need for more robust and reliable evaluation methods.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Long-Context Modeling.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.3">We also discuss the impact of long-context modeling w.r.t. RAG performance. In summary, we find that retrieving more documents is generally beneficial for RAG performance, as demonstrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4" title="4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Nevertheless, naïvely extending the context length in each generation step does not always lead to better results. Specifically, DRAG performance peaks at around <math alttext="10^{5}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S6.SS0.SSS0.Px3.p1.1.m1.1a"><msup id="S6.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">10</mn><mn id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1">superscript</csymbol><cn id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2">10</cn><cn id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.1.m1.1c">10^{5}</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px3.p1.1.m1.1d">10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math> tokens, while IterDRAG achieves optimal performance at around <math alttext="10^{6}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S6.SS0.SSS0.Px3.p1.2.m2.1a"><msup id="S6.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">10</mn><mn id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1">superscript</csymbol><cn id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2">10</cn><cn id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.2.m2.1c">10^{6}</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px3.p1.2.m2.1d">10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT</annotation></semantics></math> tokens by leveraging multiple rounds of generation. For instance, as seen in the performance plateau in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F1" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A4.F10" title="In Appendix D Additional Results on Inference Scaling Laws for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>, LLMs struggle to effectively utilize very long contexts (<math alttext="\geq 10^{5}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S6.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S6.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml"></mi><mo id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">≥</mo><msup id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mn id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">10</mn><mn id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">5</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1"><geq id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.1"></geq><csymbol cd="latexml" id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2">absent</csymbol><apply id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3">superscript</csymbol><cn id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.2">10</cn><cn id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.3.m3.1c">\geq 10^{5}</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px3.p1.3.m3.1d">≥ 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math> tokens) in each iteration, potentially due to inherent limitations of long-context modeling. Our observations suggest that: (1) the model’s ability to identify relevant information from extensive context remains to be improved, especially when presented with large quantity of “similar” documents; (2) the long-context modeling should be further refined to enhance in-context learning capabilities, where multiple lengthy demonstrations are provided.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we explore inference scaling in long-context RAG. By systematically studying the performance with different inference configurations, we demonstrate that RAG performance improves almost linearly with the increasing order of magnitude of the test-time compute under optimal inference parameters. Based on our observations, we derive inference scaling laws for RAG and the corresponding computation allocation model, designed to predict RAG performance on varying hyperparameters. Through extensive experiments, we show that optimal configurations can be accurately estimated and align closely with the experimental results. These insights provide a strong foundation for future research in optimizing inference strategies for long-context RAG.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, L. Rosias, S. C. Chan, B. Zhang, A. Faust, and H. Larochelle.

</span>
<span class="ltx_bibblock">Many-shot in-context learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ICML 2024 Workshop on In-Context Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beck et al. (2024)</span>
<span class="ltx_bibblock">
M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.

</span>
<span class="ltx_bibblock">xLSTM: Extended long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2405.04517</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
I. Beltagy, M. E. Peters, and A. Cohan.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2004.05150</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertsch et al. (2024)</span>
<span class="ltx_bibblock">
A. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig.

</span>
<span class="ltx_bibblock">In-context learning with long-context models: An in-depth exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2405.00200</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. (2022)</span>
<span class="ltx_bibblock">
S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International conference on machine learning</em>, pages 2206–2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
S. Chen, S. Wong, L. Chen, and Y. Tian.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2306.15595</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski et al. (2020)</span>
<span class="ltx_bibblock">
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al.

</span>
<span class="ltx_bibblock">Rethinking attention with performers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2009.14794</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. (2022)</span>
<span class="ltx_bibblock">
T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with IO-awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2021)</span>
<span class="ltx_bibblock">
M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant.

</span>
<span class="ltx_bibblock">Did Aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions of the Association for Computational Linguistics</em>, 9:346–361, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu and Dao (2023)</span>
<span class="ltx_bibblock">
A. Gu and T. Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2312.00752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2021)</span>
<span class="ltx_bibblock">
A. Gu, K. Goel, and C. Ré.

</span>
<span class="ltx_bibblock">Efficiently modeling long sequences with structured state spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2111.00396</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2023)</span>
<span class="ltx_bibblock">
Y. Gu, L. Dong, F. Wei, and M. Huang.

</span>
<span class="ltx_bibblock">Pre-training to learn in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 4849–4870, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutiérrez et al. (2024)</span>
<span class="ltx_bibblock">
B. J. Gutiérrez, Y. Shu, Y. Gu, M. Yasunaga, and Y. Su.

</span>
<span class="ltx_bibblock">HippoRAG: Neurobiologically inspired long-term memory for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2405.14831</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International conference on machine learning</em>, pages 3929–3938. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa.

</span>
<span class="ltx_bibblock">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6609–6625, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
G. Izacard and É. Grave.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 874–880, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2023)</span>
<span class="ltx_bibblock">
G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Journal of Machine Learning Research</em>, 24(251):1–43, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. (2023)</span>
<span class="ltx_bibblock">
S. A. Jacobs, M. Tanaka, C. Zhang, M. Zhang, L. Song, S. Rajbhandari, and Y. He.

</span>
<span class="ltx_bibblock">DeepSpeed Ulysses: System optimizations for enabling training of extreme long sequence transformer models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2309.14509</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 7969–7992, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Z. Jiang, X. Ma, and W. Chen.

</span>
<span class="ltx_bibblock">LongRAG: Enhancing retrieval-augmented generation with long-context LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2406.15319</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer.

</span>
<span class="ltx_bibblock">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769–6781, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al. (2019)</span>
<span class="ltx_bibblock">
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitaev et al. (2019)</span>
<span class="ltx_bibblock">
N. Kitaev, L. Kaiser, and A. Levskaya.

</span>
<span class="ltx_bibblock">Reformer: The efficient transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koo et al. (2024)</span>
<span class="ltx_bibblock">
T. Koo, F. Liu, and L. He.

</span>
<span class="ltx_bibblock">Automata-based constraints for language model decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2407.08103</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuratov et al. (2024)</span>
<span class="ltx_bibblock">
Y. Kuratov, A. Bulatov, P. Anokhin, I. Rodkin, D. Sorokin, A. Sorokin, and M. Burtsev.

</span>
<span class="ltx_bibblock">BABILong: Testing the limits of LLMs with long context reasoning-in-a-haystack.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2406.10149</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024a)</span>
<span class="ltx_bibblock">
J. Lee, A. Chen, Z. Dai, D. Dua, D. S. Sachan, M. Boratko, Y. Luan, S. M. Arnold, V. Perot, S. Dalmia, et al.

</span>
<span class="ltx_bibblock">Can long-context language models subsume retrieval, RAG, SQL, and more?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2406.13121</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024b)</span>
<span class="ltx_bibblock">
J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, et al.

</span>
<span class="ltx_bibblock">Gecko: Versatile text embeddings distilled from large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2403.20327</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
M. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong.

</span>
<span class="ltx_bibblock">In-context learning with many demonstration examples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2302.04931</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
T. Li, G. Zhang, Q. D. Do, X. Yue, and W. Chen.

</span>
<span class="ltx_bibblock">Long-context LLMs struggle with long in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2404.02060</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis, et al.

</span>
<span class="ltx_bibblock">RA-DIT: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
H. Liu, M. Zaharia, and P. Abbeel.

</span>
<span class="ltx_bibblock">Ring attention with blockwise transformers for near-infinite context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2310.01889</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
J. Liu, D. Shen, Y. Zhang, W. B. Dolan, L. Carin, and W. Chen.

</span>
<span class="ltx_bibblock">What makes good in-context examples for GPT-3?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</em>, pages 100–114, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
S. Liu, H. Ye, L. Xing, and J. Y. Zou.

</span>
<span class="ltx_bibblock">In-context vectors: Making in context learning more effective and controllable through latent space steering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Forty-first International Conference on Machine Learning</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Z. Liu, W. Ping, R. Roy, P. Xu, C. Lee, M. Shoeybi, and B. Catanzaro.

</span>
<span class="ltx_bibblock">ChatQA: Surpassing GPT-4 on conversational QA and RAG.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2401.10225</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp.

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8086–8098, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan.

</span>
<span class="ltx_bibblock">Query rewriting in retrieval-augmented large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 5303–5315, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2022)</span>
<span class="ltx_bibblock">
S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi.

</span>
<span class="ltx_bibblock">MetaICL: Learning to learn in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2791–2809, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023a)</span>
<span class="ltx_bibblock">
B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, et al.

</span>
<span class="ltx_bibblock">RWKV: Reinventing rnns for the transformer era.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14048–14077, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023b)</span>
<span class="ltx_bibblock">
B. Peng, J. Quesnelle, H. Fan, and E. Shippole.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.00071</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2020)</span>
<span class="ltx_bibblock">
F. Petroni, A. Piktus, A. Fan, P. Lewis, M. Yazdani, N. De Cao, J. Thorne, Y. Jernite, V. Karpukhin, J. Maillard, et al.

</span>
<span class="ltx_bibblock">KILT: a benchmark for knowledge intensive language tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2009.02252</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2021)</span>
<span class="ltx_bibblock">
O. Press, N. A. Smith, and M. Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input length extrapolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2108.12409</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2023)</span>
<span class="ltx_bibblock">
O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 5687–5711, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. (2023)</span>
<span class="ltx_bibblock">
O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2024)</span>
<span class="ltx_bibblock">
M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin et al. (2022)</span>
<span class="ltx_bibblock">
O. Rubin, J. Herzig, and J. Berant.

</span>
<span class="ltx_bibblock">Learning to retrieve prompts for in-context learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2655–2671, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarthi et al. (2024)</span>
<span class="ltx_bibblock">
P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning.

</span>
<span class="ltx_bibblock">RAPTOR: Recursive abstractive processing for tree-organized retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
R. Shao, J. He, A. Asai, W. Shi, T. Dettmers, S. Min, L. Zettlemoyer, and P. W. Koh.

</span>
<span class="ltx_bibblock">Scaling retrieval-based language models with a trillion-token datastore.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2407.12854</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih.

</span>
<span class="ltx_bibblock">REPLUG: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pages 8364–8377, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et al. (2024)</span>
<span class="ltx_bibblock">
C. Snell, J. Lee, K. Xu, and A. Kumar.

</span>
<span class="ltx_bibblock">Scaling LLM test-time compute optimally can be more effective than scaling model parameters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2408.03314</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
P. Sun, D. Simcha, D. Dopson, R. Guo, and S. Kumar.

</span>
<span class="ltx_bibblock">SOAR: improved indexing for approximate nearest neighbor search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei.

</span>
<span class="ltx_bibblock">A length-extrapolatable transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14590–14604, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2023)</span>
<span class="ltx_bibblock">
G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2022)</span>
<span class="ltx_bibblock">
H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal.

</span>
<span class="ltx_bibblock">MuSiQue: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Transactions of the Association for Computational Linguistics</em>, 10:539–554, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2023)</span>
<span class="ltx_bibblock">
H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal.

</span>
<span class="ltx_bibblock">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 10014–10037, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
X. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang.

</span>
<span class="ltx_bibblock">Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in neural information processing systems</em>, 35:24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, et al.

</span>
<span class="ltx_bibblock">Symbol tuning improves in-context learning in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 968–979, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
K. Wu, E. Wu, and J. Zou.

</span>
<span class="ltx_bibblock">How faithful are RAG models? quantifying the tug-of-war between RAG and LLMs’ internal prior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2404.10198</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Z. Wu, Y. Wang, J. Ye, and L. Kong.

</span>
<span class="ltx_bibblock">Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1423–1436, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2024)</span>
<span class="ltx_bibblock">
S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2401.15884</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2369–2380, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. (2024)</span>
<span class="ltx_bibblock">
O. Yoran, T. Wolfson, O. Ram, and J. Berant.

</span>
<span class="ltx_bibblock">Making retrieval-augmented language models robust to irrelevant context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu.

</span>
<span class="ltx_bibblock">Chain-of-note: Enhancing robustness in retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2311.09210</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaheer et al. (2020)</span>
<span class="ltx_bibblock">
M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al.

</span>
<span class="ltx_bibblock">Big bird: Transformers for longer sequences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Advances in neural information processing systems</em>, 33:17283–17297, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E. Gonzalez.

</span>
<span class="ltx_bibblock">RAFT: Adapting language model to domain specific rag.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2403.10131</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Retrieval Quality</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We assess the retrieval quality of DRAG and IterDRAG using the Gecko-1B model <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib31" title="">2024b</a>)</cite> and evaluate their impact on final RAG performance. Specifically, we retrieve varying numbers of documents per input query and measure the retrieval quality using three metrics: Recall, NDCG, and MRR, with document counts ranging from 1 to 2k. The retrieval results of DRAG are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1.F7" title="In Appendix A Retrieval Quality ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>. In addition, we evaluate the quality of iterative retrieval, where a maximum of five interleaving retrieval steps are performed. Here, we retrieve 50 documents at each step and use a 2-shot setting, with the results in comparison to DRAG in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1.T5" title="In Appendix A Retrieval Quality ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="A1.F7.g1" src="x6.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="A1.F7.3.2" style="font-size:90%;">Retrieval performance of DRAG on different datasets.</span></figcaption>
</figure>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">In <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1.F7" title="In Appendix A Retrieval Quality ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>, recall demonstrates consistent improvements as the number of documents increases, approaching near-perfect scores when large document sets (e.g., 1k) are retrieved. However, both NDCG and MRR metrics plateau early at around 100 documents, with diminishing gains as the document count further rises. This divergence suggests that while more documents lead to better recall, the relevance and ranking quality (captured by NDCG and MRR) do not improve proportionally, and even introduce extensive noise. Therefore, higher recall doesn’t necessarily translate into better final answer quality when the retrieved documents aren’t effectively ranked or filtered.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T5.6.3.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="A1.T5.4.2" style="font-size:90%;">Retrieval performance of DRAG and IterDRAG (<math alttext="k=50" class="ltx_Math" display="inline" id="A1.T5.3.1.m1.1"><semantics id="A1.T5.3.1.m1.1b"><mrow id="A1.T5.3.1.m1.1.1" xref="A1.T5.3.1.m1.1.1.cmml"><mi id="A1.T5.3.1.m1.1.1.2" xref="A1.T5.3.1.m1.1.1.2.cmml">k</mi><mo id="A1.T5.3.1.m1.1.1.1" xref="A1.T5.3.1.m1.1.1.1.cmml">=</mo><mn id="A1.T5.3.1.m1.1.1.3" xref="A1.T5.3.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.3.1.m1.1c"><apply id="A1.T5.3.1.m1.1.1.cmml" xref="A1.T5.3.1.m1.1.1"><eq id="A1.T5.3.1.m1.1.1.1.cmml" xref="A1.T5.3.1.m1.1.1.1"></eq><ci id="A1.T5.3.1.m1.1.1.2.cmml" xref="A1.T5.3.1.m1.1.1.2">𝑘</ci><cn id="A1.T5.3.1.m1.1.1.3.cmml" type="integer" xref="A1.T5.3.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.1.m1.1d">k=50</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.1.m1.1e">italic_k = 50</annotation></semantics></math> documents, <math alttext="m=2" class="ltx_Math" display="inline" id="A1.T5.4.2.m2.1"><semantics id="A1.T5.4.2.m2.1b"><mrow id="A1.T5.4.2.m2.1.1" xref="A1.T5.4.2.m2.1.1.cmml"><mi id="A1.T5.4.2.m2.1.1.2" xref="A1.T5.4.2.m2.1.1.2.cmml">m</mi><mo id="A1.T5.4.2.m2.1.1.1" xref="A1.T5.4.2.m2.1.1.1.cmml">=</mo><mn id="A1.T5.4.2.m2.1.1.3" xref="A1.T5.4.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.4.2.m2.1c"><apply id="A1.T5.4.2.m2.1.1.cmml" xref="A1.T5.4.2.m2.1.1"><eq id="A1.T5.4.2.m2.1.1.1.cmml" xref="A1.T5.4.2.m2.1.1.1"></eq><ci id="A1.T5.4.2.m2.1.1.2.cmml" xref="A1.T5.4.2.m2.1.1.2">𝑚</ci><cn id="A1.T5.4.2.m2.1.1.3.cmml" type="integer" xref="A1.T5.4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.2.m2.1d">m=2</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.2.m2.1e">italic_m = 2</annotation></semantics></math> shots).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T5.7" style="width:433.6pt;height:60.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.6pt,6.1pt) scale(0.829298834613251,0.829298834613251) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T5.7.1.1.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.T5.7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.1.1.2.1">Bamboogle</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.T5.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.1.1.3.1">HotpotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.T5.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.1.1.4.1">MuSiQue</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.T5.7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.1.1.5.1">2WikiMultiHopQA</span></th>
</tr>
<tr class="ltx_tr" id="A1.T5.7.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.1">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.2">NDCG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.3">MRR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.4">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.5">NDCG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.6">MRR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.7">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.8">NDCG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.9">MRR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.10">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.11">NDCG</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.7.1.2.2.12">MRR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.7.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.7.1.3.1.1">DRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.2">0.632</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.3">0.321</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.4">0.239</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.5">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.6">0.535</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.7">0.465</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.8">0.509</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.9">0.255</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.10">0.188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.11">0.722</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.12">0.421</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T5.7.1.3.1.13">0.336</td>
</tr>
<tr class="ltx_tr" id="A1.T5.7.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T5.7.1.4.2.1">IterDRAG</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.2"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.2.1">0.736</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.3"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.3.1">0.420</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.4"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.4.1">0.346</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.5"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.5.1">0.855</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.6"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.6.1">0.549</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.7"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.7.1">0.478</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.8"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.8.1">0.670</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.9"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.9.1">0.365</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.10"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.10.1">0.291</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.11"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.11.1">0.935</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.12"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.12.1">0.605</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T5.7.1.4.2.13"><span class="ltx_text ltx_font_bold" id="A1.T5.7.1.4.2.13.1">0.528</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">Unlike the one-step retrieval in DRAG, iterative retrieval based on query decomposition often yields simpler sub-queries, facilitating more effective retrieval. In addition, merging the retrieved documents from different steps typically results in higher overall retrieval performance, as evidenced in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A1.T5" title="In Appendix A Retrieval Quality ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>. With IterDRAG, the performance gains are consistent and reach the average of 30.5%. Specifically, we observe higher gains for complex multi-hop queries (e.g., 2WikiMultiHopQA), where metric improvements can be as high as 57.1%. Moreover, the gains on ranking-discounted metrics (30.7% in NDCG and 39.9% MRR) show greater improvements compared to recall (21.7%). In summary, these findings highlight the superiority of iterative retrieval with query decomposition over one-step methods, which effectively contribute to the overall performance of IterDRAG.</p>
</div>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Chain-of-Thought vs. IterDRAG.</h2>
<figure class="ltx_table" id="A2.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Chain-of-thought (CoT) vs. IterDRAG results (<math alttext="k=5" class="ltx_Math" display="inline" id="A2.T6.3.m1.1"><semantics id="A2.T6.3.m1.1b"><mrow id="A2.T6.3.m1.1.1" xref="A2.T6.3.m1.1.1.cmml"><mi id="A2.T6.3.m1.1.1.2" xref="A2.T6.3.m1.1.1.2.cmml">k</mi><mo id="A2.T6.3.m1.1.1.1" xref="A2.T6.3.m1.1.1.1.cmml">=</mo><mn id="A2.T6.3.m1.1.1.3" xref="A2.T6.3.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T6.3.m1.1c"><apply id="A2.T6.3.m1.1.1.cmml" xref="A2.T6.3.m1.1.1"><eq id="A2.T6.3.m1.1.1.1.cmml" xref="A2.T6.3.m1.1.1.1"></eq><ci id="A2.T6.3.m1.1.1.2.cmml" xref="A2.T6.3.m1.1.1.2">𝑘</ci><cn id="A2.T6.3.m1.1.1.3.cmml" type="integer" xref="A2.T6.3.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T6.3.m1.1d">k=5</annotation><annotation encoding="application/x-llamapun" id="A2.T6.3.m1.1e">italic_k = 5</annotation></semantics></math> documents, <math alttext="m=4" class="ltx_Math" display="inline" id="A2.T6.4.m2.1"><semantics id="A2.T6.4.m2.1b"><mrow id="A2.T6.4.m2.1.1" xref="A2.T6.4.m2.1.1.cmml"><mi id="A2.T6.4.m2.1.1.2" xref="A2.T6.4.m2.1.1.2.cmml">m</mi><mo id="A2.T6.4.m2.1.1.1" xref="A2.T6.4.m2.1.1.1.cmml">=</mo><mn id="A2.T6.4.m2.1.1.3" xref="A2.T6.4.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T6.4.m2.1c"><apply id="A2.T6.4.m2.1.1.cmml" xref="A2.T6.4.m2.1.1"><eq id="A2.T6.4.m2.1.1.1.cmml" xref="A2.T6.4.m2.1.1.1"></eq><ci id="A2.T6.4.m2.1.1.2.cmml" xref="A2.T6.4.m2.1.1.2">𝑚</ci><cn id="A2.T6.4.m2.1.1.3.cmml" type="integer" xref="A2.T6.4.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T6.4.m2.1d">m=4</annotation><annotation encoding="application/x-llamapun" id="A2.T6.4.m2.1e">italic_m = 4</annotation></semantics></math> shots).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T6.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T6.10.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T6.10.1.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T6.10.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T6.10.1.1.2.1" style="font-size:90%;">HotpotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T6.10.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T6.10.1.1.3.1" style="font-size:90%;">MuSiQue</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T6.10.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T6.10.1.1.4.1" style="font-size:90%;">2WikiMultiHopQA</span></th>
</tr>
<tr class="ltx_tr" id="A2.T6.10.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.1"><span class="ltx_text" id="A2.T6.10.2.2.1.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.2"><span class="ltx_text" id="A2.T6.10.2.2.2.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.3"><span class="ltx_text" id="A2.T6.10.2.2.3.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.4"><span class="ltx_text" id="A2.T6.10.2.2.4.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.5"><span class="ltx_text" id="A2.T6.10.2.2.5.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.6"><span class="ltx_text" id="A2.T6.10.2.2.6.1" style="font-size:90%;">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.7"><span class="ltx_text" id="A2.T6.10.2.2.7.1" style="font-size:90%;">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.8"><span class="ltx_text" id="A2.T6.10.2.2.8.1" style="font-size:90%;">F1</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.10.2.2.9"><span class="ltx_text" id="A2.T6.10.2.2.9.1" style="font-size:90%;">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.10.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.10.3.1.1"><span class="ltx_text" id="A2.T6.10.3.1.1.1" style="font-size:90%;">CoT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.2"><span class="ltx_text" id="A2.T6.10.3.1.2.1" style="font-size:90%;">40.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.3"><span class="ltx_text" id="A2.T6.10.3.1.3.1" style="font-size:90%;">51.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.4"><span class="ltx_text" id="A2.T6.10.3.1.4.1" style="font-size:90%;">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.5"><span class="ltx_text" id="A2.T6.10.3.1.5.1" style="font-size:90%;">8.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.6"><span class="ltx_text" id="A2.T6.10.3.1.6.1" style="font-size:90%;">16.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.7"><span class="ltx_text" id="A2.T6.10.3.1.7.1" style="font-size:90%;">10.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.8"><span class="ltx_text" id="A2.T6.10.3.1.8.1" style="font-size:90%;">33.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.10.3.1.9"><span class="ltx_text" id="A2.T6.10.3.1.9.1" style="font-size:90%;">37.9</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A2.T6.10.3.1.10"><span class="ltx_text" id="A2.T6.10.3.1.10.1" style="font-size:90%;">36.7</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.10.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T6.10.4.2.1"><span class="ltx_text" id="A2.T6.10.4.2.1.1" style="font-size:90%;">IterDRAG</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.2"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.2.1" style="font-size:90%;">44.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.3"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.3.1" style="font-size:90%;">59.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.4"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.4.1" style="font-size:90%;">52.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.5"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.5.1" style="font-size:90%;">17.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.6"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.6.1" style="font-size:90%;">30.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.7"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.7.1" style="font-size:90%;">25.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.8"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.8.1" style="font-size:90%;">57.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.9"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.9.1" style="font-size:90%;">69.9</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A2.T6.10.4.2.10"><span class="ltx_text ltx_font_bold" id="A2.T6.10.4.2.10.1" style="font-size:90%;">72.3</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">To evaluate different iterative strategies, we compare the commonly used chain-of-thought (CoT) with IterDRAG <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib61" title="">2022</a>)</cite>. In particular, we generate the CoT examples following <cite class="ltx_cite ltx_citemacro_cite">Trivedi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib59" title="">2023</a>)</cite> and adopt the 4-shot setting with 5 documents. The results on three larger datasets (HotpotQA, MuSiQue and 2WikiMultiHopQA), as reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A2.T6" title="In Appendix B Chain-of-Thought vs. IterDRAG. ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>, highlight the performance differences between these strategies, in which IterDRAG consistently outperforms CoT with significant improvements. Such difference can be traced back to three key factors: (1) the retrieval quality of CoT is limited without interleaving retrieval as in IterDRAG; (2) Gemini 1.5 Flash is relatively small and may not perform well in free-form reasoning in comparison to larger LLMs; and (3) the generated CoT examples are less informative than handcrafted ones and underperform compared to constrained decoding with Self-Ask <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib47" title="">2023</a>; Koo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib27" title="">2024</a>)</cite>. Consequently, IterDRAG demonstrates its effectiveness as a scalable method for knowledge-intensive tasks.</p>
</div>
</section>
<section class="ltx_appendix" id="A3" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional RAG Results</h2>
<figure class="ltx_figure" id="A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="A3.F8.g1" src="x7.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="A3.F8.3.2" style="font-size:90%;">IterDRAG performance heatmap for different metrics averaged across datasets.</span></figcaption>
</figure>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We report the IterDRAG results averaged across datasets in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3.F8" title="In Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, shown as heatmaps where the x-axis represents the number of documents and the y-axis represents the number of shots. Performance is color-coded, with blue indicating lower values and red indicating higher values. The best-performing combinations are located toward the bottom right of each heatmap, which corresponds to longer context lengths. In comparison to DRAG, as reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S4.F5.sf1" title="In Figure 5 ‣ 4.3 Inference Scaling Laws for RAG ‣ 4 RAG Performance and Inference Computation Scale ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a>, the optimal number of in-context examples is higher at 32, which highlights the importance of in-context demonstrations in enabling better query decomposition and interleaved retrieval. Combined with multiple generation steps, IterDRAG further improves RAG performance over DRAG.</p>
</div>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="A3.F9.g1" src="x8.png" width="897"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="A3.F9.3.2" style="font-size:90%;">Evaluation accuracy of DRAG on TriviaQA and Natural Questions (NaturalQ.).</span></figcaption>
</figure>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">In addition to multi-hop question answering datasets, we also report results on one-hop datasets, specifically TriviaQA and Natural Questions <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib23" title="">2017</a>; Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib29" title="">2019</a>)</cite>. The evaluations for one-hop datasets are performed with DRAG and presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3.F9" title="In Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>, similar to <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3.F8" title="In Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>. For TriviaQA, increasing the number of documents generally leads to improved accuracy, where the highest accuracy of 69.0% is achieved with 50 documents. In Natural Questions, performance increases with the number of documents up to about 10 or 20 documents, but further increases in the document count lead to diminishing returns or even slight declines in accuracy. The highest accuracy of 54.6% is achieved with 20 documents in 1-shot, and performance drops slightly when more documents are included. In summary, the optimal number of shots falls between 1 and 4. While increasing the number of shots and documents leads to initial performance gains, these improvements plateau beyond certain thresholds. This trend, in contrast to multi-hop datasets, may be partially attributed to the nature of the one-hop questions and retrieval relevance.</p>
</div>
<figure class="ltx_table" id="A3.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>StrategyQA accuracy results.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T7.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T7.4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A3.T7.4.1.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A3.T7.4.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T7.4.1.1.2.1" style="font-size:90%;">StrategyQA</span></th>
</tr>
<tr class="ltx_tr" id="A3.T7.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.2.2.1"><span class="ltx_text" id="A3.T7.4.2.2.1.1" style="font-size:90%;">Zero-shot QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.2.2.2"><span class="ltx_text" id="A3.T7.4.2.2.2.1" style="font-size:90%;">Many-shot QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.2.2.3"><span class="ltx_text" id="A3.T7.4.2.2.3.1" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.2.2.4"><span class="ltx_text" id="A3.T7.4.2.2.4.1" style="font-size:90%;">DRAG</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.4.2.2.5"><span class="ltx_text" id="A3.T7.4.2.2.5.1" style="font-size:90%;">IterDRAG</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.1"><span class="ltx_text" id="A3.T7.4.3.3.1.1" style="font-size:90%;">Acc</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.2"><span class="ltx_text" id="A3.T7.4.3.3.2.1" style="font-size:90%;">61.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.3"><span class="ltx_text" id="A3.T7.4.3.3.3.1" style="font-size:90%;">74.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.4"><span class="ltx_text" id="A3.T7.4.3.3.4.1" style="font-size:90%;">74.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.5"><span class="ltx_text" id="A3.T7.4.3.3.5.1" style="font-size:90%;">79.0</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A3.T7.4.3.3.6"><span class="ltx_text" id="A3.T7.4.3.3.6.1" style="font-size:90%;">83.4</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">We also include the multi-hop and binary StrategyQA dataset in our experiments, see <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A3.T7" title="In Appendix C Additional RAG Results ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a> <cite class="ltx_cite ltx_citemacro_citep">(Geva et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib11" title="">2021</a>)</cite>. Despite being binary questions, we observe similar trends to our main experiments. For example, DRAG consistently outperforms the baseline QA and RAG methods, with 29.3% accuracy improvement to for the baseline QA model. Furthermore, the performance is boosted with 83.4 accuracy using the iterative IterDRAG. These results demonstrate that even for binary, multi-hop tasks, iterative approaches provide substantial gains, confirming the effectiveness of both long-context and iterative strategies for inference scaling in RAG.</p>
</div>
</section>
<section class="ltx_appendix" id="A4" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Results on Inference Scaling Laws for RAG</h2>
<figure class="ltx_figure" id="A4.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="406" id="A4.F10.sf1.g1" src="x9.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F10.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A4.F10.sf1.3.2" style="font-size:90%;">Normalized performance vs. effective context lengths on Bamboogle.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="406" id="A4.F10.sf2.g1" src="x10.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F10.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A4.F10.sf2.3.2" style="font-size:90%;">Normalized performance vs. effective context lengths on HotpotQA.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F10.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="406" id="A4.F10.sf3.g1" src="x11.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F10.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A4.F10.sf3.3.2" style="font-size:90%;">Normalized performance vs. effective context lengths on 2WikiMultiHopQA.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="A4.F10.3.2" style="font-size:90%;">Normalized performance with increasing effective context lengths on different datasets.</span></figcaption>
</figure>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We present data-specific results on the relationship between the performance and the effective context length. <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A4.F10" title="In Appendix D Additional Results on Inference Scaling Laws for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a> presents the results on the other three datasets other than MuSiQue (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F1" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> for visualized results on MuSiQue). We observe different behavior depending on the datasets. For instance, the gains are more linear and consistent on Bamboogle and MuSiQue, and almost linear on 2WikiMultiHopQA until 1M tokens. However, HotpotQA and 2WikiMultiHopQA with effective context length longer than 100k tokens exhibit more sigmoidal patterns, likely due to the difficulty of the datasets and the quality of the retrieved documents.</p>
</div>
</section>
<section class="ltx_appendix" id="A5" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional Results on Computation Allocation Model for RAG</h2>
<figure class="ltx_figure" id="A5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="A5.F11.g1" src="x12.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="A5.F11.3.2" style="font-size:90%;">The estimated performance using the proposed computation allocation model vs. actual metric values in IterDRAG. The subplots represent different datasets, where each line corresponds to a fixed number of documents, we scale the context length by increasing the number of shots.</span></figcaption>
</figure>
<figure class="ltx_table" id="A5.T8">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Computation allocation mode of Gemini 1.5 Flash with <math alttext="p" class="ltx_Math" display="inline" id="A5.T8.3.m1.1"><semantics id="A5.T8.3.m1.1b"><mi id="A5.T8.3.m1.1.1" xref="A5.T8.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.T8.3.m1.1c"><ci id="A5.T8.3.m1.1.1.cmml" xref="A5.T8.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.3.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="A5.T8.3.m1.1e">italic_p</annotation></semantics></math>-value, <math alttext="R^{2}" class="ltx_Math" display="inline" id="A5.T8.4.m2.1"><semantics id="A5.T8.4.m2.1b"><msup id="A5.T8.4.m2.1.1" xref="A5.T8.4.m2.1.1.cmml"><mi id="A5.T8.4.m2.1.1.2" xref="A5.T8.4.m2.1.1.2.cmml">R</mi><mn id="A5.T8.4.m2.1.1.3" xref="A5.T8.4.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="A5.T8.4.m2.1c"><apply id="A5.T8.4.m2.1.1.cmml" xref="A5.T8.4.m2.1.1"><csymbol cd="ambiguous" id="A5.T8.4.m2.1.1.1.cmml" xref="A5.T8.4.m2.1.1">superscript</csymbol><ci id="A5.T8.4.m2.1.1.2.cmml" xref="A5.T8.4.m2.1.1.2">𝑅</ci><cn id="A5.T8.4.m2.1.1.3.cmml" type="integer" xref="A5.T8.4.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.4.m2.1d">R^{2}</annotation><annotation encoding="application/x-llamapun" id="A5.T8.4.m2.1e">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and MSE statistics.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T8.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T8.8.4">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="A5.T8.8.4.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A5.T8.5.1.1"><math alttext="\bm{a}" class="ltx_Math" display="inline" id="A5.T8.5.1.1.m1.1"><semantics id="A5.T8.5.1.1.m1.1a"><mi id="A5.T8.5.1.1.m1.1.1" mathsize="90%" xref="A5.T8.5.1.1.m1.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="A5.T8.5.1.1.m1.1b"><ci id="A5.T8.5.1.1.m1.1.1.cmml" xref="A5.T8.5.1.1.m1.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.5.1.1.m1.1c">\bm{a}</annotation><annotation encoding="application/x-llamapun" id="A5.T8.5.1.1.m1.1d">bold_italic_a</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A5.T8.6.2.2"><math alttext="\bm{b}" class="ltx_Math" display="inline" id="A5.T8.6.2.2.m1.1"><semantics id="A5.T8.6.2.2.m1.1a"><mi id="A5.T8.6.2.2.m1.1.1" mathsize="90%" xref="A5.T8.6.2.2.m1.1.1.cmml">𝒃</mi><annotation-xml encoding="MathML-Content" id="A5.T8.6.2.2.m1.1b"><ci id="A5.T8.6.2.2.m1.1.1.cmml" xref="A5.T8.6.2.2.m1.1.1">𝒃</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.6.2.2.m1.1c">\bm{b}</annotation><annotation encoding="application/x-llamapun" id="A5.T8.6.2.2.m1.1d">bold_italic_b</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.7.3.3"><math alttext="\bm{c}" class="ltx_Math" display="inline" id="A5.T8.7.3.3.m1.1"><semantics id="A5.T8.7.3.3.m1.1a"><mi id="A5.T8.7.3.3.m1.1.1" mathsize="90%" xref="A5.T8.7.3.3.m1.1.1.cmml">𝒄</mi><annotation-xml encoding="MathML-Content" id="A5.T8.7.3.3.m1.1b"><ci id="A5.T8.7.3.3.m1.1.1.cmml" xref="A5.T8.7.3.3.m1.1.1">𝒄</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.7.3.3.m1.1c">\bm{c}</annotation><annotation encoding="application/x-llamapun" id="A5.T8.7.3.3.m1.1d">bold_italic_c</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.8.4.4"><math alttext="\bm{R^{2}}" class="ltx_Math" display="inline" id="A5.T8.8.4.4.m1.1"><semantics id="A5.T8.8.4.4.m1.1a"><msup id="A5.T8.8.4.4.m1.1.1" xref="A5.T8.8.4.4.m1.1.1.cmml"><mi id="A5.T8.8.4.4.m1.1.1.2" mathsize="90%" xref="A5.T8.8.4.4.m1.1.1.2.cmml">𝑹</mi><mn id="A5.T8.8.4.4.m1.1.1.3" mathsize="90%" xref="A5.T8.8.4.4.m1.1.1.3.cmml">𝟐</mn></msup><annotation-xml encoding="MathML-Content" id="A5.T8.8.4.4.m1.1b"><apply id="A5.T8.8.4.4.m1.1.1.cmml" xref="A5.T8.8.4.4.m1.1.1"><csymbol cd="ambiguous" id="A5.T8.8.4.4.m1.1.1.1.cmml" xref="A5.T8.8.4.4.m1.1.1">superscript</csymbol><ci id="A5.T8.8.4.4.m1.1.1.2.cmml" xref="A5.T8.8.4.4.m1.1.1.2">𝑹</ci><cn id="A5.T8.8.4.4.m1.1.1.3.cmml" type="integer" xref="A5.T8.8.4.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.8.4.4.m1.1c">\bm{R^{2}}</annotation><annotation encoding="application/x-llamapun" id="A5.T8.8.4.4.m1.1d">bold_italic_R start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.8.4.6"><span class="ltx_text ltx_font_bold" id="A5.T8.8.4.6.1" style="font-size:90%;">MSE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T8.9.6.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A5.T8.9.6.1.1"><span class="ltx_text" id="A5.T8.9.6.1.1.1" style="font-size:90%;">Value</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.9.6.1.2"><span class="ltx_text" id="A5.T8.9.6.1.2.1" style="font-size:90%;">0.325</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.9.6.1.3"><span class="ltx_text" id="A5.T8.9.6.1.3.1" style="font-size:90%;">0.101</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A5.T8.9.6.1.4"><span class="ltx_text" id="A5.T8.9.6.1.4.1" style="font-size:90%;">0.177</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.9.6.1.5"><span class="ltx_text" id="A5.T8.9.6.1.5.1" style="font-size:90%;">-0.067</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.9.6.1.6"><span class="ltx_text" id="A5.T8.9.6.1.6.1" style="font-size:90%;">-0.008</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A5.T8.9.6.1.7"><span class="ltx_text" id="A5.T8.9.6.1.7.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A5.T8.9.6.1.8"><span class="ltx_text" id="A5.T8.9.6.1.8.1" style="font-size:90%;">-0.730</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A5.T8.9.6.1.9"><span class="ltx_text" id="A5.T8.9.6.1.9.1" style="font-size:90%;">0.903</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T8.9.6.1.10"><span class="ltx_text" id="A5.T8.9.6.1.10.1" style="font-size:90%;">0.085</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.9.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="A5.T8.9.5.1">
<math alttext="p" class="ltx_Math" display="inline" id="A5.T8.9.5.1.m1.1"><semantics id="A5.T8.9.5.1.m1.1a"><mi id="A5.T8.9.5.1.m1.1.1" mathsize="90%" xref="A5.T8.9.5.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.T8.9.5.1.m1.1b"><ci id="A5.T8.9.5.1.m1.1.1.cmml" xref="A5.T8.9.5.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.9.5.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A5.T8.9.5.1.m1.1d">italic_p</annotation></semantics></math><span class="ltx_text" id="A5.T8.9.5.1.1" style="font-size:90%;">-value</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.9.5.2"><span class="ltx_text" id="A5.T8.9.5.2.1" style="font-size:90%;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.9.5.3"><span class="ltx_text" id="A5.T8.9.5.3.1" style="font-size:90%;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A5.T8.9.5.4"><span class="ltx_text" id="A5.T8.9.5.4.1" style="font-size:90%;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.9.5.5"><span class="ltx_text" id="A5.T8.9.5.5.1" style="font-size:90%;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.9.5.6"><span class="ltx_text" id="A5.T8.9.5.6.1" style="font-size:90%;">0.092</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A5.T8.9.5.7"><span class="ltx_text" id="A5.T8.9.5.7.1" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A5.T8.9.5.8"><span class="ltx_text" id="A5.T8.9.5.8.1" style="font-size:90%;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A5.T8.9.5.9"><span class="ltx_text" id="A5.T8.9.5.9.1" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.9.5.10"><span class="ltx_text" id="A5.T8.9.5.10.1" style="font-size:90%;">N/A</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.9">We further explore the findings on the computation allocation model. In particular, we report the estimated parameters along with <math alttext="p" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mi id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><ci id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">italic_p</annotation></semantics></math>-values, <math alttext="R^{2}" class="ltx_Math" display="inline" id="A5.p1.2.m2.1"><semantics id="A5.p1.2.m2.1a"><msup id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><mi id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml">R</mi><mn id="A5.p1.2.m2.1.1.3" xref="A5.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1">superscript</csymbol><ci id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2">𝑅</ci><cn id="A5.p1.2.m2.1.1.3.cmml" type="integer" xref="A5.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, and MSE statistics in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.T8" title="In Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>. In our implementation, we constrain the last element of <math alttext="b" class="ltx_Math" display="inline" id="A5.p1.3.m3.1"><semantics id="A5.p1.3.m3.1a"><mi id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><ci id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">b</annotation><annotation encoding="application/x-llamapun" id="A5.p1.3.m3.1d">italic_b</annotation></semantics></math>, leaving six learnable parameters in total. Our analysis shows that all parameters are statistically significant, except for <math alttext="b_{1}" class="ltx_Math" display="inline" id="A5.p1.4.m4.1"><semantics id="A5.p1.4.m4.1a"><msub id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml"><mi id="A5.p1.4.m4.1.1.2" xref="A5.p1.4.m4.1.1.2.cmml">b</mi><mn id="A5.p1.4.m4.1.1.3" xref="A5.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><apply id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A5.p1.4.m4.1.1.1.cmml" xref="A5.p1.4.m4.1.1">subscript</csymbol><ci id="A5.p1.4.m4.1.1.2.cmml" xref="A5.p1.4.m4.1.1.2">𝑏</ci><cn id="A5.p1.4.m4.1.1.3.cmml" type="integer" xref="A5.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">b_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.4.m4.1d">italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, which has a <math alttext="p" class="ltx_Math" display="inline" id="A5.p1.5.m5.1"><semantics id="A5.p1.5.m5.1a"><mi id="A5.p1.5.m5.1.1" xref="A5.p1.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.p1.5.m5.1b"><ci id="A5.p1.5.m5.1.1.cmml" xref="A5.p1.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="A5.p1.5.m5.1d">italic_p</annotation></semantics></math>-value slightly above 0.05. Nonetheless, our experiments suggest that retaining <math alttext="b_{1}" class="ltx_Math" display="inline" id="A5.p1.6.m6.1"><semantics id="A5.p1.6.m6.1a"><msub id="A5.p1.6.m6.1.1" xref="A5.p1.6.m6.1.1.cmml"><mi id="A5.p1.6.m6.1.1.2" xref="A5.p1.6.m6.1.1.2.cmml">b</mi><mn id="A5.p1.6.m6.1.1.3" xref="A5.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.p1.6.m6.1b"><apply id="A5.p1.6.m6.1.1.cmml" xref="A5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A5.p1.6.m6.1.1.1.cmml" xref="A5.p1.6.m6.1.1">subscript</csymbol><ci id="A5.p1.6.m6.1.1.2.cmml" xref="A5.p1.6.m6.1.1.2">𝑏</ci><cn id="A5.p1.6.m6.1.1.3.cmml" type="integer" xref="A5.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.6.m6.1c">b_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.6.m6.1d">italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> improves generalization in many cases, such as IterDRAG on multi-hop datasets. For sigmoid scaling, we fit a custom function between the predicted <math alttext="\hat{P}" class="ltx_Math" display="inline" id="A5.p1.7.m7.1"><semantics id="A5.p1.7.m7.1a"><mover accent="true" id="A5.p1.7.m7.1.1" xref="A5.p1.7.m7.1.1.cmml"><mi id="A5.p1.7.m7.1.1.2" xref="A5.p1.7.m7.1.1.2.cmml">P</mi><mo id="A5.p1.7.m7.1.1.1" xref="A5.p1.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A5.p1.7.m7.1b"><apply id="A5.p1.7.m7.1.1.cmml" xref="A5.p1.7.m7.1.1"><ci id="A5.p1.7.m7.1.1.1.cmml" xref="A5.p1.7.m7.1.1.1">^</ci><ci id="A5.p1.7.m7.1.1.2.cmml" xref="A5.p1.7.m7.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.7.m7.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.7.m7.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math> and ground truth <math alttext="P" class="ltx_Math" display="inline" id="A5.p1.8.m8.1"><semantics id="A5.p1.8.m8.1a"><mi id="A5.p1.8.m8.1.1" xref="A5.p1.8.m8.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.p1.8.m8.1b"><ci id="A5.p1.8.m8.1.1.cmml" xref="A5.p1.8.m8.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.8.m8.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.p1.8.m8.1d">italic_P</annotation></semantics></math> values, defined as <math alttext="\sigma(x)=\frac{3.30}{1+e^{-1.81(x+0.46)}}-2.18" class="ltx_Math" display="inline" id="A5.p1.9.m9.2"><semantics id="A5.p1.9.m9.2a"><mrow id="A5.p1.9.m9.2.3" xref="A5.p1.9.m9.2.3.cmml"><mrow id="A5.p1.9.m9.2.3.2" xref="A5.p1.9.m9.2.3.2.cmml"><mi id="A5.p1.9.m9.2.3.2.2" xref="A5.p1.9.m9.2.3.2.2.cmml">σ</mi><mo id="A5.p1.9.m9.2.3.2.1" xref="A5.p1.9.m9.2.3.2.1.cmml">⁢</mo><mrow id="A5.p1.9.m9.2.3.2.3.2" xref="A5.p1.9.m9.2.3.2.cmml"><mo id="A5.p1.9.m9.2.3.2.3.2.1" stretchy="false" xref="A5.p1.9.m9.2.3.2.cmml">(</mo><mi id="A5.p1.9.m9.2.2" xref="A5.p1.9.m9.2.2.cmml">x</mi><mo id="A5.p1.9.m9.2.3.2.3.2.2" stretchy="false" xref="A5.p1.9.m9.2.3.2.cmml">)</mo></mrow></mrow><mo id="A5.p1.9.m9.2.3.1" xref="A5.p1.9.m9.2.3.1.cmml">=</mo><mrow id="A5.p1.9.m9.2.3.3" xref="A5.p1.9.m9.2.3.3.cmml"><mfrac id="A5.p1.9.m9.1.1" xref="A5.p1.9.m9.1.1.cmml"><mn id="A5.p1.9.m9.1.1.3" xref="A5.p1.9.m9.1.1.3.cmml">3.30</mn><mrow id="A5.p1.9.m9.1.1.1" xref="A5.p1.9.m9.1.1.1.cmml"><mn id="A5.p1.9.m9.1.1.1.3" xref="A5.p1.9.m9.1.1.1.3.cmml">1</mn><mo id="A5.p1.9.m9.1.1.1.2" xref="A5.p1.9.m9.1.1.1.2.cmml">+</mo><msup id="A5.p1.9.m9.1.1.1.4" xref="A5.p1.9.m9.1.1.1.4.cmml"><mi id="A5.p1.9.m9.1.1.1.4.2" xref="A5.p1.9.m9.1.1.1.4.2.cmml">e</mi><mrow id="A5.p1.9.m9.1.1.1.1.1" xref="A5.p1.9.m9.1.1.1.1.1.cmml"><mo id="A5.p1.9.m9.1.1.1.1.1a" xref="A5.p1.9.m9.1.1.1.1.1.cmml">−</mo><mrow id="A5.p1.9.m9.1.1.1.1.1.1" xref="A5.p1.9.m9.1.1.1.1.1.1.cmml"><mn id="A5.p1.9.m9.1.1.1.1.1.1.3" xref="A5.p1.9.m9.1.1.1.1.1.1.3.cmml">1.81</mn><mo id="A5.p1.9.m9.1.1.1.1.1.1.2" xref="A5.p1.9.m9.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="A5.p1.9.m9.1.1.1.1.1.1.1.1" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml"><mo id="A5.p1.9.m9.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml"><mi id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.2" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.1" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mn id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.3" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.3.cmml">0.46</mn></mrow><mo id="A5.p1.9.m9.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo id="A5.p1.9.m9.2.3.3.1" xref="A5.p1.9.m9.2.3.3.1.cmml">−</mo><mn id="A5.p1.9.m9.2.3.3.2" xref="A5.p1.9.m9.2.3.3.2.cmml">2.18</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.9.m9.2b"><apply id="A5.p1.9.m9.2.3.cmml" xref="A5.p1.9.m9.2.3"><eq id="A5.p1.9.m9.2.3.1.cmml" xref="A5.p1.9.m9.2.3.1"></eq><apply id="A5.p1.9.m9.2.3.2.cmml" xref="A5.p1.9.m9.2.3.2"><times id="A5.p1.9.m9.2.3.2.1.cmml" xref="A5.p1.9.m9.2.3.2.1"></times><ci id="A5.p1.9.m9.2.3.2.2.cmml" xref="A5.p1.9.m9.2.3.2.2">𝜎</ci><ci id="A5.p1.9.m9.2.2.cmml" xref="A5.p1.9.m9.2.2">𝑥</ci></apply><apply id="A5.p1.9.m9.2.3.3.cmml" xref="A5.p1.9.m9.2.3.3"><minus id="A5.p1.9.m9.2.3.3.1.cmml" xref="A5.p1.9.m9.2.3.3.1"></minus><apply id="A5.p1.9.m9.1.1.cmml" xref="A5.p1.9.m9.1.1"><divide id="A5.p1.9.m9.1.1.2.cmml" xref="A5.p1.9.m9.1.1"></divide><cn id="A5.p1.9.m9.1.1.3.cmml" type="float" xref="A5.p1.9.m9.1.1.3">3.30</cn><apply id="A5.p1.9.m9.1.1.1.cmml" xref="A5.p1.9.m9.1.1.1"><plus id="A5.p1.9.m9.1.1.1.2.cmml" xref="A5.p1.9.m9.1.1.1.2"></plus><cn id="A5.p1.9.m9.1.1.1.3.cmml" type="integer" xref="A5.p1.9.m9.1.1.1.3">1</cn><apply id="A5.p1.9.m9.1.1.1.4.cmml" xref="A5.p1.9.m9.1.1.1.4"><csymbol cd="ambiguous" id="A5.p1.9.m9.1.1.1.4.1.cmml" xref="A5.p1.9.m9.1.1.1.4">superscript</csymbol><ci id="A5.p1.9.m9.1.1.1.4.2.cmml" xref="A5.p1.9.m9.1.1.1.4.2">𝑒</ci><apply id="A5.p1.9.m9.1.1.1.1.1.cmml" xref="A5.p1.9.m9.1.1.1.1.1"><minus id="A5.p1.9.m9.1.1.1.1.1.2.cmml" xref="A5.p1.9.m9.1.1.1.1.1"></minus><apply id="A5.p1.9.m9.1.1.1.1.1.1.cmml" xref="A5.p1.9.m9.1.1.1.1.1.1"><times id="A5.p1.9.m9.1.1.1.1.1.1.2.cmml" xref="A5.p1.9.m9.1.1.1.1.1.1.2"></times><cn id="A5.p1.9.m9.1.1.1.1.1.1.3.cmml" type="float" xref="A5.p1.9.m9.1.1.1.1.1.1.3">1.81</cn><apply id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1"><plus id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.1.cmml" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.1"></plus><ci id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.2.cmml" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.3.cmml" type="float" xref="A5.p1.9.m9.1.1.1.1.1.1.1.1.1.3">0.46</cn></apply></apply></apply></apply></apply></apply><cn id="A5.p1.9.m9.2.3.3.2.cmml" type="float" xref="A5.p1.9.m9.2.3.3.2">2.18</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.9.m9.2c">\sigma(x)=\frac{3.30}{1+e^{-1.81(x+0.46)}}-2.18</annotation><annotation encoding="application/x-llamapun" id="A5.p1.9.m9.2d">italic_σ ( italic_x ) = divide start_ARG 3.30 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - 1.81 ( italic_x + 0.46 ) end_POSTSUPERSCRIPT end_ARG - 2.18</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p" id="A5.p2.3">We also visualize the predictions on for IterDRAG across different datasets in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F11" title="In Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>, where each subplot represents a dataset and each line corresponds to a document setting (<math alttext="k" class="ltx_Math" display="inline" id="A5.p2.1.m1.1"><semantics id="A5.p2.1.m1.1a"><mi id="A5.p2.1.m1.1.1" xref="A5.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A5.p2.1.m1.1b"><ci id="A5.p2.1.m1.1.1.cmml" xref="A5.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A5.p2.1.m1.1d">italic_k</annotation></semantics></math>). The inference compute is scaled by increasing the number of in-context examples (<math alttext="m" class="ltx_Math" display="inline" id="A5.p2.2.m2.1"><semantics id="A5.p2.2.m2.1a"><mi id="A5.p2.2.m2.1.1" xref="A5.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A5.p2.2.m2.1b"><ci id="A5.p2.2.m2.1.1.cmml" xref="A5.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="A5.p2.2.m2.1d">italic_m</annotation></semantics></math>) and generation iterations (<math alttext="n" class="ltx_Math" display="inline" id="A5.p2.3.m3.1"><semantics id="A5.p2.3.m3.1a"><mi id="A5.p2.3.m3.1.1" xref="A5.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A5.p2.3.m3.1b"><ci id="A5.p2.3.m3.1.1.cmml" xref="A5.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="A5.p2.3.m3.1d">italic_n</annotation></semantics></math>). Here, we find similar trends to those in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.F6" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, although IterDRAG shows larger variations compared to DRAG. HotpotQA and 2WikiMultiHopQA show more consistent trends with the predictions, likely due to the predominance of multi-hop queries. In summary, our findings are consistent for both DRAG and IterDRAG, demonstrating that RAG performance can be accurately modeled by our computation allocation model for RAG. For Bamboogle, HotpotQA and 2WikiMultiHopQA, we provide the normalized performance with increasing effective context lengths in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A4.F10" title="In Appendix D Additional Results on Inference Scaling Laws for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>, in which we observe similar trends to the results on MuSiQue (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S1.F1" title="In 1 Introduction ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>). We also illustrate the prediction surface for both DRAG and IterDRAG in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F12" title="In Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_figure" id="A5.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1196" id="A5.F12.sf1.g1" src="x13.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A5.F12.sf1.3.2" style="font-size:90%;">Performance vs. predicted surface for DRAG.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1196" id="A5.F12.sf2.g1" src="x14.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A5.F12.sf2.3.2" style="font-size:90%;">Performance vs. predicted surface for IterDRAG.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="A5.F12.3.2" style="font-size:90%;">Normalized performance vs. predicted surface for DRAG and IterDRAG.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A5.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.sf1"><svg class="ltx_picture" height="170.93" id="A5.F13.sf1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,170.93) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 165.02 C 0 168.28 2.64 170.93 5.91 170.93 L 594.09 170.93 C 597.36 170.93 600 168.28 600 165.02 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 149.51 L 598.03 149.51 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 155.41)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.F13.sf1.pic1.1.1.1.1.1.1.1">Inaccurate or outdated retrieval</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="123.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf1.pic1.2.2.2.1.1.1.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.1.1.1"> What is the lowest elevation of the longest railway tunnel? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.1.1.2"> 500 meters 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.1.1.3"> 312 m</span></span></span>
<span class="ltx_p" id="A5.F13.sf1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf1.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.2.1.1"> According to QS World University Rankings, where does the college that Ibrahim Shihata attended rank? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.2.1.2"> 3rd 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.2.1.3"> 551-600</span></span></span>
<span class="ltx_p" id="A5.F13.sf1.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf1.pic1.2.2.2.1.1.3.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.3.1.1"> Which battle occurred first, the Battle of Manila or the Battle of Guam? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.3.1.2"> Battle of Manila 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf1.pic1.2.2.2.1.1.3.1.3"> Battle of Guam</span></span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A5.F13.sf1.3.2" style="font-size:90%;">Example mistakes due to inaccurate or outdated retrieval.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.sf2"><svg class="ltx_picture" height="190.22" id="A5.F13.sf2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,190.22) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 184.32 C 0 187.58 2.64 190.22 5.91 190.22 L 594.09 190.22 C 597.36 190.22 600 187.58 600 184.32 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 166.11 L 598.03 166.11 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 172.02)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.F13.sf2.pic1.1.1.1.1.1.1.1">Incorrect or lack of reasoning</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="140.52" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf2.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf2.pic1.2.2.2.1.1.1.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.1.1.1"> Which mountain, Masherbrum or Khunyang Chhish, is a taller mountain? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.1.1.2"> Masherbrum 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.1.1.3"> Khunyang Chhish</span></span></span>
<span class="ltx_p" id="A5.F13.sf2.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf2.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.2.1.1"> What is the date of death of the director of film The Organization (Film)? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.2.1.2"> April 15, 2018 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.2.1.3"> December 12, 2012</span></span></span>
<span class="ltx_p" id="A5.F13.sf2.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf2.pic1.2.2.2.1.1.3.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.3.1.1"> Who introduced a system of musical notation in the 14th century that is used in the area where most of the invasion of the eastern Roman Empire took place? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.3.1.2"> Philippe de Vitry 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf2.pic1.2.2.2.1.1.3.1.3"> John Kukuzelis</span></span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A5.F13.sf2.3.2" style="font-size:90%;">Example mistakes due to incorrect or lack of reasoning.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.sf3"><svg class="ltx_picture" height="190.22" id="A5.F13.sf3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,190.22) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 184.32 C 0 187.58 2.64 190.22 5.91 190.22 L 594.09 190.22 C 597.36 190.22 600 187.58 600 184.32 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 166.11 L 598.03 166.11 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 172.02)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.F13.sf3.pic1.1.1.1.1.1.1.1">Hallucination or unfaithful reasoning</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="140.52" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf3.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf3.pic1.2.2.2.1.1.1.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.1.1.1"> Who was the last emperor of the dynasty that succeeded the Song dynasty? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.1.1.2"> Emperor Yuanzhen 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.1.1.3"> Toghon Temür</span></span></span>
<span class="ltx_p" id="A5.F13.sf3.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf3.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.2.1.1"> What is another notable work by the illustrator of Sylvester and the Magic Pebble? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.2.1.2"> Shrek! 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.2.1.3"> Doctor De Soto</span></span></span>
<span class="ltx_p" id="A5.F13.sf3.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf3.pic1.2.2.2.1.1.3.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.3.1.1"> In what movie did a Kenyan-Mexican actress, who graduated from Hampshire College, star in in 2015? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.3.1.2"> Queen of Katwe 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf3.pic1.2.2.2.1.1.3.1.3"> Star Wars: The Force Awakens</span></span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A5.F13.sf3.3.2" style="font-size:90%;">Example mistakes due to hallucination or unfaithful reasoning.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.sf4"><svg class="ltx_picture" height="137.72" id="A5.F13.sf4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,137.72) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 131.81 C 0 135.08 2.64 137.72 5.91 137.72 L 594.09 137.72 C 597.36 137.72 600 135.08 600 131.81 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 116.3 L 598.03 116.3 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 122.21)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.F13.sf4.pic1.1.1.1.1.1.1.1">Evaluation issues or refusal to answer</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="90.71" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.F13.sf4.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A5.F13.sf4.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf4.pic1.2.2.2.1.1.1.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.1.1.1"> The most populous city in Punjab is how large (area wise)? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.1.1.2"> 310 sq. km 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.1.1.3"> 310 square kilometers</span></span></span>
<span class="ltx_p" id="A5.F13.sf4.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A5.F13.sf4.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Question:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.2.1.1"> Renáta Tomanová and Larisa Neiland are former professional athletes for what sport? 
<br class="ltx_break"/></span>Prediction:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.2.1.2"> Tennis 
<br class="ltx_break"/></span>Annotation:<span class="ltx_text ltx_font_serif" id="A5.F13.sf4.pic1.2.2.2.1.1.2.1.3"> Professional tennis</span></span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="A5.F13.sf4.3.2" style="font-size:90%;">Example mistakes due to evaluation issues or refusal to answer.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.2.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="A5.F13.3.2" style="font-size:90%;">Example mistakes of DRAG and IterDRAG across datasets.</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Error Analysis</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">Despite the performance gains from scaling effective context length, RAG performance on challenging datasets like MuSiQue remain moderate, even for IterDRAG. To address this, we analyze the mistakes in both DRAG and IterDRAG to examine the limitations and errors inherent in these approaches. In the following, we explore common failure cases (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F13" title="In Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>) to understand where each method falls short and how they could be further improved.</p>
</div>
<div class="ltx_para" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">We provide selected example mistakes from <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F13.sf1" title="In Figure 13 ‣ Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(a)</span></a> to <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F13.sf4" title="In Figure 13 ‣ Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(d)</span></a>, with retrieved documents omitted for brevity. The reasons for common errors can be grouped into four categories: (1) inaccurate or outdated retrieval; (2) incorrect or lack of reasoning; (3) hallucination or unfaithful reasoning; and (4) evaluation issues or refusal to answer. We elaborate on these categories below:</p>
<ul class="ltx_itemize" id="A6.I1">
<li class="ltx_item" id="A6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i1.p1">
<p class="ltx_p" id="A6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I1.i1.p1.1.1">Inaccurate or outdated retrieval</span>: A major source of RAG errors stems from the retrieval process, where relevant knowledge is not correctly retrieved. For example, in the first question of <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F13.sf3" title="In Figure 13 ‣ Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(c)</span></a>, the top-50 retrieved documents do not contain the correct answer. A similar issue occurs in the second QA pair, where outdated retrieval results fail to provide useful information. In the third case, although both battles are retrieved, the initial documents overly focus on the Battle of Manila, leading to an incorrect response.</p>
</div>
</li>
<li class="ltx_item" id="A6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i2.p1">
<p class="ltx_p" id="A6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I1.i2.p1.1.1">Incorrect or lack of reasoning</span>: Beyond retrieval issues, incorrect reasoning chains are another common source of errors. For example, in the first case in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A5.F13.sf2" title="In Figure 13 ‣ Appendix E Additional Results on Computation Allocation Model for RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13(b)</span></a>, although the correct documents are retrieved, the reasoning process is incomplete (i.e., no explicit comparison of the mountain heights), leading to an incorrect answer in DRAG. Similarly, in the second and third cases, the reasoning is either absent (as in DRAG) or flawed. As a result, reasoning-related errors tend to occur more frequently in difficult questions and in the one-step DRAG approach.</p>
</div>
</li>
<li class="ltx_item" id="A6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i3.p1">
<p class="ltx_p" id="A6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I1.i3.p1.1.1">Hallucination or unfaithful reasoning</span>: Other than retrieval and reasoning, hallucination and unfaithful reasoning also contribute to errors in knowledge-intensive tasks. In the first case, the prediction is incorrect and cannot be found in the retrieved documents. As for the rest cases, while the answers are related, certain steps in the reasoning chain are flawed and cause errors in the final answers. These highlight the persistent challenge of hallucination in LLMs, particularly in long-context generation tasks.</p>
</div>
</li>
<li class="ltx_item" id="A6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i4.p1">
<p class="ltx_p" id="A6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I1.i4.p1.1.1">Evaluation issues or refusal to answer</span>: Finally, we observed several evaluation issues that may lead to inaccurate evaluation. For instance, the use of abbreviations or variations in date format can result in incorrect scoring across all metrics. Moreover, our experiments do not account for abstaining from answering, which could cause unfair scores.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="A6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="262" id="A6.F14.g1" src="x15.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A6.F14.6.3.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="A6.F14.4.2" style="font-size:90%;">Input prompt that comprises of <math alttext="m" class="ltx_Math" display="inline" id="A6.F14.3.1.m1.1"><semantics id="A6.F14.3.1.m1.1b"><mi id="A6.F14.3.1.m1.1.1" xref="A6.F14.3.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A6.F14.3.1.m1.1c"><ci id="A6.F14.3.1.m1.1.1.cmml" xref="A6.F14.3.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F14.3.1.m1.1d">m</annotation><annotation encoding="application/x-llamapun" id="A6.F14.3.1.m1.1e">italic_m</annotation></semantics></math> in-context examples, the test documents and query, in which each document chunk consists of <math alttext="k" class="ltx_Math" display="inline" id="A6.F14.4.2.m2.1"><semantics id="A6.F14.4.2.m2.1b"><mi id="A6.F14.4.2.m2.1.1" xref="A6.F14.4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A6.F14.4.2.m2.1c"><ci id="A6.F14.4.2.m2.1.1.cmml" xref="A6.F14.4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F14.4.2.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="A6.F14.4.2.m2.1e">italic_k</annotation></semantics></math> retrieved documents. For IterDRAG, the example answers additionally provide sub-queries and intermediate answers as demonstrations.</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A7" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Implementation</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">In our experiments, we utilize the Gecko-1B (en) embedding model to index both the documents and input queries <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib31" title="">2024b</a>)</cite>, using Wikipedia passages from the KILT benchmark as the document source <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib45" title="">2020</a>)</cite>. In test-time, the input query is compared against all embeddings in the corpus, and the top-<math alttext="k" class="ltx_Math" display="inline" id="A7.p1.1.m1.1"><semantics id="A7.p1.1.m1.1a"><mi id="A7.p1.1.m1.1.1" xref="A7.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A7.p1.1.m1.1b"><ci id="A7.p1.1.m1.1.1.cmml" xref="A7.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A7.p1.1.m1.1d">italic_k</annotation></semantics></math> neighbors are selected for inference. Each document is then truncated on the right side to a maximum of 1024 tokens using whitespace tokenization. For each example, we arrange the elements in the following order: documents, query, and label, with the retrieved documents listed in reverse order, placing the higher-ranked documents closer to the query <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib39" title="">2024b</a>)</cite>. Consequently, the prompt comprises of multiple in-context examples, followed by the test documents and test query, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A6.F14" title="In Appendix F Error Analysis ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<div class="ltx_para" id="A7.p2">
<p class="ltx_p" id="A7.p2.2">For generation, we utilize Gemini 1.5 Flash for more efficient experiments.
In DRAG, inference scaling is achieved by increasing the context length through the combination of documents (<math alttext="k" class="ltx_Math" display="inline" id="A7.p2.1.m1.1"><semantics id="A7.p2.1.m1.1a"><mi id="A7.p2.1.m1.1.1" xref="A7.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A7.p2.1.m1.1b"><ci id="A7.p2.1.m1.1.1.cmml" xref="A7.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A7.p2.1.m1.1d">italic_k</annotation></semantics></math>) and in-context examples (<math alttext="m" class="ltx_Math" display="inline" id="A7.p2.2.m2.1"><semantics id="A7.p2.2.m2.1a"><mi id="A7.p2.2.m2.1.1" xref="A7.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A7.p2.2.m2.1b"><ci id="A7.p2.2.m2.1.1.cmml" xref="A7.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p2.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="A7.p2.2.m2.1d">italic_m</annotation></semantics></math>). Then, the prompt (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A7.F15" title="In Appendix G Implementation ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a>) is provided to the model for a one-time generation using the default generation parameters. For IterDRAG, the input prompt is constructed in a similar fashion, with the example answers consisting of assembled sub-queries, intermediate answers, and the final answer (See <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A7.F16" title="In Appendix G Implementation ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a>). Here, we scale test-time compute by incorporating iterative retrieval and generation, along with the increase of documents and demonstrations. In each iteration, we restrict the generation to adhere to the Self-Ask format, in which the response should start with “Follow up: ”, “Intermediate answer: ” or “So the final answer is: ” <cite class="ltx_cite ltx_citemacro_citep">(Koo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#bib.bib27" title="">2024</a>)</cite>. Each iteration begins with the generation of a sub-query and concludes with the production of an intermediate answer. If a sub-query is generated, additional documents are retrieved and appended to the initial set (i.e., Test Documents in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#A6.F14" title="In Appendix F Error Analysis ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>), after which the model generates an intermediate answer. We allow up to five iterations, after which the model is forced to produce the final answer.</p>
</div>
<div class="ltx_para" id="A7.p3">
<p class="ltx_p" id="A7.p3.7">To evaluate the estimated parameters within computation allocation model for RAG, we normalized the performance metrics by subtracting the mean and dividing by the standard deviation for each dataset and metric. For DRAG, the effective context length is calculated by counting the tokens in the prompt, while for IterDRAG, it is determined by summing the context tokens across all inference requests. We constrain the last parameter in <math alttext="b" class="ltx_Math" display="inline" id="A7.p3.1.m1.1"><semantics id="A7.p3.1.m1.1a"><mi id="A7.p3.1.m1.1.1" xref="A7.p3.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="A7.p3.1.m1.1b"><ci id="A7.p3.1.m1.1.1.cmml" xref="A7.p3.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="A7.p3.1.m1.1d">italic_b</annotation></semantics></math> and perform ordinary least squares to estimate rest six parameters in <a class="ltx_ref" href="https://arxiv.org/html/2410.04343v1#S5.E2" title="In 5.1 Formulation and Estimation ‣ 5 Inference Computation Allocation for Long-Context RAG ‣ Inference Scaling for Long-Context Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a>. To prevent numerical instability, we shift the values in <math alttext="\theta" class="ltx_Math" display="inline" id="A7.p3.2.m2.1"><semantics id="A7.p3.2.m2.1a"><mi id="A7.p3.2.m2.1.1" xref="A7.p3.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="A7.p3.2.m2.1b"><ci id="A7.p3.2.m2.1.1.cmml" xref="A7.p3.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="A7.p3.2.m2.1d">italic_θ</annotation></semantics></math> by a small constant <math alttext="\epsilon" class="ltx_Math" display="inline" id="A7.p3.3.m3.1"><semantics id="A7.p3.3.m3.1a"><mi id="A7.p3.3.m3.1.1" xref="A7.p3.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A7.p3.3.m3.1b"><ci id="A7.p3.3.m3.1.1.cmml" xref="A7.p3.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.3.m3.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="A7.p3.3.m3.1d">italic_ϵ</annotation></semantics></math> of 0.01. When computing <math alttext="R^{2}" class="ltx_Math" display="inline" id="A7.p3.4.m4.1"><semantics id="A7.p3.4.m4.1a"><msup id="A7.p3.4.m4.1.1" xref="A7.p3.4.m4.1.1.cmml"><mi id="A7.p3.4.m4.1.1.2" xref="A7.p3.4.m4.1.1.2.cmml">R</mi><mn id="A7.p3.4.m4.1.1.3" xref="A7.p3.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="A7.p3.4.m4.1b"><apply id="A7.p3.4.m4.1.1.cmml" xref="A7.p3.4.m4.1.1"><csymbol cd="ambiguous" id="A7.p3.4.m4.1.1.1.cmml" xref="A7.p3.4.m4.1.1">superscript</csymbol><ci id="A7.p3.4.m4.1.1.2.cmml" xref="A7.p3.4.m4.1.1.2">𝑅</ci><cn id="A7.p3.4.m4.1.1.3.cmml" type="integer" xref="A7.p3.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.4.m4.1c">R^{2}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.4.m4.1d">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and MSE, we manage noisy data by excluding peak and valley outliers in our experiments. However, for domain generalization and length extrapolation, all data points are included in the evaluation. To predict downstream task performance, <math alttext="i" class="ltx_Math" display="inline" id="A7.p3.5.m5.1"><semantics id="A7.p3.5.m5.1a"><mi id="A7.p3.5.m5.1.1" xref="A7.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A7.p3.5.m5.1b"><ci id="A7.p3.5.m5.1.1.cmml" xref="A7.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="A7.p3.5.m5.1d">italic_i</annotation></semantics></math> should be computed for each task. Specifically, in each strategy and task: <math alttext="i_{\text{doc}}=P(k=1,m=0,n=1)-P(k=0,m=0,n=1)" class="ltx_Math" display="inline" id="A7.p3.6.m6.2"><semantics id="A7.p3.6.m6.2a"><mrow id="A7.p3.6.m6.2.2" xref="A7.p3.6.m6.2.2.cmml"><msub id="A7.p3.6.m6.2.2.4" xref="A7.p3.6.m6.2.2.4.cmml"><mi id="A7.p3.6.m6.2.2.4.2" xref="A7.p3.6.m6.2.2.4.2.cmml">i</mi><mtext id="A7.p3.6.m6.2.2.4.3" xref="A7.p3.6.m6.2.2.4.3a.cmml">doc</mtext></msub><mo id="A7.p3.6.m6.2.2.3" xref="A7.p3.6.m6.2.2.3.cmml">=</mo><mrow id="A7.p3.6.m6.2.2.2" xref="A7.p3.6.m6.2.2.2.cmml"><mrow id="A7.p3.6.m6.1.1.1.1" xref="A7.p3.6.m6.1.1.1.1.cmml"><mi id="A7.p3.6.m6.1.1.1.1.3" xref="A7.p3.6.m6.1.1.1.1.3.cmml">P</mi><mo id="A7.p3.6.m6.1.1.1.1.2" xref="A7.p3.6.m6.1.1.1.1.2.cmml">⁢</mo><mrow id="A7.p3.6.m6.1.1.1.1.1.1" xref="A7.p3.6.m6.1.1.1.1.cmml"><mo id="A7.p3.6.m6.1.1.1.1.1.1.2" stretchy="false" xref="A7.p3.6.m6.1.1.1.1.cmml">(</mo><mrow id="A7.p3.6.m6.1.1.1.1.1.1.1.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.3.cmml"><mrow id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mi id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mo id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.1" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.3" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A7.p3.6.m6.1.1.1.1.1.1.1.2.3" xref="A7.p3.6.m6.1.1.1.1.1.1.1.3a.cmml">,</mo><mrow id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.3.cmml"><mrow id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.cmml"><mi id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.2.cmml">m</mi><mo id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.1" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.3" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.3" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.3a.cmml">,</mo><mrow id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.2" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.2.cmml">n</mi><mo id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.1" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.3" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></mrow><mo id="A7.p3.6.m6.1.1.1.1.1.1.3" stretchy="false" xref="A7.p3.6.m6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A7.p3.6.m6.2.2.2.3" xref="A7.p3.6.m6.2.2.2.3.cmml">−</mo><mrow id="A7.p3.6.m6.2.2.2.2" xref="A7.p3.6.m6.2.2.2.2.cmml"><mi id="A7.p3.6.m6.2.2.2.2.3" xref="A7.p3.6.m6.2.2.2.2.3.cmml">P</mi><mo id="A7.p3.6.m6.2.2.2.2.2" xref="A7.p3.6.m6.2.2.2.2.2.cmml">⁢</mo><mrow id="A7.p3.6.m6.2.2.2.2.1.1" xref="A7.p3.6.m6.2.2.2.2.cmml"><mo id="A7.p3.6.m6.2.2.2.2.1.1.2" stretchy="false" xref="A7.p3.6.m6.2.2.2.2.cmml">(</mo><mrow id="A7.p3.6.m6.2.2.2.2.1.1.1.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.3.cmml"><mrow id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.cmml"><mi id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.2.cmml">k</mi><mo id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.1" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.1.cmml">=</mo><mn id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.3" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.6.m6.2.2.2.2.1.1.1.2.3" xref="A7.p3.6.m6.2.2.2.2.1.1.1.3a.cmml">,</mo><mrow id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.3.cmml"><mrow id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.cmml"><mi id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.2.cmml">m</mi><mo id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.1" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.3" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.3" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.3a.cmml">,</mo><mrow id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.cmml"><mi id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.2" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.2.cmml">n</mi><mo id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.1" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.3" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></mrow><mo id="A7.p3.6.m6.2.2.2.2.1.1.3" stretchy="false" xref="A7.p3.6.m6.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.p3.6.m6.2b"><apply id="A7.p3.6.m6.2.2.cmml" xref="A7.p3.6.m6.2.2"><eq id="A7.p3.6.m6.2.2.3.cmml" xref="A7.p3.6.m6.2.2.3"></eq><apply id="A7.p3.6.m6.2.2.4.cmml" xref="A7.p3.6.m6.2.2.4"><csymbol cd="ambiguous" id="A7.p3.6.m6.2.2.4.1.cmml" xref="A7.p3.6.m6.2.2.4">subscript</csymbol><ci id="A7.p3.6.m6.2.2.4.2.cmml" xref="A7.p3.6.m6.2.2.4.2">𝑖</ci><ci id="A7.p3.6.m6.2.2.4.3a.cmml" xref="A7.p3.6.m6.2.2.4.3"><mtext id="A7.p3.6.m6.2.2.4.3.cmml" mathsize="70%" xref="A7.p3.6.m6.2.2.4.3">doc</mtext></ci></apply><apply id="A7.p3.6.m6.2.2.2.cmml" xref="A7.p3.6.m6.2.2.2"><minus id="A7.p3.6.m6.2.2.2.3.cmml" xref="A7.p3.6.m6.2.2.2.3"></minus><apply id="A7.p3.6.m6.1.1.1.1.cmml" xref="A7.p3.6.m6.1.1.1.1"><times id="A7.p3.6.m6.1.1.1.1.2.cmml" xref="A7.p3.6.m6.1.1.1.1.2"></times><ci id="A7.p3.6.m6.1.1.1.1.3.cmml" xref="A7.p3.6.m6.1.1.1.1.3">𝑃</ci><apply id="A7.p3.6.m6.1.1.1.1.1.1.1.3.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A7.p3.6.m6.1.1.1.1.1.1.1.3a.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1"><eq id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.1.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.1"></eq><ci id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.2.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.2">𝑘</ci><cn id="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="A7.p3.6.m6.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.3.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.3a.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1"><eq id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.1.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.1"></eq><ci id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.2.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.2">𝑚</ci><cn id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.3.cmml" type="integer" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.1.1.3">0</cn></apply><apply id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2"><eq id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.1"></eq><ci id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.2">𝑛</ci><cn id="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.3.cmml" type="integer" xref="A7.p3.6.m6.1.1.1.1.1.1.1.2.2.2.2.3">1</cn></apply></apply></apply></apply><apply id="A7.p3.6.m6.2.2.2.2.cmml" xref="A7.p3.6.m6.2.2.2.2"><times id="A7.p3.6.m6.2.2.2.2.2.cmml" xref="A7.p3.6.m6.2.2.2.2.2"></times><ci id="A7.p3.6.m6.2.2.2.2.3.cmml" xref="A7.p3.6.m6.2.2.2.2.3">𝑃</ci><apply id="A7.p3.6.m6.2.2.2.2.1.1.1.3.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="A7.p3.6.m6.2.2.2.2.1.1.1.3a.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.3">formulae-sequence</csymbol><apply id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1"><eq id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.1.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.1"></eq><ci id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.2.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.2">𝑘</ci><cn id="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.3.cmml" type="integer" xref="A7.p3.6.m6.2.2.2.2.1.1.1.1.1.3">0</cn></apply><apply id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.3.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.3a.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1"><eq id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.1.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.1"></eq><ci id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.2.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.2">𝑚</ci><cn id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.3.cmml" type="integer" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.1.1.3">0</cn></apply><apply id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2"><eq id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.1.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.1"></eq><ci id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.2.cmml" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.2">𝑛</ci><cn id="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.3.cmml" type="integer" xref="A7.p3.6.m6.2.2.2.2.1.1.1.2.2.2.2.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.6.m6.2c">i_{\text{doc}}=P(k=1,m=0,n=1)-P(k=0,m=0,n=1)</annotation><annotation encoding="application/x-llamapun" id="A7.p3.6.m6.2d">italic_i start_POSTSUBSCRIPT doc end_POSTSUBSCRIPT = italic_P ( italic_k = 1 , italic_m = 0 , italic_n = 1 ) - italic_P ( italic_k = 0 , italic_m = 0 , italic_n = 1 )</annotation></semantics></math>, <math alttext="i_{\text{shot}}=P(k=0,m=1,n=1)-P(k=0,m=0,n=1)" class="ltx_Math" display="inline" id="A7.p3.7.m7.2"><semantics id="A7.p3.7.m7.2a"><mrow id="A7.p3.7.m7.2.2" xref="A7.p3.7.m7.2.2.cmml"><msub id="A7.p3.7.m7.2.2.4" xref="A7.p3.7.m7.2.2.4.cmml"><mi id="A7.p3.7.m7.2.2.4.2" xref="A7.p3.7.m7.2.2.4.2.cmml">i</mi><mtext id="A7.p3.7.m7.2.2.4.3" xref="A7.p3.7.m7.2.2.4.3a.cmml">shot</mtext></msub><mo id="A7.p3.7.m7.2.2.3" xref="A7.p3.7.m7.2.2.3.cmml">=</mo><mrow id="A7.p3.7.m7.2.2.2" xref="A7.p3.7.m7.2.2.2.cmml"><mrow id="A7.p3.7.m7.1.1.1.1" xref="A7.p3.7.m7.1.1.1.1.cmml"><mi id="A7.p3.7.m7.1.1.1.1.3" xref="A7.p3.7.m7.1.1.1.1.3.cmml">P</mi><mo id="A7.p3.7.m7.1.1.1.1.2" xref="A7.p3.7.m7.1.1.1.1.2.cmml">⁢</mo><mrow id="A7.p3.7.m7.1.1.1.1.1.1" xref="A7.p3.7.m7.1.1.1.1.cmml"><mo id="A7.p3.7.m7.1.1.1.1.1.1.2" stretchy="false" xref="A7.p3.7.m7.1.1.1.1.cmml">(</mo><mrow id="A7.p3.7.m7.1.1.1.1.1.1.1.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.3.cmml"><mrow id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml"><mi id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mo id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.1" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.3" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.7.m7.1.1.1.1.1.1.1.2.3" xref="A7.p3.7.m7.1.1.1.1.1.1.1.3a.cmml">,</mo><mrow id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.3.cmml"><mrow id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.cmml"><mi id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.2.cmml">m</mi><mo id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.1" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.3" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.3.cmml">1</mn></mrow><mo id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.3" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.3a.cmml">,</mo><mrow id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.2" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.2.cmml">n</mi><mo id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.1" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.3" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></mrow><mo id="A7.p3.7.m7.1.1.1.1.1.1.3" stretchy="false" xref="A7.p3.7.m7.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A7.p3.7.m7.2.2.2.3" xref="A7.p3.7.m7.2.2.2.3.cmml">−</mo><mrow id="A7.p3.7.m7.2.2.2.2" xref="A7.p3.7.m7.2.2.2.2.cmml"><mi id="A7.p3.7.m7.2.2.2.2.3" xref="A7.p3.7.m7.2.2.2.2.3.cmml">P</mi><mo id="A7.p3.7.m7.2.2.2.2.2" xref="A7.p3.7.m7.2.2.2.2.2.cmml">⁢</mo><mrow id="A7.p3.7.m7.2.2.2.2.1.1" xref="A7.p3.7.m7.2.2.2.2.cmml"><mo id="A7.p3.7.m7.2.2.2.2.1.1.2" stretchy="false" xref="A7.p3.7.m7.2.2.2.2.cmml">(</mo><mrow id="A7.p3.7.m7.2.2.2.2.1.1.1.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.3.cmml"><mrow id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.cmml"><mi id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.2.cmml">k</mi><mo id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.1" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.1.cmml">=</mo><mn id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.3" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.7.m7.2.2.2.2.1.1.1.2.3" xref="A7.p3.7.m7.2.2.2.2.1.1.1.3a.cmml">,</mo><mrow id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.3.cmml"><mrow id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.cmml"><mi id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.2.cmml">m</mi><mo id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.1" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.1.cmml">=</mo><mn id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.3" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.3.cmml">0</mn></mrow><mo id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.3" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.3a.cmml">,</mo><mrow id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.cmml"><mi id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.2" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.2.cmml">n</mi><mo id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.1" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.1.cmml">=</mo><mn id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.3" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.3.cmml">1</mn></mrow></mrow></mrow><mo id="A7.p3.7.m7.2.2.2.2.1.1.3" stretchy="false" xref="A7.p3.7.m7.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.p3.7.m7.2b"><apply id="A7.p3.7.m7.2.2.cmml" xref="A7.p3.7.m7.2.2"><eq id="A7.p3.7.m7.2.2.3.cmml" xref="A7.p3.7.m7.2.2.3"></eq><apply id="A7.p3.7.m7.2.2.4.cmml" xref="A7.p3.7.m7.2.2.4"><csymbol cd="ambiguous" id="A7.p3.7.m7.2.2.4.1.cmml" xref="A7.p3.7.m7.2.2.4">subscript</csymbol><ci id="A7.p3.7.m7.2.2.4.2.cmml" xref="A7.p3.7.m7.2.2.4.2">𝑖</ci><ci id="A7.p3.7.m7.2.2.4.3a.cmml" xref="A7.p3.7.m7.2.2.4.3"><mtext id="A7.p3.7.m7.2.2.4.3.cmml" mathsize="70%" xref="A7.p3.7.m7.2.2.4.3">shot</mtext></ci></apply><apply id="A7.p3.7.m7.2.2.2.cmml" xref="A7.p3.7.m7.2.2.2"><minus id="A7.p3.7.m7.2.2.2.3.cmml" xref="A7.p3.7.m7.2.2.2.3"></minus><apply id="A7.p3.7.m7.1.1.1.1.cmml" xref="A7.p3.7.m7.1.1.1.1"><times id="A7.p3.7.m7.1.1.1.1.2.cmml" xref="A7.p3.7.m7.1.1.1.1.2"></times><ci id="A7.p3.7.m7.1.1.1.1.3.cmml" xref="A7.p3.7.m7.1.1.1.1.3">𝑃</ci><apply id="A7.p3.7.m7.1.1.1.1.1.1.1.3.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A7.p3.7.m7.1.1.1.1.1.1.1.3a.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1"><eq id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.1.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.1"></eq><ci id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.2.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.2">𝑘</ci><cn id="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="A7.p3.7.m7.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.3.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.3a.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1"><eq id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.1.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.1"></eq><ci id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.2.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.2">𝑚</ci><cn id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.3.cmml" type="integer" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.1.1.3">1</cn></apply><apply id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2"><eq id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.1"></eq><ci id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.2">𝑛</ci><cn id="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.3.cmml" type="integer" xref="A7.p3.7.m7.1.1.1.1.1.1.1.2.2.2.2.3">1</cn></apply></apply></apply></apply><apply id="A7.p3.7.m7.2.2.2.2.cmml" xref="A7.p3.7.m7.2.2.2.2"><times id="A7.p3.7.m7.2.2.2.2.2.cmml" xref="A7.p3.7.m7.2.2.2.2.2"></times><ci id="A7.p3.7.m7.2.2.2.2.3.cmml" xref="A7.p3.7.m7.2.2.2.2.3">𝑃</ci><apply id="A7.p3.7.m7.2.2.2.2.1.1.1.3.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="A7.p3.7.m7.2.2.2.2.1.1.1.3a.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.3">formulae-sequence</csymbol><apply id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1"><eq id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.1.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.1"></eq><ci id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.2.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.2">𝑘</ci><cn id="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.3.cmml" type="integer" xref="A7.p3.7.m7.2.2.2.2.1.1.1.1.1.3">0</cn></apply><apply id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.3.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.3a.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1"><eq id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.1.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.1"></eq><ci id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.2.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.2">𝑚</ci><cn id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.3.cmml" type="integer" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.1.1.3">0</cn></apply><apply id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2"><eq id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.1.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.1"></eq><ci id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.2.cmml" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.2">𝑛</ci><cn id="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.3.cmml" type="integer" xref="A7.p3.7.m7.2.2.2.2.1.1.1.2.2.2.2.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.7.m7.2c">i_{\text{shot}}=P(k=0,m=1,n=1)-P(k=0,m=0,n=1)</annotation><annotation encoding="application/x-llamapun" id="A7.p3.7.m7.2d">italic_i start_POSTSUBSCRIPT shot end_POSTSUBSCRIPT = italic_P ( italic_k = 0 , italic_m = 1 , italic_n = 1 ) - italic_P ( italic_k = 0 , italic_m = 0 , italic_n = 1 )</annotation></semantics></math>. For the predicted optimal hyperparameters, we present the actual metric values to validate the efficacy of computation allocation model for RAG.</p>
</div>
<figure class="ltx_figure" id="A7.F15"><svg class="ltx_picture" height="175.89" id="A7.F15.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,175.89) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 169.99 C 0 173.25 2.64 175.89 5.91 175.89 L 594.09 175.89 C 597.36 175.89 600 173.25 600 169.99 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 151.78 L 598.03 151.78 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 157.69)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.F15.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A7.F15.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A7.F15.pic1.1.1.1.1.1.1.1">Prompt for DRAG</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="126.19" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.F15.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.1.1" style="font-size:80%;">You are an expert in question answering. I am going to give you one or more example triples of context, question and answer, in which the context may or may not be relevant to the question. The examples will be written.</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.2"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Context (which may or may not be relevant):</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A7.F15.pic1.2.2.2.1.1.3.1" style="font-size:80%;">&lt;Retrieved documents&gt;<span class="ltx_text ltx_font_serif" id="A7.F15.pic1.2.2.2.1.1.3.1.1"></span></span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.4"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.4.1" style="font-size:80%;">Question: What is the place of birth of the director of film Servant’S Entrance?</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.5"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.5.1" style="font-size:80%;">Answer: Helsingfors</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.6"><span class="ltx_text ltx_font_typewriter" id="A7.F15.pic1.2.2.2.1.1.6.1" style="font-size:80%;">&lt;Further demonstrations&gt;<span class="ltx_text ltx_font_serif" id="A7.F15.pic1.2.2.2.1.1.6.1.1"></span></span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.7"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.7.1" style="font-size:80%;">After the examples, I am going to provide another pair of context and question, in which the context may or may not be relevant to the question. I want you to answer the question. Give only the answer, and no extra commentary, formatting, or chattiness. Answer the question.</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.8"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.8.1" style="font-size:80%;">Context (which may or may not be relevant):</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.9"><span class="ltx_text ltx_font_typewriter" id="A7.F15.pic1.2.2.2.1.1.9.1" style="font-size:80%;">&lt;Retrieved documents&gt;<span class="ltx_text ltx_font_serif" id="A7.F15.pic1.2.2.2.1.1.9.1.1"></span></span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.10"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.10.1" style="font-size:80%;">Question: Who was born first out of Thomas Henry Holland and Jean-Mandé Sigogne?</span></span>
<span class="ltx_p" id="A7.F15.pic1.2.2.2.1.1.11"><span class="ltx_text" id="A7.F15.pic1.2.2.2.1.1.11.1" style="font-size:80%;">Answer:</span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A7.F15.2.1.1" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text" id="A7.F15.3.2" style="font-size:90%;">Example prompt for DRAG. The prompt comprises of instructions and varying number of demonstrations, followed by a test example.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A7.F16"><svg class="ltx_picture" height="260.02" id="A7.F16.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,260.02) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 254.12 C 0 257.38 2.64 260.02 5.91 260.02 L 594.09 260.02 C 597.36 260.02 600 257.38 600 254.12 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 235.91 L 598.03 235.91 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 241.82)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.F16.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A7.F16.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A7.F16.pic1.1.1.1.1.1.1.1">Prompt for IterDRAG</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="210.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.F16.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.1.1" style="font-size:80%;">You are an expert in question answering. I am going to give you one or more example sets of context, question, potential follow up questions and their respective answers, in which the context may or may not be relevant to the questions. The examples will be written.</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.2"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.2.1" style="font-size:80%;">Context:</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A7.F16.pic1.2.2.2.1.1.3.1" style="font-size:80%;">&lt;Retrieved documents&gt;<span class="ltx_text ltx_font_serif" id="A7.F16.pic1.2.2.2.1.1.3.1.1"></span></span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.4"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.4.1" style="font-size:80%;">Question: What nationality is the director of film Boggy Creek Ii: And The Legend Continues?</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.5"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.5.1" style="font-size:80%;">Follow up: Who is the director of the film Boggy Creek II: And The Legend Continues?</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.6"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.6.1" style="font-size:80%;">Intermediate answer: The director of the film Boggy Creek II: And The Legend Continues is Charles B. Pierce.</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.7"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.7.1" style="font-size:80%;">Follow up: What is the nationality of Charles B. Pierce?</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.8"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.8.1" style="font-size:80%;">Intermediate answer: The nationality of Charles B. Pierce is American.</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.9"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.9.1" style="font-size:80%;">So the final answer is: American</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.10"><span class="ltx_text ltx_font_typewriter" id="A7.F16.pic1.2.2.2.1.1.10.1" style="font-size:80%;">&lt;Further demonstrations&gt;<span class="ltx_text ltx_font_serif" id="A7.F16.pic1.2.2.2.1.1.10.1.1"></span></span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.11"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.11.1" style="font-size:80%;">After the examples, I am going to provide another pair of context and question, in which the context may or may not be relevant to the question. I want you to answer the question. When needed, generate follow up question(s) using the format ’Follow up: X’, where X is the follow up question. Then, answer each follow up question using ’Intermediate answer: X’ with X being the answer. Finally, answer to the main question with the format ’So the final answer is: X’, where X is the final answer.</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.12"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.12.1" style="font-size:80%;">Context:</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.13"><span class="ltx_text ltx_font_typewriter" id="A7.F16.pic1.2.2.2.1.1.13.1" style="font-size:80%;">&lt;Retrieved documents (with interleaving retrieval)&gt;<span class="ltx_text ltx_font_serif" id="A7.F16.pic1.2.2.2.1.1.13.1.1"></span></span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.14"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.14.1" style="font-size:80%;">Question: Where was the director of film Death Of A Friend born?</span></span>
<span class="ltx_p" id="A7.F16.pic1.2.2.2.1.1.15"><span class="ltx_text" id="A7.F16.pic1.2.2.2.1.1.15.1" style="font-size:80%;">Follow up: | Intermediate answer: | So the final answer is:</span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A7.F16.2.1.1" style="font-size:90%;">Figure 16</span>: </span><span class="ltx_text" id="A7.F16.3.2" style="font-size:90%;">Example prompt for IterDRAG. The prompt comprises of instructions and varying number of demonstrations, followed by a test example. In each iteration, we control the generation to follow the Self-Ask format with constrained decoding.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct  6 03:38:08 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
