<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely</title>
<!--Generated on Mon Sep 23 11:09:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %Uncommentâ£toâ£removeâ£theâ£date .-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14924v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S1" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S2" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Problem Definition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S2.SS1" title="In 2 Problem Definition â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Stratification of Queries</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Explicit Fact Queries (L1)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS1" title="In 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS1.SSS1" title="In 3.1 Overview â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Data Dependency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS1.SSS2" title="In 3.1 Overview â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Definition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS2" title="In 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Challenges and Solutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS3" title="In 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Retrieval-augmented Generation (RAG)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS3.SSS1" title="In 3.3 Retrieval-augmented Generation (RAG) â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Data Processing Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS3.SSS2" title="In 3.3 Retrieval-augmented Generation (RAG) â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Data Retrieval Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.SS3.SSS3" title="In 3.3 Retrieval-augmented Generation (RAG) â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Response Generation Enhancement</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implicit Fact Queries (L2)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS1" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS2" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Challenges and Solutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS3" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Iterative RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS4" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Graph/ Tree Question Answering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS5" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Natural Language to SQL Queries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S4.SS6" title="In 4 Implicit Fact Queries (L2) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Discussion on Fact Queries</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Interpretable Rationale Queries (L3)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5.SS1" title="In 5 Interpretable Rationale Queries (L3) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5.SS2" title="In 5 Interpretable Rationale Queries (L3) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Challenges and Solutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5.SS3" title="In 5 Interpretable Rationale Queries (L3) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Prompt Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5.SS4" title="In 5 Interpretable Rationale Queries (L3) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>CoT Prompting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Hidden Rationale Queries (L4)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.SS1" title="In 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.SS2" title="In 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Challenges and Solutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.SS3" title="In 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Offline Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.SS4" title="In 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>In Context Learning (ICL)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.SS5" title="In 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S7" title="In Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Siyun Zhao , Yuqing Yang , Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu
<br class="ltx_break"/>Microsoft Research Asia
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{siyunzhao,yuqing.yang,wangzilong,zhiyuan.he,lunaqiu,liliqiu}@microsoft.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. External data not only bolsters the modelsâ€™ domain-specific expertise and temporal relevance but also diminishes incidences of hallucination, thereby enhancing both the controllability and interpretability of outputs. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and the taskâ€™s primary focus: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have demonstrated remarkable capabilities, including extensive world knowledge and sophisticated reasoning skills. Despite these advancements, there are significant challenges in effectively deploying them across various specialized fields. These challenges include issues like model hallucinations, misalignment with domain-specific knowledge, among others. Incorporating domain-specific data, particularly private or on-premise data that could not be included in their initial training corpus, is crucial for tailoring LLM applications to meet specific industry needs.
Through techniques like RAG and fine tuning, data augmented LLM applications have demonstrated advantages over applications built solely on generic LLMs, in several aspects:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Enhanced Professionalism and Timeliness:</span> The data used to train LLMs often lags in timeliness and may not cover all domains comprehensively, especially proprietary data owned by users. <span class="ltx_ERROR undefined" id="S1.I1.i1.p1.1.2">\MFUsentencecase</span>data augmented LLM applications address this issue by providing more detailed and accurate answers for complex questions, allowing for data updates and customization.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Alignment with Domain Experts:</span> Through the use of and learning from domain-specific data, data augmented LLM applications can exhibit capabilities more like domain experts, such as doctors and lawyers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Reduction in Model Hallucination:</span> <span class="ltx_ERROR undefined" id="S1.I1.i3.p1.1.2">\MFUsentencecase</span>data augmented LLM applications generate responses based on real data, grounding their reactions in facts and significantly minimizing the possibility of hallucinations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Improved Controllability and Explainability:</span> The data used can serve as a reference for the modelâ€™s predictions, enhancing both controllability and explainability.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the enthusiasm for these advancements, developers often struggle and have to invest a significant amount of human labor to meet its expectations (<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">e.g.,Â </span>achieving a high success rate in question answering). Numerous studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib5" title="">5</a>]</cite> highlight the challenges and frustrations involved in constructing a data augmented LLM applications based on technologies like RAG and fine-tuning, particularly in specialized domains such as the legal field, healthcare, manufacturing, and others.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">These challenges span a wide range, from constructing data pipelines (<span class="ltx_text ltx_font_italic" id="S1.p4.1.1">e.g.,Â </span>data processing and indexing) to leveraging LLMsâ€™ capabilities to achieve complex intelligent reasoning. For example, in applications of finance, there is a frequent need to understand and utilize high-dimensional time series data, whereas in healthcare, medical images or time-series medical records are often essential. Enabling LLMs to comprehend these varied forms of data represents a recurring challenge. On the other hand, in legal and mathematical applications, LLMs typically struggle to grasp long-distance dependencies between different structures. Additionally, depending on the specific application domain, there are increased demands for the interpretability and consistency of LLM responses. The inherent nature of LLMs tends to be characterized by low interpretability and high uncertainty, which poses significant challenges. Enhancing the transparency of LLMs and reducing their uncertainty are critical for increasing trust and reliability in their outputs, especially in fields where precision and accountability are paramount.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Through extensive discussions with domain experts and developers, and by carefully analyzing the challenges they face, we have gained a deep understanding that data augmented LLM applicationsis not a one-size-fits-all solution. The real-world demands, particularly in expert domains, are highly complex and can vary significantly in their relationship with given data and the reasoning difficulties they require. However, developers often do not realize these distinctions and end up with a solution full of performance pitfalls (akin to a house with leaks everywhere). In contrast, if we could fully understand the demands at different levels and their unique challenges, we could build applications accordingly and make the application steadily improve (like constructing a solid and reliable house step by step).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Yet, research efforts and existing relevant surveysÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib13" title="">13</a>]</cite> frequently focus on only one of these levels or a particular topic of technologies. This has motivated us to compile this comprehensive survey, which aims to clearly define these different levels of queries, identify the unique challenges associated with each(FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">1</span></a>) , and list related works and efforts addressing them. This survey is intended to help readers construct a birdâ€™s-eye view of data augmented LLM applications and also serve as a handbook on how to approach the development of such applications systematically.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S1.F1.g1" src="extracted/5872412/contents/images/four_level_main_focus.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Main Focus of Four Level Queries</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Problem Definition</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.8">Data-augmented LLM applications can take many forms, ranging from the frequently seen Question-Answering bots based on domain-specific data, to semantic processing operators within complex data pipelines, or even agents handling specific steps in a multi-agent system. However, in general, a data-augmented LLM application can be formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:\mathcal{Q}\xrightarrow{\mathcal{D}}\mathcal{A}" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">f</mi><mo id="S2.E1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S2.E1.m1.1.1.1.cmml">:</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">ğ’¬</mi><mover accent="true" id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml"><mo id="S2.E1.m1.1.1.3.1.2" stretchy="false" xref="S2.E1.m1.1.1.3.1.2.cmml">â†’</mo><mo class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.1.1" xref="S2.E1.m1.1.1.3.1.1.cmml">ğ’Ÿ</mo></mover><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">ğ’œ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">:</ci><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">ğ‘“</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><apply id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"><ci id="S2.E1.m1.1.1.3.1.1.cmml" xref="S2.E1.m1.1.1.3.1.1">ğ’Ÿ</ci><ci id="S2.E1.m1.1.1.3.1.2.cmml" xref="S2.E1.m1.1.1.3.1.2">â†’</ci></apply><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">ğ’¬</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">ğ’œ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">f:\mathcal{Q}\xrightarrow{\mathcal{D}}\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_f : caligraphic_Q start_ARROW overcaligraphic_D â†’ end_ARROW caligraphic_A</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p1.7">where <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">caligraphic_Q</annotation></semantics></math>, <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">caligraphic_A</annotation></semantics></math>, and <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">caligraphic_D</annotation></semantics></math> represent the userâ€™s input (Query), the expected response (Answer), and the given data, respectively. The task of the application <math alttext="f" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.1d">italic_f</annotation></semantics></math> is to establish the mapping from <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.1d">caligraphic_Q</annotation></semantics></math> to <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.1d">caligraphic_A</annotation></semantics></math> based on <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p1.7.m7.1"><semantics id="S2.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.7.m7.1d">caligraphic_D</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.3">In contrast to standalone LLM systems that rely solely on pre-existing knowledge, data augmented LLM applications are characterized by their reliance on external data (<math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">caligraphic_D</annotation></semantics></math>) to accurately address the posed queries (<math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">caligraphic_Q</annotation></semantics></math>).
The incorporation of external data <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">caligraphic_D</annotation></semantics></math> can significantly bolster the capabilities of LLMs, granting them the ability to tap into current, domain-specific knowledge and to understand expert rationales. Queries can be stratified into various levels of complexity based on the extent and manner in which they utilize external data, reflecting the depth and nature of engagement required by the queries.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Stratification of Queries</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the landscape of data-augmented LLM applications, queries can be stratified based on their complexity and the depth of data interaction required. This stratification helps in understanding the varying levels of cognitive processing that an LLM must perform to generate accurate and relevant responses. From straightforward fact retrieval to the nuanced interpretation of implicit knowledge, each level represents a step up in the sophistication of the tasks that LLMs are expected to handle. Below, we delineate these levels, providing insights into the unique challenges and capabilities necessitated at each stage.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="346" id="S2.F2.g1" src="extracted/5872412/contents/images/overall_level_examples_fishbone.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Summary of Query Levels in <span class="ltx_ERROR undefined" id="S2.F2.2.1">\MFUsentencecase</span>data augmented LLM applications</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.1.1.1">Level-1</span></span>
<div class="ltx_para ltx_noindent" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Explicit Facts</span>: These queries are asking about <span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.2">explicit facts</span> directly present in the given data without requiring any additional reasoning. This is the simplest form of query, where the modelâ€™s task is primarily to locate and extract the relevant information. For example, <span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.3">"Where will the 2024 Summer Olympics be held?"</span> targets a fact contained in the external data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.1.1.1">Level-2</span></span>
<div class="ltx_para ltx_noindent" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Implicit Facts</span>: These queries ask about <span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.2">implicit facts</span> in the data, which are not immediately obvious and may require some level of common sense reasoning or basic logical deductions. The necessary information might be spread across multiple segments or require simple inferencing. For instance, the question <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.3">"What is the majority party now in the country where Canberra is located?"</span> can be answered by combining the fact that Canberra is in Australia with the information about the current majority party in Australia.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.1.1.1">Level-3</span></span>
<div class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Interpretable Rationales</span>: These queries demand not only a grasp of the factual content but also the capacity to comprehend and apply domain-specific <span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.2">rationales</span> that are integral to the dataâ€™s context. These rationales are often explicitly provided in external resources and is typically not present or rarely encountered during the pre-training phase of a general large language model. For example, in the realm of pharmaceuticals, an LLM must interpret FDA Guidance<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.fda.gov/industry/fda-basics-industry/guidances" title="">https://www.fda.gov/industry/fda-basics-industry/guidances</a></span></span></span> documentsâ€”which represent the FDAâ€™s current thinkingâ€”to evaluate whether a specific drug application adheres to regulatory requirements. Similarly, in customer support scenarios, the LLM must navigate the intricacies of a predefined workflow to process user inquiries effectively. In the medical field, many diagnostic manuals provide authoritative and standardized diagnostic criteria, such as management guidelines for patients with acute chest painÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib14" title="">14</a>]</cite>. By effectively following these external rationales, it is possible to develop a specialized LLM expert system for managing chest pain. This involves understanding the procedural steps and decision trees that guide a support agentâ€™s interactions with customers, ensuring responses are not only accurate but also comply with the companyâ€™s service standards and protocols.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.1.1.1">Level-4</span></span>
<div class="ltx_para ltx_noindent" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Hidden Rationales</span>: This category of queries delves into the more challenging realm where the rationales are not explicitly documented but must be inferred from patterns and outcomes observed in external data. The hidden rationales here refer not only to the implicit reasoning chains and logical relationships, but also to the inherently challenging and non-trivial task of identifying and extracting the external rationales required for each specific query. In IT operational scenarios, for example, a cloud operations team may have addressed numerous incidents in the past, each with its own unique set of circumstances and resolutions. The LLM must be adept at mining this rich repository of tacit knowledge to discern the implicit strategies and decision-making processes that were successful. Similarly, in software development, the debugging history of previous bugs can provide a wealth of implicit insights. While the step-by-step rationale for each debugging decision may not be systematically recorded, the LLM must be capable of extracting the underlying principles that guided those decisions. By synthesizing these hidden rationales, the LLM can generate responses that are not only accurate but also reflective of the unspoken expertise and problem-solving approaches that have been honed over time by experienced professionals.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In summary, the classification of queries into levels reflects a gradient of complexity and the type of understanding required from the LLM. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">1</span></a> and exampled by FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S2.F2" title="Figure 2 â€£ 2.1 Stratification of Queries â€£ 2 Problem Definition â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">2</span></a>, the first two levels, <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Explicit Facts</span> and <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.2">Implicit Facts</span>, focus on the retrieval of factual information, whether directly stated or requiring basic inferencing. These levels challenge the LLMâ€™s ability to extract and synthesize data into coherent facts.
Conversely, the latter two levels, <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.3">Interpretable Rationales</span> and <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.4">Hidden Rationales</span>, shift the focus towards the LLMâ€™s capacity to learn and apply the rationales behind the data. These levels demand a deeper cognitive engagement, where the LLM must align with expert thinking or extract wisdom from unstructured historical data, respectively. The classification of common factual querying datasets according to this standard is depicted in Table Â <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S2.T1" title="Table 1 â€£ 2.1 Stratification of Queries â€£ 2 Problem Definition â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.1.1.1.1">Task Categorization</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.1.1.1.2">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span> Datasets</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.3">levels</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.4">Mutiple References</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.1" rowspan="17"><span class="ltx_text" id="S2.T1.1.2.1.1.1">QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.2">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span> NQ(Natural Questions)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib15" title="">15</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.3.2.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>MS MARCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib16" title="">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.2">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.4.3.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>TriviaQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib17" title="">17</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.2">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.3">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.5.4.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>SQuADÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib18" title="">18</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.2">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.3">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.6.5.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>ASQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib19" title="">19</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.2">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.3">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.7.6.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>WebQSPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib20" title="">20</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.2">1 - Explicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.3">False</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.8.7.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>HotPotQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib21" title="">21</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.7.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.7.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.9.8.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>2WikiMultiHopQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib22" title="">22</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.10.9.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>MuSiQue<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib23" title="">23</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.11.10.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>Bamboogle<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib24" title="">24</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.10.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.10.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.12.11.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>StrategyQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib25" title="">25</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.13.12.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>ComplexWebQuestionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib26" title="">26</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.12.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.12.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.14.13.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>WebQuestionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib27" title="">27</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.13.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.13.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.15.14.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>MintakaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib28" title="">28</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.14.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.14.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.16.15.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>MetaQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib29" title="">29</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.15.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.15.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.17.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.17.16.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>qasperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.16.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.16.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.18.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.18.17.1">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>DROPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib31" title="">31</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.17.2">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.17.3">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.19.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.19.18.1">Multi-Choice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S2.T1.1.19.18.2">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>QuALITYÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib32" title="">32</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.18.3">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.18.4">True</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.20.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S2.T1.1.20.19.1">Fact Checking</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S2.T1.1.20.19.2">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span> Feverous<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib33" title="">33</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.20.19.3">2 - Implicit Facts</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.20.19.4">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Stratification of Common Datasets Providing Facts</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Each level presents its unique set of challenges and, consequently, necessitates tailored solutions to effectively address them. As we delve into the intricacies of these levels in the following sections, we will explore the specific strategies and methodologies that enable LLMs to navigate the complexities of data-augmented applications across these varied spectrums of query types. This exploration will not only highlight the current capabilities of LLMs but also shed light on the ongoing advancements and potential future developments in the field.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Explicit Fact Queries (L1)</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<span class="ltx_ERROR undefined" id="S3.SS1.p1.1">\MFUsentencecase</span>
<p class="ltx_p" id="S3.SS1.p1.2">explicit fact queries, represent the most straightforward type of data-augmented queries. Queries at this level can be answered by directly accessing specific domain documents or document snippets within the collection. The answers to these questions are often in plain text within the documents, requiring minimal reasoning or simple rationale in the response generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The defining characteristic of this level is the clear and direct dependency on specific pieces of external data.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Data Dependency</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.2">The dataset <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">caligraphic_D</annotation></semantics></math> can be segmented into documents or segments, denoted as <math alttext="D_{1},D_{2},\ldots,D_{n}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.4"><semantics id="S3.SS1.SSS1.p1.2.m2.4a"><mrow id="S3.SS1.SSS1.p1.2.m2.4.4.3" xref="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml"><msub id="S3.SS1.SSS1.p1.2.m2.2.2.1.1" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p1.2.m2.4.4.3.4" xref="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS1.p1.2.m2.3.3.2.2" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2.2.cmml">D</mi><mn id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.p1.2.m2.4.4.3.5" xref="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml">,</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS1.SSS1.p1.2.m2.4.4.3.6" xref="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS1.p1.2.m2.4.4.3.3" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.2" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3.2.cmml">D</mi><mi id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.4b"><list id="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3"><apply id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1.2">ğ·</ci><cn id="S3.SS1.SSS1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2.2">ğ·</ci><cn id="S3.SS1.SSS1.p1.2.m2.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">â€¦</ci><apply id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3.2">ğ·</ci><ci id="S3.SS1.SSS1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.3.3">ğ‘›</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.4c">D_{1},D_{2},\ldots,D_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.4d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, in various ways:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}=\left\{D_{1},D_{2},\ldots,D_{n}\right\}" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.4.5.cmml">ğ’Ÿ</mi><mo id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml">=</mo><mrow id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.4.cmml"><mo id="S3.E2.m1.4.4.3.3.4" xref="S3.E2.m1.4.4.3.4.cmml">{</mo><msub id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">D</mi><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.E2.m1.4.4.3.3.5" xref="S3.E2.m1.4.4.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.2.2.cmml"><mi id="S3.E2.m1.3.3.2.2.2.2" xref="S3.E2.m1.3.3.2.2.2.2.cmml">D</mi><mn id="S3.E2.m1.3.3.2.2.2.3" xref="S3.E2.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.E2.m1.4.4.3.3.6" xref="S3.E2.m1.4.4.3.4.cmml">,</mo><mi id="S3.E2.m1.1.1" mathvariant="normal" xref="S3.E2.m1.1.1.cmml">â€¦</mi><mo id="S3.E2.m1.4.4.3.3.7" xref="S3.E2.m1.4.4.3.4.cmml">,</mo><msub id="S3.E2.m1.4.4.3.3.3" xref="S3.E2.m1.4.4.3.3.3.cmml"><mi id="S3.E2.m1.4.4.3.3.3.2" xref="S3.E2.m1.4.4.3.3.3.2.cmml">D</mi><mi id="S3.E2.m1.4.4.3.3.3.3" xref="S3.E2.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S3.E2.m1.4.4.3.3.8" xref="S3.E2.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"></eq><ci id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4.5">ğ’Ÿ</ci><set id="S3.E2.m1.4.4.3.4.cmml" xref="S3.E2.m1.4.4.3.3"><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2">ğ·</ci><cn id="S3.E2.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.E2.m1.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2.2">ğ·</ci><cn id="S3.E2.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.E2.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">â€¦</ci><apply id="S3.E2.m1.4.4.3.3.3.cmml" xref="S3.E2.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.3.3.1.cmml" xref="S3.E2.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.3.3.3.2.cmml" xref="S3.E2.m1.4.4.3.3.3.2">ğ·</ci><ci id="S3.E2.m1.4.4.3.3.3.3.cmml" xref="S3.E2.m1.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathcal{D}=\left\{D_{1},D_{2},\ldots,D_{n}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">caligraphic_D = { italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS1.p1.3">Each segment <math alttext="D_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.3.m1.1"><semantics id="S3.SS1.SSS1.p1.3.m1.1a"><msub id="S3.SS1.SSS1.p1.3.m1.1.1" xref="S3.SS1.SSS1.p1.3.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m1.1.1.2" xref="S3.SS1.SSS1.p1.3.m1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS1.p1.3.m1.1.1.3" xref="S3.SS1.SSS1.p1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m1.1b"><apply id="S3.SS1.SSS1.p1.3.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m1.1.1.2">ğ·</ci><ci id="S3.SS1.SSS1.p1.3.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m1.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.3.m1.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is considered relatively short and contains content that is more focused and specific<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In some most recent advancements, the segment size may be as large as a whole document or even larger</span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.11">For a given query <math alttext="q\in\mathcal{Q}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.1"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mrow id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml">q</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">ğ’¬</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><in id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1"></in><ci id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">ğ’¬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">q\in\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">italic_q âˆˆ caligraphic_Q</annotation></semantics></math>, not every segment within <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.1"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">caligraphic_D</annotation></semantics></math> is requisite for formulating a response. Let <math alttext="\delta:\mathcal{Q}\times\mathcal{D}\rightarrow\{0,1\}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.3.m3.2"><semantics id="S3.SS1.SSS1.p2.3.m3.2a"><mrow id="S3.SS1.SSS1.p2.3.m3.2.3" xref="S3.SS1.SSS1.p2.3.m3.2.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.2.3.2" xref="S3.SS1.SSS1.p2.3.m3.2.3.2.cmml">Î´</mi><mo id="S3.SS1.SSS1.p2.3.m3.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS1.SSS1.p2.3.m3.2.3.1.cmml">:</mo><mrow id="S3.SS1.SSS1.p2.3.m3.2.3.3" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.cmml"><mrow id="S3.SS1.SSS1.p2.3.m3.2.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.2" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.2.cmml">ğ’¬</mi><mo id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.1.cmml">Ã—</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.3" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.3.cmml">ğ’Ÿ</mi></mrow><mo id="S3.SS1.SSS1.p2.3.m3.2.3.3.1" stretchy="false" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.1.cmml">â†’</mo><mrow id="S3.SS1.SSS1.p2.3.m3.2.3.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.3.1.cmml"><mo id="S3.SS1.SSS1.p2.3.m3.2.3.3.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.3.1.cmml">{</mo><mn id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">0</mn><mo id="S3.SS1.SSS1.p2.3.m3.2.3.3.3.2.2" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.3.1.cmml">,</mo><mn id="S3.SS1.SSS1.p2.3.m3.2.2" xref="S3.SS1.SSS1.p2.3.m3.2.2.cmml">1</mn><mo id="S3.SS1.SSS1.p2.3.m3.2.3.3.3.2.3" stretchy="false" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.2b"><apply id="S3.SS1.SSS1.p2.3.m3.2.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3"><ci id="S3.SS1.SSS1.p2.3.m3.2.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.1">:</ci><ci id="S3.SS1.SSS1.p2.3.m3.2.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.2">ğ›¿</ci><apply id="S3.SS1.SSS1.p2.3.m3.2.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3"><ci id="S3.SS1.SSS1.p2.3.m3.2.3.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.1">â†’</ci><apply id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2"><times id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.1"></times><ci id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.2">ğ’¬</ci><ci id="S3.SS1.SSS1.p2.3.m3.2.3.3.2.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.2.3">ğ’Ÿ</ci></apply><set id="S3.SS1.SSS1.p2.3.m3.2.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.3.3.3.2"><cn id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS1.SSS1.p2.3.m3.1.1">0</cn><cn id="S3.SS1.SSS1.p2.3.m3.2.2.cmml" type="integer" xref="S3.SS1.SSS1.p2.3.m3.2.2">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.2c">\delta:\mathcal{Q}\times\mathcal{D}\rightarrow\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.2d">italic_Î´ : caligraphic_Q Ã— caligraphic_D â†’ { 0 , 1 }</annotation></semantics></math> denote the necessity of data segment <math alttext="d\in\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.4.m4.1"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mrow id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p2.4.m4.1.1.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml">d</mi><mo id="S3.SS1.SSS1.p2.4.m4.1.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.4.m4.1.1.3" xref="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml">ğ’Ÿ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><apply id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1"><in id="S3.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1"></in><ci id="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.2">ğ‘‘</ci><ci id="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.3">ğ’Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">d\in\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">italic_d âˆˆ caligraphic_D</annotation></semantics></math> for a specific query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.5.m5.1"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><mi id="S3.SS1.SSS1.p2.5.m5.1.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m5.1b"><ci id="S3.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.5.m5.1d">italic_q</annotation></semantics></math>, where <math alttext="\delta(q,d)=1" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.6.m6.2"><semantics id="S3.SS1.SSS1.p2.6.m6.2a"><mrow id="S3.SS1.SSS1.p2.6.m6.2.3" xref="S3.SS1.SSS1.p2.6.m6.2.3.cmml"><mrow id="S3.SS1.SSS1.p2.6.m6.2.3.2" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.cmml"><mi id="S3.SS1.SSS1.p2.6.m6.2.3.2.2" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.2.cmml">Î´</mi><mo id="S3.SS1.SSS1.p2.6.m6.2.3.2.1" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.1.cmml">â¢</mo><mrow id="S3.SS1.SSS1.p2.6.m6.2.3.2.3.2" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.3.1.cmml"><mo id="S3.SS1.SSS1.p2.6.m6.2.3.2.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.3.1.cmml">(</mo><mi id="S3.SS1.SSS1.p2.6.m6.1.1" xref="S3.SS1.SSS1.p2.6.m6.1.1.cmml">q</mi><mo id="S3.SS1.SSS1.p2.6.m6.2.3.2.3.2.2" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.p2.6.m6.2.2" xref="S3.SS1.SSS1.p2.6.m6.2.2.cmml">d</mi><mo id="S3.SS1.SSS1.p2.6.m6.2.3.2.3.2.3" stretchy="false" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS1.p2.6.m6.2.3.1" xref="S3.SS1.SSS1.p2.6.m6.2.3.1.cmml">=</mo><mn id="S3.SS1.SSS1.p2.6.m6.2.3.3" xref="S3.SS1.SSS1.p2.6.m6.2.3.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.6.m6.2b"><apply id="S3.SS1.SSS1.p2.6.m6.2.3.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3"><eq id="S3.SS1.SSS1.p2.6.m6.2.3.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3.1"></eq><apply id="S3.SS1.SSS1.p2.6.m6.2.3.2.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3.2"><times id="S3.SS1.SSS1.p2.6.m6.2.3.2.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.1"></times><ci id="S3.SS1.SSS1.p2.6.m6.2.3.2.2.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.2">ğ›¿</ci><interval closure="open" id="S3.SS1.SSS1.p2.6.m6.2.3.2.3.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.3.2.3.2"><ci id="S3.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1">ğ‘</ci><ci id="S3.SS1.SSS1.p2.6.m6.2.2.cmml" xref="S3.SS1.SSS1.p2.6.m6.2.2">ğ‘‘</ci></interval></apply><cn id="S3.SS1.SSS1.p2.6.m6.2.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.6.m6.2.3.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.6.m6.2c">\delta(q,d)=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.6.m6.2d">italic_Î´ ( italic_q , italic_d ) = 1</annotation></semantics></math> means that data segment <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.7.m7.1"><semantics id="S3.SS1.SSS1.p2.7.m7.1a"><mi id="S3.SS1.SSS1.p2.7.m7.1.1" xref="S3.SS1.SSS1.p2.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.7.m7.1b"><ci id="S3.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.7.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.7.m7.1d">italic_d</annotation></semantics></math> is required to answer the query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.8.m8.1"><semantics id="S3.SS1.SSS1.p2.8.m8.1a"><mi id="S3.SS1.SSS1.p2.8.m8.1.1" xref="S3.SS1.SSS1.p2.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.8.m8.1b"><ci id="S3.SS1.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.8.m8.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.8.m8.1d">italic_q</annotation></semantics></math>, and <math alttext="\delta(q,d)=0" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.9.m9.2"><semantics id="S3.SS1.SSS1.p2.9.m9.2a"><mrow id="S3.SS1.SSS1.p2.9.m9.2.3" xref="S3.SS1.SSS1.p2.9.m9.2.3.cmml"><mrow id="S3.SS1.SSS1.p2.9.m9.2.3.2" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.cmml"><mi id="S3.SS1.SSS1.p2.9.m9.2.3.2.2" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.2.cmml">Î´</mi><mo id="S3.SS1.SSS1.p2.9.m9.2.3.2.1" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.1.cmml">â¢</mo><mrow id="S3.SS1.SSS1.p2.9.m9.2.3.2.3.2" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.3.1.cmml"><mo id="S3.SS1.SSS1.p2.9.m9.2.3.2.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.3.1.cmml">(</mo><mi id="S3.SS1.SSS1.p2.9.m9.1.1" xref="S3.SS1.SSS1.p2.9.m9.1.1.cmml">q</mi><mo id="S3.SS1.SSS1.p2.9.m9.2.3.2.3.2.2" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.p2.9.m9.2.2" xref="S3.SS1.SSS1.p2.9.m9.2.2.cmml">d</mi><mo id="S3.SS1.SSS1.p2.9.m9.2.3.2.3.2.3" stretchy="false" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS1.p2.9.m9.2.3.1" xref="S3.SS1.SSS1.p2.9.m9.2.3.1.cmml">=</mo><mn id="S3.SS1.SSS1.p2.9.m9.2.3.3" xref="S3.SS1.SSS1.p2.9.m9.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.9.m9.2b"><apply id="S3.SS1.SSS1.p2.9.m9.2.3.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3"><eq id="S3.SS1.SSS1.p2.9.m9.2.3.1.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3.1"></eq><apply id="S3.SS1.SSS1.p2.9.m9.2.3.2.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3.2"><times id="S3.SS1.SSS1.p2.9.m9.2.3.2.1.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.1"></times><ci id="S3.SS1.SSS1.p2.9.m9.2.3.2.2.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.2">ğ›¿</ci><interval closure="open" id="S3.SS1.SSS1.p2.9.m9.2.3.2.3.1.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.3.2.3.2"><ci id="S3.SS1.SSS1.p2.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p2.9.m9.1.1">ğ‘</ci><ci id="S3.SS1.SSS1.p2.9.m9.2.2.cmml" xref="S3.SS1.SSS1.p2.9.m9.2.2">ğ‘‘</ci></interval></apply><cn id="S3.SS1.SSS1.p2.9.m9.2.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.9.m9.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.9.m9.2c">\delta(q,d)=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.9.m9.2d">italic_Î´ ( italic_q , italic_d ) = 0</annotation></semantics></math> otherwise. Then the data dependency of query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.10.m10.1"><semantics id="S3.SS1.SSS1.p2.10.m10.1a"><mi id="S3.SS1.SSS1.p2.10.m10.1.1" xref="S3.SS1.SSS1.p2.10.m10.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.10.m10.1b"><ci id="S3.SS1.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.10.m10.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.10.m10.1d">italic_q</annotation></semantics></math>, characterized by the subset of segments indispensable for addressing query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.11.m11.1"><semantics id="S3.SS1.SSS1.p2.11.m11.1a"><mi id="S3.SS1.SSS1.p2.11.m11.1.1" xref="S3.SS1.SSS1.p2.11.m11.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.11.m11.1b"><ci id="S3.SS1.SSS1.p2.11.m11.1.1.cmml" xref="S3.SS1.SSS1.p2.11.m11.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.11.m11.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.11.m11.1d">italic_q</annotation></semantics></math>, is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Dep(q)=\{d\mid d\in\mathcal{D}\text{ and }\delta(q,d)=1\}" class="ltx_Math" display="block" id="S3.E3.m1.5"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><mrow id="S3.E3.m1.5.5.3" xref="S3.E3.m1.5.5.3.cmml"><mi id="S3.E3.m1.5.5.3.2" xref="S3.E3.m1.5.5.3.2.cmml">D</mi><mo id="S3.E3.m1.5.5.3.1" xref="S3.E3.m1.5.5.3.1.cmml">â¢</mo><mi id="S3.E3.m1.5.5.3.3" xref="S3.E3.m1.5.5.3.3.cmml">e</mi><mo id="S3.E3.m1.5.5.3.1a" xref="S3.E3.m1.5.5.3.1.cmml">â¢</mo><mi id="S3.E3.m1.5.5.3.4" xref="S3.E3.m1.5.5.3.4.cmml">p</mi><mo id="S3.E3.m1.5.5.3.1b" xref="S3.E3.m1.5.5.3.1.cmml">â¢</mo><mrow id="S3.E3.m1.5.5.3.5.2" xref="S3.E3.m1.5.5.3.cmml"><mo id="S3.E3.m1.5.5.3.5.2.1" stretchy="false" xref="S3.E3.m1.5.5.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">q</mi><mo id="S3.E3.m1.5.5.3.5.2.2" stretchy="false" xref="S3.E3.m1.5.5.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.5.5.2" xref="S3.E3.m1.5.5.2.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.2.cmml"><mo id="S3.E3.m1.5.5.1.1.2" stretchy="false" xref="S3.E3.m1.5.5.1.2.1.cmml">{</mo><mi id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml">d</mi><mo fence="true" id="S3.E3.m1.5.5.1.1.3" lspace="0em" rspace="0em" xref="S3.E3.m1.5.5.1.2.1.cmml">âˆ£</mo><mrow id="S3.E3.m1.5.5.1.1.1" xref="S3.E3.m1.5.5.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.2.cmml">d</mi><mo id="S3.E3.m1.5.5.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.3.cmml">âˆˆ</mo><mrow id="S3.E3.m1.5.5.1.1.1.4" xref="S3.E3.m1.5.5.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.5.5.1.1.1.4.2" xref="S3.E3.m1.5.5.1.1.1.4.2.cmml">ğ’Ÿ</mi><mo id="S3.E3.m1.5.5.1.1.1.4.1" xref="S3.E3.m1.5.5.1.1.1.4.1.cmml">â¢</mo><mtext id="S3.E3.m1.5.5.1.1.1.4.3" xref="S3.E3.m1.5.5.1.1.1.4.3a.cmml">Â andÂ </mtext><mo id="S3.E3.m1.5.5.1.1.1.4.1a" xref="S3.E3.m1.5.5.1.1.1.4.1.cmml">â¢</mo><mi id="S3.E3.m1.5.5.1.1.1.4.4" xref="S3.E3.m1.5.5.1.1.1.4.4.cmml">Î´</mi><mo id="S3.E3.m1.5.5.1.1.1.4.1b" xref="S3.E3.m1.5.5.1.1.1.4.1.cmml">â¢</mo><mrow id="S3.E3.m1.5.5.1.1.1.4.5.2" xref="S3.E3.m1.5.5.1.1.1.4.5.1.cmml"><mo id="S3.E3.m1.5.5.1.1.1.4.5.2.1" stretchy="false" xref="S3.E3.m1.5.5.1.1.1.4.5.1.cmml">(</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">q</mi><mo id="S3.E3.m1.5.5.1.1.1.4.5.2.2" xref="S3.E3.m1.5.5.1.1.1.4.5.1.cmml">,</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">d</mi><mo id="S3.E3.m1.5.5.1.1.1.4.5.2.3" stretchy="false" xref="S3.E3.m1.5.5.1.1.1.4.5.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.5.5.1.1.1.5" xref="S3.E3.m1.5.5.1.1.1.5.cmml">=</mo><mn id="S3.E3.m1.5.5.1.1.1.6" xref="S3.E3.m1.5.5.1.1.1.6.cmml">1</mn></mrow><mo id="S3.E3.m1.5.5.1.1.4" stretchy="false" xref="S3.E3.m1.5.5.1.2.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><eq id="S3.E3.m1.5.5.2.cmml" xref="S3.E3.m1.5.5.2"></eq><apply id="S3.E3.m1.5.5.3.cmml" xref="S3.E3.m1.5.5.3"><times id="S3.E3.m1.5.5.3.1.cmml" xref="S3.E3.m1.5.5.3.1"></times><ci id="S3.E3.m1.5.5.3.2.cmml" xref="S3.E3.m1.5.5.3.2">ğ·</ci><ci id="S3.E3.m1.5.5.3.3.cmml" xref="S3.E3.m1.5.5.3.3">ğ‘’</ci><ci id="S3.E3.m1.5.5.3.4.cmml" xref="S3.E3.m1.5.5.3.4">ğ‘</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘</ci></apply><apply id="S3.E3.m1.5.5.1.2.cmml" xref="S3.E3.m1.5.5.1.1"><csymbol cd="latexml" id="S3.E3.m1.5.5.1.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2">conditional-set</csymbol><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">ğ‘‘</ci><apply id="S3.E3.m1.5.5.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1"><and id="S3.E3.m1.5.5.1.1.1a.cmml" xref="S3.E3.m1.5.5.1.1.1"></and><apply id="S3.E3.m1.5.5.1.1.1b.cmml" xref="S3.E3.m1.5.5.1.1.1"><in id="S3.E3.m1.5.5.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.3"></in><ci id="S3.E3.m1.5.5.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.2">ğ‘‘</ci><apply id="S3.E3.m1.5.5.1.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.1.4"><times id="S3.E3.m1.5.5.1.1.1.4.1.cmml" xref="S3.E3.m1.5.5.1.1.1.4.1"></times><ci id="S3.E3.m1.5.5.1.1.1.4.2.cmml" xref="S3.E3.m1.5.5.1.1.1.4.2">ğ’Ÿ</ci><ci id="S3.E3.m1.5.5.1.1.1.4.3a.cmml" xref="S3.E3.m1.5.5.1.1.1.4.3"><mtext id="S3.E3.m1.5.5.1.1.1.4.3.cmml" xref="S3.E3.m1.5.5.1.1.1.4.3">Â andÂ </mtext></ci><ci id="S3.E3.m1.5.5.1.1.1.4.4.cmml" xref="S3.E3.m1.5.5.1.1.1.4.4">ğ›¿</ci><interval closure="open" id="S3.E3.m1.5.5.1.1.1.4.5.1.cmml" xref="S3.E3.m1.5.5.1.1.1.4.5.2"><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ‘</ci><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">ğ‘‘</ci></interval></apply></apply><apply id="S3.E3.m1.5.5.1.1.1c.cmml" xref="S3.E3.m1.5.5.1.1.1"><eq id="S3.E3.m1.5.5.1.1.1.5.cmml" xref="S3.E3.m1.5.5.1.1.1.5"></eq><share href="https://arxiv.org/html/2409.14924v1#S3.E3.m1.5.5.1.1.1.4.cmml" id="S3.E3.m1.5.5.1.1.1d.cmml" xref="S3.E3.m1.5.5.1.1.1"></share><cn id="S3.E3.m1.5.5.1.1.1.6.cmml" type="integer" xref="S3.E3.m1.5.5.1.1.1.6">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">Dep(q)=\{d\mid d\in\mathcal{D}\text{ and }\delta(q,d)=1\}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.5d">italic_D italic_e italic_p ( italic_q ) = { italic_d âˆ£ italic_d âˆˆ caligraphic_D and italic_Î´ ( italic_q , italic_d ) = 1 }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS1.p2.14">Itâ€™s easy to understand that <math alttext="Dep(q)\in\mathcal{P}(\mathcal{D})" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.12.m1.2"><semantics id="S3.SS1.SSS1.p2.12.m1.2a"><mrow id="S3.SS1.SSS1.p2.12.m1.2.3" xref="S3.SS1.SSS1.p2.12.m1.2.3.cmml"><mrow id="S3.SS1.SSS1.p2.12.m1.2.3.2" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.cmml"><mi id="S3.SS1.SSS1.p2.12.m1.2.3.2.2" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.2.cmml">D</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.2.1" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p2.12.m1.2.3.2.3" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.3.cmml">e</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.2.1a" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.1.cmml">â¢</mo><mi id="S3.SS1.SSS1.p2.12.m1.2.3.2.4" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.4.cmml">p</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.2.1b" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.1.cmml">â¢</mo><mrow id="S3.SS1.SSS1.p2.12.m1.2.3.2.5.2" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.cmml"><mo id="S3.SS1.SSS1.p2.12.m1.2.3.2.5.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.cmml">(</mo><mi id="S3.SS1.SSS1.p2.12.m1.1.1" xref="S3.SS1.SSS1.p2.12.m1.1.1.cmml">q</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.2.5.2.2" stretchy="false" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS1.p2.12.m1.2.3.1" xref="S3.SS1.SSS1.p2.12.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS1.SSS1.p2.12.m1.2.3.3" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.12.m1.2.3.3.2" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.2.cmml">ğ’«</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.3.1" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.1.cmml">â¢</mo><mrow id="S3.SS1.SSS1.p2.12.m1.2.3.3.3.2" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.cmml"><mo id="S3.SS1.SSS1.p2.12.m1.2.3.3.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.12.m1.2.2" xref="S3.SS1.SSS1.p2.12.m1.2.2.cmml">ğ’Ÿ</mi><mo id="S3.SS1.SSS1.p2.12.m1.2.3.3.3.2.2" stretchy="false" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.12.m1.2b"><apply id="S3.SS1.SSS1.p2.12.m1.2.3.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3"><in id="S3.SS1.SSS1.p2.12.m1.2.3.1.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.1"></in><apply id="S3.SS1.SSS1.p2.12.m1.2.3.2.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.2"><times id="S3.SS1.SSS1.p2.12.m1.2.3.2.1.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.1"></times><ci id="S3.SS1.SSS1.p2.12.m1.2.3.2.2.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.2">ğ·</ci><ci id="S3.SS1.SSS1.p2.12.m1.2.3.2.3.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.3">ğ‘’</ci><ci id="S3.SS1.SSS1.p2.12.m1.2.3.2.4.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.2.4">ğ‘</ci><ci id="S3.SS1.SSS1.p2.12.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.12.m1.1.1">ğ‘</ci></apply><apply id="S3.SS1.SSS1.p2.12.m1.2.3.3.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.3"><times id="S3.SS1.SSS1.p2.12.m1.2.3.3.1.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.1"></times><ci id="S3.SS1.SSS1.p2.12.m1.2.3.3.2.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.3.3.2">ğ’«</ci><ci id="S3.SS1.SSS1.p2.12.m1.2.2.cmml" xref="S3.SS1.SSS1.p2.12.m1.2.2">ğ’Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.12.m1.2c">Dep(q)\in\mathcal{P}(\mathcal{D})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.12.m1.2d">italic_D italic_e italic_p ( italic_q ) âˆˆ caligraphic_P ( caligraphic_D )</annotation></semantics></math>, where <math alttext="\mathcal{P}(\mathcal{D})" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.13.m2.1"><semantics id="S3.SS1.SSS1.p2.13.m2.1a"><mrow id="S3.SS1.SSS1.p2.13.m2.1.2" xref="S3.SS1.SSS1.p2.13.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.13.m2.1.2.2" xref="S3.SS1.SSS1.p2.13.m2.1.2.2.cmml">ğ’«</mi><mo id="S3.SS1.SSS1.p2.13.m2.1.2.1" xref="S3.SS1.SSS1.p2.13.m2.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.SSS1.p2.13.m2.1.2.3.2" xref="S3.SS1.SSS1.p2.13.m2.1.2.cmml"><mo id="S3.SS1.SSS1.p2.13.m2.1.2.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p2.13.m2.1.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.13.m2.1.1" xref="S3.SS1.SSS1.p2.13.m2.1.1.cmml">ğ’Ÿ</mi><mo id="S3.SS1.SSS1.p2.13.m2.1.2.3.2.2" stretchy="false" xref="S3.SS1.SSS1.p2.13.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.13.m2.1b"><apply id="S3.SS1.SSS1.p2.13.m2.1.2.cmml" xref="S3.SS1.SSS1.p2.13.m2.1.2"><times id="S3.SS1.SSS1.p2.13.m2.1.2.1.cmml" xref="S3.SS1.SSS1.p2.13.m2.1.2.1"></times><ci id="S3.SS1.SSS1.p2.13.m2.1.2.2.cmml" xref="S3.SS1.SSS1.p2.13.m2.1.2.2">ğ’«</ci><ci id="S3.SS1.SSS1.p2.13.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.13.m2.1.1">ğ’Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.13.m2.1c">\mathcal{P}(\mathcal{D})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.13.m2.1d">caligraphic_P ( caligraphic_D )</annotation></semantics></math> is the power set<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The power set (or powerset) of a set <math alttext="S" class="ltx_Math" display="inline" id="footnote3.m1.1"><semantics id="footnote3.m1.1b"><mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">S</annotation><annotation encoding="application/x-llamapun" id="footnote3.m1.1e">italic_S</annotation></semantics></math> is the set of all subsets of <math alttext="S" class="ltx_Math" display="inline" id="footnote3.m2.1"><semantics id="footnote3.m2.1b"><mi id="footnote3.m2.1.1" xref="footnote3.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="footnote3.m2.1c"><ci id="footnote3.m2.1.1.cmml" xref="footnote3.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m2.1d">S</annotation><annotation encoding="application/x-llamapun" id="footnote3.m2.1e">italic_S</annotation></semantics></math>, including the empty set and <math alttext="S" class="ltx_Math" display="inline" id="footnote3.m3.1"><semantics id="footnote3.m3.1b"><mi id="footnote3.m3.1.1" xref="footnote3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="footnote3.m3.1c"><ci id="footnote3.m3.1.1.cmml" xref="footnote3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m3.1d">S</annotation><annotation encoding="application/x-llamapun" id="footnote3.m3.1e">italic_S</annotation></semantics></math> itself.</span></span></span> of <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.14.m3.1"><semantics id="S3.SS1.SSS1.p2.14.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.14.m3.1.1" xref="S3.SS1.SSS1.p2.14.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.14.m3.1b"><ci id="S3.SS1.SSS1.p2.14.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.14.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.14.m3.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.14.m3.1d">caligraphic_D</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Definition</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p1">
<span class="ltx_ERROR undefined" id="S3.SS1.SSS2.p1.3">\MFUsentencecase</span>
<p class="ltx_p" id="S3.SS1.SSS2.p1.2">explicit fact queries, denoted as <math alttext="\mathcal{Q}_{1}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">ğ’¬</mi><mn id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">ğ’¬</ci><cn id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">\mathcal{Q}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.1d">caligraphic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, are characterized by the direct retrievability of answers from specific data segments within the dataset <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.2.m2.1"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.2.m2.1d">caligraphic_D</annotation></semantics></math>. These queries can be formally defined in the context of a data-augmented LLM system as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.2">For any query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.1.m1.1"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mi id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><ci id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.1.m1.1d">italic_q</annotation></semantics></math> and its corresponding answer <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.2.m2.1"><semantics id="S3.SS1.SSS2.p2.2.m2.1a"><mi id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.1b"><ci id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.2.m2.1d">italic_a</annotation></semantics></math>, an explicit fact query is one where there exists:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.7">A retrieval component <math alttext="r_{\mathcal{D}}:\mathcal{Q}\rightarrow\mathcal{P}(\mathcal{D})" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.2" xref="S3.I1.i1.p1.1.m1.1.2.cmml"><msub id="S3.I1.i1.p1.1.m1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.2.2.cmml"><mi id="S3.I1.i1.p1.1.m1.1.2.2.2" xref="S3.I1.i1.p1.1.m1.1.2.2.2.cmml">r</mi><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.2.2.3" xref="S3.I1.i1.p1.1.m1.1.2.2.3.cmml">ğ’Ÿ</mi></msub><mo id="S3.I1.i1.p1.1.m1.1.2.1" lspace="0.278em" rspace="0.278em" xref="S3.I1.i1.p1.1.m1.1.2.1.cmml">:</mo><mrow id="S3.I1.i1.p1.1.m1.1.2.3" xref="S3.I1.i1.p1.1.m1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.2.3.2" xref="S3.I1.i1.p1.1.m1.1.2.3.2.cmml">ğ’¬</mi><mo id="S3.I1.i1.p1.1.m1.1.2.3.1" stretchy="false" xref="S3.I1.i1.p1.1.m1.1.2.3.1.cmml">â†’</mo><mrow id="S3.I1.i1.p1.1.m1.1.2.3.3" xref="S3.I1.i1.p1.1.m1.1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.2.3.3.2" xref="S3.I1.i1.p1.1.m1.1.2.3.3.2.cmml">ğ’«</mi><mo id="S3.I1.i1.p1.1.m1.1.2.3.3.1" xref="S3.I1.i1.p1.1.m1.1.2.3.3.1.cmml">â¢</mo><mrow id="S3.I1.i1.p1.1.m1.1.2.3.3.3.2" xref="S3.I1.i1.p1.1.m1.1.2.3.3.cmml"><mo id="S3.I1.i1.p1.1.m1.1.2.3.3.3.2.1" stretchy="false" xref="S3.I1.i1.p1.1.m1.1.2.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">ğ’Ÿ</mi><mo id="S3.I1.i1.p1.1.m1.1.2.3.3.3.2.2" stretchy="false" xref="S3.I1.i1.p1.1.m1.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2"><ci id="S3.I1.i1.p1.1.m1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.1">:</ci><apply id="S3.I1.i1.p1.1.m1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.2.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2.2">ğ‘Ÿ</ci><ci id="S3.I1.i1.p1.1.m1.1.2.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2.3">ğ’Ÿ</ci></apply><apply id="S3.I1.i1.p1.1.m1.1.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3"><ci id="S3.I1.i1.p1.1.m1.1.2.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3.1">â†’</ci><ci id="S3.I1.i1.p1.1.m1.1.2.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3.2">ğ’¬</ci><apply id="S3.I1.i1.p1.1.m1.1.2.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3.3"><times id="S3.I1.i1.p1.1.m1.1.2.3.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3.3.1"></times><ci id="S3.I1.i1.p1.1.m1.1.2.3.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.3.3.2">ğ’«</ci><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">ğ’Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">r_{\mathcal{D}}:\mathcal{Q}\rightarrow\mathcal{P}(\mathcal{D})</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_r start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT : caligraphic_Q â†’ caligraphic_P ( caligraphic_D )</annotation></semantics></math> that identifies the relevant data segments from <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">caligraphic_D</annotation></semantics></math> necessary to answer <math alttext="q" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">italic_q</annotation></semantics></math>. This component ensures that <math alttext="r_{\mathcal{D}}(q)" class="ltx_Math" display="inline" id="S3.I1.i1.p1.4.m4.1"><semantics id="S3.I1.i1.p1.4.m4.1a"><mrow id="S3.I1.i1.p1.4.m4.1.2" xref="S3.I1.i1.p1.4.m4.1.2.cmml"><msub id="S3.I1.i1.p1.4.m4.1.2.2" xref="S3.I1.i1.p1.4.m4.1.2.2.cmml"><mi id="S3.I1.i1.p1.4.m4.1.2.2.2" xref="S3.I1.i1.p1.4.m4.1.2.2.2.cmml">r</mi><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.4.m4.1.2.2.3" xref="S3.I1.i1.p1.4.m4.1.2.2.3.cmml">ğ’Ÿ</mi></msub><mo id="S3.I1.i1.p1.4.m4.1.2.1" xref="S3.I1.i1.p1.4.m4.1.2.1.cmml">â¢</mo><mrow id="S3.I1.i1.p1.4.m4.1.2.3.2" xref="S3.I1.i1.p1.4.m4.1.2.cmml"><mo id="S3.I1.i1.p1.4.m4.1.2.3.2.1" stretchy="false" xref="S3.I1.i1.p1.4.m4.1.2.cmml">(</mo><mi id="S3.I1.i1.p1.4.m4.1.1" xref="S3.I1.i1.p1.4.m4.1.1.cmml">q</mi><mo id="S3.I1.i1.p1.4.m4.1.2.3.2.2" stretchy="false" xref="S3.I1.i1.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m4.1b"><apply id="S3.I1.i1.p1.4.m4.1.2.cmml" xref="S3.I1.i1.p1.4.m4.1.2"><times id="S3.I1.i1.p1.4.m4.1.2.1.cmml" xref="S3.I1.i1.p1.4.m4.1.2.1"></times><apply id="S3.I1.i1.p1.4.m4.1.2.2.cmml" xref="S3.I1.i1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.1.2.2.1.cmml" xref="S3.I1.i1.p1.4.m4.1.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.4.m4.1.2.2.2.cmml" xref="S3.I1.i1.p1.4.m4.1.2.2.2">ğ‘Ÿ</ci><ci id="S3.I1.i1.p1.4.m4.1.2.2.3.cmml" xref="S3.I1.i1.p1.4.m4.1.2.2.3">ğ’Ÿ</ci></apply><ci id="S3.I1.i1.p1.4.m4.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m4.1c">r_{\mathcal{D}}(q)</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.4.m4.1d">italic_r start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_q )</annotation></semantics></math> closely matches <math alttext="Dep(q)" class="ltx_Math" display="inline" id="S3.I1.i1.p1.5.m5.1"><semantics id="S3.I1.i1.p1.5.m5.1a"><mrow id="S3.I1.i1.p1.5.m5.1.2" xref="S3.I1.i1.p1.5.m5.1.2.cmml"><mi id="S3.I1.i1.p1.5.m5.1.2.2" xref="S3.I1.i1.p1.5.m5.1.2.2.cmml">D</mi><mo id="S3.I1.i1.p1.5.m5.1.2.1" xref="S3.I1.i1.p1.5.m5.1.2.1.cmml">â¢</mo><mi id="S3.I1.i1.p1.5.m5.1.2.3" xref="S3.I1.i1.p1.5.m5.1.2.3.cmml">e</mi><mo id="S3.I1.i1.p1.5.m5.1.2.1a" xref="S3.I1.i1.p1.5.m5.1.2.1.cmml">â¢</mo><mi id="S3.I1.i1.p1.5.m5.1.2.4" xref="S3.I1.i1.p1.5.m5.1.2.4.cmml">p</mi><mo id="S3.I1.i1.p1.5.m5.1.2.1b" xref="S3.I1.i1.p1.5.m5.1.2.1.cmml">â¢</mo><mrow id="S3.I1.i1.p1.5.m5.1.2.5.2" xref="S3.I1.i1.p1.5.m5.1.2.cmml"><mo id="S3.I1.i1.p1.5.m5.1.2.5.2.1" stretchy="false" xref="S3.I1.i1.p1.5.m5.1.2.cmml">(</mo><mi id="S3.I1.i1.p1.5.m5.1.1" xref="S3.I1.i1.p1.5.m5.1.1.cmml">q</mi><mo id="S3.I1.i1.p1.5.m5.1.2.5.2.2" stretchy="false" xref="S3.I1.i1.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m5.1b"><apply id="S3.I1.i1.p1.5.m5.1.2.cmml" xref="S3.I1.i1.p1.5.m5.1.2"><times id="S3.I1.i1.p1.5.m5.1.2.1.cmml" xref="S3.I1.i1.p1.5.m5.1.2.1"></times><ci id="S3.I1.i1.p1.5.m5.1.2.2.cmml" xref="S3.I1.i1.p1.5.m5.1.2.2">ğ·</ci><ci id="S3.I1.i1.p1.5.m5.1.2.3.cmml" xref="S3.I1.i1.p1.5.m5.1.2.3">ğ‘’</ci><ci id="S3.I1.i1.p1.5.m5.1.2.4.cmml" xref="S3.I1.i1.p1.5.m5.1.2.4">ğ‘</ci><ci id="S3.I1.i1.p1.5.m5.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m5.1c">Dep(q)</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.5.m5.1d">italic_D italic_e italic_p ( italic_q )</annotation></semantics></math>, the minimal subset of <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.6.m6.1"><semantics id="S3.I1.i1.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.6.m6.1.1" xref="S3.I1.i1.p1.6.m6.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.6.m6.1b"><ci id="S3.I1.i1.p1.6.m6.1.1.cmml" xref="S3.I1.i1.p1.6.m6.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.6.m6.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.6.m6.1d">caligraphic_D</annotation></semantics></math> required to respond to <math alttext="q" class="ltx_Math" display="inline" id="S3.I1.i1.p1.7.m7.1"><semantics id="S3.I1.i1.p1.7.m7.1a"><mi id="S3.I1.i1.p1.7.m7.1.1" xref="S3.I1.i1.p1.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.7.m7.1b"><ci id="S3.I1.i1.p1.7.m7.1.1.cmml" xref="S3.I1.i1.p1.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.7.m7.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.7.m7.1d">italic_q</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.5">A response generator <math alttext="\theta" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_Î¸</annotation></semantics></math>, typically a prompted LLM inference, that constructs the answer <math alttext="a" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">italic_a</annotation></semantics></math> based solely on the information retrieved by <math alttext="r_{\mathcal{D}}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><msub id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">r</mi><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">ğ’Ÿ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">ğ‘Ÿ</ci><ci id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">ğ’Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">r_{\mathcal{D}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">italic_r start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT</annotation></semantics></math>. The response <math alttext="\theta(r_{\mathcal{D}}(q))" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.2"><semantics id="S3.I1.i2.p1.4.m4.2a"><mrow id="S3.I1.i2.p1.4.m4.2.2" xref="S3.I1.i2.p1.4.m4.2.2.cmml"><mi id="S3.I1.i2.p1.4.m4.2.2.3" xref="S3.I1.i2.p1.4.m4.2.2.3.cmml">Î¸</mi><mo id="S3.I1.i2.p1.4.m4.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.cmml">â¢</mo><mrow id="S3.I1.i2.p1.4.m4.2.2.1.1" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml"><mo id="S3.I1.i2.p1.4.m4.2.2.1.1.2" stretchy="false" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml">(</mo><mrow id="S3.I1.i2.p1.4.m4.2.2.1.1.1" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml"><msub id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.cmml"><mi id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.2" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.2.cmml">r</mi><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.3" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.3.cmml">ğ’Ÿ</mi></msub><mo id="S3.I1.i2.p1.4.m4.2.2.1.1.1.1" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.1.cmml">â¢</mo><mrow id="S3.I1.i2.p1.4.m4.2.2.1.1.1.3.2" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml"><mo id="S3.I1.i2.p1.4.m4.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml">(</mo><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">q</mi><mo id="S3.I1.i2.p1.4.m4.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.I1.i2.p1.4.m4.2.2.1.1.3" stretchy="false" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.2b"><apply id="S3.I1.i2.p1.4.m4.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2"><times id="S3.I1.i2.p1.4.m4.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2"></times><ci id="S3.I1.i2.p1.4.m4.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.3">ğœƒ</ci><apply id="S3.I1.i2.p1.4.m4.2.2.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1"><times id="S3.I1.i2.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.1"></times><apply id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.1.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2">subscript</csymbol><ci id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.1.1.1.2.3">ğ’Ÿ</ci></apply><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.2c">\theta(r_{\mathcal{D}}(q))</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.2d">italic_Î¸ ( italic_r start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_q ) )</annotation></semantics></math> should be equal to or approximate <math alttext="a" class="ltx_Math" display="inline" id="S3.I1.i2.p1.5.m5.1"><semantics id="S3.I1.i2.p1.5.m5.1a"><mi id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><ci id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.5.m5.1d">italic_a</annotation></semantics></math>, demonstrating the queryâ€™s reliance on explicit, directly accessible facts.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS1.SSS2.p2.3">This definition underscores the reliance of explicit fact querieson direct data retrieval without the need for complex reasoning or inference beyond the scope of the identified data segments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Here are some examples of queries at this level:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.1">What method was used in Paper X to solve problem Y?</span> (given a collection of academic papers)</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i2.p1.1.1">Whatâ€™s the AI strategy of company X?</span> (given a series of the latest news and articles about company X)</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Challenges and Solutions</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Queries at this level primarily necessitate the correct retrieval of data for LLMs to provide accurate responses. RAGÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib6" title="">6</a>]</cite>, due to its effectiveness, flexibility, and relatively low costs, is the most commonly adopted technical solution for handling this level of queries. However, even with RAG, there are significant challenges in constructing a robust and high-quality system. These challenges include:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">Data Processing Difficulties</span>: External data is often highly unstructured and contains multi-modal components such as tables, images, videos, and more. Additionally, the process of segmenting or "chunking" this data presents challenges in maintaining the original context and meaning.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">Data Retrieval Difficulties</span>: The retrieval of relevant data segments from a large, unstructured dataset can be computationally intensive and prone to errors. The challenge lies in developing efficient and accurate retrieval mechanisms.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i3.p1.1.1">Evaluation Difficulties</span>: Evaluating the performance of a RAG system, particularly at a component level, is a complex task. It requires the development of robust metrics that can accurately assess the quality of data retrieval and response generation.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Given the popularity of RAG, a wealth of literature and tools have been developed to address these challenges. In the remainder of this section, we will highlight some of the most practical and impactful enhancements to RAG. Additionally, we will discuss alternative technical solutions that may be employed beyond RAG.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Retrieval-augmented Generation (RAG)</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Retrieval-Augmented Generation refers to a methodology where a language model augments its natural language generation capabilities by dynamically retrieving external information during the generation process. This technique blends the generative capabilities of LLMs with the information retrieval from extensive databases or documents. The process is typically implemented as data index construction, retrieval system construction and answer generation.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Data Processing Enhancement</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Document parsing at this level often involves extracting information from text, tables, and figures in a coherent manner, ensuring that the relevant snippets are accurately identified and retrieved.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS1.p2.1.1">Multi-modal Documents Parsing</span> Addressing multi-modal content in source documents, such as charts, tables, or even videos (e.g. meeting recordings), is one of the most frequently asked questions. Broadly, two approaches are employed to tackle this issue. The first approach involves converting multi-modal content into textual form. For instance, Table-to-Text methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib34" title="">34</a>]</cite> translate tables into text, while other techniques convert visual content into textual or attribute-based descriptionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib36" title="">36</a>]</cite>, which are subsequently processed by large language models. The second approach leverages multi-modal embedding techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib39" title="">39</a>]</cite>, utilizing the retrieved embeddings from multi-modal data as soft prompts for input.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS1.p3.1.1">Chunking Optimization</span> For long texts, segmenting documents into text chunks is a common and necessary operation. Larger text chunks can preserve more of the semantic coherence of the context, but they also tend to contain more noise within each chunk<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib40" title="">40</a>]</cite>. Commonly-used chunking strategiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib42" title="">42</a>]</cite> include fixed size chunking, recursive chunking, sliding window chunking, paragraph-based chunking ,semantic chunking, etc. Certain methods are designed to ascertain the level of detail a query demands and, based on this identification, select text chunks of appropriate granularity for retrieval<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib44" title="">44</a>]</cite>. Alternatively, some methods opt to process and refine the text into smaller segments that maintain a high degree of information completeness<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib45" title="">45</a>]</cite>. Additionally, there are approaches that employ vision models to segment text in accordance with the original document structure<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib46" title="">46</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Data Retrieval Enhancement</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Information Retrieval (IR) techniques can be smoothly transferred into RAG applicaitons. The primary steps involved include establishing data indexes, processing queries, retrieving and matching, re-ranking, and evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">Indexing</span> The purpose of this step is to establish mappings from search terms to text segments, determining the logic by which the retrieval system operates. Indexing methods are broadly classified into three types: sparse, dense, and hybrid retrieval. Sparse retrieval uses specific words to index text segments. In contrast, dense retrieval maps text segments into a dense vector space of features. Hybrid retrieval combines elements of both sparse and dense techniques.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p3">
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I4.i1.p1.1.1">Sparse Retrieval</span>: This was the first indexing method to be widely adopted due to its simplicity and intuitiveness. Techniques like TF-IDF and BM25<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib48" title="">48</a>]</cite> are designed to identify the most representative keywords of each text segment based on their relative frequency. These methods are still prevalent in many RAG projects<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib51" title="">51</a>]</cite>. However, word matching methods can lead to retrieval losses due to their inability to recognize synonyms. To address this issue, methods like KNN can be used for similarity-based matching of keywords <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib52" title="">52</a>]</cite>. Alternatively, indices like keywords can be changed into the prediction of the probabilities of query tokens for the corresponding text segment<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I4.i2.p1.1.1">Dense Retrieval</span>: This approach often involves using pre-trained or fine-tuned text encoders to map texts to a dense vector space that aligns with query requirements. BERT-based encodersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib55" title="">55</a>]</cite> are commonly to be fine-tuned as dense retriever on unsupervised data using methods such as DPR<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib56" title="">56</a>]</cite>, ANCE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib57" title="">57</a>]</cite>, SimCSE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib58" title="">58</a>]</cite> and TAS-B<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib59" title="">59</a>]</cite>. Others employ unsupervised contrastive learning for fine-tuning, such as Contriever<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib60" title="">60</a>]</cite>. Using feedback from LLMs to guide the training objectives of retrievers can also effectively enhance the retrieverâ€™s suitability for LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib63" title="">63</a>]</cite>. Given the powerful capabilities and expressive potential of LLMs, LLM-based dense retrieval has recently emerged as a key area of interest and explorationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib64" title="">64</a>]</cite>. LLM2vecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib65" title="">65</a>]</cite> modifies the attention mechanism of a pre-trained LLM to a bidirectional one and employs the masked next-token prediction method for unsupervised training, resulting in an LLM-based dense retrieval embedder. Similarly, Llama2VecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib66" title="">66</a>]</cite> leverages two pretext tasksâ€”Embedding-Based Auto-Encoding and Embedding-Based Auto-Regressionâ€”to train an unsupervised dense retrieval encoder based on the LLaMA architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib67" title="">67</a>]</cite>, leading to significant improvements in retrieval task performance.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I4.i3.p1">
<p class="ltx_p" id="S3.I4.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I4.i3.p1.1.1">Others</span>: Combining sparse retrieval and dense retrieval is an effective method to focus simultaneously on the central theme of text segments and global features. Feng et al. (2023) propose initially determining the knowledge domain needed to answer a query as a fixed area of expertise, and then using dense retrieval to recall supplementary information within this domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib68" title="">68</a>]</cite>. Numerous studies have explored various methods of blending dense vector indexing with sparse encoder indexing to better capture the semantic information of text blocks and enhance the precision of targeted paragraph retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib71" title="">71</a>]</cite>. On the other hand, Tang et al. (2024) have enhanced the capabilities of a LLM by fine-tuning it for indexing and retrieving, effectively integrating these abilities directly into the LLM. This allows the LLM to autonomously generate data indices and text segments for each query <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib73" title="">73</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="S3.F3.g1" src="extracted/5872412/contents/images/three_type_alignment.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Three Types of Query-Document Alignment</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS2.p4.1.1">Query Document Alignment</span> The goal of this step is to align the query with document segments in external data to identify the best document segment that can assist in answering the query. As FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S3.F3" title="Figure 3 â€£ 3.3.2 Data Retrieval Enhancement â€£ 3.3 Retrieval-augmented Generation (RAG) â€£ 3 Explicit Fact Queries (L1) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">3</span></a> illustrated, there are primarily three approaches to this alignment: traditional alignment, document domain alignment, and query domain alignment. Traditional alignment involves mapping both document segments and the query into the same encoding space. For instance, many dense retrieval architectures based on dual encoders feature specialized query encodersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib59" title="">59</a>]</cite>. Conversely, if a system like RAG employs sparse retrieval, it is necessary to extract keywords from the query for the search. Further refinement can be achieved through query rewriting techniques, which enhance search accuracy by mitigating issues related to user terminological inaccuracies or vague descriptions, effectively improving the precision of the search resultsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib74" title="">74</a>]</cite>. Document domain alignment involves generating synthetic answers first, then using these answers to recall relevant data, effectively addressing the issue of queries and retrieved data not being in the same distribution space. A notable work in this area is HyDEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib75" title="">75</a>]</cite>. Query domain alignmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib76" title="">76</a>]</cite> involves generating a set of synthetic questions for each atomic unit of text, mapping text segments into the query space, and then retrieving the synthetic questions closest to the original query along with their corresponding text segments. This method ensures that the most relevant and contextually appropriate segments are selected for responding to the query. SlimPLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib77" title="">77</a>]</cite> employs a small proxy model to generate heuristic answers, which are then used to predict the knowledge needed to answer the question. This approach also provides an effective method for aligning queries to the document space.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p5">
<p class="ltx_p" id="S3.SS3.SSS2.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS2.p5.1.1">Re-ranking and Correction</span>
After retrieving the top k text blocks, RAG systems must filter and reorder these segments. Most RAG systems use the relevance scores provided by the retriever as the basis for ranking, while some studies employ specific metrics such as perplexity or perplexity gain as ranking criteria <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib79" title="">79</a>]</cite>. Other efforts involve using LLMs to evaluate the credibility and utility of retrieved text blocks, training a pluggable reward-driven contextual adapter to refine the output of retriever<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib80" title="">80</a>]</cite>. Additionally, some research focuses on pre-training a small language model dedicated to fact verification, which is used to filter out incorrect retrieved text chunks, thus improving the quality of the recalled text<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib81" title="">81</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p6">
<p class="ltx_p" id="S3.SS3.SSS2.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS3.SSS2.p6.1.1">Recursive Retrieval or Iterative Retrieval</span>
Considering the inherent limitations in the accuracy of a single retrieval attempt, an effective mitigation strategy is to perform multiple retrievals to progressively address any omissions. Kim et al. (2023) introduced a tree-like recursive retrieval method, incorporating pruning strategies to incrementally break down ambiguous questions into disambiguated ones, ultimately arriving at the closest correct answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib82" title="">82</a>]</cite>. Similarly, SEATER uses the k-means algorithm to construct a hierarchical tree structure of items to be retrieved, and iteratively recalls nodes within the tree structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib83" title="">83</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Response Generation Enhancement</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Generating responses requires determining if the retrieved information is sufficient or if additional external data is needed. Handling conflicts between retrieved knowledge and the modelâ€™s internal prior knowledge is also essential <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib86" title="">86</a>]</cite>. Supervised fine-tuning is an effective method to enhance the generation performance in RAG systems. When faced with irrelevant or erroneous information as the retrieved context, pre-trained large language models are often easily misled, resulting in incorrect responses. Many studies have shown that by subtly designing training data for RAG systems, fine-tuning or pretraining can effectively mitigate this issue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib89" title="">89</a>]</cite>. Through experimental analysis, RAATÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib89" title="">89</a>]</cite>, demonstrated that the detrimental effects of irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise on RAG models increase progressively. By incorporating with these training process, these methods enables the LLM to internally recognize noisy contexts, leading to significant improvements in response generation quality even in the presence of noisy retrievals. Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Implicit Fact Queries (L2)</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">These queries involve data dependencies that are not immediately obvious and may require some level of common sense reasoning or basic logical deductions. The necessary information might be spread across multiple segments or require simple inferencing.
(Example in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S2.F2" title="Figure 2 â€£ 2.1 Stratification of Queries â€£ 2 Problem Definition â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">2</span></a>)</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Queries at this level require gathering and processing information from multiple documents within the collection. The collection of required information may exceed the ability of a single retrieval request, necessitating the decomposition of the original query into multiple retrieval operations and the aggregation of results into a comprehensive answer. This level often involves common-sense reasoning without requiring domain-specific expertise. This type of queries may include statistical queries, descriptive analysis queries, and basic aggregation queries. For example, operations such as counting, comparison, trend analysis, and selective summarization are common in "how many" and "whatâ€™s the most" type queries, while multi-hop reasoning is frequently used. Therefore, we can define the level-2 queries, <math alttext="\mathcal{Q}_{2}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">ğ’¬</mi><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ’¬</ci><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathcal{Q}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">caligraphic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.3">For any query <math alttext="q" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_q</annotation></semantics></math> and its corresponding answer <math alttext="a" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_a</annotation></semantics></math>, a <math alttext="\mathcal{Q}_{2}" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><msub id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">ğ’¬</mi><mn id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">ğ’¬</ci><cn id="S4.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\mathcal{Q}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">caligraphic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> fact query is one where:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.2">There exists a set of explicit fact queries<math alttext="\{q_{1},q_{2},\ldots,q_{m}\}\subset\mathcal{Q}_{1}" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.m1.4"><semantics id="S4.I1.i1.p1.1.m1.4a"><mrow id="S4.I1.i1.p1.1.m1.4.4" xref="S4.I1.i1.p1.1.m1.4.4.cmml"><mrow id="S4.I1.i1.p1.1.m1.4.4.3.3" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml"><mo id="S4.I1.i1.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S4.I1.i1.p1.1.m1.2.2.1.1.1" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.2.2.1.1.1.2" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml">q</mi><mn id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.I1.i1.p1.1.m1.4.4.3.3.5" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.I1.i1.p1.1.m1.3.3.2.2.2" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.I1.i1.p1.1.m1.3.3.2.2.2.2" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml">q</mi><mn id="S4.I1.i1.p1.1.m1.3.3.2.2.2.3" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.I1.i1.p1.1.m1.4.4.3.3.6" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.i1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S4.I1.i1.p1.1.m1.4.4.3.3.7" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.I1.i1.p1.1.m1.4.4.3.3.3" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S4.I1.i1.p1.1.m1.4.4.3.3.3.2" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3.2.cmml">q</mi><mi id="S4.I1.i1.p1.1.m1.4.4.3.3.3.3" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S4.I1.i1.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S4.I1.i1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow><mo id="S4.I1.i1.p1.1.m1.4.4.4" xref="S4.I1.i1.p1.1.m1.4.4.4.cmml">âŠ‚</mo><msub id="S4.I1.i1.p1.1.m1.4.4.5" xref="S4.I1.i1.p1.1.m1.4.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i1.p1.1.m1.4.4.5.2" xref="S4.I1.i1.p1.1.m1.4.4.5.2.cmml">ğ’¬</mi><mn id="S4.I1.i1.p1.1.m1.4.4.5.3" xref="S4.I1.i1.p1.1.m1.4.4.5.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.4b"><apply id="S4.I1.i1.p1.1.m1.4.4.cmml" xref="S4.I1.i1.p1.1.m1.4.4"><subset id="S4.I1.i1.p1.1.m1.4.4.4.cmml" xref="S4.I1.i1.p1.1.m1.4.4.4"></subset><set id="S4.I1.i1.p1.1.m1.4.4.3.4.cmml" xref="S4.I1.i1.p1.1.m1.4.4.3.3"><apply id="S4.I1.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.2">ğ‘</ci><cn id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.I1.i1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.2">ğ‘</ci><cn id="S4.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">â€¦</ci><apply id="S4.I1.i1.p1.1.m1.4.4.3.3.3.cmml" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3.2">ğ‘</ci><ci id="S4.I1.i1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S4.I1.i1.p1.1.m1.4.4.3.3.3.3">ğ‘š</ci></apply></set><apply id="S4.I1.i1.p1.1.m1.4.4.5.cmml" xref="S4.I1.i1.p1.1.m1.4.4.5"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.4.4.5.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.5">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.4.4.5.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.5.2">ğ’¬</ci><cn id="S4.I1.i1.p1.1.m1.4.4.5.3.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.4.4.5.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.4c">\{q_{1},q_{2},\ldots,q_{m}\}\subset\mathcal{Q}_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.m1.4d">{ italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } âŠ‚ caligraphic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, each of which can be directly retrieved from specific data segments within the dataset <math alttext="D" class="ltx_Math" display="inline" id="S4.I1.i1.p1.2.m2.1"><semantics id="S4.I1.i1.p1.2.m2.1a"><mi id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.1b"><ci id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.2.m2.1d">italic_D</annotation></semantics></math>, such that:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{D}(q)=\bigcup_{i=1}^{m}r_{D}(q_{i})" class="ltx_Math" display="block" id="S4.Ex1.m1.2"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mrow id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml"><msub id="S4.Ex1.m1.2.2.3.2" xref="S4.Ex1.m1.2.2.3.2.cmml"><mi id="S4.Ex1.m1.2.2.3.2.2" xref="S4.Ex1.m1.2.2.3.2.2.cmml">r</mi><mi id="S4.Ex1.m1.2.2.3.2.3" xref="S4.Ex1.m1.2.2.3.2.3.cmml">D</mi></msub><mo id="S4.Ex1.m1.2.2.3.1" xref="S4.Ex1.m1.2.2.3.1.cmml">â¢</mo><mrow id="S4.Ex1.m1.2.2.3.3.2" xref="S4.Ex1.m1.2.2.3.cmml"><mo id="S4.Ex1.m1.2.2.3.3.2.1" stretchy="false" xref="S4.Ex1.m1.2.2.3.cmml">(</mo><mi id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">q</mi><mo id="S4.Ex1.m1.2.2.3.3.2.2" stretchy="false" xref="S4.Ex1.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.2.2.2" rspace="0.111em" xref="S4.Ex1.m1.2.2.2.cmml">=</mo><mrow id="S4.Ex1.m1.2.2.1" xref="S4.Ex1.m1.2.2.1.cmml"><munderover id="S4.Ex1.m1.2.2.1.2" xref="S4.Ex1.m1.2.2.1.2.cmml"><mo id="S4.Ex1.m1.2.2.1.2.2.2" movablelimits="false" xref="S4.Ex1.m1.2.2.1.2.2.2.cmml">â‹ƒ</mo><mrow id="S4.Ex1.m1.2.2.1.2.2.3" xref="S4.Ex1.m1.2.2.1.2.2.3.cmml"><mi id="S4.Ex1.m1.2.2.1.2.2.3.2" xref="S4.Ex1.m1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S4.Ex1.m1.2.2.1.2.2.3.1" xref="S4.Ex1.m1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.2.2.1.2.2.3.3" xref="S4.Ex1.m1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex1.m1.2.2.1.2.3" xref="S4.Ex1.m1.2.2.1.2.3.cmml">m</mi></munderover><mrow id="S4.Ex1.m1.2.2.1.1" xref="S4.Ex1.m1.2.2.1.1.cmml"><msub id="S4.Ex1.m1.2.2.1.1.3" xref="S4.Ex1.m1.2.2.1.1.3.cmml"><mi id="S4.Ex1.m1.2.2.1.1.3.2" xref="S4.Ex1.m1.2.2.1.1.3.2.cmml">r</mi><mi id="S4.Ex1.m1.2.2.1.1.3.3" xref="S4.Ex1.m1.2.2.1.1.3.3.cmml">D</mi></msub><mo id="S4.Ex1.m1.2.2.1.1.2" xref="S4.Ex1.m1.2.2.1.1.2.cmml">â¢</mo><mrow id="S4.Ex1.m1.2.2.1.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.Ex1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.2.2.1.1.1.1.1.cmml">(</mo><msub id="S4.Ex1.m1.2.2.1.1.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.2.2.1.1.1.1.1.2" xref="S4.Ex1.m1.2.2.1.1.1.1.1.2.cmml">q</mi><mi id="S4.Ex1.m1.2.2.1.1.1.1.1.3" xref="S4.Ex1.m1.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex1.m1.2.2.1.1.1.1.3" stretchy="false" xref="S4.Ex1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><eq id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"></eq><apply id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3"><times id="S4.Ex1.m1.2.2.3.1.cmml" xref="S4.Ex1.m1.2.2.3.1"></times><apply id="S4.Ex1.m1.2.2.3.2.cmml" xref="S4.Ex1.m1.2.2.3.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.3.2.1.cmml" xref="S4.Ex1.m1.2.2.3.2">subscript</csymbol><ci id="S4.Ex1.m1.2.2.3.2.2.cmml" xref="S4.Ex1.m1.2.2.3.2.2">ğ‘Ÿ</ci><ci id="S4.Ex1.m1.2.2.3.2.3.cmml" xref="S4.Ex1.m1.2.2.3.2.3">ğ·</ci></apply><ci id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1">ğ‘</ci></apply><apply id="S4.Ex1.m1.2.2.1.cmml" xref="S4.Ex1.m1.2.2.1"><apply id="S4.Ex1.m1.2.2.1.2.cmml" xref="S4.Ex1.m1.2.2.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.2.1.cmml" xref="S4.Ex1.m1.2.2.1.2">superscript</csymbol><apply id="S4.Ex1.m1.2.2.1.2.2.cmml" xref="S4.Ex1.m1.2.2.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.2.2.1.cmml" xref="S4.Ex1.m1.2.2.1.2">subscript</csymbol><union id="S4.Ex1.m1.2.2.1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.1.2.2.2"></union><apply id="S4.Ex1.m1.2.2.1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.1.2.2.3"><eq id="S4.Ex1.m1.2.2.1.2.2.3.1.cmml" xref="S4.Ex1.m1.2.2.1.2.2.3.1"></eq><ci id="S4.Ex1.m1.2.2.1.2.2.3.2.cmml" xref="S4.Ex1.m1.2.2.1.2.2.3.2">ğ‘–</ci><cn id="S4.Ex1.m1.2.2.1.2.2.3.3.cmml" type="integer" xref="S4.Ex1.m1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S4.Ex1.m1.2.2.1.2.3.cmml" xref="S4.Ex1.m1.2.2.1.2.3">ğ‘š</ci></apply><apply id="S4.Ex1.m1.2.2.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1"><times id="S4.Ex1.m1.2.2.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.2"></times><apply id="S4.Ex1.m1.2.2.1.1.3.cmml" xref="S4.Ex1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.3.1.cmml" xref="S4.Ex1.m1.2.2.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.2.2.1.1.3.2.cmml" xref="S4.Ex1.m1.2.2.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.Ex1.m1.2.2.1.1.3.3.cmml" xref="S4.Ex1.m1.2.2.1.1.3.3">ğ·</ci></apply><apply id="S4.Ex1.m1.2.2.1.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.2">ğ‘</ci><ci id="S4.Ex1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">r_{D}(q)=\bigcup_{i=1}^{m}r_{D}(q_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.2d">italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q ) = â‹ƒ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.I1.i1.p1.6">where <math alttext="r_{D}(q_{i})" class="ltx_Math" display="inline" id="S4.I1.i1.p1.3.m1.1"><semantics id="S4.I1.i1.p1.3.m1.1a"><mrow id="S4.I1.i1.p1.3.m1.1.1" xref="S4.I1.i1.p1.3.m1.1.1.cmml"><msub id="S4.I1.i1.p1.3.m1.1.1.3" xref="S4.I1.i1.p1.3.m1.1.1.3.cmml"><mi id="S4.I1.i1.p1.3.m1.1.1.3.2" xref="S4.I1.i1.p1.3.m1.1.1.3.2.cmml">r</mi><mi id="S4.I1.i1.p1.3.m1.1.1.3.3" xref="S4.I1.i1.p1.3.m1.1.1.3.3.cmml">D</mi></msub><mo id="S4.I1.i1.p1.3.m1.1.1.2" xref="S4.I1.i1.p1.3.m1.1.1.2.cmml">â¢</mo><mrow id="S4.I1.i1.p1.3.m1.1.1.1.1" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.cmml"><mo id="S4.I1.i1.p1.3.m1.1.1.1.1.2" stretchy="false" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.I1.i1.p1.3.m1.1.1.1.1.1" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.cmml"><mi id="S4.I1.i1.p1.3.m1.1.1.1.1.1.2" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.2.cmml">q</mi><mi id="S4.I1.i1.p1.3.m1.1.1.1.1.1.3" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.I1.i1.p1.3.m1.1.1.1.1.3" stretchy="false" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.3.m1.1b"><apply id="S4.I1.i1.p1.3.m1.1.1.cmml" xref="S4.I1.i1.p1.3.m1.1.1"><times id="S4.I1.i1.p1.3.m1.1.1.2.cmml" xref="S4.I1.i1.p1.3.m1.1.1.2"></times><apply id="S4.I1.i1.p1.3.m1.1.1.3.cmml" xref="S4.I1.i1.p1.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.I1.i1.p1.3.m1.1.1.3.1.cmml" xref="S4.I1.i1.p1.3.m1.1.1.3">subscript</csymbol><ci id="S4.I1.i1.p1.3.m1.1.1.3.2.cmml" xref="S4.I1.i1.p1.3.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.I1.i1.p1.3.m1.1.1.3.3.cmml" xref="S4.I1.i1.p1.3.m1.1.1.3.3">ğ·</ci></apply><apply id="S4.I1.i1.p1.3.m1.1.1.1.1.1.cmml" xref="S4.I1.i1.p1.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p1.3.m1.1.1.1.1.1.1.cmml" xref="S4.I1.i1.p1.3.m1.1.1.1.1">subscript</csymbol><ci id="S4.I1.i1.p1.3.m1.1.1.1.1.1.2.cmml" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.2">ğ‘</ci><ci id="S4.I1.i1.p1.3.m1.1.1.1.1.1.3.cmml" xref="S4.I1.i1.p1.3.m1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.3.m1.1c">r_{D}(q_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.3.m1.1d">italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> identifies the relevant data segments from <math alttext="D" class="ltx_Math" display="inline" id="S4.I1.i1.p1.4.m2.1"><semantics id="S4.I1.i1.p1.4.m2.1a"><mi id="S4.I1.i1.p1.4.m2.1.1" xref="S4.I1.i1.p1.4.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.4.m2.1b"><ci id="S4.I1.i1.p1.4.m2.1.1.cmml" xref="S4.I1.i1.p1.4.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.4.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.4.m2.1d">italic_D</annotation></semantics></math> necessary to answer <math alttext="q_{i}" class="ltx_Math" display="inline" id="S4.I1.i1.p1.5.m3.1"><semantics id="S4.I1.i1.p1.5.m3.1a"><msub id="S4.I1.i1.p1.5.m3.1.1" xref="S4.I1.i1.p1.5.m3.1.1.cmml"><mi id="S4.I1.i1.p1.5.m3.1.1.2" xref="S4.I1.i1.p1.5.m3.1.1.2.cmml">q</mi><mi id="S4.I1.i1.p1.5.m3.1.1.3" xref="S4.I1.i1.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.5.m3.1b"><apply id="S4.I1.i1.p1.5.m3.1.1.cmml" xref="S4.I1.i1.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p1.5.m3.1.1.1.cmml" xref="S4.I1.i1.p1.5.m3.1.1">subscript</csymbol><ci id="S4.I1.i1.p1.5.m3.1.1.2.cmml" xref="S4.I1.i1.p1.5.m3.1.1.2">ğ‘</ci><ci id="S4.I1.i1.p1.5.m3.1.1.3.cmml" xref="S4.I1.i1.p1.5.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.5.m3.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.5.m3.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and the union of these segments provides the information necessary to answer <math alttext="q" class="ltx_Math" display="inline" id="S4.I1.i1.p1.6.m4.1"><semantics id="S4.I1.i1.p1.6.m4.1a"><mi id="S4.I1.i1.p1.6.m4.1.1" xref="S4.I1.i1.p1.6.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.6.m4.1b"><ci id="S4.I1.i1.p1.6.m4.1.1.cmml" xref="S4.I1.i1.p1.6.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.6.m4.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.6.m4.1d">italic_q</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.8">A response generator <math alttext="\theta" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">italic_Î¸</annotation></semantics></math>, typically a prompted LLM inference, constructs the answer <math alttext="a" class="ltx_Math" display="inline" id="S4.I1.i2.p1.2.m2.1"><semantics id="S4.I1.i2.p1.2.m2.1a"><mi id="S4.I1.i2.p1.2.m2.1.1" xref="S4.I1.i2.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.2.m2.1b"><ci id="S4.I1.i2.p1.2.m2.1.1.cmml" xref="S4.I1.i2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.2.m2.1d">italic_a</annotation></semantics></math> to <math alttext="q" class="ltx_Math" display="inline" id="S4.I1.i2.p1.3.m3.1"><semantics id="S4.I1.i2.p1.3.m3.1a"><mi id="S4.I1.i2.p1.3.m3.1.1" xref="S4.I1.i2.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.3.m3.1d">italic_q</annotation></semantics></math> by aggregating the responses <math alttext="\{\theta(r_{D}(q_{1})),\theta(r_{D}(q_{2})),\ldots,\theta(r_{D}(q_{m}))\}" class="ltx_Math" display="inline" id="S4.I1.i2.p1.4.m4.4"><semantics id="S4.I1.i2.p1.4.m4.4a"><mrow id="S4.I1.i2.p1.4.m4.4.4.3" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml"><mo id="S4.I1.i2.p1.4.m4.4.4.3.4" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml">{</mo><mrow id="S4.I1.i2.p1.4.m4.2.2.1.1" xref="S4.I1.i2.p1.4.m4.2.2.1.1.cmml"><mi id="S4.I1.i2.p1.4.m4.2.2.1.1.3" xref="S4.I1.i2.p1.4.m4.2.2.1.1.3.cmml">Î¸</mi><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.2" xref="S4.I1.i2.p1.4.m4.2.2.1.1.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.cmml"><msub id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.cmml"><mi id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.2" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.2.cmml">r</mi><mi id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.3" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.3.cmml">D</mi></msub><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.2" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.2" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.2.cmml">q</mi><mn id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.3" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.4.4.3.5" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml">,</mo><mrow id="S4.I1.i2.p1.4.m4.3.3.2.2" xref="S4.I1.i2.p1.4.m4.3.3.2.2.cmml"><mi id="S4.I1.i2.p1.4.m4.3.3.2.2.3" xref="S4.I1.i2.p1.4.m4.3.3.2.2.3.cmml">Î¸</mi><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.2" xref="S4.I1.i2.p1.4.m4.3.3.2.2.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.cmml">(</mo><mrow id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.cmml"><msub id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.cmml"><mi id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.2" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.2.cmml">r</mi><mi id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.3" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.3.cmml">D</mi></msub><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.2" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.cmml"><mi id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.2" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.2.cmml">q</mi><mn id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.3" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.4.4.3.6" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml">,</mo><mi id="S4.I1.i2.p1.4.m4.1.1" mathvariant="normal" xref="S4.I1.i2.p1.4.m4.1.1.cmml">â€¦</mi><mo id="S4.I1.i2.p1.4.m4.4.4.3.7" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml">,</mo><mrow id="S4.I1.i2.p1.4.m4.4.4.3.3" xref="S4.I1.i2.p1.4.m4.4.4.3.3.cmml"><mi id="S4.I1.i2.p1.4.m4.4.4.3.3.3" xref="S4.I1.i2.p1.4.m4.4.4.3.3.3.cmml">Î¸</mi><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.2" xref="S4.I1.i2.p1.4.m4.4.4.3.3.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.cmml">(</mo><mrow id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.cmml"><msub id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.cmml"><mi id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.2" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.2.cmml">r</mi><mi id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.3" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.3.cmml">D</mi></msub><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.2" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.cmml"><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.2" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.cmml"><mi id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.2" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.2.cmml">q</mi><mi id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.3" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.3" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.4.m4.4.4.3.8" stretchy="false" xref="S4.I1.i2.p1.4.m4.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.4b"><set id="S4.I1.i2.p1.4.m4.4.4.4.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3"><apply id="S4.I1.i2.p1.4.m4.2.2.1.1.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1"><times id="S4.I1.i2.p1.4.m4.2.2.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.2"></times><ci id="S4.I1.i2.p1.4.m4.2.2.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.3">ğœƒ</ci><apply id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1"><times id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.2"></times><apply id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.1.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.3.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.3.3">ğ·</ci></apply><apply id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.2">ğ‘</ci><cn id="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.2.2.1.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S4.I1.i2.p1.4.m4.3.3.2.2.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2"><times id="S4.I1.i2.p1.4.m4.3.3.2.2.2.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.2"></times><ci id="S4.I1.i2.p1.4.m4.3.3.2.2.3.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.3">ğœƒ</ci><apply id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1"><times id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.2"></times><apply id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.1.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.2.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.3.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.3.3">ğ·</ci></apply><apply id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.2">ğ‘</ci><cn id="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.3.3.2.2.1.1.1.1.1.1.3">2</cn></apply></apply></apply><ci id="S4.I1.i2.p1.4.m4.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1">â€¦</ci><apply id="S4.I1.i2.p1.4.m4.4.4.3.3.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3"><times id="S4.I1.i2.p1.4.m4.4.4.3.3.2.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.2"></times><ci id="S4.I1.i2.p1.4.m4.4.4.3.3.3.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.3">ğœƒ</ci><apply id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1"><times id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.2"></times><apply id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.1.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.2.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.3.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.3.3">ğ·</ci></apply><apply id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1">subscript</csymbol><ci id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.2">ğ‘</ci><ci id="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.4.4.3.3.1.1.1.1.1.1.3">ğ‘š</ci></apply></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.4c">\{\theta(r_{D}(q_{1})),\theta(r_{D}(q_{2})),\ldots,\theta(r_{D}(q_{m}))\}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.4.m4.4d">{ italic_Î¸ ( italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) , italic_Î¸ ( italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) , â€¦ , italic_Î¸ ( italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) ) }</annotation></semantics></math> and applying common-sense reasoning to derive an answer that is not explicitly stated in the data. The response <math alttext="\theta(r_{D}(q))" class="ltx_Math" display="inline" id="S4.I1.i2.p1.5.m5.2"><semantics id="S4.I1.i2.p1.5.m5.2a"><mrow id="S4.I1.i2.p1.5.m5.2.2" xref="S4.I1.i2.p1.5.m5.2.2.cmml"><mi id="S4.I1.i2.p1.5.m5.2.2.3" xref="S4.I1.i2.p1.5.m5.2.2.3.cmml">Î¸</mi><mo id="S4.I1.i2.p1.5.m5.2.2.2" xref="S4.I1.i2.p1.5.m5.2.2.2.cmml">â¢</mo><mrow id="S4.I1.i2.p1.5.m5.2.2.1.1" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml"><mo id="S4.I1.i2.p1.5.m5.2.2.1.1.2" stretchy="false" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml">(</mo><mrow id="S4.I1.i2.p1.5.m5.2.2.1.1.1" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml"><msub id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.cmml"><mi id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.2" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.2.cmml">r</mi><mi id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.3" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.3.cmml">D</mi></msub><mo id="S4.I1.i2.p1.5.m5.2.2.1.1.1.1" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.1.cmml">â¢</mo><mrow id="S4.I1.i2.p1.5.m5.2.2.1.1.1.3.2" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml"><mo id="S4.I1.i2.p1.5.m5.2.2.1.1.1.3.2.1" stretchy="false" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml">(</mo><mi id="S4.I1.i2.p1.5.m5.1.1" xref="S4.I1.i2.p1.5.m5.1.1.cmml">q</mi><mo id="S4.I1.i2.p1.5.m5.2.2.1.1.1.3.2.2" stretchy="false" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.I1.i2.p1.5.m5.2.2.1.1.3" stretchy="false" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.5.m5.2b"><apply id="S4.I1.i2.p1.5.m5.2.2.cmml" xref="S4.I1.i2.p1.5.m5.2.2"><times id="S4.I1.i2.p1.5.m5.2.2.2.cmml" xref="S4.I1.i2.p1.5.m5.2.2.2"></times><ci id="S4.I1.i2.p1.5.m5.2.2.3.cmml" xref="S4.I1.i2.p1.5.m5.2.2.3">ğœƒ</ci><apply id="S4.I1.i2.p1.5.m5.2.2.1.1.1.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1"><times id="S4.I1.i2.p1.5.m5.2.2.1.1.1.1.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.1"></times><apply id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.1.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2">subscript</csymbol><ci id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.2.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.3.cmml" xref="S4.I1.i2.p1.5.m5.2.2.1.1.1.2.3">ğ·</ci></apply><ci id="S4.I1.i2.p1.5.m5.1.1.cmml" xref="S4.I1.i2.p1.5.m5.1.1">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.5.m5.2c">\theta(r_{D}(q))</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.5.m5.2d">italic_Î¸ ( italic_r start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_q ) )</annotation></semantics></math> should approximate the correct answer <math alttext="a" class="ltx_Math" display="inline" id="S4.I1.i2.p1.6.m6.1"><semantics id="S4.I1.i2.p1.6.m6.1a"><mi id="S4.I1.i2.p1.6.m6.1.1" xref="S4.I1.i2.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.6.m6.1b"><ci id="S4.I1.i2.p1.6.m6.1.1.cmml" xref="S4.I1.i2.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.6.m6.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.6.m6.1d">italic_a</annotation></semantics></math>, demonstrating that the query <math alttext="q" class="ltx_Math" display="inline" id="S4.I1.i2.p1.7.m7.1"><semantics id="S4.I1.i2.p1.7.m7.1a"><mi id="S4.I1.i2.p1.7.m7.1.1" xref="S4.I1.i2.p1.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.7.m7.1b"><ci id="S4.I1.i2.p1.7.m7.1.1.cmml" xref="S4.I1.i2.p1.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.7.m7.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.7.m7.1d">italic_q</annotation></semantics></math> can be effectively answered through the aggregation of responses to the <math alttext="\mathcal{Q}_{1}" class="ltx_Math" display="inline" id="S4.I1.i2.p1.8.m8.1"><semantics id="S4.I1.i2.p1.8.m8.1a"><msub id="S4.I1.i2.p1.8.m8.1.1" xref="S4.I1.i2.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i2.p1.8.m8.1.1.2" xref="S4.I1.i2.p1.8.m8.1.1.2.cmml">ğ’¬</mi><mn id="S4.I1.i2.p1.8.m8.1.1.3" xref="S4.I1.i2.p1.8.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.8.m8.1b"><apply id="S4.I1.i2.p1.8.m8.1.1.cmml" xref="S4.I1.i2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p1.8.m8.1.1.1.cmml" xref="S4.I1.i2.p1.8.m8.1.1">subscript</csymbol><ci id="S4.I1.i2.p1.8.m8.1.1.2.cmml" xref="S4.I1.i2.p1.8.m8.1.1.2">ğ’¬</ci><cn id="S4.I1.i2.p1.8.m8.1.1.3.cmml" type="integer" xref="S4.I1.i2.p1.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.8.m8.1c">\mathcal{Q}_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.8.m8.1d">caligraphic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> queries.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.3">This definition underscores the reliance of <math alttext="\mathcal{Q}_{2}" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><msub id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">ğ’¬</mi><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">ğ’¬</ci><cn id="S4.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\mathcal{Q}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">caligraphic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> queries on the ability to decompose complex queries into a set of simpler, explicit fact queries<math alttext="\mathcal{Q}_{1}" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><msub id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">ğ’¬</mi><mn id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">ğ’¬</ci><cn id="S4.SS1.p4.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\mathcal{Q}_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">caligraphic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, whose answers can then be combined to generate the correct response to the original query <math alttext="\mathcal{Q}_{2}" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3.1"><semantics id="S4.SS1.p4.3.m3.1a"><msub id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">ğ’¬</mi><mn id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">ğ’¬</ci><cn id="S4.SS1.p4.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.p4.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">\mathcal{Q}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">caligraphic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Here are some examples of queries at this level:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">How many experiments have sample sizes greater than 1000?</span> (given a collection of experimental records)</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">What are the top 3 most frequently mentioned symptoms?</span> (given a collection of medical records)</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.1">Whatâ€™s the difference between the AI strategies of company X and company Y?</span> (given a series of the latest news and articles about companies X and Y)</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Challenges and Solutions</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">At this level, queries still revolve around factual questions, but the answers are not explicitly presented in any single text passage. Instead, they require combining multiple facts through common-sense reasoning to arrive at a conclusion. The challenges of a level-2 query primarily include:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">Adaptive retrieval volumes</span>: Different questions may require varying numbers of retrieved contexts, and the specific number of retrieved contexts can depend on both the question and the dataset. A fixed number of retrievals may result in either information noise or insufficient information.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.1">Coordination between reasoning and retrieval</span>: Reasoning can guide the focus of what needs to be retrieved, while the insights gained from retrieved information can iteratively refine reasoning strategies. Addressing these complexities calls for an intelligent integration and selective harnessing of external data, capitalizing on the inherent reasoning prowess of LLMs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Methods to address challenges at this level include iterative RAG, RAG on graph/tree, and RAG with SQL, among others.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Iterative RAG</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<span class="ltx_ERROR undefined" id="S4.SS3.p1.1">\MFUsentencecase</span>
<p class="ltx_p" id="S4.SS3.p1.2">implicit fact queries is similar to multi-hop RAG tasks. This category of methods dynamically controls multi-step RAG processes, iteratively gathering or correcting information until the correct answer is achieved.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.1">Planning-based</span>: Generating a stepwise retrieval plan during the prior-retrieval stage or dynamically within the retrieval process can refine the focus of each retrieval, efficiently guiding the iterative RAG system. For example, ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib93" title="">93</a>]</cite> progressively updates the target of each step, reducing the knowledge gap required to answer the question. IRCoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib94" title="">94</a>]</cite> and RAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib95" title="">95</a>]</cite> uses a Chain of Thought to guide the RAG pipeline, making decisions about the current retrieval target based on previously recalled information. GenGroundÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib96" title="">96</a>]</cite> enables LLMs to alternate between two stages until arriving at the final answer: (1) generating a simpler single-step question and producing a direct answer, and (2) tracing the question-answer pair back to the retrieved documents to verify and correct any inaccuracies in the predictions. This iterative process ensures more reliable and accurate responses.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.1">Information Gap Filling Based</span>: ITRG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib97" title="">97</a>]</cite> introduces an iterative retrieval-generation collaboration framework, generating answers based on existing knowledge and then continuing to retrieve and generate for the unknown parts of the response in subsequent rounds. Similarly, FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib50" title="">50</a>]</cite> revisits and modifies low-probability tokens in answers generated in each iteration. On the other hand, Self-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib92" title="">92</a>]</cite> fine-tunes a large model to autonomously decide when to search and when to stop searching and start answering questions.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Graph/ Tree Question Answering</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Addressing implicit fact queriesrequires synthesizing information from multiple references. Graphs or trees, whether knowledge-based or data-structured, naturally express the relational structure among texts, making them highly suitable for this type of data retrieval problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<ul class="ltx_itemize" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.1">Traditional Knowledge Graph</span>: One of the initial structures considered for enhancing the efficacy of LLMs is the traditional knowledge graph, where each node represents an entity and edges between nodes signify the relationships between these entities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.I5.i1.p2">
<p class="ltx_p" id="S4.I5.i1.p2.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib98" title="">98</a>]</cite> proposed a forward-looking development roadmap for LLMs and Knowledge Graphs (KGs) comprising: 1) KG-enhanced LLMs, which integrate KGs during the pre-training and inference phases of LLMs to deepen the modelsâ€™ understanding of acquired knowledge; 2) LLM-enhanced KGs, which employ LLMs for various KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) collaborative LLMs+KGs approaches, where both LLMs and KGs play complementary roles, enhancing each other through bidirectional inference driven by data and knowledge. The Rigel-KQGA modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib99" title="">99</a>]</cite>, is an end-to-end KGQA model that predicts the necessary knowledge graph nodes based on a query and combines this with an LLM to derive answers. Works like Think-on-GraphÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib100" title="">100</a>]</cite> and KnowledgeNavigatorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib101" title="">101</a>]</cite> extract entities involved in a query and then perform iterative BFS searches on the graph, using the LLM as a thinking machine to determine the optimal exploration path and perform pruning. The <math alttext="R^{3}" class="ltx_Math" display="inline" id="S4.I5.i1.p2.1.m1.1"><semantics id="S4.I5.i1.p2.1.m1.1a"><msup id="S4.I5.i1.p2.1.m1.1.1" xref="S4.I5.i1.p2.1.m1.1.1.cmml"><mi id="S4.I5.i1.p2.1.m1.1.1.2" xref="S4.I5.i1.p2.1.m1.1.1.2.cmml">R</mi><mn id="S4.I5.i1.p2.1.m1.1.1.3" xref="S4.I5.i1.p2.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.I5.i1.p2.1.m1.1b"><apply id="S4.I5.i1.p2.1.m1.1.1.cmml" xref="S4.I5.i1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I5.i1.p2.1.m1.1.1.1.cmml" xref="S4.I5.i1.p2.1.m1.1.1">superscript</csymbol><ci id="S4.I5.i1.p2.1.m1.1.1.2.cmml" xref="S4.I5.i1.p2.1.m1.1.1.2">ğ‘…</ci><cn id="S4.I5.i1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.I5.i1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I5.i1.p2.1.m1.1c">R^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.I5.i1.p2.1.m1.1d">italic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib102" title="">102</a>]</cite>introduces several possible commonsense axioms via an LLM that could address a query, sequentially searching related knowledge subgraphs to assess if the current information suffices to answer the query, continuing until the question is resolved.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I5.i2.p1">
<p class="ltx_p" id="S4.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I5.i2.p1.1.1">Data Chunk Graph/ Tree</span>: The impressive reading comprehension capabilities of LLMs enable them to effectively grasp text without needing to break it down into the finest granularities of entities and relationships. In this context, researchers have begun experimenting with using text chunks or data chunks as nodes on graphs or trees, employing edges to represent either high-level or more intricately designed relations. Knowledge-Graph-PromptingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib103" title="">103</a>]</cite> discusses three popular kinds of questions that require mining implicit facts from (a) bridging questions rely on sequential reasoning while (b) comparing questions rely on parallel reasoning over different passages. (c) structural questions rely on fetching contents in the corresponding document structures. To tackle these questions, Knowledge-Graph-Prompting utilizes entity recognition, TF-IDF, KNN, and document structure hierarchies to construct document graphs and extract subgraphs for answering questions. MoGGÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib44" title="">44</a>]</cite> treats one or two sentences as the smallest semantic units, using these as nodes and building edges based on semantic similarity between nodes. It also trains a predictor to determine the textual granularity required for answering a query by deciding how large a sub-graph is needed. To capture more high-level semantic relationships between text blocks, RAPTORÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib43" title="">43</a>]</cite>, employs clustering algorithms to hierarchically cluster the finest granularity of text blocks. It summarizes new semantic information at each hierarchical level, recalling the most necessary information within a collapsed tree of nodes. Similarly, GraphRAGÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib104" title="">104</a>]</cite>, adopts a clustering approach. It initially connects the smallest text blocks based on semantic similarity, then uses community detection algorithms to group nodes. Finally, it summarizes the global answer to a query by analyzing responses within each node community.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Natural Language to SQL Queries</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">When dealing with structured data, converting natural language queries to SQL (NL2SQL) can be an effective approach. Tools like Chat2DB facilitate this process by translating user queries into database queries. In the era of large language models, there has been significant progress in the area of text-to-SQL<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib108" title="">108</a>]</cite>, which allows us to utilize these tools to retrieve information from structured databases. This capability serves as a valuable external data source to augment the generation capabilities of LLMs. By integrating text-to-SQL toolsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib109" title="">109</a>]</cite>, LLMs can access and incorporate structured data, enhancing their ability to generate more accurate and contextually relevant responses. This integration not only improves the depth and quality of the generated content but also expands the scope of LLM applications, enabling them to perform more complex tasks that require interaction with and interpretation of database content.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Discussion on Fact Queries</h3>
<div class="ltx_para ltx_noindent" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS6.p1.1.1">Whether to Use Fine-tuning</span>. Some worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib110" title="">110</a>]</cite> have demonstrated the hardness of LLMs to acquire new factual knowledge during fine tuning. This process can lead to a deterioration in the overall performance of the LLMs in generating accurate responses, and it often results in the generation of more hallucinations. Furthermore, studyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib111" title="">111</a>]</cite> suggests that fine-tuning LLMs with new factual data may cause the models to mechanically memorize fact statements. Interestingly, altering the phrasing of these memorized facts can render the recently learned knowledge ineffective, indicating a superficial level of understanding and retention by the LLMs. This points to limitations in the current fine-tuning processes and the need for more sophisticated methods to integrate and adapt new information effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS6.p2.1.1">Whether to Separate Different Levels of Fact Queries</span>. Both explicit fact queriesand implicit fact queriesare fact-based, and it is crucial to determine which level these queries belong before constructing data augmented LLM applications. Misclassifying explicit fact queriesas implicit fact queriescan lead to the retrieval of an abundance of superficial information that is seemingly relevant but ultimately unhelpful for answering the question, which can mislead the LLM and waste computational resources. Conversely, mistaking implicit fact queriesfor explicit fact queriescan prevent the use of appropriate methods to retrieve a sufficient and comprehensive set of external auxiliary data. <span class="ltx_ERROR undefined" id="S4.SS6.p2.1.2">\MFUsentencecase</span>implicit fact queries often require dynamically integrating information specific to the context of the queries, whereas explicit fact queriesgenerally need only a single data snippet, leading to the retrieval of a fixed amount of external data. This can result in suboptimal performance of the LLM. Therefore, it is advantageous to preliminarily distinguish the level of queries based on a thorough understanding of the target task. Additionally, considerable effort has been directed towards training models to autonomously assess whether the information retrieved is sufficient, exemplified by approaches such as self-RAGÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Interpretable Rationale Queries (L3)</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this section and the next, we will explore queries that necessitate external data to furnish rationales for their resolution. These queries demand not only a grasp of the factual content but also the ability to comprehend and apply domain-specific <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">rationales</span> that are integral to the dataâ€™s context. We classify these queries into two categories based on the nature of the rationales involved: queries based on interpretable rationales and those based on hidden rationales, as illustrated in the Figure Â <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S5.F4" title="Figure 4 â€£ 5.1 Overview â€£ 5 Interpretable Rationale Queries (L3) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<span class="ltx_ERROR undefined" id="S5.SS1.p2.1">\MFUsentencecase</span>
<p class="ltx_p" id="S5.SS1.p2.2">interpretable rationale queries represent a relatively straightforward category within applications that rely on external data to provide rationales. The auxiliary data for these types of queries often include clear explanations of the thought processes used to solve problems. The data can be organized in several forms:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Plain Texts</span>: Textual descriptions are the most common form of presenting interpretable rationales. These may include specialized or official documents such as handbooks or guidelines, as well as domain-specific manuals or operational guides. These texts articulate the reasoning processes that facilitate decision-making in complex scenarios. For example, documents such as FDA Guidance for pharmaceutical factories or medication guides for physicians provide insights into how experts, like FDA officers or doctors, approach specific cases.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Structured Instructions</span>: More explicit reasoning relationships or decision pathways might be presented in a structured format. These rationales can be understood as either a Text-Conditioned Moore Machine or a Text-Conditioned Mealy Machine. In the theory of computation, a Moore machine is a finite-state machine where the output values are determined solely by its current state<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Moore_machine" title="">https://en.wikipedia.org/wiki/Moore_machine</a></span></span></span>. The conditions that control state transitions are often expressed in text, which LLMs need to interpret, unlike traditional programs that operate on native code. For instance, consider a customer supporting agent that follows a handbook to handle userâ€™s request to product changing or refunding. Similarly, a Mealy machine is a finite-state machine where output values are determined by both its current state and the inputs<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Mealy_machine" title="">https://en.wikipedia.org/wiki/Mealy_machine</a></span></span></span>. The distinction here is that actions (such as API calls) are determined not only by the state but also by the textual messages associated with transitions from the previous state.
Naturally, these domain-specific rationales can be represented in formats such as workflows, decision trees, or pseudocode.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Here are some examples of queries at this level:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.1">How should a patient with chest pain and specific symptom descriptions be diagnosed and treated</span> (given a chest pain management guideline)</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i2.p1.1.1">How to respond to a userâ€™s question in a real-life scenario?</span> (given a customer service workflow)</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S5.F4.g1" src="extracted/5872412/contents/images/level34.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Demonstration of Rationale Queries</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Challenges and Solutions</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In the realm of interpretable rationale queries, an additional challenge is integrating domain-specific rationales into LLMs in an comprehensible manner. The primary challenges are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<ul class="ltx_itemize" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p" id="S5.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i1.p1.1.1">Prompt Optimization Costs</span>: The process of optimizing prompts is marked by high time and computational demands. Distinct queries demand tailored background knowledge and decision-making criteria, necessitating diverse examples. While manually designed prompts can be highly effective, they are labor-intensive and time-consuming. Furthermore, training models to generate tailored prompts for various queries incurs significant computational overhead.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S5.I3.i2.p1">
<p class="ltx_p" id="S5.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i2.p1.1.1">Limited interpretability</span>: The impact of prompts on LLMs is opaque. In many cases, access to the internal parameters of LLMs is typically restricted, complicating efforts to determine the impact of various prompts on these models. This lack of transparency hinders our ability to consistently understand and verify the interpretability of LLM responses to different prompts.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Prompt Tuning</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">For interpretable rationale queries, the key issue is how to effectively integrate rationales provided by external data into LLMs and ensure that these models can accurately follow and react based on these rationales. Text2MDTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib112" title="">112</a>]</cite> offers a viable demonstration, introducing two methods for automatically extracting medical decision trees from medical guidelines and textbooks. This process clarifies the logical chains within lengthy medical texts, making them more comprehensible. Similarly, MedDMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib113" title="">113</a>]</cite> has developed a format for clinical guidance trees that can be executed by LLMs, proposing a methodology for reasoning on these executable CGTs and a framework for multi-turn dialogues between patients and LLMs. InstructRecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib114" title="">114</a>]</cite> aims to leverage the capabilities of LLMs in recommendation systems, designing a universal format to describe a userâ€™s preferences, intentions, task forms, and context using natural language, thereby creating a high-performing, language-based recommendation system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Integrating rationales directly as natural language instructions into LLMs does not necessarily yield optimal performance, and manually designing prompts can be time-consuming. To address this, the employment of prompt tuning techniques becomes essential to enhance the LLMsâ€™ capability to adhere to specific rationales. One effective methodology is the application of reinforcement learning, as evidenced by the TEMPERA frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib115" title="">115</a>]</cite>, which designs prompts incorporating limited instructions, examples, and verbalizers within the action space of reinforcement learning. Here, the LLMâ€™s probability of generating correct responses serves as the reward, guiding the model to discover the optimal prompt configuration across datasets. Similarly, RlpromptÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib116" title="">116</a>]</cite> adopts a reinforcement learning approach, training an adaptor to assist smaller language models in producing optimal prompts based on feedback concerning the relative accuracy of LLM responses. Another innovative strategy, Directional Stimulus Prompting, leverages the performance of LLMs on downstream tasks as a reward mechanism. This method trains models to extract and utilize directional stimuliâ€”specific cues or keywords tailored to individual instancesâ€”as prompts, thereby ensuring the LLMsâ€™ actions align more closely with expected outcomes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Additionally, for optimization within discrete prompt spaces, edit-based methodologies such as GrIPSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib117" title="">117</a>]</cite> are utilized. This technique involves using a small dataset as a scoring set to experiment with various prompt modificationsâ€”including deletions, swaps, paraphrases, and additionsâ€”to ascertain the most effective prompt configurations swiftly and effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Recent advancementsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib119" title="">119</a>]</cite> have also seen the rise of using LLMs themselves to facilitate prompt optimization. OPROÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib120" title="">120</a>]</cite> employs an LLM both to generate new prompt solutions based on historical data and their associated performance metrics and to score these prompts, thus streamlining the optimization process. Furthermore, the Reflexion frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib121" title="">121</a>]</cite> introduces a novel prompt optimization approach based on linguistic feedback, using a language model to analyze and store reflections on LLM outputs in an episodic memory buffer. This memory component aids in refining decision-making processes and evaluating outcomes in future interactions, leveraging accumulated historical insights.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>CoT Prompting</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Addressing complex rationales necessitates that LLMs engage in extended chains of reasoning, a process distinct from the reasoning across disparate factual information typical of fact queries. However, Chain-of-ThoughtsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib122" title="">122</a>]</cite>, Tree-of-ThoughtsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib123" title="">123</a>]</cite> or Graph-of-ThoughtsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib124" title="">124</a>]</cite> methodologies proves effective for such scenarios. For issues that are well-studied and have high general applicability, manually designing CoT prompts emerges as a feasible solution. Ji et al. (2023)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib125" title="">125</a>]</cite> proposed a method of self-reflection that integrates knowledge acquisition with answer generation. By utilizing external tools and designing prompts, they constructed three types of self-reflection loops: the Factual Knowledge Acquiring Loop, the Knowledge-Consistent Answering Loop, and the Question-Entailment Answering Loop, thereby incorporating external rationales into the modelâ€™s processing. Furthermore, Wu et al. (2024)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib126" title="">126</a>]</cite> conducted a manual analysis of error types in clinical records and developed three distinct CoT prompts to direct the GPT-4 modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib127" title="">127</a>]</cite> in focusing on intervention, diagnostic, and management errors. This targeted prompting facilitates the tasks of automatic error detection, span identification, and correction within clinical records.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">While manual design of CoT prompts is highly effective, it requires substantial human and temporal resources. To mitigate these costs, Automate-CoTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib128" title="">128</a>]</cite> proposed a technique for generating augmenting rational chains from a minimally labeled dataset. This approach employs a variance-reduced policy gradient strategy to evaluate the importance of each CoT chain, thus facilitating the selection of the most effective prompt combination.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Another form of utilizing Chain of Thoughts prompting involves constructing an agent workflow centered around LLMs. This typically requires the development of a more comprehensive system to address various real-world scenarios. According to Wang et al., such systems can be broadly divided into profiling, memory, planning, and action modulesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib129" title="">129</a>]</cite>. Interpretable rationales can be integrated into multiple modules in various forms, allowing the agent to adapt and iterate based on environmental or human feedback. Recent advancements, such as those by LLM ReasonersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib130" title="">130</a>]</cite> and SocREvalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib131" title="">131</a>]</cite>, have focused on automatically evaluating the quality of reasoning chains. These methodologies also assist in constructing robust data augmented LLM applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">Applications based on interpretable rationales span various domains. For instance, CoMLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib132" title="">132</a>]</cite> integrates AutoML knowledge as prompts into an LLM, dynamically retrieves useful information from historical experimental records, and combines these elements to empower the LLM to develop machine learning solutions for novel tasks. MetaGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib133" title="">133</a>]</cite> has developed a multi-agent system for software development, where different stakeholders within a project are each represented as an agent. This setup enables multiple agents to collaborate according to a real-world work pipeline, effectively completing software development tasks. Similarly, sophisticated agent systems have been designed in fields such as customer serviceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib134" title="">134</a>]</cite> and medical question answeringÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib135" title="">135</a>]</cite>. In these domains, agents are tailored to handle specific types of inquiries, which can involve understanding complex user requests or providing accurate medical information. These systems not only enhance the interaction quality but also improve the efficiency and accuracy of responses, demonstrating the versatility and potential of LLMs when integrated into well-designed agent workflows.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Hidden Rationale Queries (L4)</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<span class="ltx_ERROR undefined" id="S6.SS1.p1.1">\MFUsentencecase</span>
<p class="ltx_p" id="S6.SS1.p1.2">hidden rationale queries are the most challenging type of queries to address. Unlike interpretable rationale queries, which provide clear guidance on the rationales needed to respond to queries, hidden rationale queriesinvolve domain-specific reasoning method that may not be explicitly described and are too numerous to exhaust. These rationales often encompass a wide variety that cannot be fully explored within the typical context window and may lack clear instructions, representing a form of domain expertise that is implicit within the data. Such data might include, but is not limited to:</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">In-domain Data</span>: <span class="ltx_ERROR undefined" id="S6.I1.i1.p1.1.2">\MFUsentencecase</span>hidden rationale queries might utilize data from the same domain, such as historical question-and-answer records or artificially generated data. This in-domain data inherently contains the reasoning skills or methodologies necessary to address current queries. For instance, in the context of Python programming puzzles, solutions to historical problems often include classical algorithms and problem-solving strategies that could aid in resolving current issues.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Preliminary Knowledge</span>: Another form of hidden rationales consists of extensive, dispersed knowledge bases that vary in application across different scenarios. This preliminary knowledge may constitute a comprehensive axiomatic system, such as all local legal codes that form the basis for legal judgments. It could also include proven intermediate conclusions that simplify reasoning processes in fields like mathematical proofs. When addressing real-world issues using external data, this prior knowledge might also stem from complex accumulations of human experiences and empirical summaries.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Navigating hidden rationale queriesthus demands sophisticated analytical techniques to decode and leverage the latent wisdom embedded within disparate data sources, presenting significant challenges for RAG systems in effectively interpreting and applying such intricate and implicit information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Here are some examples of queries at this level:</p>
<ul class="ltx_itemize" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p" id="S6.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.1">How will the economic situation affect the companyâ€™s future development?</span> (given a collection of financial reports, with economic and financial rationale required)</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p" id="S6.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.1.1">How to achieve 24 points using the numbers 5, 5, 5, and 1?</span> (given a series of 24-point game examples and corresponding answers.)</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S6.I2.i3.p1">
<p class="ltx_p" id="S6.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i3.p1.1.1">Does Afghanistan permit a parent to confer his or her citizenship on a child born abroad?</span> (given the GLOBALCIT citizenship law datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib136" title="">136</a>]</cite>)</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Challenges and Solutions</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The construction of data-augmented LLM applications is significantly challenged by hidden rationale queries, with primary difficulties manifesting in the following areas:</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<ul class="ltx_itemize" id="S6.I3">
<li class="ltx_item" id="S6.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I3.i1.p1">
<p class="ltx_p" id="S6.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I3.i1.p1.1.1">Logical retrieval</span>: For questions involving hidden rationales, the helpfulness of external data does not simply depend on entity-level or semantic similarity, but rather on logical congruence or thematic alignment. Standard retrieval methods often struggle to capture the true target of the query or to identify text segments with logical similarities based on the problem presented. This necessitates the development of more sophisticated retrieval algorithms that can parse and identify underlying logical structures rather than relying solely on superficial textual similarities.</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S6.I3.i2.p1">
<p class="ltx_p" id="S6.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I3.i2.p1.1.1">Data insufficiency</span>: Fundamentally, external data may not explicitly contain the guidance or answers relevant to the current query. Instead, relevant information is often embedded in dispersed knowledges or illustrated through examples. This indirect presentation demands robust capabilities in data interpretation and synthesis, requiring LLMs to effectively derive coherent answers from fragmented or tangentially related data sources. Such challenges underscore the imperative for sophisticated data integration and reasoning capabilities within LLM frameworks to navigate the complexities of hidden rationale querieseffectively.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Offline Learning</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">To address these types of queries, a common approach is to identify and extract rules and guidelines from datasets offline, subsequently retrieving related items. For generate reasoning rationales, some work like STaRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib137" title="">137</a>]</cite> and LXSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib138" title="">138</a>]</cite> used LLM for rationale generation. The former employs an iterative few-shot example method to generate from small dataset to large dataset, the later introduced a two-role explaining extraction process where a learner model generated explanations and a critic model assess them for validation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">GLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib139" title="">139</a>]</cite> identifies errors and generalizes them into guidelines for future tasks through an in-context-learning. LEAPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib140" title="">140</a>]</cite> forms principles by generating mistakes, low-level principles, and high-level principles, incorporating these principles into prompts for final inference. RICPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib141" title="">141</a>]</cite> uses mistakes from training data to generate high-level reasoning and specific insights, then employs hierarchical clustering to group modes of errors, generating task-level and question-level principles, which are combined and retrieved for question-level insights. A Buffer-of-ThoughtÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib142" title="">142</a>]</cite> uses a problem distiller to distill a meta-buffer across many reasoning tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Some integrated methods, like MedPromptÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib143" title="">143</a>]</cite>, include GPT-4-generated chains of thought for training examples with self-validation, using these in conjunction with a KNN retrieval in-context learning approach. Agent HospitalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib144" title="">144</a>]</cite> generates rationales through reflection and utilizes both record retrieval and experience retrieval on generated data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">Although these concepts go by many different namesâ€”such as guidelines, principles, experiences, and thought templatesâ€”the main idea is to extract common useful rationales to enhance reasoning queries. These rationales may come from self-generated chains of thought (MedPrompt, Buffer-of-Thought), training set mistakes (GL, RICP, Agent Hospital), or intentionally generated mistakes (LEAP). Additionally, some principles are used across all tasks (Agent Hospital, RICP), while others are dynamically retrieved for specific questions (MedPrompt, Buffer-of-Thought). Many of these works demonstrate that learning from cases to accumulate experience as rationales is beneficial for various reasoning tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>In Context Learning (ICL)</h3>
<div class="ltx_para ltx_noindent" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Using examples for in-context learning is a common method for uncovering hidden rationales. Pre-trained large language models exhibit substantial in-context learning capabilities, which can be enhanced by retrieving examples based on similarity, thereby leveraging the modelsâ€™ few-shot learning abilitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib146" title="">146</a>]</cite>. However, the inclusion of irrelevant information in prompts can easily distract LLMs, leading to incorrect responsesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib148" title="">148</a>]</cite>. OpenICL, as developed by Wu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib149" title="">149</a>]</cite>, constructed an ICL framework that explores the impact of different traditional methods of retrieving examples and inference techniques on ICL effectiveness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Furthermore, training smaller models based on the feedback from LLMs regarding context examples to select optimal demonstrations and examples can improve the construction of context for specific tasks in a more targeted mannerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib150" title="">150</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib151" title="">151</a>]</cite>. To address the issue that semantic similarity-based example retrieval may not cover the broader range of associations needed in practical tests, Su et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib152" title="">152</a>]</cite> employed an unsupervised, graph-based selective annotation method called vote-k, constructing a more diverse and representative example database for few-shot learning. Additionally, Zhang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib153" title="">153</a>]</cite> proposed an Auto-CoT method that clusters examples into various representative types. By sampling problems diversely and generating reasoning chains, this method constructs examples that better support the learning process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1">However, enabling LLMs to master reasoning capabilities outside their trained domains through few-shot learning remains a substantial challenge. Wang et al. addressed this by sampling a variety of reasoning paths and marginalizing over these paths to select the most consistent answer, thereby enhancing the probability of LLMs selecting correct reasoning chainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib154" title="">154</a>]</cite>. Agarwal et al. introduced two scalable methods for generating usable example, namely reinforced ICL and unsupervised ICL, that aim to replace human-generated examples, thus expanding the pool of available examplesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib155" title="">155</a>]</cite>. DIN-SQLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib156" title="">156</a>]</cite> sought to decompose tasks into simpler subtasks and used the solutions to these sub-problems as prompts for LLMs, significantly improving their performance in generating SQL from text. Similarly, DUPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib157" title="">157</a>]</cite> identified three main issues LLMs face when using the chain of thought approach to solve complex mathematical word problems: semantic misunderstandings, computational errors, and missing steps, with semantic misunderstandings being a primary limiting factor. Encouraging LLMs to deeply understand the problems and extract essential information for resolution can significantly enhance their ability to solve mathematical problems by addressing these semantic misunderstandings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1">In-context learning is increasingly being utilized across various fields such as mathematics, law, medicine, and financeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib158" title="">158</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib159" title="">159</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib160" title="">160</a>]</cite>, playing a crucial role in the development of data-augmented LLM applications. This approach not only extends the functional capabilities of LLMs but also enhances their practical utility across diverse domains.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="314" id="S6.F5.g1" src="extracted/5872412/contents/images/solution_conclusion4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Summary of Main Techniques for Different Query Levels in <span class="ltx_ERROR undefined" id="S6.F5.2.1">\MFUsentencecase</span>data augmented LLM applications</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Despite the robust in-context learning capabilities of LLMs, accurately identifying rationales or optimal examples for complex and lengthy logical chains remains a significant challenge. Additionally, the provision of extensive external prior knowledge can also pose challenges to the inference capabilities of LLMs. Given these factors, fine-tuning emerges as a promising approach. It not only utilizes the extensive foundational knowledge that LLMs acquire during pretraining but also enables them to rapidly grasp new domain rationales. This method provides a viable path for enhancing the adaptability and effectiveness of LLMs in tackling advanced and specialized tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">Instruction tuning is a common method for infusing new capabilities into LLMs, typically involving supervised fine-tuning using paired (instruction, output) data. There are three primary methods for constructing an instruction dataset: a) deriving from existing datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib161" title="">161</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib162" title="">162</a>]</cite>, b) manually creating through handcrafted instructionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib165" title="">165</a>]</cite>, and c) generating synthetic data using powerful LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib154" title="">154</a>]</cite>. Additionally, numerous studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib167" title="">167</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib168" title="">168</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib169" title="">169</a>]</cite> have explored how to optimize the data distribution within instruction datasets to enhance fine-tuning effectiveness. However, when building data-augmented LLM applications, fine-tuning remains a relatively costly method in terms of time and computational resources. Recently, several efforts have been made to reduce the costs associated with fine-tuning large models. Adapter tuning, for instance, involves integrating small adapter models with LLMs while freezing the parameters of the LLM during fine-tuning and only optimizing the weights of the adapterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib171" title="">171</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib173" title="">173</a>]</cite>. Prefix Tuning and Prompt Tuning involve adding a set of trainable vectors before the input, which are optimized during training to enhance the performance of the LLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib174" title="">174</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib176" title="">176</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib177" title="">177</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib178" title="">178</a>]</cite>. Low-Rank AdaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib180" title="">180</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib181" title="">181</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib182" title="">182</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib183" title="">183</a>]</cite> reduces the number of trainable parameters needed for adapting to downstream tasks by imposing low-rank constraints on each dense layer to approximate the update matrices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p3">
<p class="ltx_p" id="S6.SS5.p3.1">In recent years, there has been a substantial amount of work using supervised fine-tuning to enhance the capabilities of LLMs in specialized domains such as mathematical reasoning, finance, law, and healthcareÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib184" title="">184</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib186" title="">186</a>]</cite>. For instance, ChatTimeLlamaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib187" title="">187</a>]</cite> introduced an interpretable time reasoning instruction tuning dataset and fine-tuned on LLaMAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib188" title="">188</a>]</cite> to significantly improve the modelâ€™s complex temporal reasoning, future event prediction capabilities, and interpretability. LISAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib189" title="">189</a>]</cite> leveraged a small set of segment data samples that involve reasoning to fine-tune the multimodal LLM LLaVA, which resulted in substantial improvements in reasoning segmentation capabilities. MAmmoTHÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib190" title="">190</a>]</cite> ingeniously constructed a mathematical example dataset that uniquely combines Chain of Thought and Program of Thought reasoning, ensuring broad coverage across different mathematical domains and enhancing the LLMâ€™s ability to solve general mathematical problems. ReFTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib191" title="">191</a>]</cite> proposes a method for learning from multiple annotated reasoning paths corresponding to the same problem. It automatically samples numerous reasoning trajectories for a given mathematical problem, leveraging the correct answer to generate reward signals. ChatDoctorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib192" title="">192</a>]</cite> utilized a large dataset of 100,000 patient-doctor dialogues from a widely-used online medical consultation platform to fine-tune LLaMA, significantly enhancing the modelâ€™s ability to understand patient needs and provide effective recommendations. FinGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib193" title="">193</a>]</cite> developed an open-source LLM fine-tuned on financial data using automated data curation and lightweight, low-rank adaptation techniques. DISC-LawLLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib194" title="">194</a>]</cite> created a supervised fine-tuning dataset for the Chinese judicial domain, fine-tuning LLMs to effectively serve a variety of users in different legal scenarios with enhanced legal reasoning capabilities.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we delineate data augmented LLM applications into four distinct categories based on the primary focus of queries, each facing unique challenges and thus requiring tailored solutions, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S6.F5" title="Figure 5 â€£ 6.4 In Context Learning (ICL) â€£ 6 Hidden Rationale Queries (L4) â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">5</span></a>. For queries related to static common knowledge, deploying a general LLM through a Chain of Thought methodology is effective. For explicit fact queries, the main challenge involves pinpointing the exact location of facts within a database, thus making a basic RAG the method of choice. In the case of implicit fact queries, which require the collation of multiple related facts, iterative RAG and RAG implementations on graph or tree structures are preferred for their ability to concurrently retrieve individual facts and interconnect multiple data points. When extensive data linkage is necessary, text-to-SQL techniques prove indispensable, leveraging database tools to facilitate external data searches. For interpretable rationale queries, advancements through prompt tuning and CoT prompting are critical to enhance LLMsâ€™ compliance with external directives. The most formidable are hidden rationale queries, which demand the autonomous synthesis of problem-solving approaches from extensive data sets. Here, offline learning, in-context learning , and fine-tuning emerge as vital methodologies.</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="250" id="S7.F6.g1" src="extracted/5872412/contents/images/three_type_input.png" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Three ways to inject specific domain data into an LLM: a) extracting part of the domain data based on the query as context input for the LLM, b) training a smaller model with specific domain data, which then guides the integration of external information subsequently input into the LLM, and c) directly using external domain knowledge to fine-tune a general large language model to become a domain-expert model.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Prior to the development of a targeted LLM application, as domain experts, we must acquire an in-depth understanding of the intended task, ascertain the complexity level of the associated queries, and select corresponding technological approaches for resolution. These methods principally inject knowledge into LLMs via three mechanisms, as depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#S7.F6" title="Figure 6 â€£ 7 Conclusion â€£ Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely"><span class="ltx_text ltx_ref_tag">6</span></a>: a) extracting part of the domain data based on the query as context input for the LLM, b) training a smaller model with specific domain data, which then guides the integration of external information subsequently input into the LLM, and c) directly using external domain knowledge to fine-tune a general large language model to become a domain-expert model. These strategies differ in their requirements for data volume, training duration, and computational resources, escalating respectively. Knowledge injection through context provides better interpretability and stability but faces limitations due to the finite context window and potential information loss in the middleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib40" title="">40</a>]</cite>, ideally suited for scenarios where data can be succinctly explained in shorter texts. However, this method challenges the modelâ€™s retrieval capabilities and knowledge extraction ability. The small model approach offers the advantage of reduced training times and the capacity to assimilate considerable amounts of data, yet its efficacy is contingent upon the modelâ€™s capabilities, potentially capping the LLMâ€™s performance for more complex tasks and incurring additional training costs with data increasing. Fine-tuning facilitates the utilization of large model capacities with extensive domain-specific data, yet its impact on the LLM strongly depends on the design of the data used. Employing out-of-domain factual data for fine-tuning may inadvertently lead to the generation of more erroneous outputs by the LLM, while also risking the loss of previously known domain knowledge and the neglect of unencountered tasks during fine-tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14924v1#bib.bib195" title="">195</a>]</cite>. Therefore, choosing an appropriate data injection strategy into the LLM requires a thorough understanding of oneâ€™s data sources and judicious decision-making based on this insight.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Moreover, in practical scenarios, data augmented LLM applicationstypically involves a combination of diverse query types, necessitating developers to engineer a routing pipeline that integrates multiple methodologies to effectively tackle these multifaceted challenges.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Yuqi Nie, Yaxuan Kong, Xiaowen Dong, JohnÂ M Mulvey, HÂ Vincent Poor, Qingsong Wen, and Stefan Zohren.

</span>
<span class="ltx_bibblock">A survey of large language models for financial applications: Progress, prospects and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2406.11903</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Yining Huang, Keke Tang, and Meilian Chen.

</span>
<span class="ltx_bibblock">A comprehensive survey on evaluating large language model applications in the medical industry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2404.15777</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Xiaoxian Yang, Zhifeng Wang, QiÂ Wang, KeÂ Wei, Kaiqi Zhang, and Jiangang Shi.

</span>
<span class="ltx_bibblock">Large language models for automated q&amp;a involving legal documents: a survey on algorithms, frameworks and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">International Journal of Web Information Systems</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Janice Ahn, Rishu Verma, Renze Lou, DiÂ Liu, Rui Zhang, and Wenpeng Yin.

</span>
<span class="ltx_bibblock">Large language models for mathematical reasoning: Progresses and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2402.00157</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and WilliamÂ Yang Wang.

</span>
<span class="ltx_bibblock">Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Meng Wang, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yujuan Ding, Wenqi Fan, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li.

</span>
<span class="ltx_bibblock">A survey on rag meets llms: Towards retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2405.06211</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou.

</span>
<span class="ltx_bibblock">From matching to generation: A survey on generative information retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2404.14851</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2402.19473</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Zeyu Han, Chao Gao, Jinyang Liu, SaiÂ Qian Zhang, etÂ al.

</span>
<span class="ltx_bibblock">Parameter-efficient fine-tuning for large models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2403.14608</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hongling Zheng, LiÂ Shen, Anke Tang, Yong Luo, Han Hu, BoÂ Du, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Learn from model beyond fine-tuning: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2310.08184</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, etÂ al.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2308.10792</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
JÂ Gruettner, TÂ Henzler, TÂ Sueselbeck, CÂ Fink, MÂ Borggrefe, and TÂ Walter.

</span>
<span class="ltx_bibblock">Clinical assessment of chest pain and guidelines for imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">European Journal of Radiology</span>, 81(12):3663â€“3668, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, etÂ al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Transactions of the Association for Computational Linguistics</span>, 7:453â€“466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Payal Bajaj, Daniel Campos, Nick Craswell, LiÂ Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, etÂ al.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1611.09268</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv e-prints</span>, page arXiv:1705.03551, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1606.05250</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ivan Stelmakh, YiÂ Luan, Bhuwan Dhingra, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Asqa: Factoid questions meet long-form answers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2204.06092</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh.

</span>
<span class="ltx_bibblock">The value of semantic parse labeling for knowledge base question answering.

</span>
<span class="ltx_bibblock">In Katrin Erk and NoahÂ A. Smith, editors, <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 201â€“206, Berlin, Germany, August 2016. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, WilliamÂ W Cohen, Ruslan Salakhutdinov, and ChristopherÂ D Manning.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:1809.09600</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-KhoaÂ Duong Nguyen, Saku Sugawara, and Akiko Aizawa.

</span>
<span class="ltx_bibblock">Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2011.01060</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Musique: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Transactions of the Association for Computational Linguistics</span>, 10:539â€“554, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, NoahÂ A Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2210.03350</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.

</span>
<span class="ltx_bibblock">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Transactions of the Association for Computational Linguistics</span>, 9:346â€“361, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Alon Talmor and Jonathan Berant.

</span>
<span class="ltx_bibblock">The web as a knowledge-base for answering complex questions.

</span>
<span class="ltx_bibblock">In Marilyn Walker, Heng Ji, and Amanda Stent, editors, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 641â€“651, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.

</span>
<span class="ltx_bibblock">Semantic parsing on freebase from question-answer pairs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2013 conference on empirical methods in natural language processing</span>, pages 1533â€“1544, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Priyanka Sen, AlhamÂ Fikri Aji, and Amir Saffari.

</span>
<span class="ltx_bibblock">Mintaka: A complex, natural, and multilingual dataset for end-to-end question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2210.01613</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, AlexanderÂ J Smola, and LeÂ Song.

</span>
<span class="ltx_bibblock">Variational reasoning for question answering with knowledge graph.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">AAAI</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, IzÂ Beltagy, Arman Cohan, NoahÂ A. Smith, and Matt Gardner.

</span>
<span class="ltx_bibblock">A dataset of information-seeking questions and answers anchored in research papers.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:1903.00161</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
RichardÂ Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, HeÂ He, etÂ al.

</span>
<span class="ltx_bibblock">Quality: Question answering with long input texts, yes!

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2112.08608</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Rami Aly, Zhijiang Guo, Michael Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal.

</span>
<span class="ltx_bibblock">Feverous: Fact extraction and verification over unstructured and structured information.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2106.05707</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, YuÂ Li, Guilin Qi, Yun Li, Nijun Li, and Qianren Wang.

</span>
<span class="ltx_bibblock">Exploring the impact of table-to-text methods on augmenting llm-based question answering with domain hybrid data, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
DÃ­dac SurÃ­s, Sachit Menon, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Vipergpt: Visual inference via python execution for reasoning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 11888â€“11898, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Difei Gao, Lei Ji, Luowei Zhou, KevinÂ Qinghong Lin, Joya Chen, Zihan Fan, and MikeÂ Zheng Shou.

</span>
<span class="ltx_bibblock">Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2306.08640</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, DavidÂ A Ross, and Alireza Fathi.

</span>
<span class="ltx_bibblock">Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 23369â€“23379, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">International conference on machine learning</span>, pages 12888â€“12900. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou.

</span>
<span class="ltx_bibblock">Generative multi-modal knowledge retrieval with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volumeÂ 38, pages 18733â€“18741, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
NelsonÂ F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Transactions of the Association for Computational Linguistics</span>, 12:157â€“173, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
LangChain.

</span>
<span class="ltx_bibblock">Text splitter.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/" title="">https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
LlamaIndex.

</span>
<span class="ltx_bibblock">Semantic splitter.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/" title="">https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and ChristopherÂ D Manning.

</span>
<span class="ltx_bibblock">Raptor: Recursive abstractive processing for tree-organized retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2401.18059</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, and Zengchang Qin.

</span>
<span class="ltx_bibblock">Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2406.00456</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Dong Yu, and Hongming Zhang.

</span>
<span class="ltx_bibblock">Dense x retrieval: What retrieval granularity should we use?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2312.06648</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
AntonioÂ Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and Leah Li.

</span>
<span class="ltx_bibblock">Financial report chunking for effective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2402.05131</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Karen SparckÂ Jones.

</span>
<span class="ltx_bibblock">A statistical interpretation of term specificity and its application in retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Journal of documentation</span>, 28(1):11â€“21, 1972.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
StephenÂ E Robertson, Steve Walker, Susan Jones, MichelineÂ M Hancock-Beaulieu, Mike Gatford, etÂ al.

</span>
<span class="ltx_bibblock">Okapi at trec-3.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Nist Special Publication Sp</span>, 109:109, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Transactions of the Association for Computational Linguistics</span>, 11:1316â€“1331, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zhengbao Jiang, FrankÂ F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2305.06983</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi.

</span>
<span class="ltx_bibblock">Recomp: Improving retrieval-augmented lms with context compression and selective augmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">The Twelfth International Conference on Learning Representations</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg.

</span>
<span class="ltx_bibblock">Off the beaten path: Letâ€™s replace term-based retrieval with k-nn search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 25th ACM international on conference on information and knowledge management</span>, pages 1099â€“1108, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Shengyao Zhuang and Guido Zuccon.

</span>
<span class="ltx_bibblock">Tilde: Term independent likelihood model for passage re-ranking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</span>, pages 1483â€“1492, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Shengyao Zhuang and Guido Zuccon.

</span>
<span class="ltx_bibblock">Fast passage re-ranking with contextualized exact term matching and efficient passage expansion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2108.08513</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Jacob Devlin.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2004.04906</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, YeÂ Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk.

</span>
<span class="ltx_bibblock">Approximate nearest neighbor negative contrastive learning for dense text retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2007.00808</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2104.08821</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury.

</span>
<span class="ltx_bibblock">Efficiently teaching an effective dense retriever with balanced topic aware sampling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</span>, pages 113â€“122, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2112.09118</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.

</span>
<span class="ltx_bibblock">Retrieve anything to augment large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2310.07554</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Fine-tuning llama for multi-stage text retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2310.08319</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, GeorgeÂ Bm Van DenÂ Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, etÂ al.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">International conference on machine learning</span>, pages 2206â€“2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, GustavoÂ HernÃ¡ndez Ãbrego, JiÂ Ma, VincentÂ Y Zhao, YiÂ Luan, KeithÂ B Hall, Ming-Wei Chang, etÂ al.

</span>
<span class="ltx_bibblock">Large dual encoders are generalizable retrievers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2112.07899</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy.

</span>
<span class="ltx_bibblock">Llm2vec: Large language models are secretly powerful text encoders.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2404.05961</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, and Defu Lian.

</span>
<span class="ltx_bibblock">Llama2vec: Unsupervised adaptation of large language models for dense retrieval.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 3490â€“3500, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, etÂ al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.

</span>
<span class="ltx_bibblock">Knowledge card: Filling llmsâ€™ knowledge gaps with plug-in specialized language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2305.09955</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Kunal Sawarkar, Abhilasha Mangal, and ShivamÂ Raj Solanki.

</span>
<span class="ltx_bibblock">Blended rag: Improving rag (retriever-augmented generation) accuracy with semantic search and hybrid query-based retrievers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2404.07220</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2402.03216</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
YiÂ Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins.

</span>
<span class="ltx_bibblock">Sparse, dense, and attentional representations for text retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">Transactions of the Association for Computational Linguistics</span>, 9:329â€“345, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, etÂ al.

</span>
<span class="ltx_bibblock">Self-retrieval: Building an information retrieval system with one large language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2403.00801</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Xin Cheng, DiÂ Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.

</span>
<span class="ltx_bibblock">Lift yourself up: Retrieval-augmented text generation with self-memory.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan.

</span>
<span class="ltx_bibblock">Query rewriting for retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2305.14283</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.

</span>
<span class="ltx_bibblock">Precise zero-shot dense retrieval without relevance labels.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2212.10496</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Vatsal Raina and Mark Gales.

</span>
<span class="ltx_bibblock">Question-based retrieval using atomic units for enterprise rag.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2405.12363</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2402.12052</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.

</span>
<span class="ltx_bibblock">Llmlingua: Compressing prompts for accelerated inference of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2310.05736</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.

</span>
<span class="ltx_bibblock">Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2310.06839</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao.

</span>
<span class="ltx_bibblock">Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2310.18347</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2401.15884</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:2310.14696</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Zihua Si, Zhongxiang Sun, Jiale Chen, Guozhang Chen, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu.

</span>
<span class="ltx_bibblock">Generative retrieval with semantic tree-structured item identifiers via contrastive learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2309.13375</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Kevin Wu, Eric Wu, and James Zou.

</span>
<span class="ltx_bibblock">How faithful are rag models? quantifying the tug-of-war between rag and llmsâ€™ internal prior, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng.

</span>
<span class="ltx_bibblock">Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2402.10612</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, QiÂ Cao, and Xueqi Cheng.

</span>
<span class="ltx_bibblock">Blinded by generated contexts: How language models merge generated and retrieved contexts for open-domain qa?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2401.11911</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, BoÂ Li, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2310.07713</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.

</span>
<span class="ltx_bibblock">Making retrieval-augmented language models robust to irrelevant context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2310.01558</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu.

</span>
<span class="ltx_bibblock">Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2405.20978</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
XiÂ Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, etÂ al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2310.01352</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Sebastian HofstÃ¤tter, Jiecao Chen, Karthik Raman, and Hamed Zamani.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</span>, pages 1437â€“1447, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:2310.11511</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2210.03629</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 10014â€“10037, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang.

</span>
<span class="ltx_bibblock">Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2403.05313</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren.

</span>
<span class="ltx_bibblock">Generate-then-ground in retrieval-augmented generation for multi-hop question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2406.14891</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 11661â€“11665. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.

</span>
<span class="ltx_bibblock">Unifying large language models and knowledge graphs: A roadmap.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">ArXiv</span>, abs/2306.08302, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Priyanka Sen, Sandeep Mavadia, and Amir Saffari.

</span>
<span class="ltx_bibblock">Knowledge graph-augmented language models for complex question answering.

</span>
<span class="ltx_bibblock">In Bhavana DalviÂ Mishra, Greg Durrett, Peter Jansen, Danilo NevesÂ Ribeiro, and Jason Wei, editors, <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)</span>, pages 1â€“8, Toronto, Canada, June 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, LionelÂ M. Ni, Heung-Yeung Shum, and Jian Guo.

</span>
<span class="ltx_bibblock">Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, and Yingyou Wen.

</span>
<span class="ltx_bibblock">Knowledgenavigator: Leveraging large language models for enhanced reasoning over knowledge graph.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2312.15880</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Armin Toroghi, Willis Guo, Mohammad MahdiÂ Abdollah Pour, and Scott Sanner.

</span>
<span class="ltx_bibblock">Right for right reasons: Large language models for verifiable commonsense knowledge graph question answering, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
YuÂ Wang, Nedim Lipka, RyanÂ A. Rossi, AlexaÂ F. Siu, Ruiyi Zhang, and Tyler Derr.

</span>
<span class="ltx_bibblock">Knowledge graph prompting for multi-document question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">AAAI Conference on Artificial Intelligence</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Darren Edge, HaÂ Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.

</span>
<span class="ltx_bibblock">From local to global: A graph rag approach to query-focused summarization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">arXiv.org</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Peng Shi, Rui Zhang, HeÂ Bai, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2210.13693</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.

</span>
<span class="ltx_bibblock">Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volumeÂ 37, pages 13067â€“13075, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Gabriel Poesia, Oleksandr Polozov, VuÂ Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani.

</span>
<span class="ltx_bibblock">Synchromesh: Reliable code generation from pre-trained language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">arXiv preprint arXiv:2201.11227</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
XiÂ Victoria Lin, Richard Socher, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Bridging textual and tabular data for cross-domain text-to-sql semantic parsing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:2012.12627</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, JosephÂ E Gonzalez, Carlos Guestrin, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Text2sql is not enough: Unifying ai and databases with tag.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2408.14717</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig.

</span>
<span class="ltx_bibblock">Does fine-tuning llms on new knowledge encourage hallucinations?, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, AsaÂ Cooper Stickland, Tomasz Korbak, and Owain Evans.

</span>
<span class="ltx_bibblock">The reversal curse: Llms trained on" a is b" fail to learn" b is a".

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">arXiv preprint arXiv:2309.12288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xiaoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, and Guotong Xie.

</span>
<span class="ltx_bibblock">Text2mdt: extracting medical decision trees from medical texts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">arXiv preprint arXiv:2401.02034</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Binbin Li, Tianxin Meng, Xiaoming Shi, Jie Zhai, and Tong Ruan.

</span>
<span class="ltx_bibblock">Meddm: Llm-executable clinical guidance tree for clinical decision-making.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">arXiv preprint arXiv:2312.02441</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Junjie Zhang, Ruobing Xie, Yupeng Hou, WayneÂ Xin Zhao, Leyu Lin, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Recommendation as instruction following: A large language model empowered recommendation approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">arXiv preprint arXiv:2305.07001</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and JosephÂ E Gonzalez.

</span>
<span class="ltx_bibblock">Tempera: Test-time prompting via reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:2211.11890</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, EricÂ P Xing, and Zhiting Hu.

</span>
<span class="ltx_bibblock">Rlprompt: Optimizing discrete text prompts with reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:2205.12548</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Grips: Gradient-free, edit-based instruction search for prompting large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">arXiv preprint arXiv:2203.07281</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Yongchao Zhou, AndreiÂ Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.

</span>
<span class="ltx_bibblock">Large language models are human-level prompt engineers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2211.01910</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Reid Pryzant, Dan Iter, Jerry Li, YinÂ Tat Lee, Chenguang Zhu, and Michael Zeng.

</span>
<span class="ltx_bibblock">Automatic prompt optimization with" gradient descent" and beam search.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:2305.03495</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, QuocÂ V Le, Denny Zhou, and Xinyun Chen.

</span>
<span class="ltx_bibblock">Large language models as optimizers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">The Twelfth International Conference on Learning Representations</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">Advances in neural information processing systems</span>, 35:24824â€“24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Yao Yao, Zuchao Li, and Hai Zhao.

</span>
<span class="ltx_bibblock">Beyond chain-of-thought, effective graph-of-thought reasoning in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">arXiv preprint arXiv:2305.16582</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.

</span>
<span class="ltx_bibblock">Towards mitigating llm hallucination via self reflection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 1827â€“1843, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Zhaolong Wu, Abul Hasan, Jinge Wu, Yunsoo Kim, JasonÂ PY Cheung, Teng Zhang, and Honghan Wu.

</span>
<span class="ltx_bibblock">Chain-of-though (cot) prompting strategies for medical error detection and correction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">arXiv preprint arXiv:2406.09103</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
KaShun Shum, Shizhe Diao, and Tong Zhang.

</span>
<span class="ltx_bibblock">Automatic prompt augmentation and selection with chain-of-thought from labeled data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">arXiv preprint arXiv:2302.12822</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, XuÂ Chen, Yankai Lin, etÂ al.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">Frontiers of Computer Science</span>, 18(6):186345, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Shibo Hao, YiÂ Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, etÂ al.

</span>
<span class="ltx_bibblock">Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:2404.05221</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth.

</span>
<span class="ltx_bibblock">Socreval: Large language models with the socratic method for reference-free reasoning evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">Findings of the Association for Computational Linguistics: NAACL 2024</span>, pages 2736â€“2764, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang.

</span>
<span class="ltx_bibblock">MLCopilot: Unleashing the power of large language models in solving machine learning tasks.

</span>
<span class="ltx_bibblock">In Yvette Graham and Matthew Purver, editors, <span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 2931â€“2959, St. Julianâ€™s, Malta, March 2024. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven KaÂ Shing Yau, Zijuan Lin, Liyang Zhou, etÂ al.

</span>
<span class="ltx_bibblock">Metagpt: Meta programming for multi-agent collaborative framework.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2308.00352</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Mahyar Abbasian, Iman Azimi, AmirÂ M Rahmani, and Ramesh Jain.

</span>
<span class="ltx_bibblock">Conversational health agents: A personalized llm-powered agent framework.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">arXiv preprint arXiv:2310.02374</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein.

</span>
<span class="ltx_bibblock">Medagents: Large language models as collaborators for zero-shot medical reasoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2311.10537</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Neel Guha, Julian Nyarko, Daniel Ho, Christopher RÃ©, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, etÂ al.

</span>
<span class="ltx_bibblock">Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">Advances in Neural Information Processing Systems</span>, 35:15476â€“15488, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Wolfgang Stammer, Felix Friedrich, David Steinmann, Hikaru Shindo, and Kristian Kersting.

</span>
<span class="ltx_bibblock">Learning by self-explaining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2309.08395</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Chaoxu Pang, Yixuan Cao, Qiang Ding, and Ping Luo.

</span>
<span class="ltx_bibblock">Guideline learning for in-context information extraction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">arXiv preprint arXiv:2310.05066</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon.

</span>
<span class="ltx_bibblock">In-context principle learning from mistakes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">arXiv preprint arXiv:2402.05403</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Hao Sun, Yong Jiang, BoÂ Wang, Yingyan Hou, Yan Zhang, Pengjun Xie, and Fei Huang.

</span>
<span class="ltx_bibblock">Retrieved in-context principles from previous mistakes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">arXiv preprint arXiv:2407.05682</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, JosephÂ E. Gonzalez, and Bin Cui.

</span>
<span class="ltx_bibblock">Buffer of thoughts: Thought-augmented reasoning with large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Harsha Nori, YinÂ Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, etÂ al.

</span>
<span class="ltx_bibblock">Can generalist foundation models outcompete special-purpose tuning? case study in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">arXiv preprint arXiv:2311.16452</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu.

</span>
<span class="ltx_bibblock">Agent hospital: A simulacrum of hospital with evolvable medical agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">arXiv preprint arXiv:2405.02957</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.

</span>
<span class="ltx_bibblock">What makes good in-context examples for gpt-<math alttext="3" class="ltx_Math" display="inline" id="bib.bib145.1.m1.1"><semantics id="bib.bib145.1.m1.1a"><mn id="bib.bib145.1.m1.1.1" xref="bib.bib145.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="bib.bib145.1.m1.1b"><cn id="bib.bib145.1.m1.1.1.cmml" type="integer" xref="bib.bib145.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="bib.bib145.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="bib.bib145.1.m1.1d">3</annotation></semantics></math>?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib145.2.1">arXiv preprint arXiv:2101.06804</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">Advances in neural information processing systems</span>, 33:1877â€“1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, EdÂ H Chi, Nathanael SchÃ¤rli, and Denny Zhou.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">International Conference on Machine Learning</span>, pages 31210â€“31227. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou.

</span>
<span class="ltx_bibblock">How many demonstrations do you need for in-context learning?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">arXiv preprint arXiv:2303.08119</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, YuÂ Qiao, and Zhiyong Wu.

</span>
<span class="ltx_bibblock">Openicl: An open-source framework for in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">arXiv preprint arXiv:2303.02913</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Unified demonstration retriever for in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2305.04320</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong.

</span>
<span class="ltx_bibblock">Compositional exemplars for in-context learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">International Conference on Machine Learning</span>, pages 39818â€“39833. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Hongjin Su, Jungo Kasai, ChenÂ Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, NoahÂ A Smith, etÂ al.

</span>
<span class="ltx_bibblock">Selective annotation makes language models better few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">arXiv preprint arXiv:2209.01975</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Aston Zhang, MuÂ Li, and Alex Smola.

</span>
<span class="ltx_bibblock">Automatic chain of thought prompting in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">arXiv preprint arXiv:2210.03493</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">arXiv preprint arXiv:2203.11171</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Rishabh Agarwal, Avi Singh, LeiÂ M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, JohnÂ D Co-Reyes, Eric Chu, etÂ al.

</span>
<span class="ltx_bibblock">Many-shot in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2404.11018</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Mohammadreza Pourreza and Davood Rafiei.

</span>
<span class="ltx_bibblock">Din-sql: Decomposed in-context learning of text-to-sql with self-correction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, BoÂ Du, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Achieving&gt; 97% on gsm8k: Deeply understanding the problems makes llms perfect reasoners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2404.14963</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Dan Schumacher and Anthony Rios.

</span>
<span class="ltx_bibblock">Team utsa-nlp at semeval 2024 task 5: Prompt ensembling for argument reasoning in civil procedures with gpt4.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2404.01961</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Chengfeng Dou, Zhi Jin, Wenping Jiao, Haiyan Zhao, Zhenwei Tao, and Yongqiang Zhao.

</span>
<span class="ltx_bibblock">Plugmed: Improving specificity in patient-centered medical dialogue generation using in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2305.11508</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen.

</span>
<span class="ltx_bibblock">Large language models in finance: A survey.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">Proceedings of the fourth ACM international conference on AI in finance</span>, pages 374â€“382, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Shayne Longpre, LeÂ Hou, TuÂ Vu, Albert Webson, HyungÂ Won Chung, YiÂ Tay, Denny Zhou, QuocÂ V Le, Barret Zoph, Jason Wei, etÂ al.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">International Conference on Machine Learning</span>, pages 22631â€“22648. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, StephenÂ H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, TevenÂ Le Scao, Arun Raja, etÂ al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">arXiv preprint arXiv:2110.08207</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">Advances in neural information processing systems</span>, 35:27730â€“27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the worldâ€™s first truly open instruction-tuned llm.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">Company Blog of Databricks</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis, ZhiÂ Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, RichÃ¡rd Nagyfi, etÂ al.

</span>
<span class="ltx_bibblock">Openassistant conversations-democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, PuÂ Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">arXiv preprint arXiv:2304.12244</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib167.1.1">Journal of machine learning research</span>, 21(140):1â€“67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Srinivasan Iyer, XiÂ Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, PunitÂ Singh Koura, etÂ al.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the lens of generalization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">arXiv preprint arXiv:2212.12017</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, NoahÂ A Smith, IzÂ Beltagy, etÂ al.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on open resources.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">Advances in Neural Information Processing Systems</span>, 36:74764â€“74786, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin DeÂ Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">International conference on machine learning</span>, pages 2790â€“2799. PMLR, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig.

</span>
<span class="ltx_bibblock">Towards a unified view of parameter-efficient transfer learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2110.04366</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Zhao, Yuexin Wu, BoÂ Li, etÂ al.

</span>
<span class="ltx_bibblock">Conditional adapters: Parameter-efficient transfer learning with fast inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">Advances in Neural Information Processing Systems</span>, 36:8152â€“8172, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, MatthewÂ E Peters, Alexander Fraser, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Adaptersoup: Weight averaging to improve generalization of pretrained language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">arXiv preprint arXiv:2302.07027</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Aleksandar Petrov, PhilipÂ HS Torr, and Adel Bibi.

</span>
<span class="ltx_bibblock">When do prompting and prefix-tuning work? a theory of capabilities and limitations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">arXiv preprint arXiv:2310.19698</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
RabeehÂ Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.

</span>
<span class="ltx_bibblock">Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2106.04489</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Wei Zhu and Ming Tan.

</span>
<span class="ltx_bibblock">Spt: learning to selectively insert prompts for better prompt tuning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 11862â€“11878, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, etÂ al.

</span>
<span class="ltx_bibblock">Aprompt: Attention prompt tuning for efficient adaptation of pre-trained language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 9147â€“9160, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Joon-Young Choi, Junho Kim, Jun-Hyung Park, Wing-Lam Mok, and SangKeun Lee.

</span>
<span class="ltx_bibblock">Smop: Towards efficient and effective prompt tuning with sparse mixture-of-prompts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 14306â€“14316, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi.

</span>
<span class="ltx_bibblock">Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">arXiv preprint arXiv:2210.07558</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, YuÂ Cheng, Weizhu Chen, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Adalora: Adaptive budget allocation for parameter-efficient fine-tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">arXiv preprint arXiv:2303.10512</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Sparse low-rank adaptation of pre-trained language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">arXiv preprint arXiv:2311.11696</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-ChiangÂ Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.

</span>
<span class="ltx_bibblock">Dora: Weight-decomposed low-rank adaptation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">arXiv preprint arXiv:2402.09353</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Ping Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona Diab, and Asli Celikyilmaz.

</span>
<span class="ltx_bibblock">Alert: Adapting language models to reasoning tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">arXiv preprint arXiv:2212.08286</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Mingyu Zong and Bhaskar Krishnamachari.

</span>
<span class="ltx_bibblock">Solving math word problems concerning systems of equations with gpt-3.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volumeÂ 37, pages 15972â€“15979, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, RyanÂ J Prenger, and Animashree Anandkumar.

</span>
<span class="ltx_bibblock">Leandojo: Theorem proving with retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou.

</span>
<span class="ltx_bibblock">Back to the future: Towards explainable temporal reasoning with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">Proceedings of the ACM on Web Conference 2024</span>, pages 1963â€“1974, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Lisa: Reasoning segmentation via large language model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 9579â€“9589, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Xiang Yue, Xingwei Qu, GeÂ Zhang, Yao Fu, Wenhao Huang, Huan Sun, YuÂ Su, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Mammoth: Building math generalist models through hybrid instruction tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">arXiv preprint arXiv:2309.05653</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li.

</span>
<span class="ltx_bibblock">Reft: Reasoning with reinforced fine-tuning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 7601â€“7614, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang.

</span>
<span class="ltx_bibblock">Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">Cureus</span>, 15(6), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Hongyang Yang, Xiao-Yang Liu, and ChristinaÂ Dan Wang.

</span>
<span class="ltx_bibblock">Fingpt: Open-source financial large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib193.1.1">arXiv preprint arXiv:2306.06031</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, etÂ al.

</span>
<span class="ltx_bibblock">Disc-lawllm: Fine-tuning large language models for intelligent legal services.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib194.1.1">arXiv preprint arXiv:2309.11325</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Kaiser Sun and Mark Dredze.

</span>
<span class="ltx_bibblock">Amuro &amp; char: Analyzing the relationship between pre-training and fine-tuning of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib195.1.1">arXiv preprint arXiv:2408.06663</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 11:09:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
