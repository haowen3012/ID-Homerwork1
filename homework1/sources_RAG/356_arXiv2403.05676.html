<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design</title>
<!--Generated on Fri Mar  8 21:12:53 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Machine Learning,  ICML" lang="en" name="keywords"/>
<base href="/html/2403.05676v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1" title="1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S2" title="2 Background and Motivation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3" title="3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Our Approach: PipeRAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.SS1" title="3.1 Performance-Centric Observations in RAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Performance-Centric Observations in RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.SS2" title="3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Algorithm-System Co-deisgn in PipeRAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.SS1" title="4.1 Experimental Setup ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.SS2" title="4.2 Perplexity Evaluation ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Perplexity Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.SS3" title="4.3 RAG Efficiency: Performance-Quality Trade-offs ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>RAG Efficiency: Performance-Quality Trade-offs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.SS4" title="4.4 Ablation Study ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S5" title="5 Related Work ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S6" title="6 Conclusion ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A1" title="Appendix A A Motivating Example of Periodic Retrievals ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>A Motivating Example of Periodic Retrievals</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A2" title="Appendix B Detailed Evaluation Setup ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Detailed Evaluation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A3" title="Appendix C Performance Trends on Evolving Hardware ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Performance Trends on Evolving Hardware</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A3.SS1" title="C.1 Factors Influencing Retrieval and Inference Performance ‣ Appendix C Performance Trends on Evolving Hardware ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Factors Influencing Retrieval and Inference Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A3.SS2" title="C.2 Performance Modeling for Future Hardware ‣ Appendix C Performance Trends on Evolving Hardware ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Performance Modeling for Future Hardware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4" title="Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4.SS1" title="D.1 The Effectiveness of Retrievals using Stale Queries ‣ Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>The Effectiveness of Retrievals using Stale Queries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4.SS2" title="D.2 PipeRAG versus Baseline Model that Supports Flexible Retrieval Intervals ‣ Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>PipeRAG versus Baseline Model that Supports Flexible Retrieval Intervals</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A5" title="Appendix E Broader Applicability of PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Broader Applicability of PipeRAG</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-NC-SA 4.0</div><div id="watermark-tr">arXiv:2403.05676v1 [cs.CL] 08 Mar 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document" style="font-size:120%;">PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenqi Jiang<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuai Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Boran Han
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Wang<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id2.1.m1.1"><semantics id="id2.1.m1.1a"><msup id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml"><mi id="id2.1.m1.1.1a" xref="id2.1.m1.1.1.cmml"></mi><mo id="id2.1.m1.1.1.1" xref="id2.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><apply id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1"><ci id="id2.1.m1.1.1.1.cmml" xref="id2.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bernie Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tim Kraska
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.1">Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality.
PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware.
Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6<math alttext="\times" class="ltx_Math" display="inline" id="id3.1.m1.1"><semantics id="id3.1.m1.1a"><mo id="id3.1.m1.1.1" xref="id3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id3.1.m1.1b"><times id="id3.1.m1.1.1.cmml" xref="id3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id3.1.m1.1d">×</annotation></semantics></math> speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div class="ltx_para ltx_noindent" id="p1">
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">${}^{\dagger}$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">${}^{\dagger}$</sup><span class="ltx_note_type">footnotetext: </span>Work conducted while at Amazon.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval-augmented generation (RAG) enhances auto-regressive large language models (LLMs) by conditioning on contextually relevant content retrieved from external databases.
While one retrieval prior to the generation process can be enough when generating short sequences <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib26" title="">2020b</a>; Izacard &amp; Grave, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib13" title="">2020</a>)</cite>, a more general approach involves periodic retrievals throughout the generation <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>; Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>; Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib35" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib18" title="">2023c</a>; Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib40" title="">2022</a>)</cite>. This necessity arises due to the potential shift in the generation context, such as changes in topics. Therefore, periodic retrievals ensure the retrieved content remains relevant to the latest context of the generation (Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A1" title="Appendix A A Motivating Example of Periodic Retrievals ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">A</span></a> showcases a concrete example).
A popular example of this category is <span class="ltx_text ltx_font_smallcaps" id="S1.p1.1.1">Retro</span> <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite>, which tailors the transformer neural network architecture to support the integration of retrieved content at regular intervals.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, periodic retrievals on large databases, potentially comprising trillions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite>, can lead to a significant slowdown of the sequence generation. <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">We ask: can we optimize the system performance of RAG while preserving or even improving generation quality?</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We propose PipeRAG, a pioneering approach to improve RAG efficiency via a collaborative algorithm-system co-design — including a system-aware RAG algorithm and an algorithm-aware retrieval system as overviewed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The foundation of PipeRAG is established on three observations centered on performance. Firstly, the dependencies between retrievals and LLM inferences lead to hardware underutilization, with either the inference or retrieval system being idle at any given time during the generation process (<span class="ltx_text ltx_font_bold" id="S1.p4.1.1">O1</span>). Secondly, the inference latency per token increases with sequence lengths, due to the growing workloads of the attention mechanism in transformer neural networks (<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">O2</span>). Lastly, the retrieval process, particularly the approximate nearest neighbor search, exhibits a trade-off between search latency and search quality (<span class="ltx_text ltx_font_bold" id="S1.p4.1.3">O3</span>).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S1.F1.g1" src="extracted/5456482/fig/overview.png" width="521"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Based on three performance-centric observations (O1<math alttext="\sim" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mo id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><csymbol cd="latexml" id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">∼</annotation></semantics></math>O3), PipeRAG combines a system-aware algorithm integrating pipeline parallelism (S1) with flexible retrieval intervals (S2) and an algorithm-aware retrieval system guided by a performance model (S3).</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_italic" id="S1.p5.1.1">The key idea of PipeRAG is to prefetch content from databases to facilitate pipeline parallelism between the inference and retrieval systems.</span> This solution reduces end-to-end generation latency by allowing simultaneous inference and retrievals, effectively addressing the hardware inefficiencies identified in O1 (<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">S1</span>).
We then enhance this key idea with two additional solutions.
On the model side, PipeRAG modifies <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">Retro</span>’s attention mechanism to support flexible retrieval intervals, because the intervals must be carefully tuned to capitalize the efficiency of pipeline parallelism (<span class="ltx_text ltx_font_bold" id="S1.p5.1.4">S2</span>).
On the system side, the retrieval system adopts a performance model informed by O2 and O3 to dynamically adjust the retrieval search space according to the latency expectation of the upcoming token inferences in the pipeline, thereby optimizing search quality without increasing end-to-end generation latency (<span class="ltx_text ltx_font_bold" id="S1.p5.1.5">S3</span>).</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our evaluation of PipeRAG, involving various evaluation datasets and using large databases based with up to 200 billion tokens, clearly illustrates its efficiency in both generation performance (latency) and generation quality (perplexity). Specifically, the quality-performance Pareto frontier of PipeRAG significantly outperforms that of <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">Retro</span>: PipeRAG can achieve up to 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><times id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">×</annotation></semantics></math> speedup in latency without compromising perplexity; alternatively, maintaining the same latency allows PipeRAG to reduce perplexity by as much as 0.93 compared to <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">Retro</span>. These encouraging results highlight the importance of algorithm-system co-design in retrieval-augmented generation, paving the way for deploying PipeRAG in future RAG systems.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Contributions:</span> We propose PipeRAG, the first algorithm-system co-design approach aimed at improving retrieval-augmented generation efficiency. Specifically:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We design a system-aware RAG algorithm that leverages pipeline parallelism, whose efficiency is further improved by supporting flexible retrieval intervals.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose an algorithm-aware retrieval system that uses performance models to dynamically balance search quality and performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We showcase the impressive performance of PipeRAG in various datasets, demonstrating the importance of algorithm-system co-design in optimizing RAG.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Motivation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Sequence generation quality of LLMs can be improved through periodically retrieving from large token databases <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>; Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>; Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib35" title="">2023</a>)</cite>. Here, periodic retrievals, instead of retrieving only once, are essential in handling potential contextual shifts during generation, such as topic changes, ensuring alignments between the retrieved content and the evolving generation context (a concrete example can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A1" title="Appendix A A Motivating Example of Periodic Retrievals ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">A</span></a>).
<span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">Retro</span> is a representative model in this category <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite>.
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S2.F2" title="Figure 2 ‣ 2 Background and Motivation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.2">Retro</span> integrates a retrieval system with an inference system for token generation. It employs an encoder for incorporating retrieved tokens and a decoder for token generation.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.12"><span class="ltx_text ltx_font_bold" id="S2.p2.12.1">Database construction.</span> A <span class="ltx_text ltx_font_smallcaps" id="S2.p2.12.2">Retro</span> database comprises a large collection of documents segmented into <math alttext="n" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_n</annotation></semantics></math> chunks of tokens <math alttext="S=(S_{1},\ldots,S_{n})" class="ltx_Math" display="inline" id="S2.p2.2.m2.3"><semantics id="S2.p2.2.m2.3a"><mrow id="S2.p2.2.m2.3.3" xref="S2.p2.2.m2.3.3.cmml"><mi id="S2.p2.2.m2.3.3.4" xref="S2.p2.2.m2.3.3.4.cmml">S</mi><mo id="S2.p2.2.m2.3.3.3" xref="S2.p2.2.m2.3.3.3.cmml">=</mo><mrow id="S2.p2.2.m2.3.3.2.2" xref="S2.p2.2.m2.3.3.2.3.cmml"><mo id="S2.p2.2.m2.3.3.2.2.3" stretchy="false" xref="S2.p2.2.m2.3.3.2.3.cmml">(</mo><msub id="S2.p2.2.m2.2.2.1.1.1" xref="S2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S2.p2.2.m2.2.2.1.1.1.2" xref="S2.p2.2.m2.2.2.1.1.1.2.cmml">S</mi><mn id="S2.p2.2.m2.2.2.1.1.1.3" xref="S2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.2.m2.3.3.2.2.4" xref="S2.p2.2.m2.3.3.2.3.cmml">,</mo><mi id="S2.p2.2.m2.1.1" mathvariant="normal" xref="S2.p2.2.m2.1.1.cmml">…</mi><mo id="S2.p2.2.m2.3.3.2.2.5" xref="S2.p2.2.m2.3.3.2.3.cmml">,</mo><msub id="S2.p2.2.m2.3.3.2.2.2" xref="S2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S2.p2.2.m2.3.3.2.2.2.2" xref="S2.p2.2.m2.3.3.2.2.2.2.cmml">S</mi><mi id="S2.p2.2.m2.3.3.2.2.2.3" xref="S2.p2.2.m2.3.3.2.2.2.3.cmml">n</mi></msub><mo id="S2.p2.2.m2.3.3.2.2.6" stretchy="false" xref="S2.p2.2.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.3b"><apply id="S2.p2.2.m2.3.3.cmml" xref="S2.p2.2.m2.3.3"><eq id="S2.p2.2.m2.3.3.3.cmml" xref="S2.p2.2.m2.3.3.3"></eq><ci id="S2.p2.2.m2.3.3.4.cmml" xref="S2.p2.2.m2.3.3.4">𝑆</ci><vector id="S2.p2.2.m2.3.3.2.3.cmml" xref="S2.p2.2.m2.3.3.2.2"><apply id="S2.p2.2.m2.2.2.1.1.1.cmml" xref="S2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S2.p2.2.m2.2.2.1.1.1.2">𝑆</ci><cn id="S2.p2.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">…</ci><apply id="S2.p2.2.m2.3.3.2.2.2.cmml" xref="S2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S2.p2.2.m2.3.3.2.2.2.2">𝑆</ci><ci id="S2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S2.p2.2.m2.3.3.2.2.2.3">𝑛</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.3c">S=(S_{1},\ldots,S_{n})</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.3d">italic_S = ( italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, where each chunk <math alttext="S_{i}" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">S</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝑆</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> spans <math alttext="m" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">italic_m</annotation></semantics></math> tokens. These token chunks are each converted into vector representations <math alttext="R(S)" class="ltx_Math" display="inline" id="S2.p2.5.m5.1"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.2" xref="S2.p2.5.m5.1.2.cmml"><mi id="S2.p2.5.m5.1.2.2" xref="S2.p2.5.m5.1.2.2.cmml">R</mi><mo id="S2.p2.5.m5.1.2.1" xref="S2.p2.5.m5.1.2.1.cmml">⁢</mo><mrow id="S2.p2.5.m5.1.2.3.2" xref="S2.p2.5.m5.1.2.cmml"><mo id="S2.p2.5.m5.1.2.3.2.1" stretchy="false" xref="S2.p2.5.m5.1.2.cmml">(</mo><mi id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">S</mi><mo id="S2.p2.5.m5.1.2.3.2.2" stretchy="false" xref="S2.p2.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.2.cmml" xref="S2.p2.5.m5.1.2"><times id="S2.p2.5.m5.1.2.1.cmml" xref="S2.p2.5.m5.1.2.1"></times><ci id="S2.p2.5.m5.1.2.2.cmml" xref="S2.p2.5.m5.1.2.2">𝑅</ci><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">R(S)</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">italic_R ( italic_S )</annotation></semantics></math>. The database is then structured as a key-value store, with keys being the vector representations <math alttext="R(S)" class="ltx_Math" display="inline" id="S2.p2.6.m6.1"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.2" xref="S2.p2.6.m6.1.2.cmml"><mi id="S2.p2.6.m6.1.2.2" xref="S2.p2.6.m6.1.2.2.cmml">R</mi><mo id="S2.p2.6.m6.1.2.1" xref="S2.p2.6.m6.1.2.1.cmml">⁢</mo><mrow id="S2.p2.6.m6.1.2.3.2" xref="S2.p2.6.m6.1.2.cmml"><mo id="S2.p2.6.m6.1.2.3.2.1" stretchy="false" xref="S2.p2.6.m6.1.2.cmml">(</mo><mi id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml">S</mi><mo id="S2.p2.6.m6.1.2.3.2.2" stretchy="false" xref="S2.p2.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.2.cmml" xref="S2.p2.6.m6.1.2"><times id="S2.p2.6.m6.1.2.1.cmml" xref="S2.p2.6.m6.1.2.1"></times><ci id="S2.p2.6.m6.1.2.2.cmml" xref="S2.p2.6.m6.1.2.2">𝑅</ci><ci id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">R(S)</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.1d">italic_R ( italic_S )</annotation></semantics></math> and values corresponding to the original token chunks <math alttext="S" class="ltx_Math" display="inline" id="S2.p2.7.m7.1"><semantics id="S2.p2.7.m7.1a"><mi id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">italic_S</annotation></semantics></math>, along with <math alttext="F" class="ltx_Math" display="inline" id="S2.p2.8.m8.1"><semantics id="S2.p2.8.m8.1a"><mi id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><ci id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">F</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m8.1d">italic_F</annotation></semantics></math>, in which <math alttext="F_{i}" class="ltx_Math" display="inline" id="S2.p2.9.m9.1"><semantics id="S2.p2.9.m9.1a"><msub id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml"><mi id="S2.p2.9.m9.1.1.2" xref="S2.p2.9.m9.1.1.2.cmml">F</mi><mi id="S2.p2.9.m9.1.1.3" xref="S2.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.1b"><apply id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p2.9.m9.1.1.1.cmml" xref="S2.p2.9.m9.1.1">subscript</csymbol><ci id="S2.p2.9.m9.1.1.2.cmml" xref="S2.p2.9.m9.1.1.2">𝐹</ci><ci id="S2.p2.9.m9.1.1.3.cmml" xref="S2.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.1c">F_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.9.m9.1d">italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> representing the immediately following token chunks of each chunk <math alttext="S_{i}" class="ltx_Math" display="inline" id="S2.p2.10.m10.1"><semantics id="S2.p2.10.m10.1a"><msub id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml"><mi id="S2.p2.10.m10.1.1.2" xref="S2.p2.10.m10.1.1.2.cmml">S</mi><mi id="S2.p2.10.m10.1.1.3" xref="S2.p2.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.1b"><apply id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p2.10.m10.1.1.1.cmml" xref="S2.p2.10.m10.1.1">subscript</csymbol><ci id="S2.p2.10.m10.1.1.2.cmml" xref="S2.p2.10.m10.1.1.2">𝑆</ci><ci id="S2.p2.10.m10.1.1.3.cmml" xref="S2.p2.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.10.m10.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Given a query vector <math alttext="q" class="ltx_Math" display="inline" id="S2.p2.11.m11.1"><semantics id="S2.p2.11.m11.1a"><mi id="S2.p2.11.m11.1.1" xref="S2.p2.11.m11.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.1b"><ci id="S2.p2.11.m11.1.1.cmml" xref="S2.p2.11.m11.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.p2.11.m11.1d">italic_q</annotation></semantics></math>, the database performs an approximate nearest neighbor (ANN) search to retrieve <math alttext="k" class="ltx_Math" display="inline" id="S2.p2.12.m12.1"><semantics id="S2.p2.12.m12.1a"><mi id="S2.p2.12.m12.1.1" xref="S2.p2.12.m12.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p2.12.m12.1b"><ci id="S2.p2.12.m12.1.1.cmml" xref="S2.p2.12.m12.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.12.m12.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p2.12.m12.1d">italic_k</annotation></semantics></math> closest token chunks and their immediately following chunks.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.12"><span class="ltx_text ltx_font_bold" id="S2.p3.12.1">Retrieval process.</span> <span class="ltx_text ltx_font_smallcaps" id="S2.p3.12.2">Retro</span> performs retrievals at regular intervals during the generation phase. Specifically, when generating a sequence of <math alttext="t" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_t</annotation></semantics></math> tokens <math alttext="X=(x_{1},\ldots,x_{t})" class="ltx_Math" display="inline" id="S2.p3.2.m2.3"><semantics id="S2.p3.2.m2.3a"><mrow id="S2.p3.2.m2.3.3" xref="S2.p3.2.m2.3.3.cmml"><mi id="S2.p3.2.m2.3.3.4" xref="S2.p3.2.m2.3.3.4.cmml">X</mi><mo id="S2.p3.2.m2.3.3.3" xref="S2.p3.2.m2.3.3.3.cmml">=</mo><mrow id="S2.p3.2.m2.3.3.2.2" xref="S2.p3.2.m2.3.3.2.3.cmml"><mo id="S2.p3.2.m2.3.3.2.2.3" stretchy="false" xref="S2.p3.2.m2.3.3.2.3.cmml">(</mo><msub id="S2.p3.2.m2.2.2.1.1.1" xref="S2.p3.2.m2.2.2.1.1.1.cmml"><mi id="S2.p3.2.m2.2.2.1.1.1.2" xref="S2.p3.2.m2.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p3.2.m2.2.2.1.1.1.3" xref="S2.p3.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p3.2.m2.3.3.2.2.4" xref="S2.p3.2.m2.3.3.2.3.cmml">,</mo><mi id="S2.p3.2.m2.1.1" mathvariant="normal" xref="S2.p3.2.m2.1.1.cmml">…</mi><mo id="S2.p3.2.m2.3.3.2.2.5" xref="S2.p3.2.m2.3.3.2.3.cmml">,</mo><msub id="S2.p3.2.m2.3.3.2.2.2" xref="S2.p3.2.m2.3.3.2.2.2.cmml"><mi id="S2.p3.2.m2.3.3.2.2.2.2" xref="S2.p3.2.m2.3.3.2.2.2.2.cmml">x</mi><mi id="S2.p3.2.m2.3.3.2.2.2.3" xref="S2.p3.2.m2.3.3.2.2.2.3.cmml">t</mi></msub><mo id="S2.p3.2.m2.3.3.2.2.6" stretchy="false" xref="S2.p3.2.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.3b"><apply id="S2.p3.2.m2.3.3.cmml" xref="S2.p3.2.m2.3.3"><eq id="S2.p3.2.m2.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3"></eq><ci id="S2.p3.2.m2.3.3.4.cmml" xref="S2.p3.2.m2.3.3.4">𝑋</ci><vector id="S2.p3.2.m2.3.3.2.3.cmml" xref="S2.p3.2.m2.3.3.2.2"><apply id="S2.p3.2.m2.2.2.1.1.1.cmml" xref="S2.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.2.2.1.1.1.1.cmml" xref="S2.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.p3.2.m2.2.2.1.1.1.2.cmml" xref="S2.p3.2.m2.2.2.1.1.1.2">𝑥</ci><cn id="S2.p3.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.p3.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">…</ci><apply id="S2.p3.2.m2.3.3.2.2.2.cmml" xref="S2.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.2.2.2.1.cmml" xref="S2.p3.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.p3.2.m2.3.3.2.2.2.2.cmml" xref="S2.p3.2.m2.3.3.2.2.2.2">𝑥</ci><ci id="S2.p3.2.m2.3.3.2.2.2.3.cmml" xref="S2.p3.2.m2.3.3.2.2.2.3">𝑡</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.3c">X=(x_{1},\ldots,x_{t})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.3d">italic_X = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>, <span class="ltx_text ltx_font_smallcaps" id="S2.p3.12.3">Retro</span> partitions <math alttext="X" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">italic_X</annotation></semantics></math> into <math alttext="l" class="ltx_Math" display="inline" id="S2.p3.4.m4.1"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.1d">italic_l</annotation></semantics></math> chunks <math alttext="(C_{1},\ldots,C_{l})" class="ltx_Math" display="inline" id="S2.p3.5.m5.3"><semantics id="S2.p3.5.m5.3a"><mrow id="S2.p3.5.m5.3.3.2" xref="S2.p3.5.m5.3.3.3.cmml"><mo id="S2.p3.5.m5.3.3.2.3" stretchy="false" xref="S2.p3.5.m5.3.3.3.cmml">(</mo><msub id="S2.p3.5.m5.2.2.1.1" xref="S2.p3.5.m5.2.2.1.1.cmml"><mi id="S2.p3.5.m5.2.2.1.1.2" xref="S2.p3.5.m5.2.2.1.1.2.cmml">C</mi><mn id="S2.p3.5.m5.2.2.1.1.3" xref="S2.p3.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.p3.5.m5.3.3.2.4" xref="S2.p3.5.m5.3.3.3.cmml">,</mo><mi id="S2.p3.5.m5.1.1" mathvariant="normal" xref="S2.p3.5.m5.1.1.cmml">…</mi><mo id="S2.p3.5.m5.3.3.2.5" xref="S2.p3.5.m5.3.3.3.cmml">,</mo><msub id="S2.p3.5.m5.3.3.2.2" xref="S2.p3.5.m5.3.3.2.2.cmml"><mi id="S2.p3.5.m5.3.3.2.2.2" xref="S2.p3.5.m5.3.3.2.2.2.cmml">C</mi><mi id="S2.p3.5.m5.3.3.2.2.3" xref="S2.p3.5.m5.3.3.2.2.3.cmml">l</mi></msub><mo id="S2.p3.5.m5.3.3.2.6" stretchy="false" xref="S2.p3.5.m5.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.3b"><vector id="S2.p3.5.m5.3.3.3.cmml" xref="S2.p3.5.m5.3.3.2"><apply id="S2.p3.5.m5.2.2.1.1.cmml" xref="S2.p3.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S2.p3.5.m5.2.2.1.1.1.cmml" xref="S2.p3.5.m5.2.2.1.1">subscript</csymbol><ci id="S2.p3.5.m5.2.2.1.1.2.cmml" xref="S2.p3.5.m5.2.2.1.1.2">𝐶</ci><cn id="S2.p3.5.m5.2.2.1.1.3.cmml" type="integer" xref="S2.p3.5.m5.2.2.1.1.3">1</cn></apply><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">…</ci><apply id="S2.p3.5.m5.3.3.2.2.cmml" xref="S2.p3.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S2.p3.5.m5.3.3.2.2.1.cmml" xref="S2.p3.5.m5.3.3.2.2">subscript</csymbol><ci id="S2.p3.5.m5.3.3.2.2.2.cmml" xref="S2.p3.5.m5.3.3.2.2.2">𝐶</ci><ci id="S2.p3.5.m5.3.3.2.2.3.cmml" xref="S2.p3.5.m5.3.3.2.2.3">𝑙</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.3c">(C_{1},\ldots,C_{l})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m5.3d">( italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math>, each consisting of <math alttext="m" class="ltx_Math" display="inline" id="S2.p3.6.m6.1"><semantics id="S2.p3.6.m6.1a"><mi id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><ci id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S2.p3.6.m6.1d">italic_m</annotation></semantics></math> tokens. Consequently, token <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.p3.7.m7.1"><semantics id="S2.p3.7.m7.1a"><msub id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">x</mi><mi id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">𝑥</ci><ci id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.7.m7.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> belongs to chunk <math alttext="C_{\lceil\frac{i}{m}\rceil}" class="ltx_Math" display="inline" id="S2.p3.8.m8.1"><semantics id="S2.p3.8.m8.1a"><msub id="S2.p3.8.m8.1.2" xref="S2.p3.8.m8.1.2.cmml"><mi id="S2.p3.8.m8.1.2.2" xref="S2.p3.8.m8.1.2.2.cmml">C</mi><mrow id="S2.p3.8.m8.1.1.1.3" xref="S2.p3.8.m8.1.1.1.2.cmml"><mo id="S2.p3.8.m8.1.1.1.3.1" stretchy="false" xref="S2.p3.8.m8.1.1.1.2.1.cmml">⌈</mo><mfrac id="S2.p3.8.m8.1.1.1.1" xref="S2.p3.8.m8.1.1.1.1.cmml"><mi id="S2.p3.8.m8.1.1.1.1.2" xref="S2.p3.8.m8.1.1.1.1.2.cmml">i</mi><mi id="S2.p3.8.m8.1.1.1.1.3" xref="S2.p3.8.m8.1.1.1.1.3.cmml">m</mi></mfrac><mo id="S2.p3.8.m8.1.1.1.3.2" stretchy="false" xref="S2.p3.8.m8.1.1.1.2.1.cmml">⌉</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.1b"><apply id="S2.p3.8.m8.1.2.cmml" xref="S2.p3.8.m8.1.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.2.1.cmml" xref="S2.p3.8.m8.1.2">subscript</csymbol><ci id="S2.p3.8.m8.1.2.2.cmml" xref="S2.p3.8.m8.1.2.2">𝐶</ci><apply id="S2.p3.8.m8.1.1.1.2.cmml" xref="S2.p3.8.m8.1.1.1.3"><ceiling id="S2.p3.8.m8.1.1.1.2.1.cmml" xref="S2.p3.8.m8.1.1.1.3.1"></ceiling><apply id="S2.p3.8.m8.1.1.1.1.cmml" xref="S2.p3.8.m8.1.1.1.1"><divide id="S2.p3.8.m8.1.1.1.1.1.cmml" xref="S2.p3.8.m8.1.1.1.1"></divide><ci id="S2.p3.8.m8.1.1.1.1.2.cmml" xref="S2.p3.8.m8.1.1.1.1.2">𝑖</ci><ci id="S2.p3.8.m8.1.1.1.1.3.cmml" xref="S2.p3.8.m8.1.1.1.1.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.1c">C_{\lceil\frac{i}{m}\rceil}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.8.m8.1d">italic_C start_POSTSUBSCRIPT ⌈ divide start_ARG italic_i end_ARG start_ARG italic_m end_ARG ⌉ end_POSTSUBSCRIPT</annotation></semantics></math>. For the generation of chunk <math alttext="C_{i}" class="ltx_Math" display="inline" id="S2.p3.9.m9.1"><semantics id="S2.p3.9.m9.1a"><msub id="S2.p3.9.m9.1.1" xref="S2.p3.9.m9.1.1.cmml"><mi id="S2.p3.9.m9.1.1.2" xref="S2.p3.9.m9.1.1.2.cmml">C</mi><mi id="S2.p3.9.m9.1.1.3" xref="S2.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.1b"><apply id="S2.p3.9.m9.1.1.cmml" xref="S2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p3.9.m9.1.1.1.cmml" xref="S2.p3.9.m9.1.1">subscript</csymbol><ci id="S2.p3.9.m9.1.1.2.cmml" xref="S2.p3.9.m9.1.1.2">𝐶</ci><ci id="S2.p3.9.m9.1.1.3.cmml" xref="S2.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.9.m9.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_smallcaps" id="S2.p3.12.4">Retro</span> employs the preceding chunk <math alttext="C_{i-1}" class="ltx_Math" display="inline" id="S2.p3.10.m10.1"><semantics id="S2.p3.10.m10.1a"><msub id="S2.p3.10.m10.1.1" xref="S2.p3.10.m10.1.1.cmml"><mi id="S2.p3.10.m10.1.1.2" xref="S2.p3.10.m10.1.1.2.cmml">C</mi><mrow id="S2.p3.10.m10.1.1.3" xref="S2.p3.10.m10.1.1.3.cmml"><mi id="S2.p3.10.m10.1.1.3.2" xref="S2.p3.10.m10.1.1.3.2.cmml">i</mi><mo id="S2.p3.10.m10.1.1.3.1" xref="S2.p3.10.m10.1.1.3.1.cmml">−</mo><mn id="S2.p3.10.m10.1.1.3.3" xref="S2.p3.10.m10.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p3.10.m10.1b"><apply id="S2.p3.10.m10.1.1.cmml" xref="S2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.1.1.cmml" xref="S2.p3.10.m10.1.1">subscript</csymbol><ci id="S2.p3.10.m10.1.1.2.cmml" xref="S2.p3.10.m10.1.1.2">𝐶</ci><apply id="S2.p3.10.m10.1.1.3.cmml" xref="S2.p3.10.m10.1.1.3"><minus id="S2.p3.10.m10.1.1.3.1.cmml" xref="S2.p3.10.m10.1.1.3.1"></minus><ci id="S2.p3.10.m10.1.1.3.2.cmml" xref="S2.p3.10.m10.1.1.3.2">𝑖</ci><cn id="S2.p3.10.m10.1.1.3.3.cmml" type="integer" xref="S2.p3.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.10.m10.1c">C_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.10.m10.1d">italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> as the query to retrieve <math alttext="k" class="ltx_Math" display="inline" id="S2.p3.11.m11.1"><semantics id="S2.p3.11.m11.1a"><mi id="S2.p3.11.m11.1.1" xref="S2.p3.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p3.11.m11.1b"><ci id="S2.p3.11.m11.1.1.cmml" xref="S2.p3.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.11.m11.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p3.11.m11.1d">italic_k</annotation></semantics></math> nearest neighbors <math alttext="\textsc{Ret}(C_{i-1})" class="ltx_Math" display="inline" id="S2.p3.12.m12.1"><semantics id="S2.p3.12.m12.1a"><mrow id="S2.p3.12.m12.1.1" xref="S2.p3.12.m12.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p3.12.m12.1.1.3" mathvariant="normal" xref="S2.p3.12.m12.1.1.3a.cmml">Ret</mtext><mo id="S2.p3.12.m12.1.1.2" xref="S2.p3.12.m12.1.1.2.cmml">⁢</mo><mrow id="S2.p3.12.m12.1.1.1.1" xref="S2.p3.12.m12.1.1.1.1.1.cmml"><mo id="S2.p3.12.m12.1.1.1.1.2" stretchy="false" xref="S2.p3.12.m12.1.1.1.1.1.cmml">(</mo><msub id="S2.p3.12.m12.1.1.1.1.1" xref="S2.p3.12.m12.1.1.1.1.1.cmml"><mi id="S2.p3.12.m12.1.1.1.1.1.2" xref="S2.p3.12.m12.1.1.1.1.1.2.cmml">C</mi><mrow id="S2.p3.12.m12.1.1.1.1.1.3" xref="S2.p3.12.m12.1.1.1.1.1.3.cmml"><mi id="S2.p3.12.m12.1.1.1.1.1.3.2" xref="S2.p3.12.m12.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p3.12.m12.1.1.1.1.1.3.1" xref="S2.p3.12.m12.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.p3.12.m12.1.1.1.1.1.3.3" xref="S2.p3.12.m12.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.p3.12.m12.1.1.1.1.3" stretchy="false" xref="S2.p3.12.m12.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.12.m12.1b"><apply id="S2.p3.12.m12.1.1.cmml" xref="S2.p3.12.m12.1.1"><times id="S2.p3.12.m12.1.1.2.cmml" xref="S2.p3.12.m12.1.1.2"></times><ci id="S2.p3.12.m12.1.1.3a.cmml" xref="S2.p3.12.m12.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p3.12.m12.1.1.3.cmml" mathvariant="normal" xref="S2.p3.12.m12.1.1.3">Ret</mtext></ci><apply id="S2.p3.12.m12.1.1.1.1.1.cmml" xref="S2.p3.12.m12.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.12.m12.1.1.1.1.1.1.cmml" xref="S2.p3.12.m12.1.1.1.1">subscript</csymbol><ci id="S2.p3.12.m12.1.1.1.1.1.2.cmml" xref="S2.p3.12.m12.1.1.1.1.1.2">𝐶</ci><apply id="S2.p3.12.m12.1.1.1.1.1.3.cmml" xref="S2.p3.12.m12.1.1.1.1.1.3"><minus id="S2.p3.12.m12.1.1.1.1.1.3.1.cmml" xref="S2.p3.12.m12.1.1.1.1.1.3.1"></minus><ci id="S2.p3.12.m12.1.1.1.1.1.3.2.cmml" xref="S2.p3.12.m12.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.p3.12.m12.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.p3.12.m12.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.12.m12.1c">\textsc{Ret}(C_{i-1})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.12.m12.1d">Ret ( italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> from the database.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.5"><span class="ltx_text ltx_font_bold" id="S2.p4.5.1">Attention mechanisms.</span>
<span class="ltx_text ltx_font_smallcaps" id="S2.p4.5.2">Retro</span> involves both decoder-to-encoder and encoder-to-decoder attention mechanisms.
The decoder within <span class="ltx_text ltx_font_smallcaps" id="S2.p4.5.3">Retro</span> utilizes chunked cross-attention to integrate the retrieved information encoded by the encoder. To preserve causality, the generation of a chunk <math alttext="C_{i}" class="ltx_Math" display="inline" id="S2.p4.1.m1.1"><semantics id="S2.p4.1.m1.1a"><msub id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mi id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml">C</mi><mi id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2">𝐶</ci><ci id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p4.1.m1.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> incorporates the retrieved tokens <math alttext="\textsc{Ret}(C_{i-1})" class="ltx_Math" display="inline" id="S2.p4.2.m2.1"><semantics id="S2.p4.2.m2.1a"><mrow id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.2.m2.1.1.3" mathvariant="normal" xref="S2.p4.2.m2.1.1.3a.cmml">Ret</mtext><mo id="S2.p4.2.m2.1.1.2" xref="S2.p4.2.m2.1.1.2.cmml">⁢</mo><mrow id="S2.p4.2.m2.1.1.1.1" xref="S2.p4.2.m2.1.1.1.1.1.cmml"><mo id="S2.p4.2.m2.1.1.1.1.2" stretchy="false" xref="S2.p4.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S2.p4.2.m2.1.1.1.1.1" xref="S2.p4.2.m2.1.1.1.1.1.cmml"><mi id="S2.p4.2.m2.1.1.1.1.1.2" xref="S2.p4.2.m2.1.1.1.1.1.2.cmml">C</mi><mrow id="S2.p4.2.m2.1.1.1.1.1.3" xref="S2.p4.2.m2.1.1.1.1.1.3.cmml"><mi id="S2.p4.2.m2.1.1.1.1.1.3.2" xref="S2.p4.2.m2.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p4.2.m2.1.1.1.1.1.3.1" xref="S2.p4.2.m2.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.p4.2.m2.1.1.1.1.1.3.3" xref="S2.p4.2.m2.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.p4.2.m2.1.1.1.1.3" stretchy="false" xref="S2.p4.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><apply id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1"><times id="S2.p4.2.m2.1.1.2.cmml" xref="S2.p4.2.m2.1.1.2"></times><ci id="S2.p4.2.m2.1.1.3a.cmml" xref="S2.p4.2.m2.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.2.m2.1.1.3.cmml" mathvariant="normal" xref="S2.p4.2.m2.1.1.3">Ret</mtext></ci><apply id="S2.p4.2.m2.1.1.1.1.1.cmml" xref="S2.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.p4.2.m2.1.1.1.1.1.1.cmml" xref="S2.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.p4.2.m2.1.1.1.1.1.2.cmml" xref="S2.p4.2.m2.1.1.1.1.1.2">𝐶</ci><apply id="S2.p4.2.m2.1.1.1.1.1.3.cmml" xref="S2.p4.2.m2.1.1.1.1.1.3"><minus id="S2.p4.2.m2.1.1.1.1.1.3.1.cmml" xref="S2.p4.2.m2.1.1.1.1.1.3.1"></minus><ci id="S2.p4.2.m2.1.1.1.1.1.3.2.cmml" xref="S2.p4.2.m2.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.p4.2.m2.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.p4.2.m2.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">\textsc{Ret}(C_{i-1})</annotation><annotation encoding="application/x-llamapun" id="S2.p4.2.m2.1d">Ret ( italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> by integrating the encoder states <math alttext="\textsc{Enc}(\textsc{Ret}(C_{i-1}))" class="ltx_Math" display="inline" id="S2.p4.3.m3.1"><semantics id="S2.p4.3.m3.1a"><mrow id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.3.m3.1.1.3" mathvariant="normal" xref="S2.p4.3.m3.1.1.3a.cmml">Enc</mtext><mo id="S2.p4.3.m3.1.1.2" xref="S2.p4.3.m3.1.1.2.cmml">⁢</mo><mrow id="S2.p4.3.m3.1.1.1.1" xref="S2.p4.3.m3.1.1.1.1.1.cmml"><mo id="S2.p4.3.m3.1.1.1.1.2" stretchy="false" xref="S2.p4.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.p4.3.m3.1.1.1.1.1" xref="S2.p4.3.m3.1.1.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.3.m3.1.1.1.1.1.3" mathvariant="normal" xref="S2.p4.3.m3.1.1.1.1.1.3a.cmml">Ret</mtext><mo id="S2.p4.3.m3.1.1.1.1.1.2" xref="S2.p4.3.m3.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.p4.3.m3.1.1.1.1.1.1.1" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.cmml"><mo id="S2.p4.3.m3.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.p4.3.m3.1.1.1.1.1.1.1.1" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.cmml"><mi id="S2.p4.3.m3.1.1.1.1.1.1.1.1.2" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.2.cmml">C</mi><mrow id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.2" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.1" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.3" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.p4.3.m3.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p4.3.m3.1.1.1.1.3" stretchy="false" xref="S2.p4.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><apply id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1"><times id="S2.p4.3.m3.1.1.2.cmml" xref="S2.p4.3.m3.1.1.2"></times><ci id="S2.p4.3.m3.1.1.3a.cmml" xref="S2.p4.3.m3.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.3.m3.1.1.3.cmml" mathvariant="normal" xref="S2.p4.3.m3.1.1.3">Enc</mtext></ci><apply id="S2.p4.3.m3.1.1.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1.1"><times id="S2.p4.3.m3.1.1.1.1.1.2.cmml" xref="S2.p4.3.m3.1.1.1.1.1.2"></times><ci id="S2.p4.3.m3.1.1.1.1.1.3a.cmml" xref="S2.p4.3.m3.1.1.1.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.3.m3.1.1.1.1.1.3.cmml" mathvariant="normal" xref="S2.p4.3.m3.1.1.1.1.1.3">Ret</mtext></ci><apply id="S2.p4.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p4.3.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p4.3.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.2">𝐶</ci><apply id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3"><minus id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.p4.3.m3.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">\textsc{Enc}(\textsc{Ret}(C_{i-1}))</annotation><annotation encoding="application/x-llamapun" id="S2.p4.3.m3.1d">Enc ( Ret ( italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) )</annotation></semantics></math>.
On the other hand, the <span class="ltx_text ltx_font_smallcaps" id="S2.p4.5.4">Retro</span> encoder states <math alttext="\textsc{Enc}(\textsc{Ret}(C_{i-1}))" class="ltx_Math" display="inline" id="S2.p4.4.m4.1"><semantics id="S2.p4.4.m4.1a"><mrow id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.4.m4.1.1.3" mathvariant="normal" xref="S2.p4.4.m4.1.1.3a.cmml">Enc</mtext><mo id="S2.p4.4.m4.1.1.2" xref="S2.p4.4.m4.1.1.2.cmml">⁢</mo><mrow id="S2.p4.4.m4.1.1.1.1" xref="S2.p4.4.m4.1.1.1.1.1.cmml"><mo id="S2.p4.4.m4.1.1.1.1.2" stretchy="false" xref="S2.p4.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S2.p4.4.m4.1.1.1.1.1" xref="S2.p4.4.m4.1.1.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.4.m4.1.1.1.1.1.3" mathvariant="normal" xref="S2.p4.4.m4.1.1.1.1.1.3a.cmml">Ret</mtext><mo id="S2.p4.4.m4.1.1.1.1.1.2" xref="S2.p4.4.m4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.p4.4.m4.1.1.1.1.1.1.1" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mo id="S2.p4.4.m4.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.p4.4.m4.1.1.1.1.1.1.1.1" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S2.p4.4.m4.1.1.1.1.1.1.1.1.2" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml">C</mi><mrow id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.2" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.1" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.3" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.p4.4.m4.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p4.4.m4.1.1.1.1.3" stretchy="false" xref="S2.p4.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><apply id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1"><times id="S2.p4.4.m4.1.1.2.cmml" xref="S2.p4.4.m4.1.1.2"></times><ci id="S2.p4.4.m4.1.1.3a.cmml" xref="S2.p4.4.m4.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.4.m4.1.1.3.cmml" mathvariant="normal" xref="S2.p4.4.m4.1.1.3">Enc</mtext></ci><apply id="S2.p4.4.m4.1.1.1.1.1.cmml" xref="S2.p4.4.m4.1.1.1.1"><times id="S2.p4.4.m4.1.1.1.1.1.2.cmml" xref="S2.p4.4.m4.1.1.1.1.1.2"></times><ci id="S2.p4.4.m4.1.1.1.1.1.3a.cmml" xref="S2.p4.4.m4.1.1.1.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.4.m4.1.1.1.1.1.3.cmml" mathvariant="normal" xref="S2.p4.4.m4.1.1.1.1.1.3">Ret</mtext></ci><apply id="S2.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p4.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.2">𝐶</ci><apply id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3"><minus id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.p4.4.m4.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">\textsc{Enc}(\textsc{Ret}(C_{i-1}))</annotation><annotation encoding="application/x-llamapun" id="S2.p4.4.m4.1d">Enc ( Ret ( italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) )</annotation></semantics></math> integrates the decoder’s states of the <math alttext="\textsc{Dec}(C_{i-1})" class="ltx_Math" display="inline" id="S2.p4.5.m5.1"><semantics id="S2.p4.5.m5.1a"><mrow id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S2.p4.5.m5.1.1.3" mathvariant="normal" xref="S2.p4.5.m5.1.1.3a.cmml">Dec</mtext><mo id="S2.p4.5.m5.1.1.2" xref="S2.p4.5.m5.1.1.2.cmml">⁢</mo><mrow id="S2.p4.5.m5.1.1.1.1" xref="S2.p4.5.m5.1.1.1.1.1.cmml"><mo id="S2.p4.5.m5.1.1.1.1.2" stretchy="false" xref="S2.p4.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S2.p4.5.m5.1.1.1.1.1" xref="S2.p4.5.m5.1.1.1.1.1.cmml"><mi id="S2.p4.5.m5.1.1.1.1.1.2" xref="S2.p4.5.m5.1.1.1.1.1.2.cmml">C</mi><mrow id="S2.p4.5.m5.1.1.1.1.1.3" xref="S2.p4.5.m5.1.1.1.1.1.3.cmml"><mi id="S2.p4.5.m5.1.1.1.1.1.3.2" xref="S2.p4.5.m5.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p4.5.m5.1.1.1.1.1.3.1" xref="S2.p4.5.m5.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.p4.5.m5.1.1.1.1.1.3.3" xref="S2.p4.5.m5.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.p4.5.m5.1.1.1.1.3" stretchy="false" xref="S2.p4.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.1b"><apply id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1"><times id="S2.p4.5.m5.1.1.2.cmml" xref="S2.p4.5.m5.1.1.2"></times><ci id="S2.p4.5.m5.1.1.3a.cmml" xref="S2.p4.5.m5.1.1.3"><mtext class="ltx_font_smallcaps" id="S2.p4.5.m5.1.1.3.cmml" mathvariant="normal" xref="S2.p4.5.m5.1.1.3">Dec</mtext></ci><apply id="S2.p4.5.m5.1.1.1.1.1.cmml" xref="S2.p4.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S2.p4.5.m5.1.1.1.1.1.1.cmml" xref="S2.p4.5.m5.1.1.1.1">subscript</csymbol><ci id="S2.p4.5.m5.1.1.1.1.1.2.cmml" xref="S2.p4.5.m5.1.1.1.1.1.2">𝐶</ci><apply id="S2.p4.5.m5.1.1.1.1.1.3.cmml" xref="S2.p4.5.m5.1.1.1.1.1.3"><minus id="S2.p4.5.m5.1.1.1.1.1.3.1.cmml" xref="S2.p4.5.m5.1.1.1.1.1.3.1"></minus><ci id="S2.p4.5.m5.1.1.1.1.1.3.2.cmml" xref="S2.p4.5.m5.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.p4.5.m5.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.p4.5.m5.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.1c">\textsc{Dec}(C_{i-1})</annotation><annotation encoding="application/x-llamapun" id="S2.p4.5.m5.1d">Dec ( italic_C start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> via a standard cross-attention (CA) mechanism, such that the encoder can blend the retrieved information with the generation context.
Because both decoder-to-encoder and encoder-to-decoder attention mechanisms operate on a chunk-wise basis, <span class="ltx_text ltx_font_smallcaps" id="S2.p4.5.5">Retro</span> avoids the excessive computational demands of attending to all previous retrieval and generation states.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="396" id="S2.F2.g1" src="extracted/5456482/fig/RETRO.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Retrieval-augmented generation with <span class="ltx_text ltx_font_smallcaps" id="S2.F2.2.1">Retro</span>.</figcaption>
</figure>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Motivation: improving RAG efficiency.</span>
Although periodically retrieving tokens from a large database can effectively improve the generation quality of LLMs, frequent retrievals can account for a considerable portion of the total generation time, thereby significantly slowing down the end-to-end generation process.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">In this paper, we ask the following question: <span class="ltx_text ltx_font_bold" id="S2.p6.1.1">is it possible to further enhance the efficiency of retrieval augmented generation?</span> Here, we conceptualize <span class="ltx_text ltx_font_italic" id="S2.p6.1.2">RAG efficiency</span> as a Pareto frontier considering two objectives: <span class="ltx_text ltx_font_italic" id="S2.p6.1.3">generation quality</span> and <span class="ltx_text ltx_font_italic" id="S2.p6.1.4">system performance</span>.
Specifically, given a quality requirement (achieving certain perplexity), can we optimize RAG’s system performance (reducing generation latency)? On the other hand, given a system performance requirement, can we improve the quality of generation?</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Approach: PipeRAG</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose PipeRAG, a novel retrieval augmented generation approach to improve the performance-quality Pareto frontier through an in-depth algorithm-system co-design.
The development of PipeRAG stems from performance-centric observations revealing (1) the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">fundamental</span> system inefficiencies in existing RAG algorithms and (2) the distinct performance characteristics of LLM inference and retrieval systems.
Based on these observations, PipeRAG includes (1) a system-aware RAG algorithm to address the system inefficiencies and (2) an algorithm-aware retrieval system to dynamically balance retrieval quality and latency.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Performance-Centric Observations in RAG</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">O1: Hardware inefficiency due to RAG dependencies.</span> A conventional RAG process introduces dependencies between retrievals and inferences: the current generation context is used as a query to retrieve relevant token chunks stored in the database; the inference process must wait for the retrieval to finish before it can continue generating a few more tokens, until the next retrieval is triggered.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">A RAG system typically comprises two sub-systems: the retrieval system and the inference system, each hosted on separate hardware platforms.
AI accelerators such as GPUs and TPUs are the ideal hardware platforms for LLM inference due to the high demands for computation and memory bandwidth during inference.
On the other hand, the retrieval systems consisting of large databases are usually not based on GPUs. This is because (1) the limited memory capacity of individual GPUs (GPUs adopt high-bandwidth memory that is fast but limited in capacity) makes the hosting of large databases cost-prohibitive, necessitating the setup comprising many GPUs, and (2) the communication bandwidth between the CPU and GPU is significantly lower compared to GPU’s device memory bandwidth, thus the CPU-GPU solution, in which database vectors are stored in CPU-side memory and then transferred to GPUs at query time, could be exceedingly slow.
Given the capacity requirements, the retrieval system is typically CPU-based <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib26" title="">2020b</a>)</cite>, with the database either held in substantial main memory (DRAM), or, in more budget-friendly setups, stored on disks.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Given that the two systems are based on separate hardware, the dependencies between retrievals and inferences in RAG result in significant underutilization of hardware resources. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates this inefficiency using RETRO as a representative example: due to the dependencies, either the inference or retrieval system is idle at any given time during the generation process, leading to hardware inefficiencies.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">O2: Increasing inference time with sequence length.</span>
In a standard transformer neural network <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib41" title="">2017</a>)</cite>, the cost of generating each new token correlates with the sequence length, rather than remaining a constant. This is due to the attention mechanism in transformers: although the workload of the fully-connected layers remains constant throughout the generation process, the cost of attention layers increases with the sequence length <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib2" title="">2020</a>)</cite>. Specifically, for each new token generated, the query states (Q) of the most recent token are compared against the key states (K) of all preceding tokens to calculate relevance scores. These scores are then utilized for a weighted sum over the value states (V) (note that the queries, keys, and values mentioned here under the context of transformers are distinct from those terms in RAG systems).
Consequently, the inference cost per token can be approximated as a linear function to sequence length.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">O3: Trade-offs between retrieval quality and latency.</span> Large-scale vector search in RAG employs approximate nearest neighbor (ANN) search instead of exact nearest neighbor search due to the latter’s prohibitive cost on large databases. In ANN search, database vectors are indexed, with popular choices including clustering-based inverted-file (IVF) indexes <cite class="ltx_cite ltx_citemacro_citep">(Sivic &amp; Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib39" title="">2003</a>)</cite> and graph-based indexes <cite class="ltx_cite ltx_citemacro_citep">(Malkov et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib28" title="">2014</a>; Malkov &amp; Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib29" title="">2018</a>)</cite>. Optionally, database vectors may also be compressed via product quantization (PQ) <cite class="ltx_cite ltx_citemacro_citep">(Jegou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib15" title="">2010</a>)</cite> to shrink database sizes and reduce memory bandwidth usage at query time at the expense of search accuracy. During a search, a query vector is only compared against a subset of database vectors selected by the index.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Regardless of the index types, there exists a <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.1">fundamental</span> trade-off between search quality and latency in ANN search. Typically, the index first directs the search towards those database vectors that are most likely to be the nearest neighbors of the query vector, and then gradually expands the search space. The number of database vectors scanned per query can be directly or indirectly controlled by ANN search hyper-parameters. Expanding the search space would enhance the probability of finding the query vector’s true nearest neighbors in the database (improved search quality), but also would also lead to higher latency (lower search performance) due to the greater number of comparisons between query vectors and database vectors.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a> visualizes the relationship between search quality and latency <cite class="ltx_cite ltx_citemacro_citep">(Jegou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib15" title="">2010</a>)</cite>. As the search space expands (number of scanned database vectors), the search quality (recall of the retrieval) gradually improves until reaching a plateau where the nearest neighbors are likely found. Simultaneously, the search cost (latency) grows linearly with the search space, with an initial cost of scanning the index (which could be zero in some graph-based indexes).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Algorithm-System Co-deisgn in PipeRAG</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Given the aforementioned performance-centric observations, we propose PipeRAG, an algorithm-system co-design approach aimed at enhancing RAG’s performance-quality Pareto frontier.
PipeRAG addresses the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">fundamental</span> issue of hardware inefficiency (O1) by employing pipeline parallelism (S1) and allowing flexible retrieval intervals (S2). Leveraging the distinct performance characteristics of the inference and retrieval sub-systems (O2, O3), PipeRAG further offers an option to enable automatic search space selection within the retrieval system, facilitating high-quality generation without introducing additional generation latency.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">S1: Pipeline parallelism across RAG sub-systems.</span>
Because the hardware under-utilization issue in RAG is caused by dependencies between retrievals and inferences, our first solution is about revisiting RAG algorithms to enable pipeline parallelism: the retrievals and inferences should be executed concurrently, thus overlapping their execution latency and improving hardware utilization.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">To facilitate pipeline parallelism, we relax the RAG dependencies as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>: instead of depending on the content retrieved using the query representing the most recent generation context (the latest generated tokens),
the inference process can utilize a slightly older, or <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">stale</span>, query window to <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">prefetch</span> content from the database.
The intuition here is that if the stale query window closely aligns with the latest generation context, it is likely to retrieve content similar to that obtained using the most recent query tokens.
Once the dependency constraint is relaxed, retrievals can be proactively initiated to <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.3">prefetch</span> content from the database, thus enabling pipeline parallelism as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.12">Formally, when generating token chunk <math alttext="C_{j+1}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">C</mi><mrow id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.p4.1.m1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝐶</ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><plus id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.1"></plus><ci id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2">𝑗</ci><cn id="S3.SS2.p4.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">C_{j+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_C start_POSTSUBSCRIPT italic_j + 1 end_POSTSUBSCRIPT</annotation></semantics></math>, PipeRAG does not use the immediately preceding chunk as the query <math alttext="Q=C_{j}=(x_{jm},\ldots,x_{jm+m-1})" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.3"><semantics id="S3.SS2.p4.2.m2.3a"><mrow id="S3.SS2.p4.2.m2.3.3" xref="S3.SS2.p4.2.m2.3.3.cmml"><mi id="S3.SS2.p4.2.m2.3.3.4" xref="S3.SS2.p4.2.m2.3.3.4.cmml">Q</mi><mo id="S3.SS2.p4.2.m2.3.3.5" xref="S3.SS2.p4.2.m2.3.3.5.cmml">=</mo><msub id="S3.SS2.p4.2.m2.3.3.6" xref="S3.SS2.p4.2.m2.3.3.6.cmml"><mi id="S3.SS2.p4.2.m2.3.3.6.2" xref="S3.SS2.p4.2.m2.3.3.6.2.cmml">C</mi><mi id="S3.SS2.p4.2.m2.3.3.6.3" xref="S3.SS2.p4.2.m2.3.3.6.3.cmml">j</mi></msub><mo id="S3.SS2.p4.2.m2.3.3.7" xref="S3.SS2.p4.2.m2.3.3.7.cmml">=</mo><mrow id="S3.SS2.p4.2.m2.3.3.2.2" xref="S3.SS2.p4.2.m2.3.3.2.3.cmml"><mo id="S3.SS2.p4.2.m2.3.3.2.2.3" stretchy="false" xref="S3.SS2.p4.2.m2.3.3.2.3.cmml">(</mo><msub id="S3.SS2.p4.2.m2.2.2.1.1.1" xref="S3.SS2.p4.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p4.2.m2.2.2.1.1.1.2" xref="S3.SS2.p4.2.m2.2.2.1.1.1.2.cmml">x</mi><mrow id="S3.SS2.p4.2.m2.2.2.1.1.1.3" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p4.2.m2.2.2.1.1.1.3.2" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.p4.2.m2.2.2.1.1.1.3.1" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.2.2.1.1.1.3.3" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.3.cmml">m</mi></mrow></msub><mo id="S3.SS2.p4.2.m2.3.3.2.2.4" xref="S3.SS2.p4.2.m2.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p4.2.m2.1.1" mathvariant="normal" xref="S3.SS2.p4.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.p4.2.m2.3.3.2.2.5" xref="S3.SS2.p4.2.m2.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p4.2.m2.3.3.2.2.2" xref="S3.SS2.p4.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p4.2.m2.3.3.2.2.2.2" xref="S3.SS2.p4.2.m2.3.3.2.2.2.2.cmml">x</mi><mrow id="S3.SS2.p4.2.m2.3.3.2.2.2.3" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.cmml"><mrow id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.cmml"><mrow id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.cmml"><mi id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.2" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.2.cmml">j</mi><mo id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.1" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.3" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.1" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.1.cmml">+</mo><mi id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.3" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p4.2.m2.3.3.2.2.2.3.1" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.1.cmml">−</mo><mn id="S3.SS2.p4.2.m2.3.3.2.2.2.3.3" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p4.2.m2.3.3.2.2.6" stretchy="false" xref="S3.SS2.p4.2.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.3b"><apply id="S3.SS2.p4.2.m2.3.3.cmml" xref="S3.SS2.p4.2.m2.3.3"><and id="S3.SS2.p4.2.m2.3.3a.cmml" xref="S3.SS2.p4.2.m2.3.3"></and><apply id="S3.SS2.p4.2.m2.3.3b.cmml" xref="S3.SS2.p4.2.m2.3.3"><eq id="S3.SS2.p4.2.m2.3.3.5.cmml" xref="S3.SS2.p4.2.m2.3.3.5"></eq><ci id="S3.SS2.p4.2.m2.3.3.4.cmml" xref="S3.SS2.p4.2.m2.3.3.4">𝑄</ci><apply id="S3.SS2.p4.2.m2.3.3.6.cmml" xref="S3.SS2.p4.2.m2.3.3.6"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.3.3.6.1.cmml" xref="S3.SS2.p4.2.m2.3.3.6">subscript</csymbol><ci id="S3.SS2.p4.2.m2.3.3.6.2.cmml" xref="S3.SS2.p4.2.m2.3.3.6.2">𝐶</ci><ci id="S3.SS2.p4.2.m2.3.3.6.3.cmml" xref="S3.SS2.p4.2.m2.3.3.6.3">𝑗</ci></apply></apply><apply id="S3.SS2.p4.2.m2.3.3c.cmml" xref="S3.SS2.p4.2.m2.3.3"><eq id="S3.SS2.p4.2.m2.3.3.7.cmml" xref="S3.SS2.p4.2.m2.3.3.7"></eq><share href="#S3.SS2.p4.2.m2.3.3.6.cmml" id="S3.SS2.p4.2.m2.3.3d.cmml" xref="S3.SS2.p4.2.m2.3.3"></share><vector id="S3.SS2.p4.2.m2.3.3.2.3.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2"><apply id="S3.SS2.p4.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1.2">𝑥</ci><apply id="S3.SS2.p4.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3"><times id="S3.SS2.p4.2.m2.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.1"></times><ci id="S3.SS2.p4.2.m2.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.2">𝑗</ci><ci id="S3.SS2.p4.2.m2.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.2.2.1.1.1.3.3">𝑚</ci></apply></apply><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">…</ci><apply id="S3.SS2.p4.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p4.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.2">𝑥</ci><apply id="S3.SS2.p4.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3"><minus id="S3.SS2.p4.2.m2.3.3.2.2.2.3.1.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.1"></minus><apply id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2"><plus id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.1.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.1"></plus><apply id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2"><times id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.1.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.1"></times><ci id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.2.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.2">𝑗</ci><ci id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.3.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.3.cmml" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.2.3">𝑚</ci></apply><cn id="S3.SS2.p4.2.m2.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS2.p4.2.m2.3.3.2.2.2.3.3">1</cn></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.3c">Q=C_{j}=(x_{jm},\ldots,x_{jm+m-1})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.3d">italic_Q = italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_j italic_m end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_j italic_m + italic_m - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> to retrieve <math alttext="\textsc{Ret}(Q)" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.2" xref="S3.SS2.p4.3.m3.1.2.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.3.m3.1.2.2" mathvariant="normal" xref="S3.SS2.p4.3.m3.1.2.2a.cmml">Ret</mtext><mo id="S3.SS2.p4.3.m3.1.2.1" xref="S3.SS2.p4.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p4.3.m3.1.2.3.2" xref="S3.SS2.p4.3.m3.1.2.cmml"><mo id="S3.SS2.p4.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">Q</mi><mo id="S3.SS2.p4.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.2.cmml" xref="S3.SS2.p4.3.m3.1.2"><times id="S3.SS2.p4.3.m3.1.2.1.cmml" xref="S3.SS2.p4.3.m3.1.2.1"></times><ci id="S3.SS2.p4.3.m3.1.2.2a.cmml" xref="S3.SS2.p4.3.m3.1.2.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.3.m3.1.2.2.cmml" mathvariant="normal" xref="S3.SS2.p4.3.m3.1.2.2">Ret</mtext></ci><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\textsc{Ret}(Q)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">Ret ( italic_Q )</annotation></semantics></math>. Instead, it opts for a stale token window <math alttext="\hat{Q}=(x_{jm-s},\ldots,x_{jm+m-1-s})" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.3"><semantics id="S3.SS2.p4.4.m4.3a"><mrow id="S3.SS2.p4.4.m4.3.3" xref="S3.SS2.p4.4.m4.3.3.cmml"><mover accent="true" id="S3.SS2.p4.4.m4.3.3.4" xref="S3.SS2.p4.4.m4.3.3.4.cmml"><mi id="S3.SS2.p4.4.m4.3.3.4.2" xref="S3.SS2.p4.4.m4.3.3.4.2.cmml">Q</mi><mo id="S3.SS2.p4.4.m4.3.3.4.1" xref="S3.SS2.p4.4.m4.3.3.4.1.cmml">^</mo></mover><mo id="S3.SS2.p4.4.m4.3.3.3" xref="S3.SS2.p4.4.m4.3.3.3.cmml">=</mo><mrow id="S3.SS2.p4.4.m4.3.3.2.2" xref="S3.SS2.p4.4.m4.3.3.2.3.cmml"><mo id="S3.SS2.p4.4.m4.3.3.2.2.3" stretchy="false" xref="S3.SS2.p4.4.m4.3.3.2.3.cmml">(</mo><msub id="S3.SS2.p4.4.m4.2.2.1.1.1" xref="S3.SS2.p4.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS2.p4.4.m4.2.2.1.1.1.2" xref="S3.SS2.p4.4.m4.2.2.1.1.1.2.cmml">x</mi><mrow id="S3.SS2.p4.4.m4.2.2.1.1.1.3" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.cmml"><mrow id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.cmml"><mi id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.2" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.2.cmml">j</mi><mo id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.1" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.3" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p4.4.m4.2.2.1.1.1.3.1" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.1.cmml">−</mo><mi id="S3.SS2.p4.4.m4.2.2.1.1.1.3.3" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.3.cmml">s</mi></mrow></msub><mo id="S3.SS2.p4.4.m4.3.3.2.2.4" xref="S3.SS2.p4.4.m4.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p4.4.m4.1.1" mathvariant="normal" xref="S3.SS2.p4.4.m4.1.1.cmml">…</mi><mo id="S3.SS2.p4.4.m4.3.3.2.2.5" xref="S3.SS2.p4.4.m4.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p4.4.m4.3.3.2.2.2" xref="S3.SS2.p4.4.m4.3.3.2.2.2.cmml"><mi id="S3.SS2.p4.4.m4.3.3.2.2.2.2" xref="S3.SS2.p4.4.m4.3.3.2.2.2.2.cmml">x</mi><mrow id="S3.SS2.p4.4.m4.3.3.2.2.2.3" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.cmml"><mrow id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.cmml"><mrow id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.cmml"><mi id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.2" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.2.cmml">j</mi><mo id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.1" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.3" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.1" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.1.cmml">+</mo><mi id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.3" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p4.4.m4.3.3.2.2.2.3.1" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.1.cmml">−</mo><mn id="S3.SS2.p4.4.m4.3.3.2.2.2.3.3" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.3.cmml">1</mn><mo id="S3.SS2.p4.4.m4.3.3.2.2.2.3.1a" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.1.cmml">−</mo><mi id="S3.SS2.p4.4.m4.3.3.2.2.2.3.4" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.4.cmml">s</mi></mrow></msub><mo id="S3.SS2.p4.4.m4.3.3.2.2.6" stretchy="false" xref="S3.SS2.p4.4.m4.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.3b"><apply id="S3.SS2.p4.4.m4.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3"><eq id="S3.SS2.p4.4.m4.3.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.3"></eq><apply id="S3.SS2.p4.4.m4.3.3.4.cmml" xref="S3.SS2.p4.4.m4.3.3.4"><ci id="S3.SS2.p4.4.m4.3.3.4.1.cmml" xref="S3.SS2.p4.4.m4.3.3.4.1">^</ci><ci id="S3.SS2.p4.4.m4.3.3.4.2.cmml" xref="S3.SS2.p4.4.m4.3.3.4.2">𝑄</ci></apply><vector id="S3.SS2.p4.4.m4.3.3.2.3.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2"><apply id="S3.SS2.p4.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.2">𝑥</ci><apply id="S3.SS2.p4.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3"><minus id="S3.SS2.p4.4.m4.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.1"></minus><apply id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2"><times id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.1.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.1"></times><ci id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.2.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.2">𝑗</ci><ci id="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.3.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.2.3">𝑚</ci></apply><ci id="S3.SS2.p4.4.m4.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.2.2.1.1.1.3.3">𝑠</ci></apply></apply><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">…</ci><apply id="S3.SS2.p4.4.m4.3.3.2.2.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p4.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.2">𝑥</ci><apply id="S3.SS2.p4.4.m4.3.3.2.2.2.3.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3"><minus id="S3.SS2.p4.4.m4.3.3.2.2.2.3.1.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.1"></minus><apply id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2"><plus id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.1.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.1"></plus><apply id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2"><times id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.1.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.1"></times><ci id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.2">𝑗</ci><ci id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.3.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.3.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.2.3">𝑚</ci></apply><cn id="S3.SS2.p4.4.m4.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.3">1</cn><ci id="S3.SS2.p4.4.m4.3.3.2.2.2.3.4.cmml" xref="S3.SS2.p4.4.m4.3.3.2.2.2.3.4">𝑠</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.3c">\hat{Q}=(x_{jm-s},\ldots,x_{jm+m-1-s})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.3d">over^ start_ARG italic_Q end_ARG = ( italic_x start_POSTSUBSCRIPT italic_j italic_m - italic_s end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_j italic_m + italic_m - 1 - italic_s end_POSTSUBSCRIPT )</annotation></semantics></math> as an approximate query, offset by <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">italic_s</annotation></semantics></math> tokens from the latest query window. Subsequently, <math alttext="\hat{\textsc{Ret}(Q)}=\textsc{Shift}(\textsc{Ret}(\hat{Q}),s)" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.4"><semantics id="S3.SS2.p4.6.m6.4a"><mrow id="S3.SS2.p4.6.m6.4.4" xref="S3.SS2.p4.6.m6.4.4.cmml"><mover accent="true" id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mrow id="S3.SS2.p4.6.m6.1.1.1" xref="S3.SS2.p4.6.m6.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.1.1.1.3" mathvariant="normal" xref="S3.SS2.p4.6.m6.1.1.1.3a.cmml">Ret</mtext><mo id="S3.SS2.p4.6.m6.1.1.1.2" xref="S3.SS2.p4.6.m6.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p4.6.m6.1.1.1.4.2" xref="S3.SS2.p4.6.m6.1.1.1.cmml"><mo id="S3.SS2.p4.6.m6.1.1.1.4.2.1" stretchy="false" xref="S3.SS2.p4.6.m6.1.1.1.cmml">(</mo><mi id="S3.SS2.p4.6.m6.1.1.1.1" xref="S3.SS2.p4.6.m6.1.1.1.1.cmml">Q</mi><mo id="S3.SS2.p4.6.m6.1.1.1.4.2.2" stretchy="false" xref="S3.SS2.p4.6.m6.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">^</mo></mover><mo id="S3.SS2.p4.6.m6.4.4.2" xref="S3.SS2.p4.6.m6.4.4.2.cmml">=</mo><mrow id="S3.SS2.p4.6.m6.4.4.1" xref="S3.SS2.p4.6.m6.4.4.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.4.4.1.3" mathvariant="normal" xref="S3.SS2.p4.6.m6.4.4.1.3a.cmml">Shift</mtext><mo id="S3.SS2.p4.6.m6.4.4.1.2" xref="S3.SS2.p4.6.m6.4.4.1.2.cmml">⁢</mo><mrow id="S3.SS2.p4.6.m6.4.4.1.1.1" xref="S3.SS2.p4.6.m6.4.4.1.1.2.cmml"><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.2" stretchy="false" xref="S3.SS2.p4.6.m6.4.4.1.1.2.cmml">(</mo><mrow id="S3.SS2.p4.6.m6.4.4.1.1.1.1" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.4.4.1.1.1.1.2" mathvariant="normal" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.2a.cmml">Ret</mtext><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.1.1" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS2.p4.6.m6.4.4.1.1.1.1.3.2" xref="S3.SS2.p4.6.m6.2.2.cmml"><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.p4.6.m6.2.2.cmml">(</mo><mover accent="true" id="S3.SS2.p4.6.m6.2.2" xref="S3.SS2.p4.6.m6.2.2.cmml"><mi id="S3.SS2.p4.6.m6.2.2.2" xref="S3.SS2.p4.6.m6.2.2.2.cmml">Q</mi><mo id="S3.SS2.p4.6.m6.2.2.1" xref="S3.SS2.p4.6.m6.2.2.1.cmml">^</mo></mover><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.p4.6.m6.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.3" xref="S3.SS2.p4.6.m6.4.4.1.1.2.cmml">,</mo><mi id="S3.SS2.p4.6.m6.3.3" xref="S3.SS2.p4.6.m6.3.3.cmml">s</mi><mo id="S3.SS2.p4.6.m6.4.4.1.1.1.4" stretchy="false" xref="S3.SS2.p4.6.m6.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.4b"><apply id="S3.SS2.p4.6.m6.4.4.cmml" xref="S3.SS2.p4.6.m6.4.4"><eq id="S3.SS2.p4.6.m6.4.4.2.cmml" xref="S3.SS2.p4.6.m6.4.4.2"></eq><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">^</ci><apply id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1.1"><times id="S3.SS2.p4.6.m6.1.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.1.2"></times><ci id="S3.SS2.p4.6.m6.1.1.1.3a.cmml" xref="S3.SS2.p4.6.m6.1.1.1.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.1.1.1.3.cmml" mathvariant="normal" xref="S3.SS2.p4.6.m6.1.1.1.3">Ret</mtext></ci><ci id="S3.SS2.p4.6.m6.1.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1.1.1">𝑄</ci></apply></apply><apply id="S3.SS2.p4.6.m6.4.4.1.cmml" xref="S3.SS2.p4.6.m6.4.4.1"><times id="S3.SS2.p4.6.m6.4.4.1.2.cmml" xref="S3.SS2.p4.6.m6.4.4.1.2"></times><ci id="S3.SS2.p4.6.m6.4.4.1.3a.cmml" xref="S3.SS2.p4.6.m6.4.4.1.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.4.4.1.3.cmml" mathvariant="normal" xref="S3.SS2.p4.6.m6.4.4.1.3">Shift</mtext></ci><interval closure="open" id="S3.SS2.p4.6.m6.4.4.1.1.2.cmml" xref="S3.SS2.p4.6.m6.4.4.1.1.1"><apply id="S3.SS2.p4.6.m6.4.4.1.1.1.1.cmml" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1"><times id="S3.SS2.p4.6.m6.4.4.1.1.1.1.1.cmml" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.1"></times><ci id="S3.SS2.p4.6.m6.4.4.1.1.1.1.2a.cmml" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.6.m6.4.4.1.1.1.1.2.cmml" mathvariant="normal" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.2">Ret</mtext></ci><apply id="S3.SS2.p4.6.m6.2.2.cmml" xref="S3.SS2.p4.6.m6.4.4.1.1.1.1.3.2"><ci id="S3.SS2.p4.6.m6.2.2.1.cmml" xref="S3.SS2.p4.6.m6.2.2.1">^</ci><ci id="S3.SS2.p4.6.m6.2.2.2.cmml" xref="S3.SS2.p4.6.m6.2.2.2">𝑄</ci></apply></apply><ci id="S3.SS2.p4.6.m6.3.3.cmml" xref="S3.SS2.p4.6.m6.3.3">𝑠</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.4c">\hat{\textsc{Ret}(Q)}=\textsc{Shift}(\textsc{Ret}(\hat{Q}),s)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m6.4d">over^ start_ARG Ret ( italic_Q ) end_ARG = Shift ( Ret ( over^ start_ARG italic_Q end_ARG ) , italic_s )</annotation></semantics></math> serves as the approximation of <math alttext="\textsc{Ret}(Q)" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7.1"><semantics id="S3.SS2.p4.7.m7.1a"><mrow id="S3.SS2.p4.7.m7.1.2" xref="S3.SS2.p4.7.m7.1.2.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.7.m7.1.2.2" mathvariant="normal" xref="S3.SS2.p4.7.m7.1.2.2a.cmml">Ret</mtext><mo id="S3.SS2.p4.7.m7.1.2.1" xref="S3.SS2.p4.7.m7.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p4.7.m7.1.2.3.2" xref="S3.SS2.p4.7.m7.1.2.cmml"><mo id="S3.SS2.p4.7.m7.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.7.m7.1.2.cmml">(</mo><mi id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml">Q</mi><mo id="S3.SS2.p4.7.m7.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.2.cmml" xref="S3.SS2.p4.7.m7.1.2"><times id="S3.SS2.p4.7.m7.1.2.1.cmml" xref="S3.SS2.p4.7.m7.1.2.1"></times><ci id="S3.SS2.p4.7.m7.1.2.2a.cmml" xref="S3.SS2.p4.7.m7.1.2.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.7.m7.1.2.2.cmml" mathvariant="normal" xref="S3.SS2.p4.7.m7.1.2.2">Ret</mtext></ci><ci id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">\textsc{Ret}(Q)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m7.1d">Ret ( italic_Q )</annotation></semantics></math>. Given that the stale query is <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p4.8.m8.1"><semantics id="S3.SS2.p4.8.m8.1a"><mi id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><ci id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.8.m8.1d">italic_s</annotation></semantics></math> tokens behind the most recent generation context, the retrieved results <math alttext="\textsc{Ret}(\hat{Q})" class="ltx_Math" display="inline" id="S3.SS2.p4.9.m9.1"><semantics id="S3.SS2.p4.9.m9.1a"><mrow id="S3.SS2.p4.9.m9.1.2" xref="S3.SS2.p4.9.m9.1.2.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.9.m9.1.2.2" mathvariant="normal" xref="S3.SS2.p4.9.m9.1.2.2a.cmml">Ret</mtext><mo id="S3.SS2.p4.9.m9.1.2.1" xref="S3.SS2.p4.9.m9.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p4.9.m9.1.2.3.2" xref="S3.SS2.p4.9.m9.1.1.cmml"><mo id="S3.SS2.p4.9.m9.1.2.3.2.1" stretchy="false" xref="S3.SS2.p4.9.m9.1.1.cmml">(</mo><mover accent="true" id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml">Q</mi><mo id="S3.SS2.p4.9.m9.1.1.1" xref="S3.SS2.p4.9.m9.1.1.1.cmml">^</mo></mover><mo id="S3.SS2.p4.9.m9.1.2.3.2.2" stretchy="false" xref="S3.SS2.p4.9.m9.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.2.cmml" xref="S3.SS2.p4.9.m9.1.2"><times id="S3.SS2.p4.9.m9.1.2.1.cmml" xref="S3.SS2.p4.9.m9.1.2.1"></times><ci id="S3.SS2.p4.9.m9.1.2.2a.cmml" xref="S3.SS2.p4.9.m9.1.2.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.9.m9.1.2.2.cmml" mathvariant="normal" xref="S3.SS2.p4.9.m9.1.2.2">Ret</mtext></ci><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.2.3.2"><ci id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1.1">^</ci><ci id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">\textsc{Ret}(\hat{Q})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.9.m9.1d">Ret ( over^ start_ARG italic_Q end_ARG )</annotation></semantics></math> are correspondingly left-shifted by <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p4.10.m10.1"><semantics id="S3.SS2.p4.10.m10.1a"><mi id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><ci id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.10.m10.1d">italic_s</annotation></semantics></math> tokens. This shift ensures that the first <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p4.11.m11.1"><semantics id="S3.SS2.p4.11.m11.1a"><mi id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><ci id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.11.m11.1d">italic_s</annotation></semantics></math> retrieved tokens, which are likely less relevant for the upcoming generation due to staleness, are excluded while maintaining the overall length of retrieval tokens. Note that the concept of stale query windows does not apply for the initial retrieval, which is conducted using the first chunk <math alttext="C_{1}" class="ltx_Math" display="inline" id="S3.SS2.p4.12.m12.1"><semantics id="S3.SS2.p4.12.m12.1a"><msub id="S3.SS2.p4.12.m12.1.1" xref="S3.SS2.p4.12.m12.1.1.cmml"><mi id="S3.SS2.p4.12.m12.1.1.2" xref="S3.SS2.p4.12.m12.1.1.2.cmml">C</mi><mn id="S3.SS2.p4.12.m12.1.1.3" xref="S3.SS2.p4.12.m12.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m12.1b"><apply id="S3.SS2.p4.12.m12.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m12.1.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p4.12.m12.1.1.2.cmml" xref="S3.SS2.p4.12.m12.1.1.2">𝐶</ci><cn id="S3.SS2.p4.12.m12.1.1.3.cmml" type="integer" xref="S3.SS2.p4.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m12.1c">C_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.12.m12.1d">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="416" id="S3.F3.g1" src="extracted/5456482/fig/Attention.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Attention mechanisms and query windows in PipeRAG.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.2.1">S2: Flexible retrieval intervals.</span>
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p5.2.2">Retro</span> utilizes a fixed retrieval interval of <math alttext="m=64" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">m</mi><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><eq id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></eq><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝑚</ci><cn id="S3.SS2.p5.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_m = 64</annotation></semantics></math>, aligning with the generation chunk size, database token chunk size, and query window size.
However, the effectiveness of pipeline parallelism (S1) is maximized when the retrieval and inference subsystems have similar latencies — generating <math alttext="m=64" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">m</mi><mo id="S3.SS2.p5.2.m2.1.1.1" xref="S3.SS2.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><eq id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1.1"></eq><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝑚</ci><cn id="S3.SS2.p5.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_m = 64</annotation></semantics></math> tokens does not always consume similar time as one retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.10">In order to improve the effectiveness of pipeline parallelism, PipeRAG supports alternative retrieval intervals <math alttext="m^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><msup id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">m</mi><mo id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝑚</ci><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">m^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> and modifies <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p6.10.1">Retro</span>’s attention mechanism accordingly.
Here, <math alttext="m^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><msup id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mi id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml">m</mi><mo id="S3.SS2.p6.2.m2.1.1.3" xref="S3.SS2.p6.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2">𝑚</ci><ci id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">m^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> remains constant during a single generation process but can vary from the default value of 64. When using shorter intervals, such as <math alttext="m^{\prime}=32" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><mrow id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><msup id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2.2" xref="S3.SS2.p6.3.m3.1.1.2.2.cmml">m</mi><mo id="S3.SS2.p6.3.m3.1.1.2.3" xref="S3.SS2.p6.3.m3.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.p6.3.m3.1.1.1" xref="S3.SS2.p6.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><eq id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1.1"></eq><apply id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.2.1.cmml" xref="S3.SS2.p6.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2.2">𝑚</ci><ci id="S3.SS2.p6.3.m3.1.1.2.3.cmml" xref="S3.SS2.p6.3.m3.1.1.2.3">′</ci></apply><cn id="S3.SS2.p6.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p6.3.m3.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">m^{\prime}=32</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 32</annotation></semantics></math>, the staleness of queries is also reduced (<math alttext="s=32" class="ltx_Math" display="inline" id="S3.SS2.p6.4.m4.1"><semantics id="S3.SS2.p6.4.m4.1a"><mrow id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml"><mi id="S3.SS2.p6.4.m4.1.1.2" xref="S3.SS2.p6.4.m4.1.1.2.cmml">s</mi><mo id="S3.SS2.p6.4.m4.1.1.1" xref="S3.SS2.p6.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS2.p6.4.m4.1.1.3" xref="S3.SS2.p6.4.m4.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><apply id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1"><eq id="S3.SS2.p6.4.m4.1.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1.1"></eq><ci id="S3.SS2.p6.4.m4.1.1.2.cmml" xref="S3.SS2.p6.4.m4.1.1.2">𝑠</ci><cn id="S3.SS2.p6.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p6.4.m4.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">s=32</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.4.m4.1d">italic_s = 32</annotation></semantics></math>, thereby improving the quality of the retrieved content to more closely resemble that obtained from a non-stale query.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.F3" title="Figure 3 ‣ 3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the differences in retrievals and attention mechanisms between <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p6.10.2">Retro</span> and PipeRAG, taking <math alttext="m^{\prime}=32" class="ltx_Math" display="inline" id="S3.SS2.p6.5.m5.1"><semantics id="S3.SS2.p6.5.m5.1a"><mrow id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><msup id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml"><mi id="S3.SS2.p6.5.m5.1.1.2.2" xref="S3.SS2.p6.5.m5.1.1.2.2.cmml">m</mi><mo id="S3.SS2.p6.5.m5.1.1.2.3" xref="S3.SS2.p6.5.m5.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.p6.5.m5.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p6.5.m5.1.1.3" xref="S3.SS2.p6.5.m5.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><eq id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1"></eq><apply id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.2.1.cmml" xref="S3.SS2.p6.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.2.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2.2">𝑚</ci><ci id="S3.SS2.p6.5.m5.1.1.2.3.cmml" xref="S3.SS2.p6.5.m5.1.1.2.3">′</ci></apply><cn id="S3.SS2.p6.5.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p6.5.m5.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">m^{\prime}=32</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.5.m5.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 32</annotation></semantics></math> as an example. As shown in the figure, while a query <math alttext="Q_{i}" class="ltx_Math" display="inline" id="S3.SS2.p6.6.m6.1"><semantics id="S3.SS2.p6.6.m6.1a"><msub id="S3.SS2.p6.6.m6.1.1" xref="S3.SS2.p6.6.m6.1.1.cmml"><mi id="S3.SS2.p6.6.m6.1.1.2" xref="S3.SS2.p6.6.m6.1.1.2.cmml">Q</mi><mi id="S3.SS2.p6.6.m6.1.1.3" xref="S3.SS2.p6.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m6.1b"><apply id="S3.SS2.p6.6.m6.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.2">𝑄</ci><ci id="S3.SS2.p6.6.m6.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m6.1c">Q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.6.m6.1d">italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> still has a window size of <math alttext="m=64" class="ltx_Math" display="inline" id="S3.SS2.p6.7.m7.1"><semantics id="S3.SS2.p6.7.m7.1a"><mrow id="S3.SS2.p6.7.m7.1.1" xref="S3.SS2.p6.7.m7.1.1.cmml"><mi id="S3.SS2.p6.7.m7.1.1.2" xref="S3.SS2.p6.7.m7.1.1.2.cmml">m</mi><mo id="S3.SS2.p6.7.m7.1.1.1" xref="S3.SS2.p6.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.p6.7.m7.1.1.3" xref="S3.SS2.p6.7.m7.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.7.m7.1b"><apply id="S3.SS2.p6.7.m7.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1"><eq id="S3.SS2.p6.7.m7.1.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1.1"></eq><ci id="S3.SS2.p6.7.m7.1.1.2.cmml" xref="S3.SS2.p6.7.m7.1.1.2">𝑚</ci><cn id="S3.SS2.p6.7.m7.1.1.3.cmml" type="integer" xref="S3.SS2.p6.7.m7.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.7.m7.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.7.m7.1d">italic_m = 64</annotation></semantics></math> tokens, the retrieval interval is halved. This necessitates adjustments in the attention regions to align with these modified intervals. For encoder-to-decoder attention, the attention is directed from the retrieved chunk to the query window whose position is different from that of <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p6.10.3">Retro</span>. For decoder-to-encoder attention, the generation of chunk <math alttext="C_{j+1}" class="ltx_Math" display="inline" id="S3.SS2.p6.8.m8.1"><semantics id="S3.SS2.p6.8.m8.1a"><msub id="S3.SS2.p6.8.m8.1.1" xref="S3.SS2.p6.8.m8.1.1.cmml"><mi id="S3.SS2.p6.8.m8.1.1.2" xref="S3.SS2.p6.8.m8.1.1.2.cmml">C</mi><mrow id="S3.SS2.p6.8.m8.1.1.3" xref="S3.SS2.p6.8.m8.1.1.3.cmml"><mi id="S3.SS2.p6.8.m8.1.1.3.2" xref="S3.SS2.p6.8.m8.1.1.3.2.cmml">j</mi><mo id="S3.SS2.p6.8.m8.1.1.3.1" xref="S3.SS2.p6.8.m8.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p6.8.m8.1.1.3.3" xref="S3.SS2.p6.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.8.m8.1b"><apply id="S3.SS2.p6.8.m8.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.8.m8.1.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p6.8.m8.1.1.2.cmml" xref="S3.SS2.p6.8.m8.1.1.2">𝐶</ci><apply id="S3.SS2.p6.8.m8.1.1.3.cmml" xref="S3.SS2.p6.8.m8.1.1.3"><plus id="S3.SS2.p6.8.m8.1.1.3.1.cmml" xref="S3.SS2.p6.8.m8.1.1.3.1"></plus><ci id="S3.SS2.p6.8.m8.1.1.3.2.cmml" xref="S3.SS2.p6.8.m8.1.1.3.2">𝑗</ci><cn id="S3.SS2.p6.8.m8.1.1.3.3.cmml" type="integer" xref="S3.SS2.p6.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.8.m8.1c">C_{j+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.8.m8.1d">italic_C start_POSTSUBSCRIPT italic_j + 1 end_POSTSUBSCRIPT</annotation></semantics></math> of length <math alttext="m^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p6.9.m9.1"><semantics id="S3.SS2.p6.9.m9.1a"><msup id="S3.SS2.p6.9.m9.1.1" xref="S3.SS2.p6.9.m9.1.1.cmml"><mi id="S3.SS2.p6.9.m9.1.1.2" xref="S3.SS2.p6.9.m9.1.1.2.cmml">m</mi><mo id="S3.SS2.p6.9.m9.1.1.3" xref="S3.SS2.p6.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.9.m9.1b"><apply id="S3.SS2.p6.9.m9.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m9.1.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1">superscript</csymbol><ci id="S3.SS2.p6.9.m9.1.1.2.cmml" xref="S3.SS2.p6.9.m9.1.1.2">𝑚</ci><ci id="S3.SS2.p6.9.m9.1.1.3.cmml" xref="S3.SS2.p6.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.9.m9.1c">m^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.9.m9.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> applies chunked cross-attention on <math alttext="\textsc{Ret}(Q_{j-1})" class="ltx_Math" display="inline" id="S3.SS2.p6.10.m10.1"><semantics id="S3.SS2.p6.10.m10.1a"><mrow id="S3.SS2.p6.10.m10.1.1" xref="S3.SS2.p6.10.m10.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p6.10.m10.1.1.3" mathvariant="normal" xref="S3.SS2.p6.10.m10.1.1.3a.cmml">Ret</mtext><mo id="S3.SS2.p6.10.m10.1.1.2" xref="S3.SS2.p6.10.m10.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p6.10.m10.1.1.1.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml"><mo id="S3.SS2.p6.10.m10.1.1.1.1.2" stretchy="false" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p6.10.m10.1.1.1.1.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.10.m10.1.1.1.1.1.2" xref="S3.SS2.p6.10.m10.1.1.1.1.1.2.cmml">Q</mi><mrow id="S3.SS2.p6.10.m10.1.1.1.1.1.3" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p6.10.m10.1.1.1.1.1.3.2" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.p6.10.m10.1.1.1.1.1.3.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p6.10.m10.1.1.1.1.1.3.3" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p6.10.m10.1.1.1.1.3" stretchy="false" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.10.m10.1b"><apply id="S3.SS2.p6.10.m10.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1"><times id="S3.SS2.p6.10.m10.1.1.2.cmml" xref="S3.SS2.p6.10.m10.1.1.2"></times><ci id="S3.SS2.p6.10.m10.1.1.3a.cmml" xref="S3.SS2.p6.10.m10.1.1.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p6.10.m10.1.1.3.cmml" mathvariant="normal" xref="S3.SS2.p6.10.m10.1.1.3">Ret</mtext></ci><apply id="S3.SS2.p6.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.2">𝑄</ci><apply id="S3.SS2.p6.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3"><minus id="S3.SS2.p6.10.m10.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.1"></minus><ci id="S3.SS2.p6.10.m10.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.2">𝑗</ci><cn id="S3.SS2.p6.10.m10.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.10.m10.1c">\textsc{Ret}(Q_{j-1})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.10.m10.1d">Ret ( italic_Q start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p7.1.1">S3: Performance-model-driven retrievals.</span>
PipeRAG has the potential to match the generation latency of LLMs that do not introduce retrievals, especially when the retrievals and inferences are completely overlapped in the pipeline.
However, achieving this ideal overlap is challenging because of the distinct performance characteristics of the retrieval and inference systems as introduced in O2 and O3.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">To address this, we propose a performance-model-driven retrieval system to automatically enable perfectly overlapped pipeline windows.
In this context, a performance model refers to any model (not limited to neural networks) designed to predict the performance characteristics of a system.
Specifically, the retrieval system takes the generation states as inputs and automatically adjusts the search space using performance models, ensuring that the retrieval latency can be hidden by the generation latency of the next token chunk. By maximizing the search space under the latency constraint, the retrieval quality is also maximized without incurring extra generation latency.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">The inference performance can be modeled as follows.
The time required to generate a token chunk is <math alttext="T_{C}=T_{\textsc{Enc}}+T_{\textsc{Dec}}" class="ltx_Math" display="inline" id="S3.SS2.p9.1.m1.1"><semantics id="S3.SS2.p9.1.m1.1a"><mrow id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><msub id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2.2" xref="S3.SS2.p9.1.m1.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p9.1.m1.1.1.2.3" xref="S3.SS2.p9.1.m1.1.1.2.3.cmml">C</mi></msub><mo id="S3.SS2.p9.1.m1.1.1.1" xref="S3.SS2.p9.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml"><msub id="S3.SS2.p9.1.m1.1.1.3.2" xref="S3.SS2.p9.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.p9.1.m1.1.1.3.2.2" xref="S3.SS2.p9.1.m1.1.1.3.2.2.cmml">T</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p9.1.m1.1.1.3.2.3" mathvariant="normal" xref="S3.SS2.p9.1.m1.1.1.3.2.3a.cmml">Enc</mtext></msub><mo id="S3.SS2.p9.1.m1.1.1.3.1" xref="S3.SS2.p9.1.m1.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p9.1.m1.1.1.3.3" xref="S3.SS2.p9.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p9.1.m1.1.1.3.3.2" xref="S3.SS2.p9.1.m1.1.1.3.3.2.cmml">T</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p9.1.m1.1.1.3.3.3" mathvariant="normal" xref="S3.SS2.p9.1.m1.1.1.3.3.3a.cmml">Dec</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><eq id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1.1"></eq><apply id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.2.1.cmml" xref="S3.SS2.p9.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2.2">𝑇</ci><ci id="S3.SS2.p9.1.m1.1.1.2.3.cmml" xref="S3.SS2.p9.1.m1.1.1.2.3">𝐶</ci></apply><apply id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3"><plus id="S3.SS2.p9.1.m1.1.1.3.1.cmml" xref="S3.SS2.p9.1.m1.1.1.3.1"></plus><apply id="S3.SS2.p9.1.m1.1.1.3.2.cmml" xref="S3.SS2.p9.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p9.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p9.1.m1.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.p9.1.m1.1.1.3.2.3a.cmml" xref="S3.SS2.p9.1.m1.1.1.3.2.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p9.1.m1.1.1.3.2.3.cmml" mathsize="70%" mathvariant="normal" xref="S3.SS2.p9.1.m1.1.1.3.2.3">Enc</mtext></ci></apply><apply id="S3.SS2.p9.1.m1.1.1.3.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p9.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p9.1.m1.1.1.3.3.2">𝑇</ci><ci id="S3.SS2.p9.1.m1.1.1.3.3.3a.cmml" xref="S3.SS2.p9.1.m1.1.1.3.3.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p9.1.m1.1.1.3.3.3.cmml" mathsize="70%" mathvariant="normal" xref="S3.SS2.p9.1.m1.1.1.3.3.3">Dec</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">T_{C}=T_{\textsc{Enc}}+T_{\textsc{Dec}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT Enc end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT Dec end_POSTSUBSCRIPT</annotation></semantics></math>. The latency of encoder inference is related to the number of retrieved neighbors and the number of tokens per neighbor, while the decoder inference latency depends on the current sequence length and the chunk size (O2).</p>
</div>
<div class="ltx_para" id="S3.SS2.p10">
<p class="ltx_p" id="S3.SS2.p10.4">On the other hand, the retrieval latency can be represented modeled as <math alttext="T_{\textsc{Ret}}=T_{Network}+T_{EncQuery}+T_{ScanIndex}+T_{ScanVec}" class="ltx_Math" display="inline" id="S3.SS2.p10.1.m1.1"><semantics id="S3.SS2.p10.1.m1.1a"><mrow id="S3.SS2.p10.1.m1.1.1" xref="S3.SS2.p10.1.m1.1.1.cmml"><msub id="S3.SS2.p10.1.m1.1.1.2" xref="S3.SS2.p10.1.m1.1.1.2.cmml"><mi id="S3.SS2.p10.1.m1.1.1.2.2" xref="S3.SS2.p10.1.m1.1.1.2.2.cmml">T</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p10.1.m1.1.1.2.3" mathvariant="normal" xref="S3.SS2.p10.1.m1.1.1.2.3a.cmml">Ret</mtext></msub><mo id="S3.SS2.p10.1.m1.1.1.1" xref="S3.SS2.p10.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p10.1.m1.1.1.3" xref="S3.SS2.p10.1.m1.1.1.3.cmml"><msub id="S3.SS2.p10.1.m1.1.1.3.2" xref="S3.SS2.p10.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.2.2" xref="S3.SS2.p10.1.m1.1.1.3.2.2.cmml">T</mi><mrow id="S3.SS2.p10.1.m1.1.1.3.2.3" xref="S3.SS2.p10.1.m1.1.1.3.2.3.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.2" xref="S3.SS2.p10.1.m1.1.1.3.2.3.2.cmml">N</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.3" xref="S3.SS2.p10.1.m1.1.1.3.2.3.3.cmml">e</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1a" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.4" xref="S3.SS2.p10.1.m1.1.1.3.2.3.4.cmml">t</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1b" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.5" xref="S3.SS2.p10.1.m1.1.1.3.2.3.5.cmml">w</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1c" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.6" xref="S3.SS2.p10.1.m1.1.1.3.2.3.6.cmml">o</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1d" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.7" xref="S3.SS2.p10.1.m1.1.1.3.2.3.7.cmml">r</mi><mo id="S3.SS2.p10.1.m1.1.1.3.2.3.1e" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.2.3.8" xref="S3.SS2.p10.1.m1.1.1.3.2.3.8.cmml">k</mi></mrow></msub><mo id="S3.SS2.p10.1.m1.1.1.3.1" xref="S3.SS2.p10.1.m1.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p10.1.m1.1.1.3.3" xref="S3.SS2.p10.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.3.2" xref="S3.SS2.p10.1.m1.1.1.3.3.2.cmml">T</mi><mrow id="S3.SS2.p10.1.m1.1.1.3.3.3" xref="S3.SS2.p10.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.2" xref="S3.SS2.p10.1.m1.1.1.3.3.3.2.cmml">E</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.3" xref="S3.SS2.p10.1.m1.1.1.3.3.3.3.cmml">n</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1a" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.4" xref="S3.SS2.p10.1.m1.1.1.3.3.3.4.cmml">c</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1b" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.5" xref="S3.SS2.p10.1.m1.1.1.3.3.3.5.cmml">Q</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1c" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.6" xref="S3.SS2.p10.1.m1.1.1.3.3.3.6.cmml">u</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1d" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.7" xref="S3.SS2.p10.1.m1.1.1.3.3.3.7.cmml">e</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1e" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.8" xref="S3.SS2.p10.1.m1.1.1.3.3.3.8.cmml">r</mi><mo id="S3.SS2.p10.1.m1.1.1.3.3.3.1f" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.3.3.9" xref="S3.SS2.p10.1.m1.1.1.3.3.3.9.cmml">y</mi></mrow></msub><mo id="S3.SS2.p10.1.m1.1.1.3.1a" xref="S3.SS2.p10.1.m1.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p10.1.m1.1.1.3.4" xref="S3.SS2.p10.1.m1.1.1.3.4.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.4.2" xref="S3.SS2.p10.1.m1.1.1.3.4.2.cmml">T</mi><mrow id="S3.SS2.p10.1.m1.1.1.3.4.3" xref="S3.SS2.p10.1.m1.1.1.3.4.3.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.2" xref="S3.SS2.p10.1.m1.1.1.3.4.3.2.cmml">S</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.3" xref="S3.SS2.p10.1.m1.1.1.3.4.3.3.cmml">c</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1a" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.4" xref="S3.SS2.p10.1.m1.1.1.3.4.3.4.cmml">a</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1b" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.5" xref="S3.SS2.p10.1.m1.1.1.3.4.3.5.cmml">n</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1c" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.6" xref="S3.SS2.p10.1.m1.1.1.3.4.3.6.cmml">I</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1d" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.7" xref="S3.SS2.p10.1.m1.1.1.3.4.3.7.cmml">n</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1e" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.8" xref="S3.SS2.p10.1.m1.1.1.3.4.3.8.cmml">d</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1f" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.9" xref="S3.SS2.p10.1.m1.1.1.3.4.3.9.cmml">e</mi><mo id="S3.SS2.p10.1.m1.1.1.3.4.3.1g" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.4.3.10" xref="S3.SS2.p10.1.m1.1.1.3.4.3.10.cmml">x</mi></mrow></msub><mo id="S3.SS2.p10.1.m1.1.1.3.1b" xref="S3.SS2.p10.1.m1.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p10.1.m1.1.1.3.5" xref="S3.SS2.p10.1.m1.1.1.3.5.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.5.2" xref="S3.SS2.p10.1.m1.1.1.3.5.2.cmml">T</mi><mrow id="S3.SS2.p10.1.m1.1.1.3.5.3" xref="S3.SS2.p10.1.m1.1.1.3.5.3.cmml"><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.2" xref="S3.SS2.p10.1.m1.1.1.3.5.3.2.cmml">S</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.3" xref="S3.SS2.p10.1.m1.1.1.3.5.3.3.cmml">c</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1a" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.4" xref="S3.SS2.p10.1.m1.1.1.3.5.3.4.cmml">a</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1b" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.5" xref="S3.SS2.p10.1.m1.1.1.3.5.3.5.cmml">n</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1c" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.6" xref="S3.SS2.p10.1.m1.1.1.3.5.3.6.cmml">V</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1d" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.7" xref="S3.SS2.p10.1.m1.1.1.3.5.3.7.cmml">e</mi><mo id="S3.SS2.p10.1.m1.1.1.3.5.3.1e" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p10.1.m1.1.1.3.5.3.8" xref="S3.SS2.p10.1.m1.1.1.3.5.3.8.cmml">c</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.1.m1.1b"><apply id="S3.SS2.p10.1.m1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1"><eq id="S3.SS2.p10.1.m1.1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1.1"></eq><apply id="S3.SS2.p10.1.m1.1.1.2.cmml" xref="S3.SS2.p10.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.2.1.cmml" xref="S3.SS2.p10.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.2.2.cmml" xref="S3.SS2.p10.1.m1.1.1.2.2">𝑇</ci><ci id="S3.SS2.p10.1.m1.1.1.2.3a.cmml" xref="S3.SS2.p10.1.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p10.1.m1.1.1.2.3.cmml" mathsize="70%" mathvariant="normal" xref="S3.SS2.p10.1.m1.1.1.2.3">Ret</mtext></ci></apply><apply id="S3.SS2.p10.1.m1.1.1.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3"><plus id="S3.SS2.p10.1.m1.1.1.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.1"></plus><apply id="S3.SS2.p10.1.m1.1.1.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.2">𝑇</ci><apply id="S3.SS2.p10.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3"><times id="S3.SS2.p10.1.m1.1.1.3.2.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.1"></times><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.2">𝑁</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.3">𝑒</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.4.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.4">𝑡</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.5.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.5">𝑤</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.6.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.6">𝑜</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.7.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.7">𝑟</ci><ci id="S3.SS2.p10.1.m1.1.1.3.2.3.8.cmml" xref="S3.SS2.p10.1.m1.1.1.3.2.3.8">𝑘</ci></apply></apply><apply id="S3.SS2.p10.1.m1.1.1.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.2">𝑇</ci><apply id="S3.SS2.p10.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3"><times id="S3.SS2.p10.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.1"></times><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.2">𝐸</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.3">𝑛</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.4.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.4">𝑐</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.5.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.5">𝑄</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.6.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.6">𝑢</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.7.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.7">𝑒</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.8.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.8">𝑟</ci><ci id="S3.SS2.p10.1.m1.1.1.3.3.3.9.cmml" xref="S3.SS2.p10.1.m1.1.1.3.3.3.9">𝑦</ci></apply></apply><apply id="S3.SS2.p10.1.m1.1.1.3.4.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.3.4.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.3.4.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.2">𝑇</ci><apply id="S3.SS2.p10.1.m1.1.1.3.4.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3"><times id="S3.SS2.p10.1.m1.1.1.3.4.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.1"></times><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.2">𝑆</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.3">𝑐</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.4.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.4">𝑎</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.5.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.5">𝑛</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.6.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.6">𝐼</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.7.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.7">𝑛</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.8.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.8">𝑑</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.9.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.9">𝑒</ci><ci id="S3.SS2.p10.1.m1.1.1.3.4.3.10.cmml" xref="S3.SS2.p10.1.m1.1.1.3.4.3.10">𝑥</ci></apply></apply><apply id="S3.SS2.p10.1.m1.1.1.3.5.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.3.5.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.3.5.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.2">𝑇</ci><apply id="S3.SS2.p10.1.m1.1.1.3.5.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3"><times id="S3.SS2.p10.1.m1.1.1.3.5.3.1.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.1"></times><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.2.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.2">𝑆</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.3">𝑐</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.4.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.4">𝑎</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.5.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.5">𝑛</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.6.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.6">𝑉</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.7.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.7">𝑒</ci><ci id="S3.SS2.p10.1.m1.1.1.3.5.3.8.cmml" xref="S3.SS2.p10.1.m1.1.1.3.5.3.8">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.1.m1.1c">T_{\textsc{Ret}}=T_{Network}+T_{EncQuery}+T_{ScanIndex}+T_{ScanVec}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p10.1.m1.1d">italic_T start_POSTSUBSCRIPT Ret end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_N italic_e italic_t italic_w italic_o italic_r italic_k end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_E italic_n italic_c italic_Q italic_u italic_e italic_r italic_y end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_S italic_c italic_a italic_n italic_I italic_n italic_d italic_e italic_x end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_S italic_c italic_a italic_n italic_V italic_e italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, encompassing the time spent on network communications, encoding the query tokens as vectors, scanning the vector index, and scanning a subset of database vectors.
In this paper, we apply the widely-adopted IVF-PQ vector search algorithm <cite class="ltx_cite ltx_citemacro_citep">(Jegou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib15" title="">2010</a>)</cite> that combines a clustering-based inverted-file (IVF) index with product quantization (PQ). The IVF index clusters the database to <math alttext="nlist" class="ltx_Math" display="inline" id="S3.SS2.p10.2.m2.1"><semantics id="S3.SS2.p10.2.m2.1a"><mrow id="S3.SS2.p10.2.m2.1.1" xref="S3.SS2.p10.2.m2.1.1.cmml"><mi id="S3.SS2.p10.2.m2.1.1.2" xref="S3.SS2.p10.2.m2.1.1.2.cmml">n</mi><mo id="S3.SS2.p10.2.m2.1.1.1" xref="S3.SS2.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.2.m2.1.1.3" xref="S3.SS2.p10.2.m2.1.1.3.cmml">l</mi><mo id="S3.SS2.p10.2.m2.1.1.1a" xref="S3.SS2.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.2.m2.1.1.4" xref="S3.SS2.p10.2.m2.1.1.4.cmml">i</mi><mo id="S3.SS2.p10.2.m2.1.1.1b" xref="S3.SS2.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.2.m2.1.1.5" xref="S3.SS2.p10.2.m2.1.1.5.cmml">s</mi><mo id="S3.SS2.p10.2.m2.1.1.1c" xref="S3.SS2.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.2.m2.1.1.6" xref="S3.SS2.p10.2.m2.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.2.m2.1b"><apply id="S3.SS2.p10.2.m2.1.1.cmml" xref="S3.SS2.p10.2.m2.1.1"><times id="S3.SS2.p10.2.m2.1.1.1.cmml" xref="S3.SS2.p10.2.m2.1.1.1"></times><ci id="S3.SS2.p10.2.m2.1.1.2.cmml" xref="S3.SS2.p10.2.m2.1.1.2">𝑛</ci><ci id="S3.SS2.p10.2.m2.1.1.3.cmml" xref="S3.SS2.p10.2.m2.1.1.3">𝑙</ci><ci id="S3.SS2.p10.2.m2.1.1.4.cmml" xref="S3.SS2.p10.2.m2.1.1.4">𝑖</ci><ci id="S3.SS2.p10.2.m2.1.1.5.cmml" xref="S3.SS2.p10.2.m2.1.1.5">𝑠</ci><ci id="S3.SS2.p10.2.m2.1.1.6.cmml" xref="S3.SS2.p10.2.m2.1.1.6">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.2.m2.1c">nlist</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p10.2.m2.1d">italic_n italic_l italic_i italic_s italic_t</annotation></semantics></math> IVF lists. At query time, <math alttext="nprobe" class="ltx_Math" display="inline" id="S3.SS2.p10.3.m3.1"><semantics id="S3.SS2.p10.3.m3.1a"><mrow id="S3.SS2.p10.3.m3.1.1" xref="S3.SS2.p10.3.m3.1.1.cmml"><mi id="S3.SS2.p10.3.m3.1.1.2" xref="S3.SS2.p10.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS2.p10.3.m3.1.1.1" xref="S3.SS2.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.3.m3.1.1.3" xref="S3.SS2.p10.3.m3.1.1.3.cmml">p</mi><mo id="S3.SS2.p10.3.m3.1.1.1a" xref="S3.SS2.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.3.m3.1.1.4" xref="S3.SS2.p10.3.m3.1.1.4.cmml">r</mi><mo id="S3.SS2.p10.3.m3.1.1.1b" xref="S3.SS2.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.3.m3.1.1.5" xref="S3.SS2.p10.3.m3.1.1.5.cmml">o</mi><mo id="S3.SS2.p10.3.m3.1.1.1c" xref="S3.SS2.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.3.m3.1.1.6" xref="S3.SS2.p10.3.m3.1.1.6.cmml">b</mi><mo id="S3.SS2.p10.3.m3.1.1.1d" xref="S3.SS2.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.3.m3.1.1.7" xref="S3.SS2.p10.3.m3.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.3.m3.1b"><apply id="S3.SS2.p10.3.m3.1.1.cmml" xref="S3.SS2.p10.3.m3.1.1"><times id="S3.SS2.p10.3.m3.1.1.1.cmml" xref="S3.SS2.p10.3.m3.1.1.1"></times><ci id="S3.SS2.p10.3.m3.1.1.2.cmml" xref="S3.SS2.p10.3.m3.1.1.2">𝑛</ci><ci id="S3.SS2.p10.3.m3.1.1.3.cmml" xref="S3.SS2.p10.3.m3.1.1.3">𝑝</ci><ci id="S3.SS2.p10.3.m3.1.1.4.cmml" xref="S3.SS2.p10.3.m3.1.1.4">𝑟</ci><ci id="S3.SS2.p10.3.m3.1.1.5.cmml" xref="S3.SS2.p10.3.m3.1.1.5">𝑜</ci><ci id="S3.SS2.p10.3.m3.1.1.6.cmml" xref="S3.SS2.p10.3.m3.1.1.6">𝑏</ci><ci id="S3.SS2.p10.3.m3.1.1.7.cmml" xref="S3.SS2.p10.3.m3.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.3.m3.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p10.3.m3.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math> out of the <math alttext="nlist" class="ltx_Math" display="inline" id="S3.SS2.p10.4.m4.1"><semantics id="S3.SS2.p10.4.m4.1a"><mrow id="S3.SS2.p10.4.m4.1.1" xref="S3.SS2.p10.4.m4.1.1.cmml"><mi id="S3.SS2.p10.4.m4.1.1.2" xref="S3.SS2.p10.4.m4.1.1.2.cmml">n</mi><mo id="S3.SS2.p10.4.m4.1.1.1" xref="S3.SS2.p10.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.4.m4.1.1.3" xref="S3.SS2.p10.4.m4.1.1.3.cmml">l</mi><mo id="S3.SS2.p10.4.m4.1.1.1a" xref="S3.SS2.p10.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.4.m4.1.1.4" xref="S3.SS2.p10.4.m4.1.1.4.cmml">i</mi><mo id="S3.SS2.p10.4.m4.1.1.1b" xref="S3.SS2.p10.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.4.m4.1.1.5" xref="S3.SS2.p10.4.m4.1.1.5.cmml">s</mi><mo id="S3.SS2.p10.4.m4.1.1.1c" xref="S3.SS2.p10.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p10.4.m4.1.1.6" xref="S3.SS2.p10.4.m4.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.4.m4.1b"><apply id="S3.SS2.p10.4.m4.1.1.cmml" xref="S3.SS2.p10.4.m4.1.1"><times id="S3.SS2.p10.4.m4.1.1.1.cmml" xref="S3.SS2.p10.4.m4.1.1.1"></times><ci id="S3.SS2.p10.4.m4.1.1.2.cmml" xref="S3.SS2.p10.4.m4.1.1.2">𝑛</ci><ci id="S3.SS2.p10.4.m4.1.1.3.cmml" xref="S3.SS2.p10.4.m4.1.1.3">𝑙</ci><ci id="S3.SS2.p10.4.m4.1.1.4.cmml" xref="S3.SS2.p10.4.m4.1.1.4">𝑖</ci><ci id="S3.SS2.p10.4.m4.1.1.5.cmml" xref="S3.SS2.p10.4.m4.1.1.5">𝑠</ci><ci id="S3.SS2.p10.4.m4.1.1.6.cmml" xref="S3.SS2.p10.4.m4.1.1.6">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.4.m4.1c">nlist</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p10.4.m4.1d">italic_n italic_l italic_i italic_s italic_t</annotation></semantics></math> IVF lists are selected to scan (database vectors within the selected lists are compared to the query vectors).</p>
</div>
<div class="ltx_para" id="S3.SS2.p11">
<p class="ltx_p" id="S3.SS2.p11.2">Given that the performance of both retrievals and inferences are related to hardware, we measure and model their performance on the deployment hardware. We record the time consumption of both encoder and decoder inferences with various input sequence lengths. For retrieval, we model the relationship between <math alttext="nprobe" class="ltx_Math" display="inline" id="S3.SS2.p11.1.m1.1"><semantics id="S3.SS2.p11.1.m1.1a"><mrow id="S3.SS2.p11.1.m1.1.1" xref="S3.SS2.p11.1.m1.1.1.cmml"><mi id="S3.SS2.p11.1.m1.1.1.2" xref="S3.SS2.p11.1.m1.1.1.2.cmml">n</mi><mo id="S3.SS2.p11.1.m1.1.1.1" xref="S3.SS2.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.1.m1.1.1.3" xref="S3.SS2.p11.1.m1.1.1.3.cmml">p</mi><mo id="S3.SS2.p11.1.m1.1.1.1a" xref="S3.SS2.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.1.m1.1.1.4" xref="S3.SS2.p11.1.m1.1.1.4.cmml">r</mi><mo id="S3.SS2.p11.1.m1.1.1.1b" xref="S3.SS2.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.1.m1.1.1.5" xref="S3.SS2.p11.1.m1.1.1.5.cmml">o</mi><mo id="S3.SS2.p11.1.m1.1.1.1c" xref="S3.SS2.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.1.m1.1.1.6" xref="S3.SS2.p11.1.m1.1.1.6.cmml">b</mi><mo id="S3.SS2.p11.1.m1.1.1.1d" xref="S3.SS2.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.1.m1.1.1.7" xref="S3.SS2.p11.1.m1.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p11.1.m1.1b"><apply id="S3.SS2.p11.1.m1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1"><times id="S3.SS2.p11.1.m1.1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1.1"></times><ci id="S3.SS2.p11.1.m1.1.1.2.cmml" xref="S3.SS2.p11.1.m1.1.1.2">𝑛</ci><ci id="S3.SS2.p11.1.m1.1.1.3.cmml" xref="S3.SS2.p11.1.m1.1.1.3">𝑝</ci><ci id="S3.SS2.p11.1.m1.1.1.4.cmml" xref="S3.SS2.p11.1.m1.1.1.4">𝑟</ci><ci id="S3.SS2.p11.1.m1.1.1.5.cmml" xref="S3.SS2.p11.1.m1.1.1.5">𝑜</ci><ci id="S3.SS2.p11.1.m1.1.1.6.cmml" xref="S3.SS2.p11.1.m1.1.1.6">𝑏</ci><ci id="S3.SS2.p11.1.m1.1.1.7.cmml" xref="S3.SS2.p11.1.m1.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p11.1.m1.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p11.1.m1.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math> and search latency using linear regression, given that <math alttext="nprobe" class="ltx_Math" display="inline" id="S3.SS2.p11.2.m2.1"><semantics id="S3.SS2.p11.2.m2.1a"><mrow id="S3.SS2.p11.2.m2.1.1" xref="S3.SS2.p11.2.m2.1.1.cmml"><mi id="S3.SS2.p11.2.m2.1.1.2" xref="S3.SS2.p11.2.m2.1.1.2.cmml">n</mi><mo id="S3.SS2.p11.2.m2.1.1.1" xref="S3.SS2.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.2.m2.1.1.3" xref="S3.SS2.p11.2.m2.1.1.3.cmml">p</mi><mo id="S3.SS2.p11.2.m2.1.1.1a" xref="S3.SS2.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.2.m2.1.1.4" xref="S3.SS2.p11.2.m2.1.1.4.cmml">r</mi><mo id="S3.SS2.p11.2.m2.1.1.1b" xref="S3.SS2.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.2.m2.1.1.5" xref="S3.SS2.p11.2.m2.1.1.5.cmml">o</mi><mo id="S3.SS2.p11.2.m2.1.1.1c" xref="S3.SS2.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.2.m2.1.1.6" xref="S3.SS2.p11.2.m2.1.1.6.cmml">b</mi><mo id="S3.SS2.p11.2.m2.1.1.1d" xref="S3.SS2.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p11.2.m2.1.1.7" xref="S3.SS2.p11.2.m2.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p11.2.m2.1b"><apply id="S3.SS2.p11.2.m2.1.1.cmml" xref="S3.SS2.p11.2.m2.1.1"><times id="S3.SS2.p11.2.m2.1.1.1.cmml" xref="S3.SS2.p11.2.m2.1.1.1"></times><ci id="S3.SS2.p11.2.m2.1.1.2.cmml" xref="S3.SS2.p11.2.m2.1.1.2">𝑛</ci><ci id="S3.SS2.p11.2.m2.1.1.3.cmml" xref="S3.SS2.p11.2.m2.1.1.3">𝑝</ci><ci id="S3.SS2.p11.2.m2.1.1.4.cmml" xref="S3.SS2.p11.2.m2.1.1.4">𝑟</ci><ci id="S3.SS2.p11.2.m2.1.1.5.cmml" xref="S3.SS2.p11.2.m2.1.1.5">𝑜</ci><ci id="S3.SS2.p11.2.m2.1.1.6.cmml" xref="S3.SS2.p11.2.m2.1.1.6">𝑏</ci><ci id="S3.SS2.p11.2.m2.1.1.7.cmml" xref="S3.SS2.p11.2.m2.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p11.2.m2.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p11.2.m2.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math> is approximately proportional to the number of scanned database vectors.</p>
</div>
<div class="ltx_para" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.4">The retrieval system then leverages these performance models to predict the maximal search space, indicated by <math alttext="nlist" class="ltx_Math" display="inline" id="S3.SS2.p12.1.m1.1"><semantics id="S3.SS2.p12.1.m1.1a"><mrow id="S3.SS2.p12.1.m1.1.1" xref="S3.SS2.p12.1.m1.1.1.cmml"><mi id="S3.SS2.p12.1.m1.1.1.2" xref="S3.SS2.p12.1.m1.1.1.2.cmml">n</mi><mo id="S3.SS2.p12.1.m1.1.1.1" xref="S3.SS2.p12.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.1.m1.1.1.3" xref="S3.SS2.p12.1.m1.1.1.3.cmml">l</mi><mo id="S3.SS2.p12.1.m1.1.1.1a" xref="S3.SS2.p12.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.1.m1.1.1.4" xref="S3.SS2.p12.1.m1.1.1.4.cmml">i</mi><mo id="S3.SS2.p12.1.m1.1.1.1b" xref="S3.SS2.p12.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.1.m1.1.1.5" xref="S3.SS2.p12.1.m1.1.1.5.cmml">s</mi><mo id="S3.SS2.p12.1.m1.1.1.1c" xref="S3.SS2.p12.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.1.m1.1.1.6" xref="S3.SS2.p12.1.m1.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.1.m1.1b"><apply id="S3.SS2.p12.1.m1.1.1.cmml" xref="S3.SS2.p12.1.m1.1.1"><times id="S3.SS2.p12.1.m1.1.1.1.cmml" xref="S3.SS2.p12.1.m1.1.1.1"></times><ci id="S3.SS2.p12.1.m1.1.1.2.cmml" xref="S3.SS2.p12.1.m1.1.1.2">𝑛</ci><ci id="S3.SS2.p12.1.m1.1.1.3.cmml" xref="S3.SS2.p12.1.m1.1.1.3">𝑙</ci><ci id="S3.SS2.p12.1.m1.1.1.4.cmml" xref="S3.SS2.p12.1.m1.1.1.4">𝑖</ci><ci id="S3.SS2.p12.1.m1.1.1.5.cmml" xref="S3.SS2.p12.1.m1.1.1.5">𝑠</ci><ci id="S3.SS2.p12.1.m1.1.1.6.cmml" xref="S3.SS2.p12.1.m1.1.1.6">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.1.m1.1c">nlist</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.1.m1.1d">italic_n italic_l italic_i italic_s italic_t</annotation></semantics></math>, given the latency constraint for generating the next token chunk, ensuring that <math alttext="T_{\textsc{Ret}}\leq T(C)" class="ltx_Math" display="inline" id="S3.SS2.p12.2.m2.1"><semantics id="S3.SS2.p12.2.m2.1a"><mrow id="S3.SS2.p12.2.m2.1.2" xref="S3.SS2.p12.2.m2.1.2.cmml"><msub id="S3.SS2.p12.2.m2.1.2.2" xref="S3.SS2.p12.2.m2.1.2.2.cmml"><mi id="S3.SS2.p12.2.m2.1.2.2.2" xref="S3.SS2.p12.2.m2.1.2.2.2.cmml">T</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p12.2.m2.1.2.2.3" mathvariant="normal" xref="S3.SS2.p12.2.m2.1.2.2.3a.cmml">Ret</mtext></msub><mo id="S3.SS2.p12.2.m2.1.2.1" xref="S3.SS2.p12.2.m2.1.2.1.cmml">≤</mo><mrow id="S3.SS2.p12.2.m2.1.2.3" xref="S3.SS2.p12.2.m2.1.2.3.cmml"><mi id="S3.SS2.p12.2.m2.1.2.3.2" xref="S3.SS2.p12.2.m2.1.2.3.2.cmml">T</mi><mo id="S3.SS2.p12.2.m2.1.2.3.1" xref="S3.SS2.p12.2.m2.1.2.3.1.cmml">⁢</mo><mrow id="S3.SS2.p12.2.m2.1.2.3.3.2" xref="S3.SS2.p12.2.m2.1.2.3.cmml"><mo id="S3.SS2.p12.2.m2.1.2.3.3.2.1" stretchy="false" xref="S3.SS2.p12.2.m2.1.2.3.cmml">(</mo><mi id="S3.SS2.p12.2.m2.1.1" xref="S3.SS2.p12.2.m2.1.1.cmml">C</mi><mo id="S3.SS2.p12.2.m2.1.2.3.3.2.2" stretchy="false" xref="S3.SS2.p12.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.2.m2.1b"><apply id="S3.SS2.p12.2.m2.1.2.cmml" xref="S3.SS2.p12.2.m2.1.2"><leq id="S3.SS2.p12.2.m2.1.2.1.cmml" xref="S3.SS2.p12.2.m2.1.2.1"></leq><apply id="S3.SS2.p12.2.m2.1.2.2.cmml" xref="S3.SS2.p12.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p12.2.m2.1.2.2.1.cmml" xref="S3.SS2.p12.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.p12.2.m2.1.2.2.2.cmml" xref="S3.SS2.p12.2.m2.1.2.2.2">𝑇</ci><ci id="S3.SS2.p12.2.m2.1.2.2.3a.cmml" xref="S3.SS2.p12.2.m2.1.2.2.3"><mtext class="ltx_font_smallcaps" id="S3.SS2.p12.2.m2.1.2.2.3.cmml" mathsize="70%" mathvariant="normal" xref="S3.SS2.p12.2.m2.1.2.2.3">Ret</mtext></ci></apply><apply id="S3.SS2.p12.2.m2.1.2.3.cmml" xref="S3.SS2.p12.2.m2.1.2.3"><times id="S3.SS2.p12.2.m2.1.2.3.1.cmml" xref="S3.SS2.p12.2.m2.1.2.3.1"></times><ci id="S3.SS2.p12.2.m2.1.2.3.2.cmml" xref="S3.SS2.p12.2.m2.1.2.3.2">𝑇</ci><ci id="S3.SS2.p12.2.m2.1.1.cmml" xref="S3.SS2.p12.2.m2.1.1">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.2.m2.1c">T_{\textsc{Ret}}\leq T(C)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.2.m2.1d">italic_T start_POSTSUBSCRIPT Ret end_POSTSUBSCRIPT ≤ italic_T ( italic_C )</annotation></semantics></math>.
Since the <math alttext="T(C)" class="ltx_Math" display="inline" id="S3.SS2.p12.3.m3.1"><semantics id="S3.SS2.p12.3.m3.1a"><mrow id="S3.SS2.p12.3.m3.1.2" xref="S3.SS2.p12.3.m3.1.2.cmml"><mi id="S3.SS2.p12.3.m3.1.2.2" xref="S3.SS2.p12.3.m3.1.2.2.cmml">T</mi><mo id="S3.SS2.p12.3.m3.1.2.1" xref="S3.SS2.p12.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p12.3.m3.1.2.3.2" xref="S3.SS2.p12.3.m3.1.2.cmml"><mo id="S3.SS2.p12.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS2.p12.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p12.3.m3.1.1" xref="S3.SS2.p12.3.m3.1.1.cmml">C</mi><mo id="S3.SS2.p12.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS2.p12.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.3.m3.1b"><apply id="S3.SS2.p12.3.m3.1.2.cmml" xref="S3.SS2.p12.3.m3.1.2"><times id="S3.SS2.p12.3.m3.1.2.1.cmml" xref="S3.SS2.p12.3.m3.1.2.1"></times><ci id="S3.SS2.p12.3.m3.1.2.2.cmml" xref="S3.SS2.p12.3.m3.1.2.2">𝑇</ci><ci id="S3.SS2.p12.3.m3.1.1.cmml" xref="S3.SS2.p12.3.m3.1.1">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.3.m3.1c">T(C)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.3.m3.1d">italic_T ( italic_C )</annotation></semantics></math> can be easily obtained from the recorded performance numbers, we can then derive the maximal <math alttext="nprobe" class="ltx_Math" display="inline" id="S3.SS2.p12.4.m4.1"><semantics id="S3.SS2.p12.4.m4.1a"><mrow id="S3.SS2.p12.4.m4.1.1" xref="S3.SS2.p12.4.m4.1.1.cmml"><mi id="S3.SS2.p12.4.m4.1.1.2" xref="S3.SS2.p12.4.m4.1.1.2.cmml">n</mi><mo id="S3.SS2.p12.4.m4.1.1.1" xref="S3.SS2.p12.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.4.m4.1.1.3" xref="S3.SS2.p12.4.m4.1.1.3.cmml">p</mi><mo id="S3.SS2.p12.4.m4.1.1.1a" xref="S3.SS2.p12.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.4.m4.1.1.4" xref="S3.SS2.p12.4.m4.1.1.4.cmml">r</mi><mo id="S3.SS2.p12.4.m4.1.1.1b" xref="S3.SS2.p12.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.4.m4.1.1.5" xref="S3.SS2.p12.4.m4.1.1.5.cmml">o</mi><mo id="S3.SS2.p12.4.m4.1.1.1c" xref="S3.SS2.p12.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.4.m4.1.1.6" xref="S3.SS2.p12.4.m4.1.1.6.cmml">b</mi><mo id="S3.SS2.p12.4.m4.1.1.1d" xref="S3.SS2.p12.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p12.4.m4.1.1.7" xref="S3.SS2.p12.4.m4.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.4.m4.1b"><apply id="S3.SS2.p12.4.m4.1.1.cmml" xref="S3.SS2.p12.4.m4.1.1"><times id="S3.SS2.p12.4.m4.1.1.1.cmml" xref="S3.SS2.p12.4.m4.1.1.1"></times><ci id="S3.SS2.p12.4.m4.1.1.2.cmml" xref="S3.SS2.p12.4.m4.1.1.2">𝑛</ci><ci id="S3.SS2.p12.4.m4.1.1.3.cmml" xref="S3.SS2.p12.4.m4.1.1.3">𝑝</ci><ci id="S3.SS2.p12.4.m4.1.1.4.cmml" xref="S3.SS2.p12.4.m4.1.1.4">𝑟</ci><ci id="S3.SS2.p12.4.m4.1.1.5.cmml" xref="S3.SS2.p12.4.m4.1.1.5">𝑜</ci><ci id="S3.SS2.p12.4.m4.1.1.6.cmml" xref="S3.SS2.p12.4.m4.1.1.6">𝑏</ci><ci id="S3.SS2.p12.4.m4.1.1.7.cmml" xref="S3.SS2.p12.4.m4.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.4.m4.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.4.m4.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math> during the search based on the retrieval performance model.</p>
</div>
<div class="ltx_para" id="S3.SS2.p13">
<p class="ltx_p" id="S3.SS2.p13.1">While an alternative approach to achieve a perfectly overlapped pipeline is adjusting the retrieval intervals in the inference system, we rule out this option due to generalizability concerns. In future deployment scenarios, a retrieval system may serve multiple inference systems. Thus, the retrieval performance is impacted by the number of concurrent queries being processed. In this case, it could be challenging for the inference system to accurately predict the retrieval latency, as it lacks the information about the retrieval system’s workload at the moment. Therefore, it is the retrieval system, instead of the inference system, that should be responsible for constructing a perfectly overlapped pipeline via performance modeling.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F4.fig1">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="89" id="S3.F4.g1" src="extracted/5456482/fig/paper_ppl_db_size_wikipedia_chunk9_1K.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F4.fig2">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="89" id="S3.F4.g2" src="extracted/5456482/fig/paper_ppl_db_size_realnews_chunk31_1K.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F4.fig3">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="86" id="S3.F4.g3" src="extracted/5456482/fig/paper_ppl_db_size_c4_chunk1023_1K.png" width="192"/></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The effect of database sizes and retrieval strategies on language modeling perplexity (lower perplexity means higher quality).</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F5.fig1">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="106" id="S3.F5.g1" src="extracted/5456482/fig/ppl_eval_wikipedia_chunk9_1K_db_c4_chunk_0_to_999.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F5.fig2">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="106" id="S3.F5.g2" src="extracted/5456482/fig/ppl_eval_realnews_chunk31_1K_db_c4_chunk_0_to_999.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S3.F5.fig3">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="106" id="S3.F5.g3" src="extracted/5456482/fig/ppl_eval_c4_chunk1023_1K_db_c4_chunk_0_to_999.png" width="192"/></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Perplexity of retrieval-augmented generation when applying various retrieval intervals and search space configurations (<math alttext="nprobe" class="ltx_Math" display="inline" id="S3.F5.2.m1.1"><semantics id="S3.F5.2.m1.1b"><mrow id="S3.F5.2.m1.1.1" xref="S3.F5.2.m1.1.1.cmml"><mi id="S3.F5.2.m1.1.1.2" xref="S3.F5.2.m1.1.1.2.cmml">n</mi><mo id="S3.F5.2.m1.1.1.1" xref="S3.F5.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.F5.2.m1.1.1.3" xref="S3.F5.2.m1.1.1.3.cmml">p</mi><mo id="S3.F5.2.m1.1.1.1b" xref="S3.F5.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.F5.2.m1.1.1.4" xref="S3.F5.2.m1.1.1.4.cmml">r</mi><mo id="S3.F5.2.m1.1.1.1c" xref="S3.F5.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.F5.2.m1.1.1.5" xref="S3.F5.2.m1.1.1.5.cmml">o</mi><mo id="S3.F5.2.m1.1.1.1d" xref="S3.F5.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.F5.2.m1.1.1.6" xref="S3.F5.2.m1.1.1.6.cmml">b</mi><mo id="S3.F5.2.m1.1.1.1e" xref="S3.F5.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.F5.2.m1.1.1.7" xref="S3.F5.2.m1.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.2.m1.1c"><apply id="S3.F5.2.m1.1.1.cmml" xref="S3.F5.2.m1.1.1"><times id="S3.F5.2.m1.1.1.1.cmml" xref="S3.F5.2.m1.1.1.1"></times><ci id="S3.F5.2.m1.1.1.2.cmml" xref="S3.F5.2.m1.1.1.2">𝑛</ci><ci id="S3.F5.2.m1.1.1.3.cmml" xref="S3.F5.2.m1.1.1.3">𝑝</ci><ci id="S3.F5.2.m1.1.1.4.cmml" xref="S3.F5.2.m1.1.1.4">𝑟</ci><ci id="S3.F5.2.m1.1.1.5.cmml" xref="S3.F5.2.m1.1.1.5">𝑜</ci><ci id="S3.F5.2.m1.1.1.6.cmml" xref="S3.F5.2.m1.1.1.6">𝑏</ci><ci id="S3.F5.2.m1.1.1.7.cmml" xref="S3.F5.2.m1.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.m1.1d">nprobe</annotation><annotation encoding="application/x-llamapun" id="S3.F5.2.m1.1e">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math>).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We briefly introduce our experimental setup below and leave more details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A2" title="Appendix B Detailed Evaluation Setup ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.2.1">Database.</span>
Our token database was constructed from the C4 corpus containing deduplicated English documents. Adhering to <cite class="ltx_cite ltx_citemacro_citet">Borgeaud et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite>, we segmented the documents into chunks of <math alttext="m=64" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">m</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑚</ci><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_m = 64</annotation></semantics></math> tokens, yielding a total of three billion chunks, and set the number of nearest neighbors per retrieval as <math alttext="k=2" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><eq id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></eq><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝑘</ci><cn id="S4.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">k=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_k = 2</annotation></semantics></math>. Following <cite class="ltx_cite ltx_citemacro_citet">Norlund et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>)</cite>, we transformed each token chunk into a 384-dimensional vector using a sentence transformer<cite class="ltx_cite ltx_citemacro_citep">(Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib36" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Model.</span>
We developed PipeRAG based on the <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.2">Retro</span> checkpoint with 582M parameters provided by <cite class="ltx_cite ltx_citemacro_citet">Norlund et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>)</cite>, the only available pre-trained <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.3">Retro</span> model when we conducted the experiments.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Evaluation Set.</span>
To evaluate language modeling quality, we used the Wikipedia dataset <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib1" title="">wik, </a>)</cite>, the RealNews subset of the C4 dataset, and C4’s English document subset <cite class="ltx_cite ltx_citemacro_citep">(Dodge et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib6" title="">2021</a>; Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib34" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Software.</span>
For model inference, we adopted the ONNX runtime, which, according to our experiments, achieves 2 to 3 times speedup in latency over PyTorch. For retrieval, we used the Faiss library <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib19" title="">2019</a>)</cite> and the IVF-PQ vector search algorithm. Communication between the inference and retrieval systems was managed via gRPC.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Hardware.</span>
For model inference, we used an NVIDIA A100 GPU (40 GB), while the retrievals were conducted on a server equipped with Intel(R) Xeon(R) Platinum 8259CL CPUs @2.50GHz (48 cores) and 384 GB memory.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Perplexity Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.F4" title="Figure 4 ‣ 3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a> shows the impact of various retrieval strategies across different database sizes. This comparison includes PipeRAG, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.1">Retro</span>, retrieval-augmented generation with only one retrieval at the beginning of generation, and generation without retrieval. For the last two strategies, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.2">Retro</span> still serves as the base model.
As indicated in the figure, retrieval, especially on large databases, plays a crucial role in improving generation quality (lower perplexity is better). Across all evaluated datasets, generation without retrieval performs the worst, followed by only retrieving once, showing the effectiveness of periodic retrieval in <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.3">Retro</span>. Additionally, perplexity decreases as the dataset size increases, highlighting the importance of comprehensive content coverage in the databases. Notably, when pairing with the largest database, PipeRAG outperforms <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.4">Retro</span> in generation quality, as we will analyze in greater detail later on.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">From now on, we report results in generation quality and performance based on the full (largest) database</span>, as using subsets significantly compromises generation quality.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.F5" title="Figure 5 ‣ 3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">5</span></a> compares the perplexity between PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.1">Retro</span> across various retrieval configurations. We assess PipeRAG with different retrieval intervals, setting the search space through <math alttext="nprobe" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">n</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">p</mi><mo id="S4.SS2.p3.1.m1.1.1.1a" xref="S4.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p3.1.m1.1.1.4" xref="S4.SS2.p3.1.m1.1.1.4.cmml">r</mi><mo id="S4.SS2.p3.1.m1.1.1.1b" xref="S4.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p3.1.m1.1.1.5" xref="S4.SS2.p3.1.m1.1.1.5.cmml">o</mi><mo id="S4.SS2.p3.1.m1.1.1.1c" xref="S4.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p3.1.m1.1.1.6" xref="S4.SS2.p3.1.m1.1.1.6.cmml">b</mi><mo id="S4.SS2.p3.1.m1.1.1.1d" xref="S4.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p3.1.m1.1.1.7" xref="S4.SS2.p3.1.m1.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝑛</ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">𝑝</ci><ci id="S4.SS2.p3.1.m1.1.1.4.cmml" xref="S4.SS2.p3.1.m1.1.1.4">𝑟</ci><ci id="S4.SS2.p3.1.m1.1.1.5.cmml" xref="S4.SS2.p3.1.m1.1.1.5">𝑜</ci><ci id="S4.SS2.p3.1.m1.1.1.6.cmml" xref="S4.SS2.p3.1.m1.1.1.6">𝑏</ci><ci id="S4.SS2.p3.1.m1.1.1.7.cmml" xref="S4.SS2.p3.1.m1.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math>, which represents the number of scanned vector lists per query in the IVF index. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.F5" title="Figure 5 ‣ 3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">5</span></a>, both PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.2">Retro</span> show reduced perplexity with an expanded search space, which leads to better search quality (O3).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<svg class="ltx_picture" height="50.41" id="S4.SS2.p4.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,50.41) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 33.68 C 3.54 39 7.86 43.32 13.19 43.32 L 593.9 43.32 C 599.22 43.32 603.54 39 603.54 33.68 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 38.99 C 0 43.34 3.53 46.87 7.87 46.87 L 592.13 46.87 C 596.47 46.87 600 43.34 600 38.99 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 37.88 C 1.11 42.23 4.63 45.76 8.98 45.76 L 591.02 45.76 C 595.37 45.76 598.89 42.23 598.89 37.88 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS2.p4.pic1.2.2.2.2.2" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.pic1.2.2.2.2.2.1">Takeaway 1:</span> The quality of retrieval-augmented generation benefits from higher retrieval quality achieved by expanding the search space during vector search.</p></foreignobject></g></g></svg>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F6.fig1">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="83" id="S4.F6.g1" src="extracted/5456482/fig/ppl_pareto_eval_wikipedia_chunk9_1K.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F6.fig2">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="85" id="S4.F6.g2" src="extracted/5456482/fig/ppl_pareto_eval_realnews_chunk31_1K.png" width="192"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F6.fig3">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_landscape" height="85" id="S4.F6.g3" src="extracted/5456482/fig/ppl_pareto_eval_c4_chunk1023_1K.png" width="192"/></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>PipeRAG significantly outperforms <span class="ltx_text ltx_font_smallcaps" id="S4.F6.2.1">Retro</span> on the latency-perplexity Pareto frontier (lower latency and perplexity are better).</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance-driven retrieval (S3) facilitates latency comparable to non-retrieval models while significantly reducing perplexity. Values in parentheses indicate the difference compared to the baseline model without retrieval (lower latency and perplexity are better).</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1" style="width:70.3pt;height:846.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.9pt,119.4pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.1" rowspan="2" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="S4.T1.1.1.1.1.1.1">Eval Set</p>
</th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T1.1.1.1.1.2">Latency (s)</th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T1.1.1.1.1.3">Perplexity</th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.1" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.1.1">No retrieval</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.2" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.2.1">RETRO</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.3" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.3.1">Performance-driven retrieval (S3)</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.4" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.4.1">No retrieval</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.5" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.5.1">RETRO</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.6" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.2.2.6.1">Performance-driven retrieval (S3)</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.3.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.3.1.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="S4.T1.1.1.3.1.1.1">Wikipedia</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.2.1">9.35</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.3" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.3.1">14.59 (+5.23)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.4" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.4.1">10.34 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1.4.1.1">+0.99</span>)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.5.1">16.74</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.6" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.6.1">13.49 (-3.25)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.3.1.7" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.3.1.7.1">13.47 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1.7.1.1">-3.28</span>)</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row" id="S4.T1.1.1.4.2.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="S4.T1.1.1.4.2.1.1">RealNews</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.2.1">9.35</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.3" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.3.1">12.36 (+3.00)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.4" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.4.1">10.58 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.2.4.1.1">+1.22</span>)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.5.1">17.37</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.6" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.6.1">14.94 (-2.43)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.1.4.2.7" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.4.2.7.1">14.87 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.2.7.1.1">-2.50</span>)</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.5.3.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="S4.T1.1.1.5.3.1.1">C4</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.2.1">9.35</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.3" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.3.1">11.13 (+1.78)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.4" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.4.1">10.58 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.4.1.1">+1.22</span>)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.5.1">24.18</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.6" style="width:60.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.6.1">19.48 (-4.70)</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S4.T1.1.1.5.3.7" style="width:140.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="S4.T1.1.1.5.3.7.1">19.36 (<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.7.1.1">-4.82</span>)</p>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Furthermore, PipeRAG demonstrates superior generation quality over <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p5.1.1">Retro</span>, particularly when using shorter retrieval intervals of no more than 32 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S3.F5" title="Figure 5 ‣ 3.2 Algorithm-System Co-deisgn in PipeRAG ‣ 3 Our Approach: PipeRAG ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">5</span></a>). This advantage is attributed to PipeRAG’s revised attention mechanism. Shorter intervals not only reduce query staleness (equivalent to the interval) but improve the content integration frequency, in contrast to <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p5.1.2">Retro</span> with a fixed interval of 64. The increased retrieval frequency in PipeRAG does not necessarily add to generation latency thanks to the pipeline parallelism, a point we will further elaborate on.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p6">
<svg class="ltx_picture" height="50.41" id="S4.SS2.p6.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,50.41) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 33.68 C 3.54 39 7.86 43.32 13.19 43.32 L 593.9 43.32 C 599.22 43.32 603.54 39 603.54 33.68 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 38.99 C 0 43.34 3.53 46.87 7.87 46.87 L 592.13 46.87 C 596.47 46.87 600 43.34 600 38.99 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 37.88 C 1.11 42.23 4.63 45.76 8.98 45.76 L 591.02 45.76 C 595.37 45.76 598.89 42.23 598.89 37.88 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS2.p6.pic1.2.2.2.2.2" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.pic1.2.2.2.2.2.1">Takeaway 2:</span> PipeRAG can surpass <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p6.pic1.2.2.2.2.2.2">Retro</span> in generation quality when using shorter retrieval intervals backed by PipeRAG’s attention mechanism.</p></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>RAG Efficiency: Performance-Quality Trade-offs</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this section, we assess the efficiency of PipeRAG. Our primary performance metric is the end-to-end latency to generate a 1024-token sequence, which we reported by taking the median latency of five individual runs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F6" title="Figure 6 ‣ 4.2 Perplexity Evaluation ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">6</span></a> compares the Pareto frontiers of the performance-quality (latency-perplexity) trade-offs between PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.2.1">Retro</span>.
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.2.2">Retro</span>, we manipulate the search space by tuning <math alttext="nprobe" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">n</mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">p</mi><mo id="S4.SS3.p2.1.m1.1.1.1a" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.4" xref="S4.SS3.p2.1.m1.1.1.4.cmml">r</mi><mo id="S4.SS3.p2.1.m1.1.1.1b" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.5" xref="S4.SS3.p2.1.m1.1.1.5.cmml">o</mi><mo id="S4.SS3.p2.1.m1.1.1.1c" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.6" xref="S4.SS3.p2.1.m1.1.1.6.cmml">b</mi><mo id="S4.SS3.p2.1.m1.1.1.1d" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.7" xref="S4.SS3.p2.1.m1.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑛</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑝</ci><ci id="S4.SS3.p2.1.m1.1.1.4.cmml" xref="S4.SS3.p2.1.m1.1.1.4">𝑟</ci><ci id="S4.SS3.p2.1.m1.1.1.5.cmml" xref="S4.SS3.p2.1.m1.1.1.5">𝑜</ci><ci id="S4.SS3.p2.1.m1.1.1.6.cmml" xref="S4.SS3.p2.1.m1.1.1.6">𝑏</ci><ci id="S4.SS3.p2.1.m1.1.1.7.cmml" xref="S4.SS3.p2.1.m1.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">nprobe</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_n italic_p italic_r italic_o italic_b italic_e</annotation></semantics></math>.
For PipeRAG, we explore a range of retrieval intervals in conjunction with either a fixed search space or the performance-model-driven search space selection (S3).
Across all datasets, the Pareto frontier of PipeRAG demonstrates significant advantages over <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.2.3">Retro</span>, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F6" title="Figure 6 ‣ 4.2 Perplexity Evaluation ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">6</span></a>. For example, PipeRAG can attain up to a 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mo id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><times id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">×</annotation></semantics></math> reduction in latency while maintaining or reducing perplexity relative to <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.2.4">Retro</span>; alternatively, under the same latency constraint, PipeRAG can lower perplexity by as much as 0.93 points compared to <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.2.5">Retro</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<svg class="ltx_picture" height="50.41" id="S4.SS3.p3.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,50.41) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 33.68 C 3.54 39 7.86 43.32 13.19 43.32 L 593.9 43.32 C 599.22 43.32 603.54 39 603.54 33.68 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 38.99 C 0 43.34 3.53 46.87 7.87 46.87 L 592.13 46.87 C 596.47 46.87 600 43.34 600 38.99 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 37.88 C 1.11 42.23 4.63 45.76 8.98 45.76 L 591.02 45.76 C 595.37 45.76 598.89 42.23 598.89 37.88 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">Takeaway 3:</span> PipeRAG shows impressive efficiency, achieving up to 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1"><semantics id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1a"><mo id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1.1" xref="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1b"><times id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1.1.cmml" xref="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.m1.1d">×</annotation></semantics></math> speedup in latency over <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.2">Retro</span> without compromising generation quality.</p></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.T1" title="Table 1 ‣ 4.2 Perplexity Evaluation ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates the effectiveness of the proposed performance-model-driven retrieval system. The objective of the performance model is to dynamically maximize search quality while minimizing additional performance costs.
To evaluate this, we compare the generation latency and quality of PipeRAG applying performance-model-driven retrievals to that of <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p4.2.1">Retro</span> as well as the same base <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p4.2.2">Retro</span> model without invoking retrievals.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.T1" title="Table 1 ‣ 4.2 Perplexity Evaluation ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>, PipeRAG achieves a notable reduction in perplexity (2.50<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><mo id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">∼</annotation></semantics></math>4.82) with a minor increase in performance overhead (merely 10.6%<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS3.p4.2.m2.1"><semantics id="S4.SS3.p4.2.m2.1a"><mo id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><csymbol cd="latexml" id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.2.m2.1d">∼</annotation></semantics></math>13.2% in latency overhead), outperformance <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p4.2.3">Retro</span> in both latency and perplexity.
This slight increase in latency is attributed to the extra computational workload of the cross-attention mechanism when integrating the retrieved content from the encoder.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<svg class="ltx_picture" height="67.01" id="S4.SS3.p5.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,67.01) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 50.28 C 3.54 55.61 7.86 59.93 13.19 59.93 L 593.9 59.93 C 599.22 59.93 603.54 55.61 603.54 50.28 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 55.6 C 0 59.94 3.53 63.47 7.87 63.47 L 592.13 63.47 C 596.47 63.47 600 59.94 600 55.6 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 54.49 C 1.11 58.84 4.63 62.36 8.98 62.36 L 591.02 62.36 C 595.37 62.36 598.89 58.84 598.89 54.49 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS3.p5.pic1.2.2.2.2.2" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.pic1.2.2.2.2.2.1">Takeaway 4:</span>
Leveraging the performance-model-driven retrieval system, PipeRAG can achieve comparable latency to models without retrievals while significantly improving generation quality.</p></foreignobject></g></g></svg>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S4.F7.g1" src="extracted/5456482/fig/ppl_alternative_system_performance_eval_c4_chunk1023_1K_db_c4_chunk_0_to_999.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Trends in PipeRAG efficiency when deployed on future hardware that enables faster retrieval or inference.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F7" title="Figure 7 ‣ 4.3 RAG Efficiency: Performance-Quality Trade-offs ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the projected performance trends of PipeRAG across a range of system and hardware configurations. Considering the rapid advancements in hardware accelerators, we expect shifts in performance of both retrieval and inference systems over years. To analyze PipeRAG’s effectiveness on future hardware, we model the latency of PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p6.3.1">Retro</span> when using faster retrieval or inference systems, with the methodology described in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A3" title="Appendix C Performance Trends on Evolving Hardware ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">C</span></a>.
The first row of Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F7" title="Figure 7 ‣ 4.3 RAG Efficiency: Performance-Quality Trade-offs ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">7</span></a> demonstrates the generation latency when the inference system becomes 4<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p6.1.m1.1"><semantics id="S4.SS3.p6.1.m1.1a"><mo id="S4.SS3.p6.1.m1.1.1" xref="S4.SS3.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.1.m1.1b"><times id="S4.SS3.p6.1.m1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p6.1.m1.1d">×</annotation></semantics></math> and 16<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p6.2.m2.1"><semantics id="S4.SS3.p6.2.m2.1a"><mo id="S4.SS3.p6.2.m2.1.1" xref="S4.SS3.p6.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.2.m2.1b"><times id="S4.SS3.p6.2.m2.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p6.2.m2.1d">×</annotation></semantics></math> faster, while the second row examines the effects of accelerated retrieval. Across all scenarios, PipeRAG achieves superior efficiency compared to <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p6.3.2">Retro</span>. When either system experiences an order of magnitude speedup (e.g., 16<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p6.3.m3.1"><semantics id="S4.SS3.p6.3.m3.1a"><mo id="S4.SS3.p6.3.m3.1.1" xref="S4.SS3.p6.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.3.m3.1b"><times id="S4.SS3.p6.3.m3.1.1.cmml" xref="S4.SS3.p6.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p6.3.m3.1d">×</annotation></semantics></math>), however, the benefits of applying PipeRAG become less significant. This trend aligns with our expectations, as the effectiveness of pipeline parallelism peaks when both system components have comparable latencies and diminishes when one component significantly outpaces the other.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p7">
<svg class="ltx_picture" height="50.41" id="S4.SS3.p7.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,50.41) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 33.68 C 3.54 39 7.86 43.32 13.19 43.32 L 593.9 43.32 C 599.22 43.32 603.54 39 603.54 33.68 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 38.99 C 0 43.34 3.53 46.87 7.87 46.87 L 592.13 46.87 C 596.47 46.87 600 43.34 600 38.99 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 37.88 C 1.11 42.23 4.63 45.76 8.98 45.76 L 591.02 45.76 C 595.37 45.76 598.89 42.23 598.89 37.88 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS3.p7.pic1.2.2.2.2.2" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS3.p7.pic1.2.2.2.2.2.1">Takeaway 5:</span> PipeRAG outperforms <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p7.pic1.2.2.2.2.2.2">Retro</span> in efficiency across different hardware, though the extent of improvements depends on sub-system performance.</p></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Since PipeRAG not only introduces pipeline parallelism but also modifies <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.1">Retro</span>’s attention mechanism to maximize the effectiveness of pipelining, it is natural to ask how a baseline model would perform if it integrates the same attention mechanism. To illustrate the effectiveness of pipeline parallelism itself, we compare PipeRAG with an enhanced variant of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.2">Retro</span>, named <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.3">Retro+</span>, which also supports flexible retrieval intervals by integrating PipeRAG’s attention mechanism.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F8" title="Figure 8 ‣ 4.4 Ablation Study ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">8</span></a> compares the performance-quality Pareto-frontier between PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.1">Retro+</span>.
Both models use retrieval intervals ranging from 8 to 64.
While <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.2">Retro+</span>, benefiting from flexible intervals, matches PipeRAG in perplexity, PipeRAG consistently achieves lower latency given the same perplexity. This is attributed to the proposed pipeline parallelism: PipeRAG effectively hides the retrieval latencies by overlapping them with generation latencies, whereas for <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.3">Retro+</span>, more frequent retrievals lead to increased total generation latency. More detailed comparisons between PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.4">Retro+</span> under identical retrieval intervals (corresponding to the same number of database requests) can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4" title="Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p3">
<svg class="ltx_picture" height="63.96" id="S4.SS4.p3.pic1" overflow="visible" version="1.1" width="603.54"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,63.96) matrix(1 0 0 -1 0 0) translate(0,-3.54) translate(0,3.54)"><g fill="#808080" fill-opacity="0.500000" stroke="#808080" stroke-opacity="0.500000"><path d="M 3.54 6.1 L 3.54 47.22 C 3.54 52.55 7.86 56.87 13.19 56.87 L 593.9 56.87 C 599.22 56.87 603.54 52.55 603.54 47.22 L 603.54 6.1 C 603.54 0.78 599.22 -3.54 593.9 -3.54 L 13.19 -3.54 C 7.86 -3.54 3.54 0.78 3.54 6.1 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.000000"><path d="M 0 7.87 L 0 52.54 C 0 56.89 3.53 60.41 7.87 60.41 L 592.13 60.41 C 596.47 60.41 600 56.89 600 52.54 L 600 7.87 C 600 3.53 596.47 0 592.13 0 L 7.87 0 C 3.53 0 0 3.53 0 7.87 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.11 8.98 L 1.11 51.43 C 1.11 55.78 4.63 59.3 8.98 59.3 L 591.02 59.3 C 595.37 59.3 598.89 55.78 598.89 51.43 L 598.89 8.98 C 598.89 4.63 595.37 1.11 591.02 1.11 L 8.98 1.11 C 4.63 1.11 1.11 4.63 1.11 8.98 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 16.85 8.98)"><foreignobject height="42.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="566.29">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.SS4.p3.pic1.2.2.2.2.2" style="width:409.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.pic1.2.2.2.2.2.1">Takeaway 6:</span> Pipeline parallelism is essential to achieve superior RAG efficiency, as PipeRAG outperforms <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p3.pic1.2.2.2.2.2.2">Retro+</span> that supports flexible retrieval intervals using PipeRAG’s attention mechanism.</p></foreignobject></g></g></svg>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F8.fig1">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_square" height="86" id="S4.F8.g1" src="extracted/5456482/fig/ppl_RETRO_flexible_interval_pareto_eval_wikipedia_chunk9_1K.png" width="106"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F8.fig2">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_square" height="86" id="S4.F8.g2" src="extracted/5456482/fig/ppl_RETRO_flexible_interval_pareto_eval_realnews_chunk31_1K.png" width="99"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_6 ltx_flex_size_many" id="S4.F8.fig3">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_6 ltx_flex_size_many ltx_img_square" height="86" id="S4.F8.g3" src="extracted/5456482/fig/ppl_RETRO_flexible_interval_pareto_eval_c4_chunk1023_1K.png" width="99"/></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Even if the baseline model supports flexible retrieval intervals (<span class="ltx_text ltx_font_smallcaps" id="S4.F8.2.1">Retro+</span>), PipeRAG still significantly outperforms it in efficiency thanks to the proposed pipeline parallelism.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To the best of our knowledge, PipeRAG represents the first endeavor to enhance RAG efficiency through an in-depth algorithm-system co-design, diverging from existing RAG research that mainly focuses on improving generation quality. We now briefly introduce these related works.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Since knowledge is primarily retrieved rather than encoded in the LLM’s parameters, RALMs, even with LLMs of one to two orders of magnitude fewer parameters, can achieve superior or comparable performance to conventional LLMs on various natural language processing (NLP) tasks <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib25" title="">2020a</a>; Izacard et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib14" title="">2022</a>; Komeili et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib24" title="">2021</a>; Guu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib11" title="">2020</a>)</cite>. While the generation may only involve a single passage retrieval at the beginning <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib26" title="">2020b</a>; Izacard &amp; Grave, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib13" title="">2020</a>; Sachan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib37" title="">2021</a>)</cite>, the generated sequence may gradually diverge from the initially retrieved contents as the sequence grows longer. Thus, a more generaral RAG approach involves multiple retrievals during text generation to improve token generation quality <cite class="ltx_cite ltx_citemacro_citep">(Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib35" title="">2023</a>; Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Another line of RAG research emphasizes token-level retrievals, exemplified by kNN-LM <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib21" title="">2019</a>)</cite> and subsequent works <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib22" title="">2020</a>; Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib30" title="">2021</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib43" title="">2023</a>)</cite>. In these models, during each token generation step, the hidden state of the last layer is used as a query to retrieve contextually similar passages as well as their subsequent tokens (with a retrieval interval of one). The next token of the current context is then predicted by interpolating the model’s next-token probability distribution with that of the retrieved contents. There are also arguments suggesting that token-level content integration may not be as effective as integrating longer passages <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib42" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Retrieval-augmented generation presents both opportunities and efficiency challenges, due to the significant overheads when retrieving from large databases.
We propose PipeRAG, a novel RAG approach that improves generation efficiency by adopting pipeline parallelism, allowing flexible retrieval intervals, and dynamically adjusting retrieval quality via performance modeling. PipeRAG achieves up to 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mo id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><times id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">×</annotation></semantics></math> speedup over <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">Retro</span> without compromising generation quality. This not only establishes a solid foundation for integrating pipeline parallelism in future RAG systems but also showcasing future research opportunities in optimizing RAG through algorithm-system co-design.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Impact Statements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This paper focuses on enhancing the system efficiency of retrieval-augmented generation, aiming to reduce both energy consumption and carbon emissions during large-scale LLM inference. As our work does not involve training new models, we anticipate minimal ethical concerns or adverse societal impacts.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
The wikipedia dataset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/datasets/community_catalog/huggingface/wikipedia" title="">https://www.tensorflow.org/datasets/community_catalog/huggingface/wikipedia</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
Beltagy, I., Peters, M. E., and Cohan, A.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2004.05150</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. (2022)</span>
<span class="ltx_bibblock">
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International conference on machine learning</em>, pp.  2206–2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai &amp; Callan (2019)</span>
<span class="ltx_bibblock">
Dai, Z. and Callan, J.

</span>
<span class="ltx_bibblock">Deeper text understanding for ir with contextual neural language modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pp.  985–988, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al. (2021)</span>
<span class="ltx_bibblock">
Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M.

</span>
<span class="ltx_bibblock">Documenting large webtext corpora: A case study on the colossal clean crawled corpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2104.08758</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doostmohammadi et al. (2023)</span>
<span class="ltx_bibblock">
Doostmohammadi, E., Norlund, T., Kuhlmann, M., and Johansson, R.

</span>
<span class="ltx_bibblock">Surface-based retrieval reduces perplexity of retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2305.16243</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al.

</span>
<span class="ltx_bibblock">Glam: Efficient scaling of language models with mixture-of-experts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Machine Learning</em>, pp.  5547–5569. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et al. (2022)</span>
<span class="ltx_bibblock">
Fedus, W., Zoph, B., and Shazeer, N.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">The Journal of Machine Learning Research</em>, 23(1):5232–5270, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et al. (2022)</span>
<span class="ltx_bibblock">
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.

</span>
<span class="ltx_bibblock">Gptq: Accurate post-training quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International conference on machine learning</em>, pp.  3929–3938. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2013)</span>
<span class="ltx_bibblock">
Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L.

</span>
<span class="ltx_bibblock">Learning deep structured semantic models for web search using clickthrough data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>, pp.  2333–2338, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard &amp; Grave (2020)</span>
<span class="ltx_bibblock">
Izacard, G. and Grave, E.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2007.01282</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2022)</span>
<span class="ltx_bibblock">
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.

</span>
<span class="ltx_bibblock">Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2208.03299</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jegou et al. (2010)</span>
<span class="ltx_bibblock">
Jegou, H., Douze, M., and Schmid, C.

</span>
<span class="ltx_bibblock">Product quantization for nearest neighbor search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 33(1):117–128, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Jiang, W., Li, S., Zhu, Y., de Fine Licht, J., He, Z., Shi, R., Renggli, C., Zhang, S., Rekatsinas, T., Hoefler, T., et al.

</span>
<span class="ltx_bibblock">Co-design hardware and algorithm for vector search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, pp.  1–15, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Jiang, W., Zeller, M., Waleffe, R., Hoefler, T., and Alonso, G.

</span>
<span class="ltx_bibblock">Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2310.09949</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023c)</span>
<span class="ltx_bibblock">
Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2305.06983</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019)</span>
<span class="ltx_bibblock">
Johnson, J., Douze, M., and Jégou, H.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE Transactions on Big Data</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2004.04906</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al. (2019)</span>
<span class="ltx_bibblock">
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:1911.00172</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al. (2020)</span>
<span class="ltx_bibblock">
Khandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and Lewis, M.

</span>
<span class="ltx_bibblock">Nearest neighbor machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2010.00710</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab &amp; Zaharia (2020)</span>
<span class="ltx_bibblock">
Khattab, O. and Zaharia, M.

</span>
<span class="ltx_bibblock">Colbert: Efficient and effective passage search via contextualized late interaction over bert.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</em>, pp.  39–48, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komeili et al. (2021)</span>
<span class="ltx_bibblock">
Komeili, M., Shuster, K., and Weston, J.

</span>
<span class="ltx_bibblock">Internet-augmented dialogue generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2107.07566</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020a)</span>
<span class="ltx_bibblock">
Lewis, M., Ghazvininejad, M., Ghosh, G., Aghajanyan, A., Wang, S., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Pre-training via paraphrasing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</em>, 33:18470–18481, 2020a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020b)</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacAvaney et al. (2019)</span>
<span class="ltx_bibblock">
MacAvaney, S., Yates, A., Cohan, A., and Goharian, N.

</span>
<span class="ltx_bibblock">Cedr: Contextualized embeddings for document ranking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</em>, pp.  1101–1104, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov et al. (2014)</span>
<span class="ltx_bibblock">
Malkov, Y., Ponomarenko, A., Logvinov, A., and Krylov, V.

</span>
<span class="ltx_bibblock">Approximate nearest neighbor algorithm based on navigable small world graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Information Systems</em>, 45:61–68, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov &amp; Yashunin (2018)</span>
<span class="ltx_bibblock">
Malkov, Y. A. and Yashunin, D. A.

</span>
<span class="ltx_bibblock">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 42(4):824–836, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2021)</span>
<span class="ltx_bibblock">
Meng, Y., Li, X., Zheng, X., Wu, F., Sun, X., Zhang, T., and Li, J.

</span>
<span class="ltx_bibblock">Fast nearest neighbor machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2105.14528</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira &amp; Cho (2019)</span>
<span class="ltx_bibblock">
Nogueira, R. and Cho, K.

</span>
<span class="ltx_bibblock">Passage re-ranking with bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:1901.04085</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Norlund et al. (2023)</span>
<span class="ltx_bibblock">
Norlund, T., Doostmohammadi, E., Johansson, R., and Kuhlmann, M.

</span>
<span class="ltx_bibblock">On the generalization ability of retrieval-enhanced transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2302.12128</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. (2023)</span>
<span class="ltx_bibblock">
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2302.00083</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers &amp; Gurevych (2019)</span>
<span class="ltx_bibblock">
Reimers, N. and Gurevych, I.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1908.10084</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan et al. (2021)</span>
<span class="ltx_bibblock">
Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B.

</span>
<span class="ltx_bibblock">End-to-end training of neural retrievers for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2101.00408</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et al. (2021)</span>
<span class="ltx_bibblock">
Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M.

</span>
<span class="ltx_bibblock">Colbertv2: Effective and efficient retrieval via lightweight late interaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2112.01488</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sivic &amp; Zisserman (2003)</span>
<span class="ltx_bibblock">
Sivic, J. and Zisserman, A.

</span>
<span class="ltx_bibblock">Video google: A text retrieval approach to object matching in videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Computer Vision, IEEE International Conference on</em>, volume 3, pp.  1470–1470. IEEE Computer Society, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2022)</span>
<span class="ltx_bibblock">
Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A.

</span>
<span class="ltx_bibblock">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2212.10509</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Wang, S., Song, Y., Drozdov, A., Garimella, A., Manjunatha, V., and Iyyer, M.

</span>
<span class="ltx_bibblock">Knn-lm does not improve open-ended text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2305.14625</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Xu, F. F., Alon, U., and Neubig, G.

</span>
<span class="ltx_bibblock">Why do nearest neighbor language models work?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2301.02828</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>A Motivating Example of Periodic Retrievals</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we present a concrete example demonstrating the effectiveness of <span class="ltx_text ltx_font_italic" id="A1.p1.1.1">periodic retrievals</span> during sequence generation, a strategy that has been proven to significantly enhance the quality of language modeling <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>; Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib35" title="">2023</a>; Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A1.F9" title="Figure 9 ‣ Appendix A A Motivating Example of Periodic Retrievals ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the example, wherein the model is asked to describe a high-impact machine learning paper. In crafting its response, the model uses the Transformer neural network <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib41" title="">2017</a>)</cite> as the target paper, covering several aspects related to the paper. The narrative evolves from a brief introduction of the model, through its impacts on various natural language processing tasks, to its influence on subsequent research, its cross-disciplinary applications, and ultimately, to emerging trends in research. Given these shifts in topic, the content initially retrieved about the Transformer architecture might lose relevance in the context of discussing future research trends. Therefore, periodic retrievals, in this instance, are vital to ensure that the retrieved content remains pertinent to the current context of generation.</p>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="398" id="A1.F9.g1" src="extracted/5456482/fig/Example_periodic_retrievals.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A motivating example of utilizing periodic retrievals during sequence generation.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Detailed Evaluation Setup</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">Datasets.</span>
We constructed the token database from the C4 dataset, using deduplicated English documents. We did not choose the Pile dataset used in previous works <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib3" title="">2022</a>)</cite> due to its current copyright issues.
By segmenting these documents into chunks of <math alttext="m=64" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mi id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">m</mi><mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">=</mo><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><eq id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></eq><ci id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">𝑚</ci><cn id="A2.p1.1.m1.1.1.3.cmml" type="integer" xref="A2.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">italic_m = 64</annotation></semantics></math> tokens, we generated a total of three billion chunks.
Subsequently, each chunk was converted into a 384-dimensional vector using a sentence transformer <cite class="ltx_cite ltx_citemacro_citep">(Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib36" title="">2019</a>)</cite> checkpoint <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">all-MiniLM-L6-v2</span>.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1"><span class="ltx_text ltx_font_bold" id="A2.p2.1.1">Software.</span>
Our implementation of the PipeRAG model is based on a <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.2">Retro</span> baseline obtained from <cite class="ltx_cite ltx_citemacro_citep">(Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>)</cite>, which is built on top of PyTorch. To enhance inference performance, we supported the caching of key-value states in the transformer and converted the model to ONNX format, enabling model inference by ONNX runtime. With the above optimizations, the inference latency on GPU is improved by around 3<math alttext="\times" class="ltx_Math" display="inline" id="A2.p2.1.m1.1"><semantics id="A2.p2.1.m1.1a"><mo id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><times id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A2.p2.1.m1.1d">×</annotation></semantics></math> over the original Pytorch implementation. We maintained the fp32 (32-bit floating point) precision of the model.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.2">For the retrieval system, we used the Faiss library <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib19" title="">2019</a>)</cite>, which is known for its efficient product-quantization-based vector search implementation.
We adopted the IVF-PQ vector search algorithm, setting the number of IVF list centroids to <math alttext="nlist=16384" class="ltx_Math" display="inline" id="A2.p3.1.m1.1"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mrow id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml"><mi id="A2.p3.1.m1.1.1.2.2" xref="A2.p3.1.m1.1.1.2.2.cmml">n</mi><mo id="A2.p3.1.m1.1.1.2.1" xref="A2.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.p3.1.m1.1.1.2.3" xref="A2.p3.1.m1.1.1.2.3.cmml">l</mi><mo id="A2.p3.1.m1.1.1.2.1a" xref="A2.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.p3.1.m1.1.1.2.4" xref="A2.p3.1.m1.1.1.2.4.cmml">i</mi><mo id="A2.p3.1.m1.1.1.2.1b" xref="A2.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.p3.1.m1.1.1.2.5" xref="A2.p3.1.m1.1.1.2.5.cmml">s</mi><mo id="A2.p3.1.m1.1.1.2.1c" xref="A2.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.p3.1.m1.1.1.2.6" xref="A2.p3.1.m1.1.1.2.6.cmml">t</mi></mrow><mo id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">=</mo><mn id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml">16384</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><eq id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1"></eq><apply id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2"><times id="A2.p3.1.m1.1.1.2.1.cmml" xref="A2.p3.1.m1.1.1.2.1"></times><ci id="A2.p3.1.m1.1.1.2.2.cmml" xref="A2.p3.1.m1.1.1.2.2">𝑛</ci><ci id="A2.p3.1.m1.1.1.2.3.cmml" xref="A2.p3.1.m1.1.1.2.3">𝑙</ci><ci id="A2.p3.1.m1.1.1.2.4.cmml" xref="A2.p3.1.m1.1.1.2.4">𝑖</ci><ci id="A2.p3.1.m1.1.1.2.5.cmml" xref="A2.p3.1.m1.1.1.2.5">𝑠</ci><ci id="A2.p3.1.m1.1.1.2.6.cmml" xref="A2.p3.1.m1.1.1.2.6">𝑡</ci></apply><cn id="A2.p3.1.m1.1.1.3.cmml" type="integer" xref="A2.p3.1.m1.1.1.3">16384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">nlist=16384</annotation><annotation encoding="application/x-llamapun" id="A2.p3.1.m1.1d">italic_n italic_l italic_i italic_s italic_t = 16384</annotation></semantics></math> and quantizing each 384-dimensional vector into 64 bytes of PQ code.
During retrievals, we set the number of nearest neighbors as <math alttext="k=2" class="ltx_Math" display="inline" id="A2.p3.2.m2.1"><semantics id="A2.p3.2.m2.1a"><mrow id="A2.p3.2.m2.1.1" xref="A2.p3.2.m2.1.1.cmml"><mi id="A2.p3.2.m2.1.1.2" xref="A2.p3.2.m2.1.1.2.cmml">k</mi><mo id="A2.p3.2.m2.1.1.1" xref="A2.p3.2.m2.1.1.1.cmml">=</mo><mn id="A2.p3.2.m2.1.1.3" xref="A2.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.2.m2.1b"><apply id="A2.p3.2.m2.1.1.cmml" xref="A2.p3.2.m2.1.1"><eq id="A2.p3.2.m2.1.1.1.cmml" xref="A2.p3.2.m2.1.1.1"></eq><ci id="A2.p3.2.m2.1.1.2.cmml" xref="A2.p3.2.m2.1.1.2">𝑘</ci><cn id="A2.p3.2.m2.1.1.3.cmml" type="integer" xref="A2.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.2.m2.1c">k=2</annotation><annotation encoding="application/x-llamapun" id="A2.p3.2.m2.1d">italic_k = 2</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">The communication between the inference and retrieval systems was managed via the gRPC library.</p>
</div>
<div class="ltx_para" id="A2.p5">
<p class="ltx_p" id="A2.p5.1"><span class="ltx_text ltx_font_bold" id="A2.p5.1.1">Hardware.</span>
We used two separate platforms for inference and retrievals. For model inference, we utilized an NVIDIA A100 GPU (40 GB). The retrieval process was handled by a server with substantial memory capacity to accommodate the large encoded dataset. The server was equipped with dual-socket Intel(R) Xeon(R) Platinum 8259CL CPUs @2.50GHz (48 cores and 96 threads) and 384 GB memory.
The retrieval and inference servers were interconnected through a network, with a round-trip time (RTT) of around 1 ms.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Performance Trends on Evolving Hardware</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we begin by enumerating the factors that influence retrieval and inference performance. We then introduce the performance modeling methodology employed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>, which projects PipeRAG’s efficiency on future hardware configurations.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Factors Influencing Retrieval and Inference Performance</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.1">Retrieval performance</span> depends on the following factors:</p>
</div>
<div class="ltx_para" id="A3.SS1.p2">
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i1.p1.1.1">Hardware.</span> The memory bandwidth and computational capacity of the hardware used for retrieval are key factors influencing performance. It is worth noticing that there are emerging hardware accelerators that are specialized for retrievals <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib16" title="">2023a</a>)</cite> and integrated into RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib17" title="">2023b</a>)</cite>, offering impressive retrieval performance as well as cost efficiency.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i2.p1.1.1">Document numbers.</span> The total number of documents, along with encoding granularity as introduced below, determines the vector count in the database.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i3.p1.1.1">Encoding granularity.</span> Documents can be encoded in various granularities by LLMs, ranging from one vector per document <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib12" title="">2013</a>; Karpukhin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib20" title="">2020</a>)</cite> to one vector per passage <cite class="ltx_cite ltx_citemacro_citep">(Dai &amp; Callan, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib5" title="">2019</a>; Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib36" title="">2019</a>)</cite> or even per token <cite class="ltx_cite ltx_citemacro_citep">(Khattab &amp; Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib23" title="">2020</a>; Santhanam et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib38" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i4.p1.1.1">Dimensionality.</span> The dimensionality of the database vectors, as well as the compression ratio when employing product quantization, are critical to retrieval performance.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i5.p1">
<p class="ltx_p" id="A3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i5.p1.1.1">Indexes.</span> The selection of indexes, such as IVF or graph-based ones, and their parameter configurations are crucial for retrieval efficiency.
</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i6.p1">
<p class="ltx_p" id="A3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i6.p1.1.1">Reranking.</span> Optionally, the retrieved content can be reranked using LLMs, which often yields better ranking quality than relying solely on vector similarity <cite class="ltx_cite ltx_citemacro_citep">(Nogueira &amp; Cho, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib31" title="">2019</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p3.1.1">LLM inference performance</span> is influenced by the following factors:</p>
</div>
<div class="ltx_para" id="A3.SS1.p4">
<ul class="ltx_itemize" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">Hardware.</span> The performance of inference is heavily dependent on the hardware, particularly its memory bandwidth and computational capacity. LLM accelerators such as GPUs are evolving rapidly in these metrics.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">Software.</span> The choice of software for inference also plays a significant role. For instance, PyTorch’s eager execution mode might not fully exploit hardware accelerators due to the slow execution speed of Python programs. In such cases, software overhead could exceed the GPU kernel execution time.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.2"><span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.2.1">Quantization</span>. Quantizing models to lower precisions can markedly reduce inference time, thanks to reduced memory footprint and bandwidth usage. For instance, converting models to 3-bit precision can lead to a 3<math alttext="\sim" class="ltx_Math" display="inline" id="A3.I2.i3.p1.1.m1.1"><semantics id="A3.I2.i3.p1.1.m1.1a"><mo id="A3.I2.i3.p1.1.m1.1.1" xref="A3.I2.i3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A3.I2.i3.p1.1.m1.1b"><csymbol cd="latexml" id="A3.I2.i3.p1.1.m1.1.1.cmml" xref="A3.I2.i3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.I2.i3.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="A3.I2.i3.p1.1.m1.1d">∼</annotation></semantics></math>5<math alttext="\times" class="ltx_Math" display="inline" id="A3.I2.i3.p1.2.m2.1"><semantics id="A3.I2.i3.p1.2.m2.1a"><mo id="A3.I2.i3.p1.2.m2.1.1" xref="A3.I2.i3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.I2.i3.p1.2.m2.1b"><times id="A3.I2.i3.p1.2.m2.1.1.cmml" xref="A3.I2.i3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.I2.i3.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.I2.i3.p1.2.m2.1d">×</annotation></semantics></math> speedup compared to 16-bit floating point formats <cite class="ltx_cite ltx_citemacro_citep">(Frantar et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib10" title="">2022</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">Sparsity</span>. Techniques like mixture-of-experts allow for scaling LLMs without proportionate increases in computational costs <cite class="ltx_cite ltx_citemacro_citep">(Fedus et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib9" title="">2022</a>; Du et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib8" title="">2022</a>)</cite>, because only a small subset of neurons are activated during inference.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Performance Modeling for Future Hardware</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">To estimate PipeRAG’s efficiency on future hardware, we model its performance using hypothetical hardware with enhanced inference and/or retrieval performance. We included the modeled performance in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>, with a detailed explanation of our performance modeling approach provided here.</p>
</div>
<div class="ltx_para" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1">For <span class="ltx_text ltx_font_smallcaps" id="A3.SS2.p2.1.1">Retro</span>, the end-to-end generation latency is the sum of inference and retrieval time. In PipeRAG, due to the parallelism, the latency for generating a chunk of tokens is determined by the maximum value of the inference and retrieval latency of that chunk, except for the first chunk where the pipeline is not yet active (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="A3.SS2.p3">
<p class="ltx_p" id="A3.SS2.p3.2">We then input the measured performance of inference and retrievals into the performance model. This allows us to simulate performance scaling, such as a 4<math alttext="\times" class="ltx_Math" display="inline" id="A3.SS2.p3.1.m1.1"><semantics id="A3.SS2.p3.1.m1.1a"><mo id="A3.SS2.p3.1.m1.1.1" xref="A3.SS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p3.1.m1.1b"><times id="A3.SS2.p3.1.m1.1.1.cmml" xref="A3.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p3.1.m1.1d">×</annotation></semantics></math> improvement in retrieval or a 16<math alttext="\times" class="ltx_Math" display="inline" id="A3.SS2.p3.2.m2.1"><semantics id="A3.SS2.p3.2.m2.1a"><mo id="A3.SS2.p3.2.m2.1.1" xref="A3.SS2.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p3.2.m2.1b"><times id="A3.SS2.p3.2.m2.1.1.cmml" xref="A3.SS2.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p3.2.m2.1d">×</annotation></semantics></math> enhancement in inference. The result generation latency as well as the respective conclusions are included in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>. The model’s accuracy is then verified by comparing these projected results against actual experimental data, with deviations found to be within a reasonable range (the median difference is only 5.7%).</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Experimental Results</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we include additional experimental results to further illustrate the effectiveness of PipeRAG.
First, we demonstrate the fundamental applicability of pipeline parallelism by illustrating the effectiveness of prefetching content with stale queries. Second, we show the advantages of PipeRAG over a modified version of RETRO, which, similar to PipeRAG, supports flexible retrieval intervals, highlighting the benefits of pipeline parallelism.</p>
</div>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>The Effectiveness of Retrievals using Stale Queries</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.2">We investigate the fundamental applicability of prefetching content using stale queries. For this purpose, we compare the <math alttext="k=1" class="ltx_Math" display="inline" id="A4.SS1.p1.1.m1.1"><semantics id="A4.SS1.p1.1.m1.1a"><mrow id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml"><mi id="A4.SS1.p1.1.m1.1.1.2" xref="A4.SS1.p1.1.m1.1.1.2.cmml">k</mi><mo id="A4.SS1.p1.1.m1.1.1.1" xref="A4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A4.SS1.p1.1.m1.1.1.3" xref="A4.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><apply id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1"><eq id="A4.SS1.p1.1.m1.1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1.1"></eq><ci id="A4.SS1.p1.1.m1.1.1.2.cmml" xref="A4.SS1.p1.1.m1.1.1.2">𝑘</ci><cn id="A4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="A4.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.1.m1.1d">italic_k = 1</annotation></semantics></math> nearest neighbors retrieved by non-stale queries in our evaluation set with their staleness versions. Same as Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>, we use the largest C4 database, which consists of three billion token chunks, and set <math alttext="nprobe=64" class="ltx_Math" display="inline" id="A4.SS1.p1.2.m2.1"><semantics id="A4.SS1.p1.2.m2.1a"><mrow id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml"><mrow id="A4.SS1.p1.2.m2.1.1.2" xref="A4.SS1.p1.2.m2.1.1.2.cmml"><mi id="A4.SS1.p1.2.m2.1.1.2.2" xref="A4.SS1.p1.2.m2.1.1.2.2.cmml">n</mi><mo id="A4.SS1.p1.2.m2.1.1.2.1" xref="A4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A4.SS1.p1.2.m2.1.1.2.3" xref="A4.SS1.p1.2.m2.1.1.2.3.cmml">p</mi><mo id="A4.SS1.p1.2.m2.1.1.2.1a" xref="A4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A4.SS1.p1.2.m2.1.1.2.4" xref="A4.SS1.p1.2.m2.1.1.2.4.cmml">r</mi><mo id="A4.SS1.p1.2.m2.1.1.2.1b" xref="A4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A4.SS1.p1.2.m2.1.1.2.5" xref="A4.SS1.p1.2.m2.1.1.2.5.cmml">o</mi><mo id="A4.SS1.p1.2.m2.1.1.2.1c" xref="A4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A4.SS1.p1.2.m2.1.1.2.6" xref="A4.SS1.p1.2.m2.1.1.2.6.cmml">b</mi><mo id="A4.SS1.p1.2.m2.1.1.2.1d" xref="A4.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="A4.SS1.p1.2.m2.1.1.2.7" xref="A4.SS1.p1.2.m2.1.1.2.7.cmml">e</mi></mrow><mo id="A4.SS1.p1.2.m2.1.1.1" xref="A4.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A4.SS1.p1.2.m2.1.1.3" xref="A4.SS1.p1.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><apply id="A4.SS1.p1.2.m2.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1"><eq id="A4.SS1.p1.2.m2.1.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1.1"></eq><apply id="A4.SS1.p1.2.m2.1.1.2.cmml" xref="A4.SS1.p1.2.m2.1.1.2"><times id="A4.SS1.p1.2.m2.1.1.2.1.cmml" xref="A4.SS1.p1.2.m2.1.1.2.1"></times><ci id="A4.SS1.p1.2.m2.1.1.2.2.cmml" xref="A4.SS1.p1.2.m2.1.1.2.2">𝑛</ci><ci id="A4.SS1.p1.2.m2.1.1.2.3.cmml" xref="A4.SS1.p1.2.m2.1.1.2.3">𝑝</ci><ci id="A4.SS1.p1.2.m2.1.1.2.4.cmml" xref="A4.SS1.p1.2.m2.1.1.2.4">𝑟</ci><ci id="A4.SS1.p1.2.m2.1.1.2.5.cmml" xref="A4.SS1.p1.2.m2.1.1.2.5">𝑜</ci><ci id="A4.SS1.p1.2.m2.1.1.2.6.cmml" xref="A4.SS1.p1.2.m2.1.1.2.6">𝑏</ci><ci id="A4.SS1.p1.2.m2.1.1.2.7.cmml" xref="A4.SS1.p1.2.m2.1.1.2.7">𝑒</ci></apply><cn id="A4.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="A4.SS1.p1.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">nprobe=64</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.2.m2.1d">italic_n italic_p italic_r italic_o italic_b italic_e = 64</annotation></semantics></math> to ensure high retrieval quality. We then employ the <span class="ltx_text ltx_font_italic" id="A4.SS1.p1.2.1">msmarco-bert-base-dot-v5</span> checkpoint from sentence transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib36" title="">2019</a>)</cite> to evaluate the cosine similarity between contents retrieved by stale and non-stale queries.</p>
</div>
<div class="ltx_para" id="A4.SS1.p2">
<p class="ltx_p" id="A4.SS1.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4.T2" title="Table 2 ‣ D.1 The Effectiveness of Retrievals using Stale Queries ‣ Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">2</span></a> presents the retrieval quality using stale queries. Here, we use different degrees of staleness, ranging from 1 token to 64 tokens, while maintaining a consistent retrieval interval of <math alttext="m=64" class="ltx_Math" display="inline" id="A4.SS1.p2.1.m1.1"><semantics id="A4.SS1.p2.1.m1.1a"><mrow id="A4.SS1.p2.1.m1.1.1" xref="A4.SS1.p2.1.m1.1.1.cmml"><mi id="A4.SS1.p2.1.m1.1.1.2" xref="A4.SS1.p2.1.m1.1.1.2.cmml">m</mi><mo id="A4.SS1.p2.1.m1.1.1.1" xref="A4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="A4.SS1.p2.1.m1.1.1.3" xref="A4.SS1.p2.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.SS1.p2.1.m1.1b"><apply id="A4.SS1.p2.1.m1.1.1.cmml" xref="A4.SS1.p2.1.m1.1.1"><eq id="A4.SS1.p2.1.m1.1.1.1.cmml" xref="A4.SS1.p2.1.m1.1.1.1"></eq><ci id="A4.SS1.p2.1.m1.1.1.2.cmml" xref="A4.SS1.p2.1.m1.1.1.2">𝑚</ci><cn id="A4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="A4.SS1.p2.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p2.1.m1.1c">m=64</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p2.1.m1.1d">italic_m = 64</annotation></semantics></math>. The results indicate that, despite the staleness, the retrieved content closely resembles that obtained through non-stale queries, with around 90% cosine similarity across datasets. As expected, this similarity shows a gradual decline as the staleness increases.</p>
</div>
<figure class="ltx_table" id="A4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Cosine similarity between content retrieved by stale and non-stale queries. The results indicate that stale queries are still highly effective in identifying relevant token chunks from the database.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="A4.T2.1" style="width:89.9pt;height:391.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.9pt,34.6pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A4.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T2.1.1.1.1">
<th class="ltx_td ltx_align_middle ltx_th ltx_th_row ltx_border_tt" id="A4.T2.1.1.1.1.1" style="width:40.0pt;"></th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt" id="A4.T2.1.1.1.1.2" rowspan="2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.1.1.2.1">No staleness</p>
</th>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_tt" colspan="7" id="A4.T2.1.1.1.1.3">Staleness (number of stale tokens in the query)</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.1.2.2">
<th class="ltx_td ltx_align_middle ltx_th ltx_th_row" id="A4.T2.1.1.2.2.1" style="width:40.0pt;"></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.2.1">1</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.3" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.3.1">2</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.4" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.4.1">4</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.5.1">8</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.6" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.6.1">16</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.7" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.7.1">32</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.2.2.8" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.2.2.8.1">64</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.1.3.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="A4.T2.1.1.3.3.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="A4.T2.1.1.3.3.1.1">Wikipedia</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="A4.T2.1.1.3.3.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.2.1">1.0000</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.3" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.3.1">0.9262</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.4" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.4.1">0.9204</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.5.1">0.9138</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.6" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.6.1">0.9062</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.7" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.7.1">0.8990</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.8" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.8.1">0.8921</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A4.T2.1.1.3.3.9" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.3.3.9.1">0.8875</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.1.4.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row" id="A4.T2.1.1.4.4.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="A4.T2.1.1.4.4.1.1">RealNews</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row" id="A4.T2.1.1.4.4.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.2.1">1.0000</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.3" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.3.1">0.9219</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.4" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.4.1">0.9147</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.5.1">0.9073</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.6" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.6.1">0.8996</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.7" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.7.1">0.8925</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.8" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.8.1">0.8850</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A4.T2.1.1.4.4.9" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.4.4.9.1">0.8794</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.1.5.5">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb" id="A4.T2.1.1.5.5.1" style="width:40.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top" id="A4.T2.1.1.5.5.1.1">C4</p>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb" id="A4.T2.1.1.5.5.2" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.2.1">1.0000</p>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.3" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.3.1">0.9323</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.4" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.4.1">0.9263</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.5" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.5.1">0.9193</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.6" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.6.1">0.9127</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.7" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.7.1">0.9052</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.8" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.8.1">0.8980</p>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="A4.T2.1.1.5.5.9" style="width:50.0pt;">
<p class="ltx_p ltx_align_center ltx_align_top" id="A4.T2.1.1.5.5.9.1">0.8929</p>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>PipeRAG versus Baseline Model that Supports Flexible Retrieval Intervals</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">To further show the efficiency gains of pipeline parallelism, we also compare PipeRAG with a modified version of <span class="ltx_text ltx_font_smallcaps" id="A4.SS2.p1.1.1">Retro</span>, termed <span class="ltx_text ltx_font_smallcaps" id="A4.SS2.p1.1.2">Retro+</span>, which also supports flexible retrieval intervals as PipeRAG. Here, we extend the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4.F8" title="Figure 8 ‣ 4.4 Ablation Study ‣ 4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">8</span></a>
presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="A4.SS2.p2">
<p class="ltx_p" id="A4.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4.F10" title="Figure 10 ‣ D.2 PipeRAG versus Baseline Model that Supports Flexible Retrieval Intervals ‣ Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">10</span></a> presents a performance-quality comparison between PipeRAG and <span class="ltx_text ltx_font_smallcaps" id="A4.SS2.p2.1.1">Retro+</span> under identical retrieval intervals (corresponding to the same number of database requests). For most retrieval intervals, ranging from 8 to 32, PipeRAG demonstrates superior efficiency compared to <span class="ltx_text ltx_font_smallcaps" id="A4.SS2.p2.1.2">Retro+</span>. When the staleness is high (at a retrieval interval of 64), <span class="ltx_text ltx_font_smallcaps" id="A4.SS2.p2.1.3">Retro+</span> has the potential to outperform PipeRAG in scenarios of low perplexity, attributable to the effects of high query staleness.</p>
</div>
<figure class="ltx_figure" id="A4.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3" id="A4.F10.fig1">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_3 ltx_img_landscape" height="103" id="A4.F10.g1" src="extracted/5456482/fig/performance_ppl_stale_vs_RETRO_same_interval_eval_wikipedia_chunk9_1K_db_c4_chunk_0_to_999.png" width="598"/></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3" id="A4.F10.fig2">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_2 ltx_img_landscape" height="102" id="A4.F10.g2" src="extracted/5456482/fig/performance_ppl_stale_vs_RETRO_same_interval_eval_realnews_chunk31_1K_db_c4_chunk_0_to_999.png" width="598"/></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="A4.F10.fig3">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_flex_size_1 ltx_img_landscape" height="102" id="A4.F10.g3" src="extracted/5456482/fig/performance_ppl_stale_vs_RETRO_same_interval_eval_c4_chunk1023_1K_db_c4_chunk_0_to_999.png" width="598"/></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The latency-perplexity comparison between PipeRAG versus <span class="ltx_text ltx_font_smallcaps" id="A4.F10.2.1">Retro</span> given the same retrieval intervals.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Broader Applicability of PipeRAG</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1"><span class="ltx_text ltx_font_italic" id="A5.p1.1.1">The idea of improving RAG efficiency through pipeline parallelism is broadly applicable across various RAG configurations, as long as they include periodic retrievals.</span>
In this paper, we have focused on improving RAG efficiency based on the <span class="ltx_text ltx_font_smallcaps" id="A5.p1.1.2">Retro</span> model and evaluated generation performance using specific hardware and software setups described in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#S4" title="4 Evaluation ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">4</span></a>.
In the future, RAG can evolve in several ways: models may adopt a decoder-only transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib33" title="">2018</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib4" title="">2020</a>)</cite> although the high cost of periodically appending the retrieved content has to be addressed <cite class="ltx_cite ltx_citemacro_citep">(Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib35" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib18" title="">2023c</a>)</cite>; retrieval engines could incorporate LLM-based or BM25-based result reranking <cite class="ltx_cite ltx_citemacro_citep">(Nogueira &amp; Cho, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib31" title="">2019</a>; MacAvaney et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib27" title="">2019</a>; Doostmohammadi et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib7" title="">2023</a>)</cite>, instead of solely relying on vector-level similarity; and hardware may evolve to include dedicated retrieval accelerators <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib16" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib17" title="">b</a>)</cite>.
However, regardless of these potential advancements in algorithms and hardware (detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A3" title="Appendix C Performance Trends on Evolving Hardware ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">C</span></a>), the dependencies between retrievals and inferences in RAG systems — especially when retrievals are periodic — remains a <span class="ltx_text ltx_font_italic" id="A5.p1.1.3">fundamental</span> obstacle to fully leveraging hardware resources and achieving maximal inference efficiency. Thus, whenever the time consumption of one retrieval and multiple steps of inferences are on a similar scale, pipeline parallelism by <span class="ltx_text ltx_font_italic" id="A5.p1.1.4">prefetching</span> content from databases should be a great option to improve generation efficiency.</p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p" id="A5.p2.1"><span class="ltx_text ltx_font_italic" id="A5.p2.1.1">Prefetching content from databases using stale queries is applicable regardless of the specific models used for generation.</span> To demonstrate this, we show that using a stale query window can retrieve content very similar to that obtained via a regular query window, with detailed results included in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#A4" title="Appendix D Additional Experimental Results ‣ PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"><span class="ltx_text ltx_ref_tag">D</span></a>.
These findings address a potential limitation in our evaluation, as our experimentation with PipeRAG was conducted using the <span class="ltx_text ltx_font_smallcaps" id="A5.p2.1.2">Retro</span> checkpoint provided by <cite class="ltx_cite ltx_citemacro_citep">(Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05676v1#bib.bib32" title="">2023</a>)</cite>, which was the only available <span class="ltx_text ltx_font_smallcaps" id="A5.p2.1.3">Retro</span> checkpoint at the time of our research.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Mar  8 21:12:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
