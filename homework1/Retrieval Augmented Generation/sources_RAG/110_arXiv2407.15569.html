<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought</title>
<!--Generated on Fri Aug 30 14:40:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.15569v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S1" title="In An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S2" title="In An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S2.SS1" title="In 2 Method ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>RAFT Finetuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S2.SS2" title="In 2 Method ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Dataset Construction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S3" title="In An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S3.SS1" title="In 3 Experiment Setup ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S3.SS2" title="In 3 Experiment Setup ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S3.SS3" title="In 3 Experiment Setup ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4" title="In An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment Result</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.SS1" title="In 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Long-form QA Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.SS2" title="In 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Chinese Dataset Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.SS3" title="In 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Performance Across Different Types of Reasoning Tasks by RAFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.SS4" title="In 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Effect of CoT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S5" title="In An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1" style="font-size:90%;">Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Augmented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model’s information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:90%;">Index Terms</span><span class="ltx_text" id="p1.1.2" style="font-size:90%;">: generative dialogue model, large language model, chain-of-thought, retrieval augmented generation</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="font-size:90%;">In recent years, with the rapid development of human-computer dialogue, a key technology in this field, generative dialogue models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib2" title="">2</a><span class="ltx_text" id="S1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.4" style="font-size:90%;">, has shown great potential and wide application prospects. From the early sequence-to-sequence (Seq2Seq) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib3" title="">3</a><span class="ltx_text" id="S1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.7" style="font-size:90%;"> architecture to recent innovations based on the Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib4" title="">4</a><span class="ltx_text" id="S1.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.10" style="font-size:90%;"> model with attention mechanisms, more advanced models are constantly emerging. However, generative dialogue models still confront significant challenges in accuracy, consistency, coherence, security, and resource efficiency. Enhancing their performance is a critical issue that demands attention.</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="font-size:90%;">To tackle more complex and diverse NLP tasks, the chain-of-thought (CoT) method </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib8" title="">8</a><span class="ltx_text" id="S1.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.4" style="font-size:90%;"> has been proposed. Chain-of thought breaks down complex reasoning tasks into multiple intermediate steps that are computed sequentially to obtain the final result. It not only improves the logical consistency of the model’s responses but also enhances user interaction experiences. However, recent studies have shown that chain-of-thought prompting method requires models of ~100 billion parameters to fully release their reasoning ability </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib5" title="">5</a><span class="ltx_text" id="S1.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.7" style="font-size:90%;">, and thus will have a significant demands on computational resources.</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1" style="font-size:90%;">Retrieval Augmented Generation (RAG) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib9" title="">9</a><span class="ltx_text" id="S1.p3.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.4" style="font-size:90%;"> is also a promising method to improve the performance of generative dialogue models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib13" title="">13</a><span class="ltx_text" id="S1.p3.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.7" style="font-size:90%;">. Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge.</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text" id="S1.p4.1.1" style="font-size:90%;">This paper studies a method that combines the chain-of-thought with retrieval augmented generation for Supervised Fine-Tuning (SFT) small-scale models to optimize their performance in reasoning tasks, which is called RAFT (Retrieval Augmented Fine-Tuning) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p4.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p4.1.4" style="font-size:90%;">. This method not only avoids the reliance of the chain-of-thought prompting on large-scale models, but also alleviates the hallucination </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p4.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib15" title="">15</a><span class="ltx_text" id="S1.p4.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p4.1.7" style="font-size:90%;"> and maintenance challenges of the knowledge retrieval process in RAG, enhancing the model’s ability to extract information and perform logical reasoning. In this work, we provide comprehensive optimization and evaluation of RAFT method across different types of reasoning tasks, including short-form QA and long-form QA, English tasks and Chinese tasks, bridge type and comparison tasks, particularly focusing on long-form QA and Chinese datasets. In addition, we evaluated the benefits of the chain-of-thought in the RAFT method and conducted a detailed analysis of the performance across various type of tasks above.</span></p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S1.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of RAFT</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Construction of RAFT fine-tuning dataset</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="460" id="S1.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of Chinese chain-of-thought style response generation process via GPT-3.5 in RAFT.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>RAFT Finetuning</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:90%;">RAFT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib14" title="">14</a><span class="ltx_text" id="S2.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.4" style="font-size:90%;"> is derived from RAG+SFT, which combines retrieval augmented generation (RAG) and Supervised Fine-Tuning (SFT). For better understanding, let us make an analogy between these modeling techniques in generative dialogue models and various kinds of examinations faced by human.</span></p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text" id="S2.SS1.p2.1.1" style="font-size:90%;">For supervised fine-tuning, the pre-trained language model is fine-tuned for a specific task by introducing a labeled dataset tailored for the task. Supervised fine-tuning is similar to a closed-book exam taken after class, where students answer questions using only the problem-solving methods learned in class without any reference materials.</span></p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text" id="S2.SS1.p3.1.1" style="font-size:90%;">For the retrieval augmented generation method, the RAG model uses input prompts as query keywords to retrieve relevant documents. These retrieved contents are added to the model’s input, and the model generates responses based on the augmented input. In the examination analogy, this method can be regarded as finding relevant passages from the open-book knowledge according to the question and reasoning the answer.</span></p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.5"><span class="ltx_text" id="S2.SS1.p4.5.1" style="font-size:90%;">The RAFT method combines retrieval augmented generation and supervised fine-tuning, as well as incorporating the idea of chain-of-thought. This is akin to training the model to compute results from relevant information before taking an exam. Consequently, during an open-book exam, the model can deduce correct answers more quickly and accurately using the reference materials. </span><em class="ltx_emph ltx_font_italic" id="S2.SS1.p4.5.2" style="font-size:90%;">In summary, the RAFT method has two key features.</em><span class="ltx_text" id="S2.SS1.p4.5.3" style="font-size:90%;"> First, in addition to the oracle documents, irrelevant distractor documents are also included in the reference documents to improve model’s robustness against irrelevant information retrieved during the retrieval process. Second, chain-of-thought style responses are used as the target text in the fine-tuning dataset rather than plain short answers to improve model’s reasoning capability. To be specific, each data in RAFT dataset contains a question (</span><math alttext="Q" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" mathsize="90%" xref="S2.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">italic_Q</annotation></semantics></math><span class="ltx_text" id="S2.SS1.p4.5.4" style="font-size:90%;">), several distractor documents (</span><math alttext="D_{k}" class="ltx_Math" display="inline" id="S2.SS1.p4.2.m2.1"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1.2" mathsize="90%" xref="S2.SS1.p4.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS1.p4.2.m2.1.1.3" mathsize="90%" xref="S2.SS1.p4.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">𝐷</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">D_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS1.p4.5.5" style="font-size:90%;">), a oracle document containing the effective information to answer the question (</span><math alttext="D^{*}" class="ltx_Math" display="inline" id="S2.SS1.p4.3.m3.1"><semantics id="S2.SS1.p4.3.m3.1a"><msup id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.2" mathsize="90%" xref="S2.SS1.p4.3.m3.1.1.2.cmml">D</mi><mo id="S2.SS1.p4.3.m3.1.1.3" mathsize="90%" xref="S2.SS1.p4.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">superscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2">𝐷</ci><times id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">D^{*}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.3.m3.1d">italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS1.p4.5.6" style="font-size:90%;">), and a chain-of-thought style response (</span><math alttext="A^{*}" class="ltx_Math" display="inline" id="S2.SS1.p4.4.m4.1"><semantics id="S2.SS1.p4.4.m4.1a"><msup id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml"><mi id="S2.SS1.p4.4.m4.1.1.2" mathsize="90%" xref="S2.SS1.p4.4.m4.1.1.2.cmml">A</mi><mo id="S2.SS1.p4.4.m4.1.1.3" mathsize="90%" xref="S2.SS1.p4.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><apply id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.4.m4.1.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">superscript</csymbol><ci id="S2.SS1.p4.4.m4.1.1.2.cmml" xref="S2.SS1.p4.4.m4.1.1.2">𝐴</ci><times id="S2.SS1.p4.4.m4.1.1.3.cmml" xref="S2.SS1.p4.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">A^{*}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.4.m4.1d">italic_A start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS1.p4.5.7" style="font-size:90%;">) generated from the oracle document (</span><math alttext="D^{*}" class="ltx_Math" display="inline" id="S2.SS1.p4.5.m5.1"><semantics id="S2.SS1.p4.5.m5.1a"><msup id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2" mathsize="90%" xref="S2.SS1.p4.5.m5.1.1.2.cmml">D</mi><mo id="S2.SS1.p4.5.m5.1.1.3" mathsize="90%" xref="S2.SS1.p4.5.m5.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">superscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2">𝐷</ci><times id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">D^{*}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.5.m5.1d">italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S2.SS1.p4.5.8" style="font-size:90%;">). Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S1.F1" style="font-size:90%;" title="Figure 1 ‣ 1 Introduction ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S2.SS1.p4.5.9" style="font-size:90%;"> shows overview of RAFT.</span></p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="RAFT\ training:Q+D^{*}+D_{1}+D_{2}+\dots+D_{k}\rightarrow A^{*}" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mrow id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" mathsize="90%" xref="S2.E1.m1.1.1.2.2.cmml">R</mi><mo id="S2.E1.m1.1.1.2.1" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.3" mathsize="90%" xref="S2.E1.m1.1.1.2.3.cmml">A</mi><mo id="S2.E1.m1.1.1.2.1a" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.4" mathsize="90%" xref="S2.E1.m1.1.1.2.4.cmml">F</mi><mo id="S2.E1.m1.1.1.2.1b" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.5" mathsize="90%" xref="S2.E1.m1.1.1.2.5.cmml">T</mi><mo id="S2.E1.m1.1.1.2.1c" lspace="0.450em" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.6" mathsize="90%" xref="S2.E1.m1.1.1.2.6.cmml">t</mi><mo id="S2.E1.m1.1.1.2.1d" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.7" mathsize="90%" xref="S2.E1.m1.1.1.2.7.cmml">r</mi><mo id="S2.E1.m1.1.1.2.1e" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.8" mathsize="90%" xref="S2.E1.m1.1.1.2.8.cmml">a</mi><mo id="S2.E1.m1.1.1.2.1f" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.9" mathsize="90%" xref="S2.E1.m1.1.1.2.9.cmml">i</mi><mo id="S2.E1.m1.1.1.2.1g" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.10" mathsize="90%" xref="S2.E1.m1.1.1.2.10.cmml">n</mi><mo id="S2.E1.m1.1.1.2.1h" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.11" mathsize="90%" xref="S2.E1.m1.1.1.2.11.cmml">i</mi><mo id="S2.E1.m1.1.1.2.1i" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.12" mathsize="90%" xref="S2.E1.m1.1.1.2.12.cmml">n</mi><mo id="S2.E1.m1.1.1.2.1j" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.2.13" mathsize="90%" xref="S2.E1.m1.1.1.2.13.cmml">g</mi></mrow><mo id="S2.E1.m1.1.1.1" lspace="0.278em" mathsize="90%" rspace="0.278em" xref="S2.E1.m1.1.1.1.cmml">:</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" mathsize="90%" xref="S2.E1.m1.1.1.3.2.2.cmml">Q</mi><mo id="S2.E1.m1.1.1.3.2.1" mathsize="90%" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><msup id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.2.3.2" mathsize="90%" xref="S2.E1.m1.1.1.3.2.3.2.cmml">D</mi><mo id="S2.E1.m1.1.1.3.2.3.3" mathsize="90%" xref="S2.E1.m1.1.1.3.2.3.3.cmml">∗</mo></msup><mo id="S2.E1.m1.1.1.3.2.1a" mathsize="90%" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><msub id="S2.E1.m1.1.1.3.2.4" xref="S2.E1.m1.1.1.3.2.4.cmml"><mi id="S2.E1.m1.1.1.3.2.4.2" mathsize="90%" xref="S2.E1.m1.1.1.3.2.4.2.cmml">D</mi><mn id="S2.E1.m1.1.1.3.2.4.3" mathsize="90%" xref="S2.E1.m1.1.1.3.2.4.3.cmml">1</mn></msub><mo id="S2.E1.m1.1.1.3.2.1b" mathsize="90%" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><msub id="S2.E1.m1.1.1.3.2.5" xref="S2.E1.m1.1.1.3.2.5.cmml"><mi id="S2.E1.m1.1.1.3.2.5.2" mathsize="90%" xref="S2.E1.m1.1.1.3.2.5.2.cmml">D</mi><mn id="S2.E1.m1.1.1.3.2.5.3" mathsize="90%" xref="S2.E1.m1.1.1.3.2.5.3.cmml">2</mn></msub><mo id="S2.E1.m1.1.1.3.2.1c" mathsize="90%" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.2.6" mathsize="90%" mathvariant="normal" xref="S2.E1.m1.1.1.3.2.6.cmml">⋯</mi><mo id="S2.E1.m1.1.1.3.2.1d" mathsize="90%" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><msub id="S2.E1.m1.1.1.3.2.7" xref="S2.E1.m1.1.1.3.2.7.cmml"><mi id="S2.E1.m1.1.1.3.2.7.2" mathsize="90%" xref="S2.E1.m1.1.1.3.2.7.2.cmml">D</mi><mi id="S2.E1.m1.1.1.3.2.7.3" mathsize="90%" xref="S2.E1.m1.1.1.3.2.7.3.cmml">k</mi></msub></mrow><mo id="S2.E1.m1.1.1.3.1" mathsize="90%" stretchy="false" xref="S2.E1.m1.1.1.3.1.cmml">→</mo><msup id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2" mathsize="90%" xref="S2.E1.m1.1.1.3.3.2.cmml">A</mi><mo id="S2.E1.m1.1.1.3.3.3" mathsize="90%" xref="S2.E1.m1.1.1.3.3.3.cmml">∗</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">:</ci><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><times id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑅</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">𝐴</ci><ci id="S2.E1.m1.1.1.2.4.cmml" xref="S2.E1.m1.1.1.2.4">𝐹</ci><ci id="S2.E1.m1.1.1.2.5.cmml" xref="S2.E1.m1.1.1.2.5">𝑇</ci><ci id="S2.E1.m1.1.1.2.6.cmml" xref="S2.E1.m1.1.1.2.6">𝑡</ci><ci id="S2.E1.m1.1.1.2.7.cmml" xref="S2.E1.m1.1.1.2.7">𝑟</ci><ci id="S2.E1.m1.1.1.2.8.cmml" xref="S2.E1.m1.1.1.2.8">𝑎</ci><ci id="S2.E1.m1.1.1.2.9.cmml" xref="S2.E1.m1.1.1.2.9">𝑖</ci><ci id="S2.E1.m1.1.1.2.10.cmml" xref="S2.E1.m1.1.1.2.10">𝑛</ci><ci id="S2.E1.m1.1.1.2.11.cmml" xref="S2.E1.m1.1.1.2.11">𝑖</ci><ci id="S2.E1.m1.1.1.2.12.cmml" xref="S2.E1.m1.1.1.2.12">𝑛</ci><ci id="S2.E1.m1.1.1.2.13.cmml" xref="S2.E1.m1.1.1.2.13">𝑔</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><ci id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1">→</ci><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><plus id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></plus><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑄</ci><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3">superscript</csymbol><ci id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2">𝐷</ci><times id="S2.E1.m1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.2.3.3"></times></apply><apply id="S2.E1.m1.1.1.3.2.4.cmml" xref="S2.E1.m1.1.1.3.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.4.1.cmml" xref="S2.E1.m1.1.1.3.2.4">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.4.2.cmml" xref="S2.E1.m1.1.1.3.2.4.2">𝐷</ci><cn id="S2.E1.m1.1.1.3.2.4.3.cmml" type="integer" xref="S2.E1.m1.1.1.3.2.4.3">1</cn></apply><apply id="S2.E1.m1.1.1.3.2.5.cmml" xref="S2.E1.m1.1.1.3.2.5"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.5.1.cmml" xref="S2.E1.m1.1.1.3.2.5">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.5.2.cmml" xref="S2.E1.m1.1.1.3.2.5.2">𝐷</ci><cn id="S2.E1.m1.1.1.3.2.5.3.cmml" type="integer" xref="S2.E1.m1.1.1.3.2.5.3">2</cn></apply><ci id="S2.E1.m1.1.1.3.2.6.cmml" xref="S2.E1.m1.1.1.3.2.6">⋯</ci><apply id="S2.E1.m1.1.1.3.2.7.cmml" xref="S2.E1.m1.1.1.3.2.7"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.7.1.cmml" xref="S2.E1.m1.1.1.3.2.7">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.7.2.cmml" xref="S2.E1.m1.1.1.3.2.7.2">𝐷</ci><ci id="S2.E1.m1.1.1.3.2.7.3.cmml" xref="S2.E1.m1.1.1.3.2.7.3">𝑘</ci></apply></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3">superscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2">𝐴</ci><times id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">RAFT\ training:Q+D^{*}+D_{1}+D_{2}+\dots+D_{k}\rightarrow A^{*}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_R italic_A italic_F italic_T italic_t italic_r italic_a italic_i italic_n italic_i italic_n italic_g : italic_Q + italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT + italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ⋯ + italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT → italic_A start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="711" id="S2.F4.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of English chain-of-thought style response generation process via GPT-3.5 in RAFT.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Dataset Construction</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text" id="S2.SS2.p1.1.1" style="font-size:90%;">Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S1.F2" style="font-size:90%;" title="Figure 2 ‣ 1 Introduction ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S2.SS2.p1.1.2" style="font-size:90%;"> shows our RAFT fine-tuning dataset construction process.
In order to make the datasets tailored to RAFT fine-tuning, we use two methods to process open-source datasets. When dealing with a dataset where a question corresponds to several reference documents (include oracle documents and distractor documents), we use the first method: For each question, we extract all the oracle documents from the question’s corresponding documents, then randomly select a specified number of documents from the remaining corresponding documents as the distractor documents. When dealing with a dataset where a question corresponds to only one oracle document, we use the second method: For each question, we take its corresponding document as oracle document and randomly select a specified number of documents from </span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3" style="font-size:90%;">other</span><span class="ltx_text" id="S2.SS2.p1.1.4" style="font-size:90%;"> questions’ reference documents as the question’s distractor documents. In this study, the dataset HotpotQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib16" title="">16</a><span class="ltx_text" id="S2.SS2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.7" style="font-size:90%;"> was processed using the first method, while the datasets like PubMedQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib17" title="">17</a><span class="ltx_text" id="S2.SS2.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.10" style="font-size:90%;"> and DuReader_robust </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib18" title="">18</a><span class="ltx_text" id="S2.SS2.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.13" style="font-size:90%;"> were processed using the second method. In our RAFT experiments, we use four distractor documents for each question.</span></p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text" id="S2.SS2.p2.1.1" style="font-size:90%;">After selecting the oracle and distractor documents, we used GPT-3.5 with to generate chain-of-thought style response. We require the model to generate a chain-of-thought reasoning process based on the input question as well as its corresponding oracle documents. During this reasoning process, the model are prompted to cite the referenced content from the oracle documents and provide a final answer separately at the end. Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S1.F3" style="font-size:90%;" title="Figure 3 ‣ 1 Introduction ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S2.SS2.p2.1.2" style="font-size:90%;"> and figure </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S2.F4" style="font-size:90%;" title="Figure 4 ‣ 2.1 RAFT Finetuning ‣ 2 Method ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S2.SS2.p2.1.3" style="font-size:90%;"> show our CoT answer generation process via GPT-3.5 in Chinese and English respectively.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment Setup</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS1.p1">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text" id="S3.I1.i1.p1.1.1" style="font-size:90%;">HotpotQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.I1.i1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib16" title="">16</a><span class="ltx_text" id="S3.I1.i1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.I1.i1.p1.1.4" style="font-size:90%;">: HotpotQA dataset contains 113,000 multi-hop reasoning question-answer pairs from Wikipedia. It includes two types of QA tasks: bridge and comparison. Bridge QA tasks require the model to find relevant information from multiple reference documents to provide an answer, while comparison QA tasks require the model to compare multiple entities or events. Each data item includes a question, several reference documents, and a short answer.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text" id="S3.I1.i2.p1.1.1" style="font-size:90%;">PubMedQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.I1.i2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib17" title="">17</a><span class="ltx_text" id="S3.I1.i2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.I1.i2.p1.1.4" style="font-size:90%;">: A biomedical question-answering dataset. It extracts data from PubMed abstracts and answers research questions based on these abstracts. Answers are presented in the form of ”yes/no/maybe.” Each data item consists of a question, a reference document, a long answer, and a short answer.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text" id="S3.I1.i3.p1.1.1" style="font-size:90%;">DuReader_robust </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.I1.i3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib18" title="">18</a><span class="ltx_text" id="S3.I1.i3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.I1.i3.p1.1.4" style="font-size:90%;">: DuReader_robust is a Chinese dataset used to evaluate the robustness and generalization ability of model’s reading comprehension function. Each data item includes a question, a reference document, and a short answer. All datas items are sourced from Baidu users’ search queries and responses.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baselines</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text" id="S3.SS2.p1.1.1" style="font-size:90%;">In this study, we evaluated the Chinese dataset DuReader_robust using Qwen-1.5-7B-chat </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib19" title="">19</a><span class="ltx_text" id="S3.SS2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.1.4" style="font-size:90%;"> and the English datasets, HotpotQA and PubMedQA, using LLaMA2-7B-chat </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib21" title="">21</a><span class="ltx_text" id="S3.SS2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.1.7" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text" id="S3.I2.i1.p1.1.1" style="font-size:90%;">LLaMA2-7B-chat / Qwen-1.5-7B-chat + zero-shot prompting: Provide the model with clear instructions and the question it needs to answer, without providing any external reference documents, and require the model to generate an answer.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text" id="S3.I2.i2.p1.1.1" style="font-size:90%;">LLaMA2-7B-chat / Qwen-1.5-7B-chat + RAG: Provide the model with instructions and the question, supplemented with external reference documents, and require the model to derive an answer using the content in these reference documents.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text" id="S3.I2.i3.p1.1.1" style="font-size:90%;">DSF (Domain Specific Finetuning) + zero-shot prompting: For each dataset standard supervised fine-tuning is performed without reference documents, using the question as the input text and the answer as the target text for fine-tuning. The fine-tuned model is then given the question and instructions and required to respond without referencing external documents.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text" id="S3.I2.i4.p1.1.1" style="font-size:90%;">DSF + RAG: Standard supervised fine-tuning is performed without reference documents for each dataset, but during testing, the fine-tuned model is supplemented with reference documents for the question. The model is required to derive the answer using external knowledge.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Method</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text" id="S3.SS3.p1.1.1" style="font-size:90%;">In our experiments, we primarily use F1 score and EM score (Exact Match) to evaluate the performance of the models. We standardize the answers by normalizing the answer text through several steps, including converting all text to lowercase, removing punctuation, removing articles (a, an, the), and standardizing spaces, which ensure that the answers are more uniform in format </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#bib.bib23" title="">23</a><span class="ltx_text" id="S3.SS3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.1.4" style="font-size:90%;">. Subsequently, the standardized answers are used to calculate their EM scores and F1 scores.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment Result</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation in EM score</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.1" style="font-size:90%;">PubMedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.1.1.1.1" style="font-size:90%;">HotpotQA</span><sub class="ltx_sub" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T1.1.1.1.2.1" style="font-size:90%;">[Oracle]</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.1" style="font-size:90%;">HotpotQA</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.2.1.1.1" style="font-size:90%;">zero-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.2.1.2.1" style="font-size:90%;">50.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.2.1.3.1" style="font-size:90%;">15.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.2.1.4.1" style="font-size:90%;">15.06</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.3.2.1.1" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.3.2.2.1" style="font-size:90%;">56.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.3.2.3.1" style="font-size:90%;">12.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.3.2.4.1" style="font-size:90%;">8.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.4.3.1.1" style="font-size:90%;">DSF + zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.4.3.2.1" style="font-size:90%;">53.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.4.3.3.1" style="font-size:90%;">20.04</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.4.3.4.1" style="font-size:90%;">20.04</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.5.4.1.1" style="font-size:90%;">DSF + RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.5.4.2.1" style="font-size:90%;">71.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.5.4.3.1" style="font-size:90%;">45.26</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.5.4.4.1" style="font-size:90%;">27.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.6.5.1.1" style="font-size:90%;">RAFT w.o. CoT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.6.5.2.1" style="font-size:90%;">54.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.6.5.3.1" style="font-size:90%;">52.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.6.5.4.1" style="font-size:90%;">28.74</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.7.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.7.6.1.1" style="font-size:90%;">RAFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.7.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.7.6.2.1" style="font-size:90%;">74.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.7.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.7.6.3.1" style="font-size:90%;">54.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.7.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.1.7.6.4.1" style="font-size:90%;">39.48</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation in F1 score</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.3"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1">
<span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:90%;">PubMedQA</span><sub class="ltx_sub" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.1.2.1" style="font-size:90%;">[long]</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.2">
<span class="ltx_text" id="S4.T2.2.2.2.1" style="font-size:90%;">HotpotQA</span><sub class="ltx_sub" id="S4.T2.2.2.2.2"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.2.2.1" style="font-size:90%;">[Oracle]</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.4"><span class="ltx_text" id="S4.T2.2.2.4.1" style="font-size:90%;">HotpotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.5"><span class="ltx_text" id="S4.T2.2.2.5.1" style="font-size:90%;">DuReader</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.1"><span class="ltx_text" id="S4.T2.2.3.1.1.1" style="font-size:90%;">zero-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.2"><span class="ltx_text" id="S4.T2.2.3.1.2.1" style="font-size:90%;">1.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.3"><span class="ltx_text" id="S4.T2.2.3.1.3.1" style="font-size:90%;">22.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.4"><span class="ltx_text" id="S4.T2.2.3.1.4.1" style="font-size:90%;">22.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.1.5"><span class="ltx_text" id="S4.T2.2.3.1.5.1" style="font-size:90%;">13.47</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.2">
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.1"><span class="ltx_text" id="S4.T2.2.4.2.1.1" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.2"><span class="ltx_text" id="S4.T2.2.4.2.2.1" style="font-size:90%;">3.05</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.3"><span class="ltx_text" id="S4.T2.2.4.2.3.1" style="font-size:90%;">25.05</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.4"><span class="ltx_text" id="S4.T2.2.4.2.4.1" style="font-size:90%;">18.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2.5"><span class="ltx_text" id="S4.T2.2.4.2.5.1" style="font-size:90%;">26.06</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.3">
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.1"><span class="ltx_text" id="S4.T2.2.5.3.1.1" style="font-size:90%;">DSF + zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.2"><span class="ltx_text" id="S4.T2.2.5.3.2.1" style="font-size:90%;">7.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.3"><span class="ltx_text" id="S4.T2.2.5.3.3.1" style="font-size:90%;">27.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.4"><span class="ltx_text" id="S4.T2.2.5.3.4.1" style="font-size:90%;">27.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.3.5"><span class="ltx_text" id="S4.T2.2.5.3.5.1" style="font-size:90%;">20.90</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.4">
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.1"><span class="ltx_text" id="S4.T2.2.6.4.1.1" style="font-size:90%;">DSF + RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.2"><span class="ltx_text" id="S4.T2.2.6.4.2.1" style="font-size:90%;">10.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.3"><span class="ltx_text" id="S4.T2.2.6.4.3.1" style="font-size:90%;">58.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.4"><span class="ltx_text" id="S4.T2.2.6.4.4.1" style="font-size:90%;">34.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.5"><span class="ltx_text" id="S4.T2.2.6.4.5.1" style="font-size:90%;">39.91</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.1"><span class="ltx_text" id="S4.T2.2.7.5.1.1" style="font-size:90%;">RAFT w.o. CoT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.2"><span class="ltx_text" id="S4.T2.2.7.5.2.1" style="font-size:90%;">——</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.3"><span class="ltx_text" id="S4.T2.2.7.5.3.1" style="font-size:90%;">64.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.4"><span class="ltx_text" id="S4.T2.2.7.5.4.1" style="font-size:90%;">37.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.5"><span class="ltx_text" id="S4.T2.2.7.5.5.1" style="font-size:90%;">42.25</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.8.6.1"><span class="ltx_text" id="S4.T2.2.8.6.1.1" style="font-size:90%;">RAFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.8.6.2"><span class="ltx_text" id="S4.T2.2.8.6.2.1" style="font-size:90%;">14.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.8.6.3"><span class="ltx_text" id="S4.T2.2.8.6.3.1" style="font-size:90%;">67.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.8.6.4"><span class="ltx_text" id="S4.T2.2.8.6.4.1" style="font-size:90%;">51.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.8.6.5"><span class="ltx_text" id="S4.T2.2.8.6.5.1" style="font-size:90%;">57.81</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text" id="S4.p1.1.1" style="font-size:90%;">We compared the performance of the models using the RAFT method and the baselines. Table </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.T1" style="font-size:90%;" title="Table 1 ‣ 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.p1.1.2" style="font-size:90%;"> and Table </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.T2" style="font-size:90%;" title="Table 2 ‣ 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.p1.1.3" style="font-size:90%;"> show the results for the EM score and F1 score respectively. In the HotpotQA</span><sub class="ltx_sub" id="S4.p1.1.4"><span class="ltx_text ltx_font_italic" id="S4.p1.1.4.1" style="font-size:90%;">[Oracle]</span></sub><span class="ltx_text" id="S4.p1.1.5" style="font-size:90%;"> experiment group, only oracle documents were provided as references for the model in the RAG experiments. For all other groups, distractor documents were included alongside the reference documents in the RAG experiments.</span></p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text" id="S4.p2.1.1" style="font-size:90%;">From the experimental results, we can see that the RAFT method consistently outperforms four baseline methods across all datasets, demonstrating superior information extraction and complex problem reasoning capabilities in the models fine-tuned with RAFT method. On the HotpotQA dataset, the RAFT method (with CoT) achieved a performance gain of 42.13% in EM score and 42.78% in F1 score over the plain RAG baseline (without using DSF model) experiments. Even with the inclusion of distractor documents, it still achieved gains of 30.76% in EM score and 32.94% in F1 score. Furthermore, we observed that although the scores for RAFT degrade with the addition of distractor documents in the experiments (comparing the table columns corresponding to HotpotQA</span><sub class="ltx_sub" id="S4.p2.1.2"><span class="ltx_text ltx_font_italic" id="S4.p2.1.2.1" style="font-size:90%;">[Oracle]</span></sub><span class="ltx_text" id="S4.p2.1.3" style="font-size:90%;"> and HotpotQA), it achieved a higher performance gain over the DSF+RAG baseline. This indicates that the RAFT method can significantly enhance the model’s robustness in the retrieval process in RAG.</span></p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text" id="S4.p3.1.1" style="font-size:90%;">Before fine-tuning, the model’s performance was poor, regardless of whether RAG was included or not. Fine-tuning the model for specific domains, i.e., DSF, can significantly improve its performance by aligning model outputs with the answering patterns of those domains. Through the RAFT method (with CoT), the model not only learned specific domain answering patterns but also significantly improved its ability to extract effective information from complex data.</span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Long-form QA Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text" id="S4.SS1.p1.1.1" style="font-size:90%;">Since the ”yes/no” QA of PubMedQA and QA of HotpotQA are both short-form, we also assessed the long-form QA in dataset PubMedQA. The experiment results are shown in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.T2" style="font-size:90%;" title="Table 2 ‣ 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS1.p1.1.2" style="font-size:90%;"> under the PubMedQA</span><sub class="ltx_sub" id="S4.SS1.p1.1.3"><span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3.1" style="font-size:90%;">[long]</span></sub><span class="ltx_text" id="S4.SS1.p1.1.4" style="font-size:90%;"> group. The results in F1 score of long-form QA indicate that RAFT method brought about a 13% performance improvement for long-answer questions over zero-shot prompting baseline. However, compared to the DSF+RAG baseline, the performance gain was less prominent than for the short-form QA. This is because the content of long answers is more focused on induction and summarization, rather than definitive results derived from reasoning, as is common with short answers. The study of long-form QA with chain-of-thought needs further exploration.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Chinese Dataset Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:90%;">We also conducted evaluation on DuReader_robust to assess the effectiveness of the RAFT method on the Chinese datasets. Since the questions in this dataset heavily rely on information from reference documents, the gain brought by the use of DSF is only 7.43% over the zero-shot prompting baseline (in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.T2" style="font-size:90%;" title="Table 2 ‣ 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS2.p1.1.2" style="font-size:90%;"> comparing the ’zero-shot’ and ’DSF+zero-shot’ rows in the DuReader group). In this case, the use of RAG to supplement reference documents with the question is more effective, which obtains a 12.59% performance gain over the zero-shot baseline. After RAFT fine-tuning, the model’s ability to extract and process information, as well as its reasoning capability can be significantly improved. It achieves 44.34% and 19.9% performance gain in F1 score over zero-shot prompting baseline and DSF+RAG baseline respectively. These results demonstrate that the RAFT method performs exceptionally well on both English and Chinese datasets.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Performance Across Different Types of Reasoning Tasks by RAFT</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text" id="S4.SS3.p1.1.1" style="font-size:90%;">We evaluated the RAFT method separately on bridge-type QA and comparison-type QA in HotpotQA dataset, as shown in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2407.15569v2#S4.T3" style="font-size:90%;" title="Table 3 ‣ 4.3 Performance Across Different Types of Reasoning Tasks by RAFT ‣ 4 Experiment Result ‣ An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS3.p1.1.2" style="font-size:90%;">. The results indicate that RAFT performs better on comparison-type questions. This is likely because comparison-type questions typically involve comparing features between two or more entities, which can rely on direct information retrieval and simple comparison operations. In contrast, bridge-type questions often require the model to extract relevant information from multiple documents, involving longer reasoning chains and multiple intermediate steps so it demands a higher level of understanding and reasoning ability from the model.</span></p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance gains across different types of reasoning tasks by RAFT</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T3.3.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.1.1.2"><span class="ltx_text" id="S4.T3.3.1.1.2.1" style="font-size:90%;">bridge</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.1.1.3"><span class="ltx_text" id="S4.T3.3.1.1.3.1" style="font-size:90%;">comparison</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.1"><span class="ltx_text" id="S4.T3.3.2.2.1.1" style="font-size:90%;">RAFT-EM score</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.2"><span class="ltx_text" id="S4.T3.3.2.2.2.1" style="font-size:90%;">36.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.3"><span class="ltx_text" id="S4.T3.3.2.2.3.1" style="font-size:90%;">50.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.3.3.1"><span class="ltx_text" id="S4.T3.3.3.3.1.1" style="font-size:90%;">RAFT-F1 score</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.3.3.2"><span class="ltx_text" id="S4.T3.3.3.3.2.1" style="font-size:90%;">48.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.3.3.3"><span class="ltx_text" id="S4.T3.3.3.3.3.1" style="font-size:90%;">60.11</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Effect of CoT</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text" id="S4.SS4.p1.1.1" style="font-size:90%;">To evaluate the benefit of the chain-of-thought (CoT) in the RAFT method, we conducted an ablation experiment (RAFT w.o. CoT). In this experiment, we removed the chain-of-thought style response from the RAFT training dataset and only included the final answer for each question as target text in the fine-tuning process. Comparing the HotpotQA dataset tested with only oracle documents for RAG and the one tested with distractor documents, the CoT method achieved more significant performance gains in the latter setting. This demonstrates that CoT can obtain more considerable benefit in the face of more complex knowledge and more serious information noise. Moreover, the performance of RAFT was consistently superior to the performance of RAFT without CoT across various datasets. Therefore, adding CoT effectively guides the model the correct information from complex input and enhances the model’s logical rigor and accuracy.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="font-size:90%;">In this study, we evaluated the RAFT method across multiple datasets, addressing the gaps in previous research regarding long-form QA and Chinese datasets. The results indicate that the RAFT method combined with CoT not only improves the models’ ability to robustly extract and process information in the face of noise, but also enhances their logical reasoning ability in reasoning tasks. Significant performance gains were observed in evaluations on both English and Chinese datasets, as well as on long-form QA and short-form QA. Additionally, we conducted an ablation experiment where we removed the chain-of-thought style response from the RAFT training dataset to fine-tune the model. This experiment verifies the critical role of the chain-of-thought in enhancing the performance of generative dialogue models.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Y. Zhang, Z. Ou, and Z. Yu, “Task-oriented dialog systems that consider multiple appropriate responses under the same context,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="font-size:90%;">Proc. AAAI</em><span class="ltx_text" id="bib.bib1.3.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
H. Liu, Y. Cai, Z. Ou, Y. Huang, and J. Feng, “Building markovian generative architectures over pretrained lm backbones for efficient task-oriented dialog systems,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2" style="font-size:90%;">Proc. SLT</em><span class="ltx_text" id="bib.bib2.3.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="font-size:90%;">Proc. NeurIPS</em><span class="ltx_text" id="bib.bib3.3.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="font-size:90%;">Proc. NeurIPS</em><span class="ltx_text" id="bib.bib4.3.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib5.3.3" style="font-size:90%;">, “Chain-of-Thought prompting elicits reasoning in large language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.4" style="font-size:90%;">Proc. NeurIPS</em><span class="ltx_text" id="bib.bib5.5.5" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib6.3.3" style="font-size:90%;">, “Challenging BIG-Bench tasks and whether Chain-of-Thought can solve them,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.4" style="font-size:90%;">Proc. ACL</em><span class="ltx_text" id="bib.bib6.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib7.3.3" style="font-size:90%;">, “Training verifiers to solve math word problems,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.4.4" style="font-size:90%;">arXiv preprint arXiv:2110.14168</em><span class="ltx_text" id="bib.bib7.5.5" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, “ReAct: Synergizing reasoning and acting in language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2" style="font-size:90%;">Proc. ICLR</em><span class="ltx_text" id="bib.bib8.3.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib9.3.3" style="font-size:90%;">, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.4" style="font-size:90%;">Proc. NeurIPS</em><span class="ltx_text" id="bib.bib9.5.5" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Atlas: Few-shot learning with retrieval augmented language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2" style="font-size:90%;">Journal of Machine Learning Research</em><span class="ltx_text" id="bib.bib10.3.3" style="font-size:90%;">, vol. 24, no. 251, pp. 1–43, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib11.3.3" style="font-size:90%;">, “Improving language models by retrieving from trillions of tokens,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.4.4" style="font-size:90%;">Proc. ICML</em><span class="ltx_text" id="bib.bib11.5.5" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented language model pre-training,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2" style="font-size:90%;">Proc. ICML</em><span class="ltx_text" id="bib.bib12.3.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis, “Generalization through memorization: Nearest neighbor language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="font-size:90%;">Proc. ICLR</em><span class="ltx_text" id="bib.bib13.3.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E. Gonzalez, “RAFT: Adapting language model to domain specific RAG,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="font-size:90%;">arXiv preprint arXiv:2403.10131</em><span class="ltx_text" id="bib.bib14.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib15.3.3" style="font-size:90%;">, “A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.4" style="font-size:90%;">arXiv preprint arXiv:2311.05232</em><span class="ltx_text" id="bib.bib15.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, “HotpotQA: A dataset for diverse, explainable multi-hop question answering,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2" style="font-size:90%;">Proc. EMNLP</em><span class="ltx_text" id="bib.bib16.3.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, “PubMedQA: A dataset for biomedical research question answering,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2" style="font-size:90%;">Proc. EMNLP</em><span class="ltx_text" id="bib.bib17.3.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
H. Tang, H. Li, J. Liu, Y. Hong, H. Wu, and H. Wang, “DuReader_robust: A chinese dataset towards evaluating robustness and generalization of machine reading comprehension in real-world applications,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2" style="font-size:90%;">Proc. ACL</em><span class="ltx_text" id="bib.bib18.3.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib19.3.3" style="font-size:90%;">, “Qwen technical report,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.4.4" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span class="ltx_text" id="bib.bib19.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib20.3.3" style="font-size:90%;">, “LLaMA: Open and efficient foundation language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.4.4" style="font-size:90%;">arXiv preprint arXiv:2302.13971</em><span class="ltx_text" id="bib.bib20.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib21.3.3" style="font-size:90%;">, “LLAMA 2: Open foundation and fine-tuned chat models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.4.4" style="font-size:90%;">arXiv preprint arXiv:2307.09288</em><span class="ltx_text" id="bib.bib21.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Baidu, “Dureader robust,” https://github.com/baidu/DuReader.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
HotpotQA, “Hotpotqa,” https://github.com/hotpotqa/hotpot.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 30 14:40:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
