<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need</title>
<!--Generated on Fri Jul  5 09:40:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.18064v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S1" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S2" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Motivation and Problem Setting</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.SS1" title="In 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.SS2" title="In 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retrieval, Augmentation, and Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.SS3" title="In 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Grading Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.SS1" title="In 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>GPT-4</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.SS2" title="In 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Human</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.SS3" title="In 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Compare GPT-4 with Human</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.SS4" title="In 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Llama 3</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S5" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S6" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A1" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>tRAG: Knowledge Preprocessing Workflow and Configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A2" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>tRAG: Document Retrieval and Answer Generation Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A3" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Answer Quality Evaluation Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A4" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>An Anomaly Transaction Question and Its Label</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A5" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>tRAG’s Answer to the Anomaly Transaction Question and GPT-4’s Grading</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A6" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Early Design of Grading Instructions, Rubric, and GPT-4’s Assessment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A7" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>A Transaction Code Question and Label</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A8" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>tRAG’s Answer to the Transaction Code Question and GPT-4’s grading justification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A9" title="In Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Llama 3</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Evaluating Quality of Answers for Retrieval-Augmented Generation: 
<br class="ltx_break"/>A Strong LLM Is All You Need</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Wang 
<br class="ltx_break"/>Visa Data &amp; AI Platform 
<br class="ltx_break"/>Foster City, California 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">yangwang@visa.com</span>
&amp;Alberto Garcia Hernandez 
<br class="ltx_break"/>Visa Consulting &amp; Analytics 
<br class="ltx_break"/>London, United Kingdom 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">garciaha@visa.com</span>
<span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Roman Kyslyi 
<br class="ltx_break"/>Visa Consulting &amp; Analytics 
<br class="ltx_break"/>Kyiv, Ukraine 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">rkyslyi@visa.com</span>
&amp;Nicholas Kersting 
<br class="ltx_break"/>Visa Data &amp; AI Platform 
<br class="ltx_break"/>Austin, Texas 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">nkerstin@visa.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">We present a comprehensive study of answer quality evaluation in Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel grading system that is designed to assess correctness, completeness, and honesty. We further map the grading of quality aspects aforementioned into a binary score, indicating an accept or reject decision, mirroring the intuitive "thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This approach suits factual business settings where a clear decision opinion is essential. Our assessment applies vRAG-Eval to two Large Language Models (LLMs), evaluating the quality of answers generated by a vanilla RAG application. We compare these evaluations with human expert judgments and find a substantial alignment between GPT-4’s assessments and those of human experts, reaching 83% agreement on accept or reject decisions. This study highlights the potential of LLMs as reliable evaluators in closed-domain, closed-ended settings, particularly when human evaluations require significant resources.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Evaluating Quality of Answers for Retrieval-Augmented Generation: 
<br class="ltx_break"/>A Strong LLM Is All You Need</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Yang Wang</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">Visa Data &amp; AI Platform</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">Foster City, California</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.1">yangwang@visa.com</span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Alberto Garcia Hernandez</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">Visa Consulting &amp; Analytics</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1">London, United Kingdom</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.4.4.1.1">garciaha@visa.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.3" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.3.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.3.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.1.1.1.1.1.1">Roman Kyslyi</span></span></span>
<span class="ltx_tr" id="p1.1.3.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.2.2.1">Visa Consulting &amp; Analytics</span></span>
<span class="ltx_tr" id="p1.1.3.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.3.3.1">Kyiv, Ukraine</span></span>
<span class="ltx_tr" id="p1.1.3.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.3.1.1.4.4.1.1">rkyslyi@visa.com</span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.3.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.3.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.2.1.1.1.1.1">Nicholas Kersting</span></span></span>
<span class="ltx_tr" id="p1.1.3.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.2.2.1">Visa Data &amp; AI Platform</span></span>
<span class="ltx_tr" id="p1.1.3.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.3.3.1">Austin, Texas</span></span>
<span class="ltx_tr" id="p1.1.3.2.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.3.2.1.4.4.1.1">nkerstin@visa.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Since the launch of ChatGPT in November 2022, Large Language Models (LLMs) have become increasingly popular integrated components for organizations seeking to enhance productivity and enrich their product portfolio offerings. However, it is well known that GPT-4 Turbo’s training corpora cut-off date is April 2023, rendering the model lacking in current events knowledge. Furthermore, as LLMs are pre-trained on public domain text and do not possess proprietary information, their capabilities are limited when it comes to our company’s knowledge-intensive applications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib11" title="">2018</a>; Dodge et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib1" title="">2020</a>)</cite> is a technique that can be used to inject new knowledge into pre-trained LLMs by adjusting their gradient parameters through further, specialized training. However, OpenAI’s fine-tuning API is currently only available through an experimental access program, and GPT-4 fine-tuning requires more effort to achieve significant improvements, as noted by <cite class="ltx_cite ltx_citemacro_citet">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib9" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Retrieval-augmented generation (RAG) is proposed initially by <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib5" title="">2021</a>)</cite>. This method involves storing extra knowledge in a non-parametric dense vector index and using a pre-trained neural retriever to search relevant context, followed by generating content with a pre-trained sequence-to-sequence (seq2seq) model. <cite class="ltx_cite ltx_citemacro_citet">Ovadia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib10" title="">2024</a>)</cite> argue that RAG consistently outperforms unsupervised fine-tuning on a wide range of knowledge-intensive tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A RAG application leveraging LLMs enhanced with the company’s proprietary knowledge has become one of the pivotal factors advancing the adoption of GPT-4-based applications at our enterprise, a global payment technology company. This phenomenon highlights the need to apply viable approaches to evaluate RAG applications, as lacking performance metrics poses risks to the enterprise’ business and may have negative consequences.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">One approach to evaluate the quality of RAGs focuses on their unique characteristic: that they consist of a Retriever model and a Generator model. Therefore, studies have used metrics such as context relevance, and answer relevance to evaluate the two components separately then assess the answer faithfulness between them <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib14" title="">2024</a>; Es et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib2" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The other paradigm for evaluating RAG applications treats them as traditional question answering systems. Researchers have proposed metrics such as SAS, a semantic answer similarity estimation <cite class="ltx_cite ltx_citemacro_citep">(Risch et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib13" title="">2021</a>)</cite>, and F1/EM scores <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib15" title="">2024</a>)</cite> to evaluate these applications on public question answering (QA) datasets. <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib17" title="">2023</a>)</cite> propose using strong LLMs such as GPT-4 to evaluate other LLMs. They argue that traditional benchmarks cannot effectively align quality measurement with human preferences in open-ended tasks.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In some companies, RAG applications are mostly developed for a closed-domain, closed-ended setting, where employees seek factual answers based on proprietary knowledge. Meanwhile, they often develop external-facing RAG applications that allow clients to search for authoritative information that is not necessarily available on the Internet. Therefore, it is crucial to establish definitive answer quality evaluation practices that are suitable for these businesses’ needs.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To study this, we introduce a benchmark dataset called VND-Bench, comprising 155 high-quality questions across 14 subject areas in a proprietary payment network data domain. We also collect their corresponding answers from a communication channel on an internal employee collaboration platform to serve as ground truth labels. We build a vanilla RAG application, tRAG, as a test bed and ask it to answer the 155 questions. We then request five human experts, GPT-4, and Llama 3 <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib7" title="">2024</a>)</cite> to assess tRAG’s answer quality based on the labels and vRAG-Eval, a set of grading instructions and a rubric. Our results show that GPT-4’s evaluation agrees with human evaluators’ scores at a rate of 83% based on a binary accept/reject category.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">We summarize our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We design a RAG evaluation mechanism, vRAG-Eval, which includes a set of grading instructions and a rubric that measures answer quality in three aspects: correctness, completeness, and honesty, resulting in one single score. It can be readily converted to a binary accept/reject decision that suits business settings.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We build a high-quality benchmark dataset, VND-Bench, with 155 questions/answers pairs that cover 14 subject areas in data domain.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct experiments using LLMs as a RAG’s quality evaluators in a closed-domain closed-ended single-turn QA setting and find that GPT-4’s grading agrees with human experts’ opinions at a rate of 83% in terms of answer accepted or rejected decisions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">The rest of the paper is organized as follows: in section 2, we introduce related work. And in section 3, we explain the motivation, problem setting, and our method of experimentation. In section 4, we provide our experimental results. We suggest future research directions and make concluding remarks in sections 5&amp;6.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Embedding and semantic similarity</span> <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib16" title="">2020</a>)</cite> propose the BERTScore metric as an automatic evaluation method for text generation tasks. Unlike traditional lexical approaches that rely on word matching, BERTScore sums the cosine similarities between the token embeddings of both two sentences: the reference answer and the QA system-generated
answer. <cite class="ltx_cite ltx_citemacro_citet">Risch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib13" title="">2021</a>)</cite> introduce SAS, a metric designed to estimate the semantic similarity between ground-truth annotations and answers generated by a QA model. It is found that semantic similarity metrics, especially those based on contemporary transformer models <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib12" title="">2019</a>)</cite>, align more accurately with human assessments compared to the conventional lexical similarity metrics.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">LLM as a judge</span> Two projects closely related to our research are LLM-as-a-Judge <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib17" title="">2023</a>)</cite> and A Case Study on the Databricks Documentation Bot <cite class="ltx_cite ltx_citemacro_citep">(Leng et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib4" title="">2023</a>)</cite>. To evaluate LLM-based chat
assistants, <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib17" title="">2023</a>)</cite> examine the usage and limitations of LLMs as judges for open-ended questions. This setting may not be suitable for most RAG applications in enterprises, where users typically seek definitive answers. <cite class="ltx_cite ltx_citemacro_citet">Leng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib4" title="">2023</a>)</cite> employ LLMs to generate grades as a weighted composite score of Correctness, Comprehensiveness, and Readability. While they report an alignment rate of 80% with human scores on individual factors, their grading system lacks a crucial element: one single metric that provides a decision-making-ready quality score. In business-centric settings, this limitation becomes particularly noticeable.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Reference-free evaluation</span> In scenarios where human annotations are not readily available, <cite class="ltx_cite ltx_citemacro_citet">Es et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib2" title="">2023</a>)</cite> present RAGAS, a framework for automating the assessment of RAG systems. This framework considers RAG’s 2-module composition and proposes three key metrics: Context Relevance to evaluate the Retriever, Answer Relevance to assess the Generator, and Faithfulness to measure the coherence between the two modules. Similarly, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib14" title="">2024</a>)</cite> evaluates along the same 3 dimensions to assess the quality of RAG components. While not entirely reference-free, ARES leverages a few hundred human annotations during evaluation and employs a lightweight fine-tuned LLM as a judge, rather than relying on a frozen LLM.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Motivation and Problem Setting</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Prior research has shown that LLMs are powerful tools and become increasingly popular for evaluating RAG answer qualities. However, existing studies have largely focused on open-ended settings or in public domains. The objective of our work is to determine the reliability of LLMs in a real-world business setting where informed decision-making is of top priorities as well.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preparation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We first develop a test bed RAG application, tRAG. Its proprietary knowledge base consists of a corpus of 18 PDF documents, totaling 15M tokens, which includes payment processing specifications, API reference guides, data standard manuals, and others.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.7">During the preprocessing stage, for each document <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_d</annotation></semantics></math> in the knowledge corpus, we split its content into chunks such that <math alttext="d=\{c_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">d</mi><mo id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml"><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml"><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.2.m2.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.2.m2.1.1.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.2.m2.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.3.cmml">n</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><eq id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"></eq><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑑</ci><apply id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1">superscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1">subscript</csymbol><set id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS1.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.3"><eq id="S3.SS1.p2.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.3.1"></eq><ci id="S3.SS1.p2.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.3.2">𝑖</ci><cn id="S3.SS1.p2.2.m2.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.2.m2.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">d=\{c_{i}\}_{i=1}^{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_d = { italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, and vectorize each text chunk <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑐</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> using an embedding model <math alttext="M" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_M</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib8" title="">2022</a>)</cite>, where <math alttext="v_{i}=M(c_{i})" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><msub id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">v</mi><mi id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.1.3" xref="S3.SS1.p2.5.m5.1.1.1.3.cmml">M</mi><mo id="S3.SS1.p2.5.m5.1.1.1.2" xref="S3.SS1.p2.5.m5.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.5.m5.1.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.5.m5.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.5.m5.1.1.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.1.2" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.1.3" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.5.m5.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><eq id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2"></eq><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.2">𝑣</ci><ci id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3">𝑖</ci></apply><apply id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"><times id="S3.SS1.p2.5.m5.1.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.1.2"></times><ci id="S3.SS1.p2.5.m5.1.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.1.3">𝑀</ci><apply id="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">v_{i}=M(c_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_M ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>. Then we store the key value pair <math alttext="&lt;v_{i},c_{i}&gt;" class="ltx_math_unparsed" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1b"><mo id="S3.SS1.p2.6.m6.1.1">&lt;</mo><msub id="S3.SS1.p2.6.m6.1.2"><mi id="S3.SS1.p2.6.m6.1.2.2">v</mi><mi id="S3.SS1.p2.6.m6.1.2.3">i</mi></msub><mo id="S3.SS1.p2.6.m6.1.3">,</mo><msub id="S3.SS1.p2.6.m6.1.4"><mi id="S3.SS1.p2.6.m6.1.4.2">c</mi><mi id="S3.SS1.p2.6.m6.1.4.3">i</mi></msub><mo id="S3.SS1.p2.6.m6.1.5">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">&lt;v_{i},c_{i}&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">&lt; italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt;</annotation></semantics></math> into a local vector store instance <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_S</annotation></semantics></math>. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A1" title="Appendix A tRAG: Knowledge Preprocessing Workflow and Configurations ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">A</span></a> illustrates the high-level workflow, and configuration details.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Next, we curate a benchmark dataset, VND-Bench, by collecting question-answer pairs directly from an internal employee communication channel. This channel is frequently used by a diverse range of data users across the organization to seek help in understanding specific topics related to a major Payment Network’s operations and transactional data semantics. We collect a total of 155 closed-ended questions along with their corresponding answers, which we consider to be the ground truth for our experiment. When multiple people respond to a question on the channel, we aggregate their answers into one single response. The set of questions are categorized into subject areas specified in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.T1" title="Table 1 ‣ 3.1 Preparation ‣ 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1">1. Acceptance</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.1.2">8. Issuing</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.1.2.2.1">2. Authentication</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.2.2.2">9. Master Data</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.1.3.3.1">3. Authorization</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.3.3.2">10. OCT</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.1.4.4.1">4. Clearing and Settlement</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.4.2">11. Other</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.1.5.5.1">5. Commercial</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.5.2">12. Processing</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.1.6.6.1">6. Dispute</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.6.2">13. Product</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r" id="S3.T1.1.7.7.1">7. Fraud</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.1.7.7.2">14. Token</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Subject areas of the questions considered in the experiment.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Retrieval, Augmentation, and Generation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.5">For each question <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_Q</annotation></semantics></math> in the VND-Bench dataset, tRAG embeds the text into a fixed-length dense vector using model <math alttext="M:q=M(Q)" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.2.2" xref="S3.SS2.p1.2.m2.1.2.2.cmml">M</mi><mo id="S3.SS2.p1.2.m2.1.2.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p1.2.m2.1.2.1.cmml">:</mo><mrow id="S3.SS2.p1.2.m2.1.2.3" xref="S3.SS2.p1.2.m2.1.2.3.cmml"><mi id="S3.SS2.p1.2.m2.1.2.3.2" xref="S3.SS2.p1.2.m2.1.2.3.2.cmml">q</mi><mo id="S3.SS2.p1.2.m2.1.2.3.1" xref="S3.SS2.p1.2.m2.1.2.3.1.cmml">=</mo><mrow id="S3.SS2.p1.2.m2.1.2.3.3" xref="S3.SS2.p1.2.m2.1.2.3.3.cmml"><mi id="S3.SS2.p1.2.m2.1.2.3.3.2" xref="S3.SS2.p1.2.m2.1.2.3.3.2.cmml">M</mi><mo id="S3.SS2.p1.2.m2.1.2.3.3.1" xref="S3.SS2.p1.2.m2.1.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.p1.2.m2.1.2.3.3.3.2" xref="S3.SS2.p1.2.m2.1.2.3.3.cmml"><mo id="S3.SS2.p1.2.m2.1.2.3.3.3.2.1" stretchy="false" xref="S3.SS2.p1.2.m2.1.2.3.3.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Q</mi><mo id="S3.SS2.p1.2.m2.1.2.3.3.3.2.2" stretchy="false" xref="S3.SS2.p1.2.m2.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml" xref="S3.SS2.p1.2.m2.1.2"><ci id="S3.SS2.p1.2.m2.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.1">:</ci><ci id="S3.SS2.p1.2.m2.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2">𝑀</ci><apply id="S3.SS2.p1.2.m2.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.2.3"><eq id="S3.SS2.p1.2.m2.1.2.3.1.cmml" xref="S3.SS2.p1.2.m2.1.2.3.1"></eq><ci id="S3.SS2.p1.2.m2.1.2.3.2.cmml" xref="S3.SS2.p1.2.m2.1.2.3.2">𝑞</ci><apply id="S3.SS2.p1.2.m2.1.2.3.3.cmml" xref="S3.SS2.p1.2.m2.1.2.3.3"><times id="S3.SS2.p1.2.m2.1.2.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.2.3.3.1"></times><ci id="S3.SS2.p1.2.m2.1.2.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.2.3.3.2">𝑀</ci><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">M:q=M(Q)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_M : italic_q = italic_M ( italic_Q )</annotation></semantics></math>, then conducts distance search <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS2.p1.3.m3.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">roman_Δ</annotation></semantics></math> within the knowledge database <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_S</annotation></semantics></math>, and returns the top <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_K</annotation></semantics></math> most relevant document chunks matching the query.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K=\underset{i}{\mathrm{argmin}}\,\Delta(v_{i},q)" class="ltx_Math" display="block" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml">K</mi><mo id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.cmml"><munder accentunder="true" id="S3.Ex1.m1.2.2.1.3" xref="S3.Ex1.m1.2.2.1.3.cmml"><mi id="S3.Ex1.m1.2.2.1.3.2" xref="S3.Ex1.m1.2.2.1.3.2.cmml">argmin</mi><mo id="S3.Ex1.m1.2.2.1.3.1" xref="S3.Ex1.m1.2.2.1.3.1.cmml">𝑖</mo></munder><mo id="S3.Ex1.m1.2.2.1.2" xref="S3.Ex1.m1.2.2.1.2.cmml">⁢</mo><mi id="S3.Ex1.m1.2.2.1.4" mathvariant="normal" xref="S3.Ex1.m1.2.2.1.4.cmml">Δ</mi><mo id="S3.Ex1.m1.2.2.1.2a" xref="S3.Ex1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.2.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.2.cmml">(</mo><msub id="S3.Ex1.m1.2.2.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.2.cmml">v</mi><mi id="S3.Ex1.m1.2.2.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.2.2.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.2.cmml">,</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">q</mi><mo id="S3.Ex1.m1.2.2.1.1.1.4" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><eq id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2"></eq><ci id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3">𝐾</ci><apply id="S3.Ex1.m1.2.2.1.cmml" xref="S3.Ex1.m1.2.2.1"><times id="S3.Ex1.m1.2.2.1.2.cmml" xref="S3.Ex1.m1.2.2.1.2"></times><apply id="S3.Ex1.m1.2.2.1.3.cmml" xref="S3.Ex1.m1.2.2.1.3"><ci id="S3.Ex1.m1.2.2.1.3.1.cmml" xref="S3.Ex1.m1.2.2.1.3.1">𝑖</ci><ci id="S3.Ex1.m1.2.2.1.3.2.cmml" xref="S3.Ex1.m1.2.2.1.3.2">argmin</ci></apply><ci id="S3.Ex1.m1.2.2.1.4.cmml" xref="S3.Ex1.m1.2.2.1.4">Δ</ci><interval closure="open" id="S3.Ex1.m1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1"><apply id="S3.Ex1.m1.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.2">𝑣</ci><ci id="S3.Ex1.m1.2.2.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.3">𝑖</ci></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝑞</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">K=\underset{i}{\mathrm{argmin}}\,\Delta(v_{i},q)</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">italic_K = underitalic_i start_ARG roman_argmin end_ARG roman_Δ ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.7">During the Augmentation stage tRAG concatenates those top <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m1.1"><semantics id="S3.SS2.p1.6.m1.1a"><mi id="S3.SS2.p1.6.m1.1.1" xref="S3.SS2.p1.6.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m1.1b"><ci id="S3.SS2.p1.6.m1.1.1.cmml" xref="S3.SS2.p1.6.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m1.1d">italic_K</annotation></semantics></math> chunks as the context <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m2.1"><semantics id="S3.SS2.p1.7.m2.1a"><mi id="S3.SS2.p1.7.m2.1.1" xref="S3.SS2.p1.7.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m2.1b"><ci id="S3.SS2.p1.7.m2.1.1.cmml" xref="S3.SS2.p1.7.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m2.1d">italic_C</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C=\{c_{j}|j\in K\}" class="ltx_Math" display="block" id="S3.Ex2.m1.2"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mi id="S3.Ex2.m1.2.2.4" xref="S3.Ex2.m1.2.2.4.cmml">C</mi><mo id="S3.Ex2.m1.2.2.3" xref="S3.Ex2.m1.2.2.3.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.3.cmml"><mo id="S3.Ex2.m1.2.2.2.2.3" stretchy="false" xref="S3.Ex2.m1.2.2.2.3.1.cmml">{</mo><msub id="S3.Ex2.m1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.Ex2.m1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.2.2.2.2.4" lspace="0em" rspace="0em" xref="S3.Ex2.m1.2.2.2.3.1.cmml">|</mo><mrow id="S3.Ex2.m1.2.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.2.2.cmml">j</mi><mo id="S3.Ex2.m1.2.2.2.2.2.1" xref="S3.Ex2.m1.2.2.2.2.2.1.cmml">∈</mo><mi id="S3.Ex2.m1.2.2.2.2.2.3" xref="S3.Ex2.m1.2.2.2.2.2.3.cmml">K</mi></mrow><mo id="S3.Ex2.m1.2.2.2.2.5" stretchy="false" xref="S3.Ex2.m1.2.2.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><eq id="S3.Ex2.m1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.3"></eq><ci id="S3.Ex2.m1.2.2.4.cmml" xref="S3.Ex2.m1.2.2.4">𝐶</ci><apply id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.2"><csymbol cd="latexml" id="S3.Ex2.m1.2.2.2.3.1.cmml" xref="S3.Ex2.m1.2.2.2.2.3">conditional-set</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2">𝑐</ci><ci id="S3.Ex2.m1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.Ex2.m1.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2"><in id="S3.Ex2.m1.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.1"></in><ci id="S3.Ex2.m1.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2">𝑗</ci><ci id="S3.Ex2.m1.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.2.2.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">C=\{c_{j}|j\in K\}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.2d">italic_C = { italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j ∈ italic_K }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.8">And combines with the question to form a prompt: <math alttext="Q\oplus C" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m1.1"><semantics id="S3.SS2.p1.8.m1.1a"><mrow id="S3.SS2.p1.8.m1.1.1" xref="S3.SS2.p1.8.m1.1.1.cmml"><mi id="S3.SS2.p1.8.m1.1.1.2" xref="S3.SS2.p1.8.m1.1.1.2.cmml">Q</mi><mo id="S3.SS2.p1.8.m1.1.1.1" xref="S3.SS2.p1.8.m1.1.1.1.cmml">⊕</mo><mi id="S3.SS2.p1.8.m1.1.1.3" xref="S3.SS2.p1.8.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m1.1b"><apply id="S3.SS2.p1.8.m1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.8.m1.1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1.1">direct-sum</csymbol><ci id="S3.SS2.p1.8.m1.1.1.2.cmml" xref="S3.SS2.p1.8.m1.1.1.2">𝑄</ci><ci id="S3.SS2.p1.8.m1.1.1.3.cmml" xref="S3.SS2.p1.8.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m1.1c">Q\oplus C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m1.1d">italic_Q ⊕ italic_C</annotation></semantics></math> . This prompt is then sent to the answer generator.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We utilize GPT-4 model as the tRAG’s answer generator model <math alttext="G" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_G</annotation></semantics></math>. The resulting response, which constitutes the model’s inference output, is evaluated in our experiment. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A2" title="Appendix B tRAG: Document Retrieval and Answer Generation Workflow ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">B</span></a> illustrates tRAG’s single-turn question answering workflow.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Grading Method</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We compile a fielded dataset containing 155 entries with the following columns: Subject Area, Question, Label, tRAG’s Answer. Additionally, we design a grading rubric as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.T2" title="Table 2 ‣ 3.3 Grading Method ‣ 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">2</span></a> to evaluate tRAG’s Answer.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t" id="S3.T2.1.1.1.1">1:</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.1.2.1.1.1">The response is not aligned with the Label or is off-topic; includes hallucination.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l" id="S3.T2.1.2.2.1">2:</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S3.T2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.2.2.1">
<span class="ltx_p" id="S3.T2.1.2.2.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.2.2.2.1.1.1">The response admits it cannot provide an answer or lacks context; honest.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l" id="S3.T2.1.3.3.1">3:</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S3.T2.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.3.2.1">
<span class="ltx_p" id="S3.T2.1.3.3.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.3.3.2.1.1.1">The response is relevant but contains notable discrepancies or inaccuracies.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l" id="S3.T2.1.4.4.1">4:</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S3.T2.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.4.2.1">
<span class="ltx_p" id="S3.T2.1.4.4.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.4.4.2.1.1.1">The response is acceptable, sufficient but not exhaustive.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l" id="S3.T2.1.5.5.1">5:</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r" id="S3.T2.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.5.2.1">
<span class="ltx_p" id="S3.T2.1.5.5.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.5.5.2.1.1.1">The response is fully accurate and comprehensive, based on the Label.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Grading rubric</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Each score assesses a distinct aspect of answer quality. Specifically, although both incorrect answers and responses of unknown lack value, we argue that hallucination may impose significant harm to mission-critical businesses. Consequently, we penalize hallucination with the lowest score of 1, while honestly acknowledging an unknown response earns a score of 2.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">A correct answer warrants a score of 4, while a RAG application’s extra effort to provide supplementary details is commended with a score of 5, reflecting the value of answer completeness. When a score of 3 is assigned, it indicates that the answer quality is borderline, potentially containing false information.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">LLMs grade answer quality through zero-shot learning, wherein a constant template as part of the vRAG-Eval, is designed to populate prompts for each question. Table <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S3.T3" title="Table 3 ‣ 3.3 Grading Method ‣ 3 Motivation and Problem Setting ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the grading template.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1.1" style="width:411.9pt;">You are an AI assistant. In the following task, you are given a Question,</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.2.1.1">
<span class="ltx_p" id="S3.T3.1.2.2.1.1.1" style="width:411.9pt;">a RAG application’s response,
and a Ground-truth Answer referred to as ’Label’.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.3.1.1">
<span class="ltx_p" id="S3.T3.1.3.3.1.1.1" style="width:411.9pt;">Assess how well the RAG application’s response aligns with the Label,</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.4.1.1">
<span class="ltx_p" id="S3.T3.1.4.4.1.1.1" style="width:411.9pt;">using the grading rubric below:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.5.1.1">
<span class="ltx_p" id="S3.T3.1.5.5.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.5.5.1.1.1.1">[Start of Grading Rubric]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.6.1.1">
<span class="ltx_p" id="S3.T3.1.6.6.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.6.6.1.1.1.1">{rubric}</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.7.1.1">
<span class="ltx_p" id="S3.T3.1.7.7.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.7.7.1.1.1.1">[End of Grading Rubric]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.8.1.1">
<span class="ltx_p" id="S3.T3.1.8.8.1.1.1" style="width:411.9pt;">Treat the Label as the definitive answer. Present your final score in the format: "[[score]]",</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.9.1.1">
<span class="ltx_p" id="S3.T3.1.9.9.1.1.1" style="width:411.9pt;">followed by your justification. Example:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.10.10.1.1">
<span class="ltx_p" id="S3.T3.1.10.10.1.1.1" style="width:411.9pt;">Score: [[3]], Reason: [[The RAG’s response partially aligns with the Label</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.11.1.1">
<span class="ltx_p" id="S3.T3.1.11.11.1.1.1" style="width:411.9pt;">but with some discrepancies]].</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.12.12.1.1">
<span class="ltx_p" id="S3.T3.1.12.12.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.12.12.1.1.1.1">[Start of User Question]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.13.13.1.1">
<span class="ltx_p" id="S3.T3.1.13.13.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.13.13.1.1.1.1">{question}</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.14.1.1">
<span class="ltx_p" id="S3.T3.1.14.14.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.14.14.1.1.1.1">[End of User Question]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.15.15.1.1">
<span class="ltx_p" id="S3.T3.1.15.15.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.15.15.1.1.1.1">[Start of Label]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.16.16.1.1">
<span class="ltx_p" id="S3.T3.1.16.16.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.16.16.1.1.1.1">{label}</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.17.17.1.1">
<span class="ltx_p" id="S3.T3.1.17.17.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.17.17.1.1.1.1">[End of Label]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.18.18">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.18.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.18.18.1.1">
<span class="ltx_p" id="S3.T3.1.18.18.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.18.18.1.1.1.1">[Start of RAG’s Application Response]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.19.19">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S3.T3.1.19.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.19.19.1.1">
<span class="ltx_p" id="S3.T3.1.19.19.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.19.19.1.1.1.1">{tRAG answer}</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.20.20">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S3.T3.1.20.20.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.20.20.1.1">
<span class="ltx_p" id="S3.T3.1.20.20.1.1.1" style="width:411.9pt;"><code class="ltx_verbatim ltx_font_typewriter" id="S3.T3.1.20.20.1.1.1.1">[End of RAG’s Application Response]</code></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Prompt template used for LLM grading.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.4">In our experiment, inputs to the evaluators are a sequence of tRAG’s answers <math alttext="\hat{y}_{1},\hat{y}_{2},\cdots,\hat{y}_{k}" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.4"><semantics id="S3.SS3.p5.1.m1.4a"><mrow id="S3.SS3.p5.1.m1.4.4.3" xref="S3.SS3.p5.1.m1.4.4.4.cmml"><msub id="S3.SS3.p5.1.m1.2.2.1.1" xref="S3.SS3.p5.1.m1.2.2.1.1.cmml"><mover accent="true" id="S3.SS3.p5.1.m1.2.2.1.1.2" xref="S3.SS3.p5.1.m1.2.2.1.1.2.cmml"><mi id="S3.SS3.p5.1.m1.2.2.1.1.2.2" xref="S3.SS3.p5.1.m1.2.2.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p5.1.m1.2.2.1.1.2.1" xref="S3.SS3.p5.1.m1.2.2.1.1.2.1.cmml">^</mo></mover><mn id="S3.SS3.p5.1.m1.2.2.1.1.3" xref="S3.SS3.p5.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p5.1.m1.4.4.3.4" xref="S3.SS3.p5.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS3.p5.1.m1.3.3.2.2" xref="S3.SS3.p5.1.m1.3.3.2.2.cmml"><mover accent="true" id="S3.SS3.p5.1.m1.3.3.2.2.2" xref="S3.SS3.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS3.p5.1.m1.3.3.2.2.2.2" xref="S3.SS3.p5.1.m1.3.3.2.2.2.2.cmml">y</mi><mo id="S3.SS3.p5.1.m1.3.3.2.2.2.1" xref="S3.SS3.p5.1.m1.3.3.2.2.2.1.cmml">^</mo></mover><mn id="S3.SS3.p5.1.m1.3.3.2.2.3" xref="S3.SS3.p5.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p5.1.m1.4.4.3.5" xref="S3.SS3.p5.1.m1.4.4.4.cmml">,</mo><mi id="S3.SS3.p5.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p5.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS3.p5.1.m1.4.4.3.6" xref="S3.SS3.p5.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS3.p5.1.m1.4.4.3.3" xref="S3.SS3.p5.1.m1.4.4.3.3.cmml"><mover accent="true" id="S3.SS3.p5.1.m1.4.4.3.3.2" xref="S3.SS3.p5.1.m1.4.4.3.3.2.cmml"><mi id="S3.SS3.p5.1.m1.4.4.3.3.2.2" xref="S3.SS3.p5.1.m1.4.4.3.3.2.2.cmml">y</mi><mo id="S3.SS3.p5.1.m1.4.4.3.3.2.1" xref="S3.SS3.p5.1.m1.4.4.3.3.2.1.cmml">^</mo></mover><mi id="S3.SS3.p5.1.m1.4.4.3.3.3" xref="S3.SS3.p5.1.m1.4.4.3.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.4b"><list id="S3.SS3.p5.1.m1.4.4.4.cmml" xref="S3.SS3.p5.1.m1.4.4.3"><apply id="S3.SS3.p5.1.m1.2.2.1.1.cmml" xref="S3.SS3.p5.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p5.1.m1.2.2.1.1">subscript</csymbol><apply id="S3.SS3.p5.1.m1.2.2.1.1.2.cmml" xref="S3.SS3.p5.1.m1.2.2.1.1.2"><ci id="S3.SS3.p5.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS3.p5.1.m1.2.2.1.1.2.1">^</ci><ci id="S3.SS3.p5.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.1.1.2.2">𝑦</ci></apply><cn id="S3.SS3.p5.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.p5.1.m1.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.p5.1.m1.3.3.2.2.cmml" xref="S3.SS3.p5.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.3.3.2.2.1.cmml" xref="S3.SS3.p5.1.m1.3.3.2.2">subscript</csymbol><apply id="S3.SS3.p5.1.m1.3.3.2.2.2.cmml" xref="S3.SS3.p5.1.m1.3.3.2.2.2"><ci id="S3.SS3.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS3.p5.1.m1.3.3.2.2.2.1">^</ci><ci id="S3.SS3.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.3.3.2.2.2.2">𝑦</ci></apply><cn id="S3.SS3.p5.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.p5.1.m1.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">⋯</ci><apply id="S3.SS3.p5.1.m1.4.4.3.3.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.4.4.3.3.1.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3">subscript</csymbol><apply id="S3.SS3.p5.1.m1.4.4.3.3.2.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3.2"><ci id="S3.SS3.p5.1.m1.4.4.3.3.2.1.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3.2.1">^</ci><ci id="S3.SS3.p5.1.m1.4.4.3.3.2.2.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3.2.2">𝑦</ci></apply><ci id="S3.SS3.p5.1.m1.4.4.3.3.3.cmml" xref="S3.SS3.p5.1.m1.4.4.3.3.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.4c">\hat{y}_{1},\hat{y}_{2},\cdots,\hat{y}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.4d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> together with a sequence of labels <math alttext="y_{1},y_{2},\cdots,y_{k}" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.4"><semantics id="S3.SS3.p5.2.m2.4a"><mrow id="S3.SS3.p5.2.m2.4.4.3" xref="S3.SS3.p5.2.m2.4.4.4.cmml"><msub id="S3.SS3.p5.2.m2.2.2.1.1" xref="S3.SS3.p5.2.m2.2.2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.2.2.1.1.2" xref="S3.SS3.p5.2.m2.2.2.1.1.2.cmml">y</mi><mn id="S3.SS3.p5.2.m2.2.2.1.1.3" xref="S3.SS3.p5.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p5.2.m2.4.4.3.4" xref="S3.SS3.p5.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS3.p5.2.m2.3.3.2.2" xref="S3.SS3.p5.2.m2.3.3.2.2.cmml"><mi id="S3.SS3.p5.2.m2.3.3.2.2.2" xref="S3.SS3.p5.2.m2.3.3.2.2.2.cmml">y</mi><mn id="S3.SS3.p5.2.m2.3.3.2.2.3" xref="S3.SS3.p5.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p5.2.m2.4.4.3.5" xref="S3.SS3.p5.2.m2.4.4.4.cmml">,</mo><mi id="S3.SS3.p5.2.m2.1.1" mathvariant="normal" xref="S3.SS3.p5.2.m2.1.1.cmml">⋯</mi><mo id="S3.SS3.p5.2.m2.4.4.3.6" xref="S3.SS3.p5.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS3.p5.2.m2.4.4.3.3" xref="S3.SS3.p5.2.m2.4.4.3.3.cmml"><mi id="S3.SS3.p5.2.m2.4.4.3.3.2" xref="S3.SS3.p5.2.m2.4.4.3.3.2.cmml">y</mi><mi id="S3.SS3.p5.2.m2.4.4.3.3.3" xref="S3.SS3.p5.2.m2.4.4.3.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.4b"><list id="S3.SS3.p5.2.m2.4.4.4.cmml" xref="S3.SS3.p5.2.m2.4.4.3"><apply id="S3.SS3.p5.2.m2.2.2.1.1.cmml" xref="S3.SS3.p5.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.2.2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p5.2.m2.2.2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.2.2.1.1.2">𝑦</ci><cn id="S3.SS3.p5.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.p5.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.p5.2.m2.3.3.2.2.cmml" xref="S3.SS3.p5.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.3.3.2.2.1.cmml" xref="S3.SS3.p5.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p5.2.m2.3.3.2.2.2.cmml" xref="S3.SS3.p5.2.m2.3.3.2.2.2">𝑦</ci><cn id="S3.SS3.p5.2.m2.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.p5.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">⋯</ci><apply id="S3.SS3.p5.2.m2.4.4.3.3.cmml" xref="S3.SS3.p5.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.4.4.3.3.1.cmml" xref="S3.SS3.p5.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS3.p5.2.m2.4.4.3.3.2.cmml" xref="S3.SS3.p5.2.m2.4.4.3.3.2">𝑦</ci><ci id="S3.SS3.p5.2.m2.4.4.3.3.3.cmml" xref="S3.SS3.p5.2.m2.4.4.3.3.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.4c">y_{1},y_{2},\cdots,y_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.4d">italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. The quality scores given by evaluators can be generally denoted as <math alttext="S_{E}:=P(\hat{Y}=Y)" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.1"><semantics id="S3.SS3.p5.3.m3.1a"><mrow id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><msub id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3.cmml"><mi id="S3.SS3.p5.3.m3.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.3.2.cmml">S</mi><mi id="S3.SS3.p5.3.m3.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.3.3.cmml">E</mi></msub><mo id="S3.SS3.p5.3.m3.1.1.2" lspace="0.278em" rspace="0.278em" xref="S3.SS3.p5.3.m3.1.1.2.cmml">:=</mo><mrow id="S3.SS3.p5.3.m3.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.1.3" xref="S3.SS3.p5.3.m3.1.1.1.3.cmml">P</mi><mo id="S3.SS3.p5.3.m3.1.1.1.2" xref="S3.SS3.p5.3.m3.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.p5.3.m3.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS3.p5.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p5.3.m3.1.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.2.cmml">Y</mi><mo id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml">=</mo><mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.3" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo id="S3.SS3.p5.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><csymbol cd="latexml" id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">assign</csymbol><apply id="S3.SS3.p5.3.m3.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2">𝑆</ci><ci id="S3.SS3.p5.3.m3.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3">𝐸</ci></apply><apply id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1"><times id="S3.SS3.p5.3.m3.1.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.2"></times><ci id="S3.SS3.p5.3.m3.1.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.3">𝑃</ci><apply id="S3.SS3.p5.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1"><eq id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1"></eq><apply id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2"><ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.1">^</ci><ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.2.2">𝑌</ci></apply><ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.3">𝑌</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">S_{E}:=P(\hat{Y}=Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT := italic_P ( over^ start_ARG italic_Y end_ARG = italic_Y )</annotation></semantics></math>, where <math alttext="E\in\{GPT-4,Llama3,Human\}" class="ltx_Math" display="inline" id="S3.SS3.p5.4.m4.3"><semantics id="S3.SS3.p5.4.m4.3a"><mrow id="S3.SS3.p5.4.m4.3.3" xref="S3.SS3.p5.4.m4.3.3.cmml"><mi id="S3.SS3.p5.4.m4.3.3.5" xref="S3.SS3.p5.4.m4.3.3.5.cmml">E</mi><mo id="S3.SS3.p5.4.m4.3.3.4" xref="S3.SS3.p5.4.m4.3.3.4.cmml">∈</mo><mrow id="S3.SS3.p5.4.m4.3.3.3.3" xref="S3.SS3.p5.4.m4.3.3.3.4.cmml"><mo id="S3.SS3.p5.4.m4.3.3.3.3.4" stretchy="false" xref="S3.SS3.p5.4.m4.3.3.3.4.cmml">{</mo><mrow id="S3.SS3.p5.4.m4.1.1.1.1.1" xref="S3.SS3.p5.4.m4.1.1.1.1.1.cmml"><mrow id="S3.SS3.p5.4.m4.1.1.1.1.1.2" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p5.4.m4.1.1.1.1.1.2.2" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.2.cmml">G</mi><mo id="S3.SS3.p5.4.m4.1.1.1.1.1.2.1" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.1.1.1.1.1.2.3" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.3.cmml">P</mi><mo id="S3.SS3.p5.4.m4.1.1.1.1.1.2.1a" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.1.1.1.1.1.2.4" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.4.cmml">T</mi></mrow><mo id="S3.SS3.p5.4.m4.1.1.1.1.1.1" xref="S3.SS3.p5.4.m4.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS3.p5.4.m4.1.1.1.1.1.3" xref="S3.SS3.p5.4.m4.1.1.1.1.1.3.cmml">4</mn></mrow><mo id="S3.SS3.p5.4.m4.3.3.3.3.5" xref="S3.SS3.p5.4.m4.3.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.4.m4.2.2.2.2.2" xref="S3.SS3.p5.4.m4.2.2.2.2.2.cmml"><mi id="S3.SS3.p5.4.m4.2.2.2.2.2.2" xref="S3.SS3.p5.4.m4.2.2.2.2.2.2.cmml">L</mi><mo id="S3.SS3.p5.4.m4.2.2.2.2.2.1" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.2.2.2.2.2.3" xref="S3.SS3.p5.4.m4.2.2.2.2.2.3.cmml">l</mi><mo id="S3.SS3.p5.4.m4.2.2.2.2.2.1a" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.2.2.2.2.2.4" xref="S3.SS3.p5.4.m4.2.2.2.2.2.4.cmml">a</mi><mo id="S3.SS3.p5.4.m4.2.2.2.2.2.1b" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.2.2.2.2.2.5" xref="S3.SS3.p5.4.m4.2.2.2.2.2.5.cmml">m</mi><mo id="S3.SS3.p5.4.m4.2.2.2.2.2.1c" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.2.2.2.2.2.6" xref="S3.SS3.p5.4.m4.2.2.2.2.2.6.cmml">a</mi><mo id="S3.SS3.p5.4.m4.2.2.2.2.2.1d" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml">⁢</mo><mn id="S3.SS3.p5.4.m4.2.2.2.2.2.7" xref="S3.SS3.p5.4.m4.2.2.2.2.2.7.cmml">3</mn></mrow><mo id="S3.SS3.p5.4.m4.3.3.3.3.6" xref="S3.SS3.p5.4.m4.3.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.4.m4.3.3.3.3.3" xref="S3.SS3.p5.4.m4.3.3.3.3.3.cmml"><mi id="S3.SS3.p5.4.m4.3.3.3.3.3.2" xref="S3.SS3.p5.4.m4.3.3.3.3.3.2.cmml">H</mi><mo id="S3.SS3.p5.4.m4.3.3.3.3.3.1" xref="S3.SS3.p5.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.3.3.3.3.3.3" xref="S3.SS3.p5.4.m4.3.3.3.3.3.3.cmml">u</mi><mo id="S3.SS3.p5.4.m4.3.3.3.3.3.1a" xref="S3.SS3.p5.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.3.3.3.3.3.4" xref="S3.SS3.p5.4.m4.3.3.3.3.3.4.cmml">m</mi><mo id="S3.SS3.p5.4.m4.3.3.3.3.3.1b" xref="S3.SS3.p5.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.3.3.3.3.3.5" xref="S3.SS3.p5.4.m4.3.3.3.3.3.5.cmml">a</mi><mo id="S3.SS3.p5.4.m4.3.3.3.3.3.1c" xref="S3.SS3.p5.4.m4.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.4.m4.3.3.3.3.3.6" xref="S3.SS3.p5.4.m4.3.3.3.3.3.6.cmml">n</mi></mrow><mo id="S3.SS3.p5.4.m4.3.3.3.3.7" stretchy="false" xref="S3.SS3.p5.4.m4.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.3b"><apply id="S3.SS3.p5.4.m4.3.3.cmml" xref="S3.SS3.p5.4.m4.3.3"><in id="S3.SS3.p5.4.m4.3.3.4.cmml" xref="S3.SS3.p5.4.m4.3.3.4"></in><ci id="S3.SS3.p5.4.m4.3.3.5.cmml" xref="S3.SS3.p5.4.m4.3.3.5">𝐸</ci><set id="S3.SS3.p5.4.m4.3.3.3.4.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3"><apply id="S3.SS3.p5.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1"><minus id="S3.SS3.p5.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.1"></minus><apply id="S3.SS3.p5.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2"><times id="S3.SS3.p5.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.1"></times><ci id="S3.SS3.p5.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.2">𝐺</ci><ci id="S3.SS3.p5.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.3">𝑃</ci><ci id="S3.SS3.p5.4.m4.1.1.1.1.1.2.4.cmml" xref="S3.SS3.p5.4.m4.1.1.1.1.1.2.4">𝑇</ci></apply><cn id="S3.SS3.p5.4.m4.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p5.4.m4.1.1.1.1.1.3">4</cn></apply><apply id="S3.SS3.p5.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2"><times id="S3.SS3.p5.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.1"></times><ci id="S3.SS3.p5.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.2">𝐿</ci><ci id="S3.SS3.p5.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.3">𝑙</ci><ci id="S3.SS3.p5.4.m4.2.2.2.2.2.4.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.4">𝑎</ci><ci id="S3.SS3.p5.4.m4.2.2.2.2.2.5.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.5">𝑚</ci><ci id="S3.SS3.p5.4.m4.2.2.2.2.2.6.cmml" xref="S3.SS3.p5.4.m4.2.2.2.2.2.6">𝑎</ci><cn id="S3.SS3.p5.4.m4.2.2.2.2.2.7.cmml" type="integer" xref="S3.SS3.p5.4.m4.2.2.2.2.2.7">3</cn></apply><apply id="S3.SS3.p5.4.m4.3.3.3.3.3.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3"><times id="S3.SS3.p5.4.m4.3.3.3.3.3.1.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.1"></times><ci id="S3.SS3.p5.4.m4.3.3.3.3.3.2.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.2">𝐻</ci><ci id="S3.SS3.p5.4.m4.3.3.3.3.3.3.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.3">𝑢</ci><ci id="S3.SS3.p5.4.m4.3.3.3.3.3.4.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.4">𝑚</ci><ci id="S3.SS3.p5.4.m4.3.3.3.3.3.5.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.5">𝑎</ci><ci id="S3.SS3.p5.4.m4.3.3.3.3.3.6.cmml" xref="S3.SS3.p5.4.m4.3.3.3.3.3.6">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.3c">E\in\{GPT-4,Llama3,Human\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.4.m4.3d">italic_E ∈ { italic_G italic_P italic_T - 4 , italic_L italic_l italic_a italic_m italic_a 3 , italic_H italic_u italic_m italic_a italic_n }</annotation></semantics></math>. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A3" title="Appendix C Answer Quality Evaluation Workflow ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">C</span></a> depicts the workflow of the grading experiments.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">To ensure reproducibility and prevent potential grading hallucinations, we fix
the temperature parameter for each LLM call at <math alttext="T=0.0" class="ltx_Math" display="inline" id="S3.SS3.p6.1.m1.1"><semantics id="S3.SS3.p6.1.m1.1a"><mrow id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">T</mi><mo id="S3.SS3.p6.1.m1.1.1.1" xref="S3.SS3.p6.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">0.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><eq id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1"></eq><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">𝑇</ci><cn id="S3.SS3.p6.1.m1.1.1.3.cmml" type="float" xref="S3.SS3.p6.1.m1.1.1.3">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">T=0.0</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.1.m1.1d">italic_T = 0.0</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We ask GPT-4, Llama 3 8B, and human experts at our company to evaluate the quality of the aforementioned tRAG’s answers respectively and independently. Their assessment is guided by the vRAG-Eval instructions and grading rubric.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>GPT-4</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">vRAG-Eval instructs that grading responses should be in the format of “Score: [[score]], Reason: [[explanation]]”. GPT-4 demonstrates the capability to strictly adhere to the instructions. For example:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.ix1.p1.1.1">Score<span class="ltx_text ltx_font_medium" id="S4.I1.ix1.p1.1.1.1">: [[2]], </span>Reason<span class="ltx_text ltx_font_medium" id="S4.I1.ix1.p1.1.1.2">: [[The RAG’s response provides general insights into the user’s queries but does not align with the specific answer provided in the Label. ...]]</span></span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A4" title="Appendix D An Anomaly Transaction Question and Its Label ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">D</span></a> shows an anomaly transaction question, and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A5" title="Appendix E tRAG’s Answer to the Anomaly Transaction Question and GPT-4’s Grading ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">E</span></a> illustrates the full text of GPT-4’s grading response.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We prompt GPT-4 to explain the justifications that substantiate the grading. It allows us to understand LLM’s thinking process. In our earlier design of vRAG-Eval, we explicitly emphasized Correctness, Completeness, and Honesty. It became evident that the GPT-4 evaluator partially deviated from the Label then graded answers based on the language model’s own definitions of those quality metrics. For instance, as illustrated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A6" title="Appendix F Early Design of Grading Instructions, Rubric, and GPT-4’s Assessment ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">F</span></a>, tRAG’s answer for the same question was graded 4 instead of 2:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix1.p1">
<p class="ltx_p" id="S4.I2.ix1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I2.ix1.p1.1.1">Rating: [[4]], Reason: [[The RAG application’s answer is acceptable and provides
a general understanding of the possible reasons for the user’s queries. ...]]</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">This example underscores the importance of developing clear and distinct grading criteria tailored to individual businesses’ specific requirements, contrasting with vague guidelines that are commonly applied in public domains.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Human</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We gather human grading that score tRAG’s answer quality from experts. They are experienced data and machine learning practitioners employed by our company.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The 155 Q&amp;A pairs in VND-Bench are collected in two phases. tRAG’s answers to the first 52 questions are graded by three human experts, while the next set of 103 answers are graded by two others.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To obtain a robust and representative view of human’s opinion, we choose the median function to vote the first 52 grading scores by the three human experts, and then randomly pick the other two experts’ assessment to sample remaining 103 scores.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">The score distributions are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.F1" title="Figure 1 ‣ 4.2 Human ‣ 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.F2" title="Figure 2 ‣ 4.2 Human ‣ 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">2</span></a> respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="482" id="S4.F1.g1" src="extracted/5712610/images/3humans_score_distribution.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Distribution of human grading scores for tRAG’s answers to the first 52 questions.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="482" id="S4.F2.g1" src="extracted/5712610/images/2humans_score_distribution.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of human grading scores for tRAG’s answers to the next 103 questions.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Compare GPT-4 with Human</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The overall agreement between GPT-4 and Human evaluators at each of 5 quality score levels defined by the vRAG-Eval rubric is 29.7%. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#S4.F3" title="Figure 3 ‣ 4.3 Compare GPT-4 with Human ‣ 4 Experiments ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">3</span></a> shows comparison at each level.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="482" id="S4.F3.g1" src="extracted/5712610/images/human_vs_gpt4_each_level.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Score distribution comparison at each quality level.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">A closer analysis of grading samples reveals that the two evaluators may have subtle opinion differences at exact quality levels to assign scores. For example, Human grades the tRAG’s answer to a transaction code question shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A7" title="Appendix G A Transaction Code Question and Label ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">G</span></a> at quality level 2, while GPT-4 assigns 1:</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I3.ix1.p1">
<p class="ltx_p" id="S4.I3.ix1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I3.ix1.p1.1.1">Score<span class="ltx_text ltx_font_medium" id="S4.I3.ix1.p1.1.1.1">: [[1]], </span>Reason<span class="ltx_text ltx_font_medium" id="S4.I3.ix1.p1.1.1.2">: [[The RAG’s response does not align with the Label. ...]]</span></span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A8" title="Appendix H tRAG’s Answer to the Transaction Code Question and GPT-4’s grading justification ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">H</span></a> displays the full text of GPT-4’s grading justification.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">In the vRAG-Eval grading rubric, the criteria are “2: The response admits it cannot provide an answer or lacks context; honest.”. Meanwhile, as the reason explained by GPT-4 evaluator, it believes that tRAG’s answer does not directly address the question, so its assessment is based on “1: The response is not aligned with the Label or is off-topic; includes hallucination.” We also believe that GPT-4 explanation justifies the score 1. It is evident that both evaluators agree that tRAG’s response is not a good answer to the question, yet each grade on different basis of incorrectness.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><cite class="ltx_cite ltx_citemacro_citet">Leng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib4" title="">2023</a>)</cite> confirms that both humans and LLMs struggle to hold the same standard for the same score when grading on high precision. However, the definitions of “high precision” and “low precision” are subjective and can vary based on individual perspectives. To address this issue, we propose aligning with the typical thumbs-up and thumbs-down gestures used in chat applications. It is also appropriate in business settings, where a binary quality scale effectively predicts that whether an answer can be accepted or rejected by clients.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">Therefore, we convert the 5-level grading scale to binary: 1&amp;2&amp;3 (answer rejected), 4&amp;5 (answer accepted). We observe an impressive 82.6% agreement rate between GPT-4 and human evaluators. Our subsequent findings reveal that GPT-4 and human evaluators both agree the tRAG’s answer quality being at the lower end: 77% and 72% reject rate respectively. Note that tRAG is a preliminarily developed application that serves as a test bed. However, it highlights the seriousness of the issue if we blindly deploy RAG applications without answer quality control measures in place. This could have adverse consequences for any enterprise’ business operations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Llama 3</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Llama 3 is an open-source LLM available in two variants: 8B and 70B parameters. Due to our infrastructure constraints, we are only able to utilize the smaller 8B version.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Similar to the GPT-4 experiment, we ask Llama 3 to assess the quality of tRAG’s responses based on vRAG-Eval’s grading instructions and rubric. Details of results can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A9" title="Appendix I Llama 3 ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">The overall agreement between 8B and Human evaluators at each of 5 quality score levels is 23.9%. Next, we convert the scaled grading to answer rejected/accepted categories and observe only 36.8% alignment.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">At this relatively small parameter size, we hypothesize that 8B may not have sufficient intelligence to serve as a reliable evaluator to assess the answer quality on par with human experts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Directions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">While LLMs continue to grow in size and intelligence, utilizing powerful LLMs as an automated alternative to human expert graders is a promising and prominent area of research. This approach complements traditional lexical and semantic measurements and demonstrates exceptional explainability and human preference alignment capabilities. We identify several areas for potential future research and practice directions:</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Enterprise’s Values</span> In designing the vRAG-Eval grading system, we incorporate honesty into the grading rubric to reflect that human evaluators prefer answers that admit uncertainty over hallucination. We also conduct preliminary experiments on the subject of "verbalized probability" to explore the potential of using self-expressed confidence levels as a factor in measuring a LLM evaluator’s own honesty. Additionally, we consider integrating more ethical aspects, such as inclusiveness and fairness, into the grading metrics to ensure that the system is both effective and responsible.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">More Language Support</span> The VND-Bench dataset currently only includes Q&amp;A pairs in English, yet our enterprise is a global company that supports businesses worldwide. We consider extending the dataset to include other languages, reflecting the company’s value of inclusiveness and diversity.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Domain Adaptation</span> We curate the VND-Bench dataset, focusing on transactional data domain knowledge. We consider building benchmark datasets in other areas and study vRAG-Eval’s applicability to explore its potential in those business priorities.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">Few-shot Evaluation.</span> Our experiments demonstrate that the GPT-4 evaluator exhibits high agreement rates in accept/reject decisions. We consider analyzing the score discrepancy at each of the five grading levels to better understand the relationship between human and LLM evaluators’ thinking process. By leveraging "few-shot learning", we hypothesize that we can improve the alignment of LLM evaluators with human preferences at a greater refined granularity.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.p6.1.1">More Question Categories.</span> The VND-Bench dataset currently focuses on factual knowledge questions in the transactional data domain. We consider expanding to include math and logical reasoning questions, which are essential for evaluating business-centric RAG applications. By developing high-order grading capabilities, we can better support the company’s growth and innovation.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1">Llama 3 (70B).</span> We contemplate the quantization of the 70B model into 4-bit integers and evaluate its grading agreement with human experts. This experiment could explore the possibility of open source LLMs becoming more economical and less risky alternatives, suitable for deployment on commodity hardware.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we thoroughly study evaluating Retrieval-Augmented Generation (RAG) applications in a high-stakes business environment where answer quality is paramount.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We design a novel system to evaluate answer correctness, completeness, and honesty through one 5-level grading scale. Furthermore, we uniquely cast fine-grained scores into either accepted or rejected categories. This mapping leads to a clearer, more direct comparison between human evaluator and LLMs.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">We observe a remarkably high agreement rate of 82.6% between GPT-4 and human evaluators, which underscores the potential of LLMs in understanding and aligning with human judgement criteria through exceptional explainability.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Furthermore, our study reveals a significant disparity between Llama 3 8B’s evaluation and human grading, highlighting the importance of selecting a strong LLM.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Ranjan Dutta, Toni Wang, and Ajit Patil for their thoughtful discussions and assistance with data annotation. We acknowledge Ping Zhu, Chi Wo Chung, Jiayin Zheng, Mohammad Rahman and their team for providing robust OpenAI API access. We appreciate our managers Salila Khilani, Dmitrii Krainov, and Yu Gu for their support. We also thank Stephen Shin, Nissa Strottman, Allee McDermott, Kathleen Patel, Tuesday Uhland, and Devon Grant for their support on compliance matters.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al. (2020)</span>
<span class="ltx_bibblock">
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2002.06305" title="">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv:2002.06305</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et al. (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.15217" title="">Ragas: Automated evaluation of retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">arXiv:2309.15217.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2009.03300" title="">Measuring massive multitask language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ICLR 2021</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leng et al. (2023)</span>
<span class="ltx_bibblock">
Quinn Leng, Kasey Uhlenhuth, and Alkis Polyzotis. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG" title="">Best practices for llm evaluation of rag applications</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Databrick Engineering Blog</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2005.11401" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock">arXiv:2005.11401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2205.14334" title="">Teaching models to express their uncertainty in words</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv:2205.14334</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024)</span>
<span class="ltx_bibblock">
Meta. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://ai.meta.com/blog/meta-llama-3/" title="">Introducing meta llama 3: The most capable openly available llm to date</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/index/new-and-improved-embedding-model/" title="">New and improved embedding model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/index/new-models-and-developer-products-announced-at-devday/" title="">New models and developer products announced at devday</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et al. (2024)</span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.05934" title="">Fine-tuning or retrieval? comparing knowledge injection in llms</a>.

</span>
<span class="ltx_bibblock">arXiv:2312.05934.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" title="">Improving language understanding by generative pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">OpenAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1908.10084" title="">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Risch et al. (2021)</span>
<span class="ltx_bibblock">
Julian Risch, Timo Möller, Julian Gutsch, and Malte Pietsch. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2108.06130" title="">Semantic answer similarity for evaluating question answering models</a>.

</span>
<span class="ltx_bibblock">arXiv:2108.06130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et al. (2024)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.09476" title="">Ares: An automated evaluation framework for retrieval-augmented generation systems</a>.

</span>
<span class="ltx_bibblock">arXiv:2311.09476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.07713" title="">Instructretro: Instruction tuning post retrieval-augmented pretraining</a>.

</span>
<span class="ltx_bibblock">arXiv:2310.07713.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ICLR 2020</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>tRAG: Knowledge Preprocessing Workflow and Configurations</h2>
<figure class="ltx_figure" id="A1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="116" id="A1.1.g1" src="extracted/5712610/images/preprocessing.png" width="359"/>
</figure>
<figure class="ltx_table" id="A1.tab1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.tab1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.tab1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.1.1.1">Chunk size</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.tab1.1.1.1.2">1000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.tab1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.2.1.1">Chunk overlap</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.tab1.1.2.1.2">25%</td>
</tr>
<tr class="ltx_tr" id="A1.tab1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.3.2.1">Embedding model</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.tab1.1.3.2.2">text-embedding-ada-002</td>
</tr>
<tr class="ltx_tr" id="A1.tab1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.4.3.1">Indexing</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.tab1.1.4.3.2">Hierarchical Navigable Small World (HNSW)</td>
</tr>
<tr class="ltx_tr" id="A1.tab1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.5.4.1">Distance</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.tab1.1.5.4.2">Squared L2 Norm</td>
</tr>
<tr class="ltx_tr" id="A1.tab1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.6.5.1">Top-K</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.tab1.1.6.5.2">3</td>
</tr>
<tr class="ltx_tr" id="A1.tab1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.tab1.1.7.6.1">Embeddings</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.tab1.1.7.6.2">13644</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>tRAG: Document Retrieval and Answer Generation Workflow</h2>
<figure class="ltx_figure" id="A2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="99" id="A2.1.g1" src="extracted/5712610/images/rag.png" width="359"/>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Answer Quality Evaluation Workflow</h2>
<figure class="ltx_figure" id="A3.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="112" id="A3.1.g1" src="extracted/5712610/images/qa.png" width="359"/>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>An Anomaly Transaction Question and Its Label</h2>
<figure class="ltx_table" id="A4.T1">
<table class="ltx_tabular ltx_align_middle" id="A4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.1.1">
<span class="ltx_p" id="A4.T1.1.1.1.1.1.1" style="width:303.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.1.1.1.1.1.1.1">Question</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.2.1">
<span class="ltx_p" id="A4.T1.1.1.1.2.1.1" style="width:104.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.1.1.1.2.1.1.1">Label</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.2.1.1">
<span class="ltx_p" id="A4.T1.1.2.2.1.1.1" style="width:303.5pt;"><span class="ltx_text ltx_font_typewriter" id="A4.T1.1.2.2.1.1.1.1">I have following queries related to data in [anomaly_tranx_table_name]. 
<br class="ltx_break"/>
<br class="ltx_break"/>There are around [number_of_records] records in this table where [purchase_date_field_name] is greater than [post_date_field_name]. May I know what are the scenarios where a purchase date can be greater than [anomaly] posted date? 
<br class="ltx_break"/>
<br class="ltx_break"/>There are records where same [tranx_id_field_name] have more than one record. For ref, [tranx_id_field_name] [tranx_id_value] has [number_of_entries] entries in dedupe table and each record has different [tranx_amount_field_name] reported. May I know why there can be more than 1 entry for the same [tranx_id_field_name]</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.2.2.1">
<span class="ltx_p" id="A4.T1.1.2.2.2.1.1" style="width:104.1pt;"><span class="ltx_text ltx_font_typewriter" id="A4.T1.1.2.2.2.1.1.1">[tranx_id_field_name] may be used to tie multiple clearing records to one authorisation, as is the case with split shipments (one order placed online will have original authorisation, followed by multiple shipments, each one cleared separately)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A1: </span>Note that we retain spelling errors in the original label text. Exact table names, column names, and values are masked in square brackets for compliance purposes.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>tRAG’s Answer to the Anomaly Transaction Question and GPT-4’s Grading</h2>
<figure class="ltx_table" id="A5.tab1">
<table class="ltx_tabular ltx_align_middle" id="A5.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.tab1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A5.tab1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.tab1.1.1.1.1.1">
<span class="ltx_p" id="A5.tab1.1.1.1.1.1.1" style="width:294.9pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.1">tRAG’s Answer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.tab1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A5.tab1.1.1.1.2.1">
<span class="ltx_p" id="A5.tab1.1.1.1.2.1.1" style="width:117.1pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.2.1.1.1">Assessment</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.tab1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A5.tab1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A5.tab1.1.2.2.1.1">
<span class="ltx_p" id="A5.tab1.1.2.2.1.1.1" style="width:294.9pt;"><span class="ltx_text ltx_font_typewriter" id="A5.tab1.1.2.2.1.1.1.1">Based on the context provided, I can provide some general insights into your queries. However, please note that the specific reasons for these scenarios in your data may require a more detailed understanding of the data and the business rules applied. 
<br class="ltx_break"/>
<br class="ltx_break"/>1. Scenarios where purchase date ([purchase_date_field_name]) is greater than [anomaly] posted date ([post_date_field_name]): This could occur due to time zone differences, system errors, or late reporting of transactions. For instance, a transaction could be made in one time zone and recorded in another, causing the purchase date to appear later than the [anomaly] posted date. It could also be due to delays in transaction posting or reporting. 
<br class="ltx_break"/>
<br class="ltx_break"/>2. Multiple entries for the same transaction ID ([tranx_id_field_name]) with different [anomaly] transaction amounts ([tranx_amount_field_name]): This could be due to multiple instances of [anomaly] associated with the same transaction, or updates to the [anomaly] amount as more information becomes available. It could also be a result of data errors or duplications. 
<br class="ltx_break"/>
<br class="ltx_break"/>Without more specific information about the data and the business rules applied, it’s difficult to provide a more precise explanation. I would recommend further investigation into these records and consultation with the team responsible for data management and [anomaly] detection.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A5.tab1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="A5.tab1.1.2.2.2.1">
<span class="ltx_p" id="A5.tab1.1.2.2.2.1.1" style="width:117.1pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A5.tab1.1.2.2.2.1.1.1">Score<span class="ltx_text ltx_font_medium" id="A5.tab1.1.2.2.2.1.1.1.1">: [[2]], </span>Reason<span class="ltx_text ltx_font_medium" id="A5.tab1.1.2.2.2.1.1.1.2">: [[The RAG’s response provides general insights into the user’s queries but does not align with the specific answer provided in the Label. The Label explains that multiple entries for the same transaction ID could be due to split shipments, which is not mentioned in the RAG’s response. The RAG’s response also does not provide a specific reason for the scenario where the purchase date is greater than the fraud posted date, which is not addressed in the Label. Therefore, the RAG’s response lacks the specific context provided in the Label.]]</span></span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Early Design of Grading Instructions, Rubric, and GPT-4’s Assessment</h2>
<figure class="ltx_table" id="A6.tab1">
<table class="ltx_tabular ltx_align_middle" id="A6.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.tab1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A6.tab1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.1.1.1.1">
<span class="ltx_p" id="A6.tab1.1.1.1.1.1.1" style="width:420.6pt;">Please act as an impartial judge and evaluate the quality of the answer provided by a RAG application.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.2.2.1.1">
<span class="ltx_p" id="A6.tab1.1.2.2.1.1.1" style="width:420.6pt;">Your evaluation should consider correctness, completeness, and honesty.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.3.3.1.1">
<span class="ltx_p" id="A6.tab1.1.3.3.1.1.1" style="width:420.6pt;">You will be given a reference answer, and the RAG application’s answer,</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.4.4.1.1">
<span class="ltx_p" id="A6.tab1.1.4.4.1.1.1" style="width:420.6pt;">then grade the quality of the RAG application’s answer based on rubric below:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.5.5.1.1">
<span class="ltx_p" id="A6.tab1.1.5.5.1.1.1" style="width:420.6pt;"><code class="ltx_verbatim ltx_font_typewriter" id="A6.tab1.1.5.5.1.1.1.1">[The Start of Grading Rubric]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.6.6.1.1">
<span class="ltx_p" id="A6.tab1.1.6.6.1.1.1" style="width:420.6pt;"><code class="ltx_verbatim ltx_font_typewriter" id="A6.tab1.1.6.6.1.1.1.1">{rubric}</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.7.7.1.1">
<span class="ltx_p" id="A6.tab1.1.7.7.1.1.1" style="width:420.6pt;"><code class="ltx_verbatim ltx_font_typewriter" id="A6.tab1.1.7.7.1.1.1.1">[The End of Grading Rubric]</code></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.8.8.1.1">
<span class="ltx_p" id="A6.tab1.1.8.8.1.1.1" style="width:420.6pt;">Be as objective as possible. Output your final verdict by strictly following this format: "[[rating]]",</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.9.9.1.1">
<span class="ltx_p" id="A6.tab1.1.9.9.1.1.1" style="width:420.6pt;">then providing your explanation.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.10.10.1.1">
<span class="ltx_p" id="A6.tab1.1.10.10.1.1.1" style="width:420.6pt;">For example: "Rating: [[5]], Reason: [[The answer is correct and comprehensive]].</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt" id="A6.tab1.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.11.11.1.1">
<span class="ltx_p" id="A6.tab1.1.11.11.1.1.1" style="width:420.6pt;">1: completely incorrect, hallucination</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.12.12.1.1">
<span class="ltx_p" id="A6.tab1.1.12.12.1.1.1" style="width:420.6pt;">2: admits it cannot answer or lack of context, honest</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.13.13.1.1">
<span class="ltx_p" id="A6.tab1.1.13.13.1.1.1" style="width:420.6pt;">3: pertinent but contains noticeable errors or inaccuracies</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.14.14.1.1">
<span class="ltx_p" id="A6.tab1.1.14.14.1.1.1" style="width:420.6pt;">4: acceptable answer, adequate but not comprehensive</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="A6.tab1.1.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.15.15.1.1">
<span class="ltx_p" id="A6.tab1.1.15.15.1.1.1" style="width:420.6pt;">5: fully accurate and exhaustive</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A6.tab1.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_tt" id="A6.tab1.1.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="A6.tab1.1.16.16.1.1">
<span class="ltx_p" id="A6.tab1.1.16.16.1.1.1" style="width:420.6pt;"><span class="ltx_text ltx_font_typewriter" id="A6.tab1.1.16.16.1.1.1.1">Rating: [[4]], Reason: [[The RAG application’s answer is acceptable and provides a general understanding of the possible reasons for the user’s queries. However, it does not specifically address the scenario of multiple clearing records tied to one authorization, as mentioned in the reference answer. The RAG application’s answer is not incorrect, but it is not as comprehensive as the reference answer.]]</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>A Transaction Code Question and Label</h2>
<figure class="ltx_table" id="A7.tab1">
<table class="ltx_tabular ltx_align_middle" id="A7.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A7.tab1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A7.tab1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A7.tab1.1.1.1.1.1">
<span class="ltx_p" id="A7.tab1.1.1.1.1.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="A7.tab1.1.1.1.1.1.1.1">Question</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A7.tab1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A7.tab1.1.1.1.2.1">
<span class="ltx_p" id="A7.tab1.1.1.1.2.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_bold" id="A7.tab1.1.1.1.2.1.1.1">Label</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A7.tab1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A7.tab1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A7.tab1.1.2.2.1.1">
<span class="ltx_p" id="A7.tab1.1.2.2.1.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_typewriter" id="A7.tab1.1.2.2.1.1.1.1">I would like to browse transactions - invoice from [tranx_code_value]. Could you please advice if the data is availabe somewhere.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A7.tab1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="A7.tab1.1.2.2.2.1">
<span class="ltx_p" id="A7.tab1.1.2.2.2.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_typewriter" id="A7.tab1.1.2.2.2.1.1.1">[tranx_code_value]s are carried in [tranx_detail_table_name] [tranx_code_field_name] = [tranx_code_value]. [tranx_code_value]s are used for all types of information transferance such as additional hotel portfolio information that could not be fit in the [tranx_component_record_type] record.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>tRAG’s Answer to the Transaction Code Question and GPT-4’s grading justification</h2>
<figure class="ltx_table" id="A8.T2">
<table class="ltx_tabular ltx_align_middle" id="A8.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A8.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A8.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T2.1.1.1.1.1">
<span class="ltx_p" id="A8.T2.1.1.1.1.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_bold" id="A8.T2.1.1.1.1.1.1.1">tRAG’s Answer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T2.1.1.1.2.1">
<span class="ltx_p" id="A8.T2.1.1.1.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="A8.T2.1.1.1.2.1.1.1">GPT-4’s Grading</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A8.T2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T2.1.2.2.1.1">
<span class="ltx_p" id="A8.T2.1.2.2.1.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_typewriter" id="A8.T2.1.2.2.1.1.1.1">Based on the context provided, it appears that the data for [tranx_code_value] - [tranx_component_record_type] - Commercial Card - Invoice (Line Detail) is contained within specific tables. However, the exact location or access method for these tables is not specified in the context. As a data engineer, I would typically expect this data to be stored in a secure database or data warehouse, accessible to authorized personnel or systems. If you are an authorized user, you may need to use a specific tool or application to browse this data, or you may need to write a query or script to extract the data from the database. If you are not sure how to access this data, I would recommend reaching out to your organization’s IT or data management team for assistance. They should be able to provide you with the necessary access and instructions. Please note that this is a general advice based on typical data management practices and the actual process may vary depending on your organization’s specific systems and policies.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A8.T2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T2.1.2.2.2.1">
<span class="ltx_p" id="A8.T2.1.2.2.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_typewriter" id="A8.T2.1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A8.T2.1.2.2.2.1.1.1.1">Score</span>: [[1]], <span class="ltx_text ltx_font_bold" id="A8.T2.1.2.2.2.1.1.1.2">Reason</span>: [[The RAG’s response does not align with the Label. The Label specifically mentions that [tranx_code_value]s are carried in [tranx_detail_table_name] with [tranx_code_field_name] = [tranx_code_value], but the RAG’s response does not provide this information. Instead, it gives a general advice on how to access data, which is not asked in the question.]]</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A2: </span>tRAG’s answer and GPT-4’s grading justification.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Llama 3</h2>
<div class="ltx_para" id="A9.p1">
<p class="ltx_p" id="A9.p1.1">Llama 3 8B assigns a quality score of 4 to 80% of tRAG’s answers, and a score of 3 to just one answer. The score distribution is depicted in the left-hand figure of Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A9.F1" title="Figure A1 ‣ Appendix I Llama 3 ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">A1</span></a>.</p>
</div>
<figure class="ltx_figure" id="A9.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="225" id="A9.F1.g1" src="extracted/5712610/images/llama3.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="219" id="A9.F1.g2" src="extracted/5712610/images/llama3correlation.png" width="287"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure A1: </span>Left: Distribution of Llama 3 8B grading scores for tRAG’s answers. Right: The heatmap illustrates the correlation between quality scores and grading confidence.</figcaption>
</figure>
<div class="ltx_para" id="A9.p2">
<p class="ltx_p" id="A9.p2.1">The skewness leads us to speculate that at this not-so-large parameter size, 8B may not be intelligent enough to discern the answer quality on the borderline. And it may avoid assigning extreme scores when not confident in its grading. To test the hypothesis, we add an additional instruction text, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A9.T3" title="Table A3 ‣ Appendix I Llama 3 ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">A3</span></a>, to elicit Llama 3’s confidence level. This approach, coined as “verbalized probability” by <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib6" title="">2022</a>)</cite>, allows the LLM to express its confidence level in natural language without use of model logits.</p>
</div>
<figure class="ltx_table" id="A9.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A9.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A9.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A9.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A9.T3.1.1.1.1.1">
<span class="ltx_p" id="A9.T3.1.1.1.1.1.1" style="width:411.9pt;">Present your final score in the format: "[[score]]",

<br class="ltx_break"/>followed by your confidence level of the grading in the range of 0 to 100,

<br class="ltx_break"/>with 100 being very confident and 0 being not sure about your grading at all.

<br class="ltx_break"/>At the end, disclose your justification. Example:

<br class="ltx_break"/>Score: [[3]], Confidence: [[50]], Reason: [[The RAG’s response partially aligns with the Label but with some discrepancies]].</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table A3: </span>Addition instruction to request Llama 3 disclosing its internal confidence level.</figcaption>
</figure>
<div class="ltx_para" id="A9.p3">
<p class="ltx_p" id="A9.p3.1">The right-hand figure in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#A9.F1" title="Figure A1 ‣ Appendix I Llama 3 ‣ Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need"><span class="ltx_text ltx_ref_tag">A1</span></a> illustrates the correlation between the quality scores of answers and the confidence of the 8B evaluator.</p>
</div>
<div class="ltx_para" id="A9.p4">
<p class="ltx_p" id="A9.p4.1"><cite class="ltx_cite ltx_citemacro_citet">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.18064v2#bib.bib3" title="">2021</a>)</cite> show that LLMs should be calibrated. However, we argue that the observation agrees with our hypothesis that when the model’s grading confidence is relatively low (at 80%), 8B tends to assign a quality score of 4 as a "default" choice.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jul  5 09:40:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
