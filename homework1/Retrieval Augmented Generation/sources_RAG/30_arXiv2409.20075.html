<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain</title>
<!--Generated on Mon Sep 30 08:25:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.20075v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S2" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">LLMs for the E-Commerce Domain.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Retrieval Augmented Generation for LLMs.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS1" title="In 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS2" title="In 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Strategies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS2.SSS0.Px1" title="In 3.2 Training Strategies ‣ 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Stage 1: Backbone Continual Pre-training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS2.SSS0.Px2" title="In 3.2 Training Strategies ‣ 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Stage 2: Training Retriever with Hard Negative Mining.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS2.SSS0.Px3" title="In 3.2 Training Strategies ‣ 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Stage 3: Training Generator with Retrieval Augmented Instruction Tuning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.SS3" title="In 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S4" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>WorthBuying Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S4.SS0.SSS0.Px1" title="In 4 WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Data Collection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S4.SS0.SSS0.Px2" title="In 4 WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">QDA Tuples Generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S4.SS0.SSS0.Px3" title="In 4 WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title">Dataset Statistics and Comparisons.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS1" title="In 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS1.SSS1" title="In 5.1 Experiment Setup ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Datasets and Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS1.SSS2" title="In 5.1 Experiment Setup ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS2" title="In 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS2.SSS1" title="In 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Retrieval Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS2.SSS2" title="In 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Generation Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS2.SSS3" title="In 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Influence of Retrieval to Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS3" title="In 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Analysis of BSharedRAG Performance Gain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS4" title="In 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Inference space cost and time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S6" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A1" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of WorthBuying Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A1.SS1" title="In Appendix A Details of WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A1.SS2" title="In Appendix A Details of WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>QA Generation with the help of GPT-4 API</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A1.SS2.SSS1" title="In A.2 QA Generation with the help of GPT-4 API ‣ Appendix A Details of WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Question Generation Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A1.SS2.SSS2" title="In A.2 QA Generation with the help of GPT-4 API ‣ Appendix A Details of WorthBuying Dataset ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Answer Generation Prompt</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Model Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2.SS1" title="In Appendix B Model Training Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Domain-specific Continual Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2.SS1.SSS1" title="In B.1 Domain-specific Continual Pretraining ‣ Appendix B Model Training Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1.1 </span>Training Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2.SS1.SSS2" title="In B.1 Domain-specific Continual Pretraining ‣ Appendix B Model Training Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1.2 </span>Training Configurations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2.SS2" title="In Appendix B Model Training Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Contrastive Learning Training Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A3" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Generation Evaluation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A3.SS1" title="In Appendix C Generation Evaluation Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span> Stage1: Extracting key points from answer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A3.SS2" title="In Appendix C Generation Evaluation Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span> Stage2: Judging the key points</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A4" title="In BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Analysis</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">BSharedRAG: Backbone Shared Retrieval-Augmented Generation 
<br class="ltx_break"/>for the E-commerce Domain</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Kaisi Guan
 Qian Cao  Yuchong Sun  Xiting Wang   Ruihua Song <span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break"/>Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="1.1.1">{guankaisi,caoqian4real,ycsun,xitingwang,rsong}@ruc.edu.com
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">  Corresponding authors.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="2.1">Retrieval Augmented Generation (RAG) system is important in domains such as e-commerce, which has many long-tail entities and frequently updated information.
Most existing works adopt separate modules for retrieval and generation, which may be suboptimal since the retrieval task and the generation task cannot benefit from each other to improve performance.
In this paper, we construct a high-quality e-commerce dataset and use e-commerce as an example domain to show how to ensure effective transfer between the retrieval and generation tasks.
We propose a novel Backbone Shared RAG framework (<span class="ltx_text ltx_font_bold" id="2.1.1">BSharedRAG</span>). It first uses a domain-specific corpus to continually pre-train a base model as a domain-specific backbone model and then trains two plug-and-play Low-Rank Adaptation (LoRA) modules based on the shared backbone to minimize retrieval and generation losses respectively. Experimental results indicate that our proposed BSharedRAG outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation.
Our codes, models, and dataset are available at <a class="ltx_ref ltx_href" href="https://bsharedrag.github.io" title="">https://bsharedrag.github.io</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">BSharedRAG: Backbone Shared Retrieval-Augmented Generation 
<br class="ltx_break"/>for the E-commerce Domain</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Kaisi Guan
 Qian Cao  Yuchong Sun  Xiting Wang <span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>  Corresponding authors.</span></span></span>  Ruihua Song <span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">{guankaisi,caoqian4real,ycsun,xitingwang,rsong}@ruc.edu.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have shown impressive performance in reasoning and remembering extensive knowledge <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib38" title="">2019</a>); Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib5" title="">2020</a>); Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib1" title="">2023</a>); Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib2" title="">2024</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib62" title="">2023b</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib46" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib47" title="">b</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib6" title="">2024</a>)</cite>.
However, when facing domain-specific questions, especially the e-commerce domain that has many long-tail entities and frequently updated information, LLMs often have issues such as hallucinations <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib45" title="">2023c</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib52" title="">2023</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib60" title="">2023a</a>)</cite>.
Retrieval-Augmented Generation (RAG) has been proposed as an effective method for such cases <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib64" title="">2024</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib15" title="">2024</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib61" title="">2024</a>)</cite>.
However, previous works <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib59" title="">2023</a>); Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib44" title="">2023b</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib25" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib57" title="">2024</a>)</cite> mainly adopt a general-domain retriever and generator directly, which may be suboptimal due to the lack of domain-specific knowledge.
How to construct a domain-specific RAG system is an important research problem. To answer this question, we need to solve the following two challenges.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparing three categories of possible RAG frameworks: (a) most previous works lie in the separate RAG category, in which the retrieval task and the generation task cannot benefit from each other; (b) only a few trials are about the fully shared RAG, which may suffer from performance decrease due to negative transfer and require effort-taking loss balancing to determine <math alttext="\lambda" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">italic_λ</annotation></semantics></math>; (c) what we proposed is a backbone shared RAG, which ensures effective knowledge transfer between the two tasks without the need to perform effort-taking loss balancing.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">C1 (dataset)</em>: How to build a high-quality RAG dataset with an informative knowledge base and correct question-answer (QA) pairs? For important domains like e-commerce, the existing knowledge bases are usually short and noisy, as most of them are from user-generated product reviews. The QA pairs are also of various qualities <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib9" title="">2023</a>)</cite> and may contain subjective or conflicting information because the answers are provided by users.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">C2 (framework)</em>: How to design a suitable RAG framework for domain adaptation? As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a> (a), existing works typically adopt separate modules for retrieval and generation <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib44" title="">2023b</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib25" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib59" title="">2023</a>); langchain (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib22" title="">2023</a>); Liu (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib26" title="">2022</a>)</cite>, for example, using BERT-like <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib10" title="">2019</a>)</cite> embedding models for retrieving documents and an LLM for generating answers.
In this design, retrieval and generation operate independently, hindering their potential to achieve optimal performance. It is often challenging for the retriever to leverage the continuous pre-training of the generator, while the generator’s performance may be constrained by retrieved documents that, although similar to the query, are less effective in addressing the questions (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.SS2.SSS3" title="5.2.3 Influence of Retrieval to Generation ‣ 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>).
Although we can build the retriever and generator both on a fully shared LLM and optimize it with multi-task learning as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a> (b), it can suffer from negative transfer between tasks, <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">i.e.</span>, performance decrease due to the potential conflicts in the retrieval task and the generation task. Moreover, balancing the retrieval loss and the generation loss is non-trivial and may lead to an effort-taking hyperparameter search.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address the two challenges, we first propose the WorthBuying dataset with 735K high-quality documents, 50K Question-Document-Answer (QDA) tuples, and human annotated test data of relevant documents for 1K questions and 500 QA pairs (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">C1</em>).
The knowledge base in our dataset comes from professional users, reducing conflicts and errors, and is more informative, with 1.1K words per document rather than a few dozen words as in existing e-commerce knowledge bases. We also annotate high-quality QA pairs with GPT-4 <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib1" title="">2023</a>)</cite> and manually review the test set.
We then propose a specifically designed BSharedRAG framework that effectively adapts RAG to the e-commerce domain (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">C2</em>).
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a> (c) we apply two task-specific modules to independently minimize the retrieval loss and generation loss, which avoids effort-taking loss balancing.
With the shared backbone that benefits from domain-specific continual pre-training, the retriever and the generator can benefit from each other to improve both retrieval and generation performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions are summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">∙</annotation></semantics></math> We construct a high-quality dataset called <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">WorthBuying</span> for the e-commerce domain, which contains 735K documents, 50K QDA tuples for training and some high-quality human labeled test data to facilitate further research.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">∙</annotation></semantics></math> We propose a backbone-shared RAG framework (<span class="ltx_text ltx_font_bold" id="S1.p7.1.1">BSharedRAG</span>) for the e-commerce domain, which enables effective knowledge transfer between the retrieval task and the generation task to improve the performance of both tasks.
Although we focus on e-commerce, the framework can be generalized to other domain-specific RAG models.
</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">∙</annotation></semantics></math> Experimental results demonstrate that our BSharedRAG, with affordable efficiency, outperforms traditional RAG methods in both retrieval and generation evaluation.

</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">LLMs for the E-Commerce Domain.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">To adapt LLMs for the e-commerce domain, some studies leverage LLMs like GPT-3.5-turbo and GPT-4 to generate e-commerce instruction tuning datasets <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib23" title="">2024</a>); Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib35" title="">2024</a>)</cite>.
Continual pre-training on e-commerce data has been shown to further enhance LLM performance in e-commerce tasks <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib29" title="">2023a</a>)</cite>. Despite these advancements, issues such as hallucination hindering real-world application, especially for Product Question Answering (PQA) tasks that heavily rely on product knowledge <cite class="ltx_cite ltx_citemacro_cite">Miller et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib31" title="">2020</a>); Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib43" title="">2023a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib23" title="">2024</a>)</cite>. Existing PQA datasets only contain short, non-informative knowledge bases about products, and QA pairs are of various quality <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib16" title="">2019a</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib13" title="">2019a</a>); Shen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib40" title="">2023a</a>); Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib9" title="">2023</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib7" title="">2019a</a>)</cite>, since they contain subjective or conflicting answers provided by users.
In this paper, we address the aforementioned issues by proposing a high-quality WorthBuying dataset. Moreover, we propose a BSharedRAG framework to improve retrieval and generation performance significantly.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Retrieval Augmented Generation for LLMs.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Retrieval Augmented Generation (RAG), where the retriever provides relevant knowledge from external sources and LLMs answer the query based on the retrieved results, significantly reduces problems such as hallucinations and untimely knowledge updating of LLMs <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib64" title="">2024</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib61" title="">2024</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib15" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="470" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of training and inference of our proposed BSharedRAG Framework.
</figcaption>
</figure>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">In previous SeperateRAG frameworks, the retriever and the generator are typically separate and share no parameters, with the retrievers being off-the-shelf embedding models like DPR <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib19" title="">2020</a>)</cite>, BGE <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib54" title="">2023</a>)</cite>, E5 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib49" title="">2022</a>)</cite>, or BERT-like architectures <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib10" title="">2019</a>)</cite>, and the generators being LLMs <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib46" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib47" title="">b</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>)</cite>. Some RAG frameworks, such as RETRO <cite class="ltx_cite ltx_citemacro_cite">Borgeaud et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib4" title="">2022</a>)</cite>, REPLUG <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib44" title="">2023b</a>)</cite>, and RA-DIT <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib25" title="">2023</a>)</cite>, make some performance improvements within the SeperateRAG framework. These frameworks require an independent retriever alongside the language model, without any shared parameters between the retriever and generator, creating a gap between these components. It is challenging for them to share the benefits derived from continual pre-training (CPT), making it difficult for them hard to adapt to a specific domain.
Recently, GritLM <cite class="ltx_cite ltx_citemacro_cite">Muennighoff et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib32" title="">2024</a>)</cite> is proposed, in which the retriever and generator share all parameters and thus the the retriever can benefit from the capacity of LLM. However, it is unclear how GritLM can be adapted to domain-specific scenarios: it requires large-scale training data and high computation costs that can hardly be satisfied in a specific domain. In comparison, we propose a lightweight framework that can efficiently and effectively convert a continually pre-trained domain-specific LLM to a backbone-shared domain-specific RAG model.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce our BSharedRAG for scenarios that require extensive domain knowledge, such as e-commerce.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Framework</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.5.1">Separate RAG</span>. Traditional RAG frameworks treat retrieval and generation as two distinct tasks, where the retriever and generator share no parameters (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a>(a)).
Formally, they optimize two groups of parameters separately:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\ \mathcal{L}_{r}(\theta_{r}),\ \ \min\mathcal{L}_{g}(\theta_{g})" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.3.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">min</mi><mo id="S3.E1.m1.1.1.1.1.3a" lspace="0.667em" xref="S3.E1.m1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">ℒ</mi><mi id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">r</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">r</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.3" rspace="1.167em" xref="S3.E1.m1.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.3.1.cmml">min</mi><mo id="S3.E1.m1.2.2.2.2.3a" lspace="0.167em" xref="S3.E1.m1.2.2.2.2.3.cmml">⁡</mo><msub id="S3.E1.m1.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.2.2.3.2.2" xref="S3.E1.m1.2.2.2.2.3.2.2.cmml">ℒ</mi><mi id="S3.E1.m1.2.2.2.2.3.2.3" xref="S3.E1.m1.2.2.2.2.3.2.3.cmml">g</mi></msub></mrow><mo id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mo id="S3.E1.m1.2.2.2.2.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">θ</mi><mi id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml">g</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><list id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.2"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><min id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></min><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">𝑟</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝑟</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><times id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"></times><apply id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3"><min id="S3.E1.m1.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.3.1"></min><apply id="S3.E1.m1.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.3.2.2">ℒ</ci><ci id="S3.E1.m1.2.2.2.2.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3.2.3">𝑔</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">𝜃</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">𝑔</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\min\ \mathcal{L}_{r}(\theta_{r}),\ \ \min\mathcal{L}_{g}(\theta_{g})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">roman_min caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) , roman_min caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.4">where <math alttext="\theta_{r}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">θ</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝜃</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\theta_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_θ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{r}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">ℒ</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ℒ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{L}_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> are the parameters and loss for retrieval model, and <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">θ</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝜃</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="L_{g}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">L</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝐿</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">L_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> are those for generation model.
This paradigm cannot benefit from the shared information between the two related tasks as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1" title="1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a>, which limits the performance of both retrieval and generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.4.1">Fully Shared RAG</span>.
One straightforward method to overcome the problems is to build a shared model that can be used for retrieval and generation based on multi-task learning (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a> (b)):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\mathcal{L}_{r}(\theta)+\lambda\mathcal{L}_{g}(\theta)" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><mrow id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><mrow id="S3.E2.m1.2.3.2.2" xref="S3.E2.m1.2.3.2.2.cmml"><mi id="S3.E2.m1.2.3.2.2.1" xref="S3.E2.m1.2.3.2.2.1.cmml">min</mi><mo id="S3.E2.m1.2.3.2.2a" lspace="0.167em" xref="S3.E2.m1.2.3.2.2.cmml">⁡</mo><msub id="S3.E2.m1.2.3.2.2.2" xref="S3.E2.m1.2.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.3.2.2.2.2" xref="S3.E2.m1.2.3.2.2.2.2.cmml">ℒ</mi><mi id="S3.E2.m1.2.3.2.2.2.3" xref="S3.E2.m1.2.3.2.2.2.3.cmml">r</mi></msub></mrow><mo id="S3.E2.m1.2.3.2.1" xref="S3.E2.m1.2.3.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.3.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><mo id="S3.E2.m1.2.3.2.3.2.1" stretchy="false" xref="S3.E2.m1.2.3.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">θ</mi><mo id="S3.E2.m1.2.3.2.3.2.2" stretchy="false" xref="S3.E2.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">+</mo><mrow id="S3.E2.m1.2.3.3" xref="S3.E2.m1.2.3.3.cmml"><mi id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.2.cmml">λ</mi><mo id="S3.E2.m1.2.3.3.1" xref="S3.E2.m1.2.3.3.1.cmml">⁢</mo><msub id="S3.E2.m1.2.3.3.3" xref="S3.E2.m1.2.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.3.3.3.2" xref="S3.E2.m1.2.3.3.3.2.cmml">ℒ</mi><mi id="S3.E2.m1.2.3.3.3.3" xref="S3.E2.m1.2.3.3.3.3.cmml">g</mi></msub><mo id="S3.E2.m1.2.3.3.1a" xref="S3.E2.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.3.3.4.2" xref="S3.E2.m1.2.3.3.cmml"><mo id="S3.E2.m1.2.3.3.4.2.1" stretchy="false" xref="S3.E2.m1.2.3.3.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">θ</mi><mo id="S3.E2.m1.2.3.3.4.2.2" stretchy="false" xref="S3.E2.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><plus id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></plus><apply id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2"><times id="S3.E2.m1.2.3.2.1.cmml" xref="S3.E2.m1.2.3.2.1"></times><apply id="S3.E2.m1.2.3.2.2.cmml" xref="S3.E2.m1.2.3.2.2"><min id="S3.E2.m1.2.3.2.2.1.cmml" xref="S3.E2.m1.2.3.2.2.1"></min><apply id="S3.E2.m1.2.3.2.2.2.cmml" xref="S3.E2.m1.2.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.2.2.1.cmml" xref="S3.E2.m1.2.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.3.2.2.2.2.cmml" xref="S3.E2.m1.2.3.2.2.2.2">ℒ</ci><ci id="S3.E2.m1.2.3.2.2.2.3.cmml" xref="S3.E2.m1.2.3.2.2.2.3">𝑟</ci></apply></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝜃</ci></apply><apply id="S3.E2.m1.2.3.3.cmml" xref="S3.E2.m1.2.3.3"><times id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.1"></times><ci id="S3.E2.m1.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.2">𝜆</ci><apply id="S3.E2.m1.2.3.3.3.cmml" xref="S3.E2.m1.2.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.1.cmml" xref="S3.E2.m1.2.3.3.3">subscript</csymbol><ci id="S3.E2.m1.2.3.3.3.2.cmml" xref="S3.E2.m1.2.3.3.3.2">ℒ</ci><ci id="S3.E2.m1.2.3.3.3.3.cmml" xref="S3.E2.m1.2.3.3.3.3">𝑔</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝜃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\min\mathcal{L}_{r}(\theta)+\lambda\mathcal{L}_{g}(\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">roman_min caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_θ ) + italic_λ caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.3">where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_θ</annotation></semantics></math> is the parameters of the shared model, and <math alttext="\lambda&gt;0" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">λ</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">&gt;</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><gt id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></gt><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝜆</ci><cn id="S3.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\lambda&gt;0</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_λ &gt; 0</annotation></semantics></math> is the weight for balancing the two losses.
This framework may suffer from negative transfer, i.e., performance decrease in both tasks due to the potential conflicts between them, as verified in our experiments. Moreover, finding an appropriate <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_λ</annotation></semantics></math> to ensure loss balancing is non-trivial and may require an effort-taking hyperparameter search.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Backbone Shared RAG</span>.
To make both retrieval and generation benefit from the capability of LLM, and avoid the difficulty of balancing two objectives, we propose a BSharedRAG framework.
The basic idea is to use a frozen LLM backbone as a shared information processor and two task-specific modules for retrieval and generation tasks.We use LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib18" title="">2021</a>)</cite> method to finish this idea.
Under this design, the two tasks can share knowledge from the backbone, <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">e.g.</span>, domain-specific continual pre-training, while using task-specific parameters for different purposes to avoid the difficulty of balancing two tasks and negative transfer:</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\mathcal{L}_{r}(\theta_{r_{s}};\theta_{s}),\ \ \min\mathcal{L}_{g}(\theta_%
{g_{s}};\theta_{s})" class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.3.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.1.4.1" xref="S3.E3.m1.1.1.1.1.4.1.cmml">min</mi><mo id="S3.E3.m1.1.1.1.1.4a" lspace="0.167em" xref="S3.E3.m1.1.1.1.1.4.cmml">⁡</mo><msub id="S3.E3.m1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.4.2.2" xref="S3.E3.m1.1.1.1.1.4.2.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.4.2.3" xref="S3.E3.m1.1.1.1.1.4.2.3.cmml">r</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.3.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml">θ</mi><msub id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml">r</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml">s</mi></msub></msub><mo id="S3.E3.m1.1.1.1.1.2.2.4" xref="S3.E3.m1.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.cmml">θ</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml">s</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.2.3" rspace="1.167em" xref="S3.E3.m1.2.2.3.cmml">,</mo><mrow id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.4.cmml"><mi id="S3.E3.m1.2.2.2.2.4.1" xref="S3.E3.m1.2.2.2.2.4.1.cmml">min</mi><mo id="S3.E3.m1.2.2.2.2.4a" lspace="0.167em" xref="S3.E3.m1.2.2.2.2.4.cmml">⁡</mo><msub id="S3.E3.m1.2.2.2.2.4.2" xref="S3.E3.m1.2.2.2.2.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.2.2.4.2.2" xref="S3.E3.m1.2.2.2.2.4.2.2.cmml">ℒ</mi><mi id="S3.E3.m1.2.2.2.2.4.2.3" xref="S3.E3.m1.2.2.2.2.4.2.3.cmml">g</mi></msub></mrow><mo id="S3.E3.m1.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.3.cmml"><mo id="S3.E3.m1.2.2.2.2.2.2.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.3.cmml">(</mo><msub id="S3.E3.m1.2.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.1.2.cmml">θ</mi><msub id="S3.E3.m1.2.2.2.2.1.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2.cmml">g</mi><mi id="S3.E3.m1.2.2.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.2.2.1.1.1.3.3.cmml">s</mi></msub></msub><mo id="S3.E3.m1.2.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.2.3.cmml">;</mo><msub id="S3.E3.m1.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml">θ</mi><mi id="S3.E3.m1.2.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.2.3.cmml">s</mi></msub><mo id="S3.E3.m1.2.2.2.2.2.2.5" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><list id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></times><apply id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4"><min id="S3.E3.m1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.4.1"></min><apply id="S3.E3.m1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.2.1.cmml" xref="S3.E3.m1.1.1.1.1.4.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.2.2.cmml" xref="S3.E3.m1.1.1.1.1.4.2.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.4.2.3.cmml" xref="S3.E3.m1.1.1.1.1.4.2.3">𝑟</ci></apply></apply><list id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2">𝜃</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2">𝑟</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3">𝑠</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2">𝜃</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.3">𝑠</ci></apply></list></apply><apply id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"><times id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.3"></times><apply id="S3.E3.m1.2.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.2.4"><min id="S3.E3.m1.2.2.2.2.4.1.cmml" xref="S3.E3.m1.2.2.2.2.4.1"></min><apply id="S3.E3.m1.2.2.2.2.4.2.cmml" xref="S3.E3.m1.2.2.2.2.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.4.2.1.cmml" xref="S3.E3.m1.2.2.2.2.4.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.4.2.2.cmml" xref="S3.E3.m1.2.2.2.2.4.2.2">ℒ</ci><ci id="S3.E3.m1.2.2.2.2.4.2.3.cmml" xref="S3.E3.m1.2.2.2.2.4.2.3">𝑔</ci></apply></apply><list id="S3.E3.m1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><apply id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.2">𝜃</ci><apply id="S3.E3.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.2">𝑔</ci><ci id="S3.E3.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.3.3">𝑠</ci></apply></apply><apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2">𝜃</ci><ci id="S3.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.3">𝑠</ci></apply></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\min\mathcal{L}_{r}(\theta_{r_{s}};\theta_{s}),\ \ \min\mathcal{L}_{g}(\theta_%
{g_{s}};\theta_{s})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">roman_min caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , roman_min caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p4.3">where <math alttext="\theta_{r_{s}}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">θ</mi><msub id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.3.2.cmml">r</mi><mi id="S3.SS1.p4.1.m1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝜃</ci><apply id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.3.2">𝑟</ci><ci id="S3.SS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\theta_{r_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_θ start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\theta_{r_{s}}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">θ</mi><msub id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.p4.2.m2.1.1.3.2" xref="S3.SS1.p4.2.m2.1.1.3.2.cmml">r</mi><mi id="S3.SS1.p4.2.m2.1.1.3.3" xref="S3.SS1.p4.2.m2.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝜃</ci><apply id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.p4.2.m2.1.1.3.2">𝑟</ci><ci id="S3.SS1.p4.2.m2.1.1.3.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\theta_{r_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_θ start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> are task-specific modules unpon a pre-trained shared backbone model <math alttext="\theta_{s}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">θ</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝜃</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\theta_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Strategies</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S2.F2" title="Figure 2 ‣ Retrieval Augmented Generation for LLMs. ‣ 2 Related Work ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">2</span></a>,
we adopt the following three training stages to optimize Equation (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.E3" title="In 3.1 Framework ‣ 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Stage 1: Backbone Continual Pre-training.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We leverage continual pre-training to adapt a general domain LLM (Baichuan2-7B-Base <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>)</cite> here) to a specific domain such as e-commerce (here called Ecom-base).
The domain-specific continual pre-training objective is the next token prediction task:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max\limits_{\theta_{s}}\ \sum_{i=1}^{N}\mathrm{log}\ p(y_{i}|y_{&lt;i};\theta_{s})" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><munder id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">max</mi><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">θ</mi><mi id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml">s</mi></msub></munder><mo id="S3.E4.m1.1.1.2" lspace="0.667em" xref="S3.E4.m1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><munderover id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.E4.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E4.m1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.2.2.3.1" xref="S3.E4.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml">log</mi><mo id="S3.E4.m1.1.1.1.1.2" lspace="0.500em" xref="S3.E4.m1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml">p</mi><mo id="S3.E4.m1.1.1.1.1.2a" xref="S3.E4.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.1.1.1.4.2.cmml">y</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo fence="false" id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">θ</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.3.cmml">s</mi></msub></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><times id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></times><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><max id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"></max><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝜃</ci><ci id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">𝑠</ci></apply></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><apply id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.2">subscript</csymbol><sum id="S3.E4.m1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.2.2.2"></sum><apply id="S3.E4.m1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.2.2.3"><eq id="S3.E4.m1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E4.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3">log</ci><ci id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4">𝑝</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.4.2">𝑦</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2">𝑦</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.2">𝜃</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.2.3">𝑠</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\max\limits_{\theta_{s}}\ \sum_{i=1}^{N}\mathrm{log}\ p(y_{i}|y_{&lt;i};\theta_{s})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">roman_max start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.2">For the training data, we mix corpus from the e-commerce domain and the general domain to avoid catastrophic forgetting following <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib55" title="">2023</a>); Ke et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib20" title="">2023</a>); Que et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib37" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.7" style="width:577.7pt;height:122.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.0pt,10.8pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.7.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.7.7.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.2">Document Source</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.3"># Categories</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.4"># Document</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.5">Avg words per Doc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.7.8.1.6">Release</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.2">AmazonQA <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib17" title="">2019b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.3">PR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.4">17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.5">923K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.6">63.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mi id="S3.T1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T1.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2">SubjQA <cite class="ltx_cite ltx_citemacro_cite">Bjerva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib3" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.3">PR</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.4">6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.5">10,098</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.6">289.87</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T1.2.2.2.1.m1.1"><semantics id="S3.T1.2.2.2.1.m1.1a"><mi id="S3.T1.2.2.2.1.m1.1.1" mathvariant="normal" xref="S3.T1.2.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.1.m1.1b"><ci id="S3.T1.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.2">semiPQA <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib42" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3">PI</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.4">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.5">11,243</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.6">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.1"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.1.m1.1a"><mo id="S3.T1.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.m1.1b"><times id="S3.T1.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.1.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.2">xPQA <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib41" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.3">PR</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.5">2,500</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.6">76.87</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.1.m1.1a"><mi id="S3.T1.4.4.4.1.m1.1.1" mathvariant="normal" xref="S3.T1.4.4.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><ci id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.2">JD <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib14" title="">2019b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.3">PR &amp; PI</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.4">38</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5">469,955</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.6">16.94</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T1.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.1.m1.1a"><mi id="S3.T1.5.5.5.1.m1.1.1" mathvariant="normal" xref="S3.T1.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.1b"><ci id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.2">Taobao <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib8" title="">2019b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.3">PR</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.4">2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.5">1,155,530</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.6">74.15</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.1"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.6.6.6.1.m1.1"><semantics id="S3.T1.6.6.6.1.m1.1a"><mo id="S3.T1.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.1.m1.1b"><times id="S3.T1.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.1.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.2">WorthyBuying (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.3">PA</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.4">25 &amp; 128</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.5">735,937</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.6"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.7.6.1">1171.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.7.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T1.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.1.m1.1a"><mi id="S3.T1.7.7.7.1.m1.1.1" mathvariant="normal" xref="S3.T1.7.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><ci id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Product Question Answering (PQA) datasets.
The document types are classified into Product Reviews (PR), Product Information (PI), and <span class="ltx_text ltx_font_bold" id="S3.T1.9.1">Product Analysis from professional users (PA) </span>.
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Stage 2: Training Retriever with Hard Negative Mining.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.3">We then minimize <math alttext="\mathcal{L}_{r}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{L}_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> by fine-tuning a domain-specific retrieval model based on a frozen base LLM <math alttext="\theta_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">θ</mi><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝜃</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">\theta_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.1d">italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> with trainable LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib18" title="">2021</a>)</cite> module <math alttext="\theta_{r_{s}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">θ</mi><msub id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">r</mi><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">𝜃</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2">𝑟</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\theta_{r_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">italic_θ start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>.
As our retrieval model adopts a GPT-like architecture, we adhere to the RepLLaMA paradigm <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib30" title="">2023b</a>)</cite> to derive sentence embeddings from the End of Sentence (EOS) token.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">Following the approach of the bi-encoder dense retriever DPR <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib19" title="">2020</a>)</cite>, we employ in-batch contrastive learning to fine-tune Ecom-base with InfoNCE loss:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{r}=-\log\ \frac{e^{sim(q,D^{+})}}{e^{sim(q,D^{+})}+\sum\limits_{D%
_{i}^{-}\in{D^{-}}}e^{sim(q,D_{i}^{-})}}" class="ltx_Math" display="block" id="S3.E5.m1.6"><semantics id="S3.E5.m1.6a"><mrow id="S3.E5.m1.6.7" xref="S3.E5.m1.6.7.cmml"><msub id="S3.E5.m1.6.7.2" xref="S3.E5.m1.6.7.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.6.7.2.2" xref="S3.E5.m1.6.7.2.2.cmml">ℒ</mi><mi id="S3.E5.m1.6.7.2.3" xref="S3.E5.m1.6.7.2.3.cmml">r</mi></msub><mo id="S3.E5.m1.6.7.1" xref="S3.E5.m1.6.7.1.cmml">=</mo><mrow id="S3.E5.m1.6.7.3" xref="S3.E5.m1.6.7.3.cmml"><mo id="S3.E5.m1.6.7.3a" rspace="0.167em" xref="S3.E5.m1.6.7.3.cmml">−</mo><mrow id="S3.E5.m1.6.7.3.2" xref="S3.E5.m1.6.7.3.2.cmml"><mi id="S3.E5.m1.6.7.3.2.1" xref="S3.E5.m1.6.7.3.2.1.cmml">log</mi><mo id="S3.E5.m1.6.7.3.2a" lspace="0.667em" xref="S3.E5.m1.6.7.3.2.cmml">⁡</mo><mfrac id="S3.E5.m1.6.6" xref="S3.E5.m1.6.6.cmml"><msup id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml"><mi id="S3.E5.m1.2.2.2.4" xref="S3.E5.m1.2.2.2.4.cmml">e</mi><mrow id="S3.E5.m1.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.cmml"><mi id="S3.E5.m1.2.2.2.2.2.4" xref="S3.E5.m1.2.2.2.2.2.4.cmml">s</mi><mo id="S3.E5.m1.2.2.2.2.2.3" xref="S3.E5.m1.2.2.2.2.2.3.cmml">⁢</mo><mi id="S3.E5.m1.2.2.2.2.2.5" xref="S3.E5.m1.2.2.2.2.2.5.cmml">i</mi><mo id="S3.E5.m1.2.2.2.2.2.3a" xref="S3.E5.m1.2.2.2.2.2.3.cmml">⁢</mo><mi id="S3.E5.m1.2.2.2.2.2.6" xref="S3.E5.m1.2.2.2.2.2.6.cmml">m</mi><mo id="S3.E5.m1.2.2.2.2.2.3b" xref="S3.E5.m1.2.2.2.2.2.3.cmml">⁢</mo><mrow id="S3.E5.m1.2.2.2.2.2.2.1" xref="S3.E5.m1.2.2.2.2.2.2.2.cmml"><mo id="S3.E5.m1.2.2.2.2.2.2.1.2" stretchy="false" xref="S3.E5.m1.2.2.2.2.2.2.2.cmml">(</mo><mi id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">q</mi><mo id="S3.E5.m1.2.2.2.2.2.2.1.3" xref="S3.E5.m1.2.2.2.2.2.2.2.cmml">,</mo><msup id="S3.E5.m1.2.2.2.2.2.2.1.1" xref="S3.E5.m1.2.2.2.2.2.2.1.1.cmml"><mi id="S3.E5.m1.2.2.2.2.2.2.1.1.2" xref="S3.E5.m1.2.2.2.2.2.2.1.1.2.cmml">D</mi><mo id="S3.E5.m1.2.2.2.2.2.2.1.1.3" xref="S3.E5.m1.2.2.2.2.2.2.1.1.3.cmml">+</mo></msup><mo id="S3.E5.m1.2.2.2.2.2.2.1.4" stretchy="false" xref="S3.E5.m1.2.2.2.2.2.2.2.cmml">)</mo></mrow></mrow></msup><mrow id="S3.E5.m1.6.6.6" xref="S3.E5.m1.6.6.6.cmml"><msup id="S3.E5.m1.6.6.6.6" xref="S3.E5.m1.6.6.6.6.cmml"><mi id="S3.E5.m1.6.6.6.6.2" xref="S3.E5.m1.6.6.6.6.2.cmml">e</mi><mrow id="S3.E5.m1.4.4.4.2.2" xref="S3.E5.m1.4.4.4.2.2.cmml"><mi id="S3.E5.m1.4.4.4.2.2.4" xref="S3.E5.m1.4.4.4.2.2.4.cmml">s</mi><mo id="S3.E5.m1.4.4.4.2.2.3" xref="S3.E5.m1.4.4.4.2.2.3.cmml">⁢</mo><mi id="S3.E5.m1.4.4.4.2.2.5" xref="S3.E5.m1.4.4.4.2.2.5.cmml">i</mi><mo id="S3.E5.m1.4.4.4.2.2.3a" xref="S3.E5.m1.4.4.4.2.2.3.cmml">⁢</mo><mi id="S3.E5.m1.4.4.4.2.2.6" xref="S3.E5.m1.4.4.4.2.2.6.cmml">m</mi><mo id="S3.E5.m1.4.4.4.2.2.3b" xref="S3.E5.m1.4.4.4.2.2.3.cmml">⁢</mo><mrow id="S3.E5.m1.4.4.4.2.2.2.1" xref="S3.E5.m1.4.4.4.2.2.2.2.cmml"><mo id="S3.E5.m1.4.4.4.2.2.2.1.2" stretchy="false" xref="S3.E5.m1.4.4.4.2.2.2.2.cmml">(</mo><mi id="S3.E5.m1.3.3.3.1.1.1" xref="S3.E5.m1.3.3.3.1.1.1.cmml">q</mi><mo id="S3.E5.m1.4.4.4.2.2.2.1.3" xref="S3.E5.m1.4.4.4.2.2.2.2.cmml">,</mo><msup id="S3.E5.m1.4.4.4.2.2.2.1.1" xref="S3.E5.m1.4.4.4.2.2.2.1.1.cmml"><mi id="S3.E5.m1.4.4.4.2.2.2.1.1.2" xref="S3.E5.m1.4.4.4.2.2.2.1.1.2.cmml">D</mi><mo id="S3.E5.m1.4.4.4.2.2.2.1.1.3" xref="S3.E5.m1.4.4.4.2.2.2.1.1.3.cmml">+</mo></msup><mo id="S3.E5.m1.4.4.4.2.2.2.1.4" stretchy="false" xref="S3.E5.m1.4.4.4.2.2.2.2.cmml">)</mo></mrow></mrow></msup><mo id="S3.E5.m1.6.6.6.5" rspace="0.055em" xref="S3.E5.m1.6.6.6.5.cmml">+</mo><mrow id="S3.E5.m1.6.6.6.7" xref="S3.E5.m1.6.6.6.7.cmml"><munder id="S3.E5.m1.6.6.6.7.1" xref="S3.E5.m1.6.6.6.7.1.cmml"><mo id="S3.E5.m1.6.6.6.7.1.2" movablelimits="false" xref="S3.E5.m1.6.6.6.7.1.2.cmml">∑</mo><mrow id="S3.E5.m1.6.6.6.7.1.3" xref="S3.E5.m1.6.6.6.7.1.3.cmml"><msubsup id="S3.E5.m1.6.6.6.7.1.3.2" xref="S3.E5.m1.6.6.6.7.1.3.2.cmml"><mi id="S3.E5.m1.6.6.6.7.1.3.2.2.2" xref="S3.E5.m1.6.6.6.7.1.3.2.2.2.cmml">D</mi><mi id="S3.E5.m1.6.6.6.7.1.3.2.2.3" xref="S3.E5.m1.6.6.6.7.1.3.2.2.3.cmml">i</mi><mo id="S3.E5.m1.6.6.6.7.1.3.2.3" xref="S3.E5.m1.6.6.6.7.1.3.2.3.cmml">−</mo></msubsup><mo id="S3.E5.m1.6.6.6.7.1.3.1" xref="S3.E5.m1.6.6.6.7.1.3.1.cmml">∈</mo><msup id="S3.E5.m1.6.6.6.7.1.3.3" xref="S3.E5.m1.6.6.6.7.1.3.3.cmml"><mi id="S3.E5.m1.6.6.6.7.1.3.3.2" xref="S3.E5.m1.6.6.6.7.1.3.3.2.cmml">D</mi><mo id="S3.E5.m1.6.6.6.7.1.3.3.3" xref="S3.E5.m1.6.6.6.7.1.3.3.3.cmml">−</mo></msup></mrow></munder><msup id="S3.E5.m1.6.6.6.7.2" xref="S3.E5.m1.6.6.6.7.2.cmml"><mi id="S3.E5.m1.6.6.6.7.2.2" xref="S3.E5.m1.6.6.6.7.2.2.cmml">e</mi><mrow id="S3.E5.m1.6.6.6.4.2" xref="S3.E5.m1.6.6.6.4.2.cmml"><mi id="S3.E5.m1.6.6.6.4.2.4" xref="S3.E5.m1.6.6.6.4.2.4.cmml">s</mi><mo id="S3.E5.m1.6.6.6.4.2.3" xref="S3.E5.m1.6.6.6.4.2.3.cmml">⁢</mo><mi id="S3.E5.m1.6.6.6.4.2.5" xref="S3.E5.m1.6.6.6.4.2.5.cmml">i</mi><mo id="S3.E5.m1.6.6.6.4.2.3a" xref="S3.E5.m1.6.6.6.4.2.3.cmml">⁢</mo><mi id="S3.E5.m1.6.6.6.4.2.6" xref="S3.E5.m1.6.6.6.4.2.6.cmml">m</mi><mo id="S3.E5.m1.6.6.6.4.2.3b" xref="S3.E5.m1.6.6.6.4.2.3.cmml">⁢</mo><mrow id="S3.E5.m1.6.6.6.4.2.2.1" xref="S3.E5.m1.6.6.6.4.2.2.2.cmml"><mo id="S3.E5.m1.6.6.6.4.2.2.1.2" stretchy="false" xref="S3.E5.m1.6.6.6.4.2.2.2.cmml">(</mo><mi id="S3.E5.m1.5.5.5.3.1.1" xref="S3.E5.m1.5.5.5.3.1.1.cmml">q</mi><mo id="S3.E5.m1.6.6.6.4.2.2.1.3" xref="S3.E5.m1.6.6.6.4.2.2.2.cmml">,</mo><msubsup id="S3.E5.m1.6.6.6.4.2.2.1.1" xref="S3.E5.m1.6.6.6.4.2.2.1.1.cmml"><mi id="S3.E5.m1.6.6.6.4.2.2.1.1.2.2" xref="S3.E5.m1.6.6.6.4.2.2.1.1.2.2.cmml">D</mi><mi id="S3.E5.m1.6.6.6.4.2.2.1.1.2.3" xref="S3.E5.m1.6.6.6.4.2.2.1.1.2.3.cmml">i</mi><mo id="S3.E5.m1.6.6.6.4.2.2.1.1.3" xref="S3.E5.m1.6.6.6.4.2.2.1.1.3.cmml">−</mo></msubsup><mo id="S3.E5.m1.6.6.6.4.2.2.1.4" stretchy="false" xref="S3.E5.m1.6.6.6.4.2.2.2.cmml">)</mo></mrow></mrow></msup></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.6b"><apply id="S3.E5.m1.6.7.cmml" xref="S3.E5.m1.6.7"><eq id="S3.E5.m1.6.7.1.cmml" xref="S3.E5.m1.6.7.1"></eq><apply id="S3.E5.m1.6.7.2.cmml" xref="S3.E5.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E5.m1.6.7.2.1.cmml" xref="S3.E5.m1.6.7.2">subscript</csymbol><ci id="S3.E5.m1.6.7.2.2.cmml" xref="S3.E5.m1.6.7.2.2">ℒ</ci><ci id="S3.E5.m1.6.7.2.3.cmml" xref="S3.E5.m1.6.7.2.3">𝑟</ci></apply><apply id="S3.E5.m1.6.7.3.cmml" xref="S3.E5.m1.6.7.3"><minus id="S3.E5.m1.6.7.3.1.cmml" xref="S3.E5.m1.6.7.3"></minus><apply id="S3.E5.m1.6.7.3.2.cmml" xref="S3.E5.m1.6.7.3.2"><log id="S3.E5.m1.6.7.3.2.1.cmml" xref="S3.E5.m1.6.7.3.2.1"></log><apply id="S3.E5.m1.6.6.cmml" xref="S3.E5.m1.6.6"><divide id="S3.E5.m1.6.6.7.cmml" xref="S3.E5.m1.6.6"></divide><apply id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2">superscript</csymbol><ci id="S3.E5.m1.2.2.2.4.cmml" xref="S3.E5.m1.2.2.2.4">𝑒</ci><apply id="S3.E5.m1.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2"><times id="S3.E5.m1.2.2.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.2.2.3"></times><ci id="S3.E5.m1.2.2.2.2.2.4.cmml" xref="S3.E5.m1.2.2.2.2.2.4">𝑠</ci><ci id="S3.E5.m1.2.2.2.2.2.5.cmml" xref="S3.E5.m1.2.2.2.2.2.5">𝑖</ci><ci id="S3.E5.m1.2.2.2.2.2.6.cmml" xref="S3.E5.m1.2.2.2.2.2.6">𝑚</ci><interval closure="open" id="S3.E5.m1.2.2.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1"><ci id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">𝑞</ci><apply id="S3.E5.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.1">superscript</csymbol><ci id="S3.E5.m1.2.2.2.2.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.1.2">𝐷</ci><plus id="S3.E5.m1.2.2.2.2.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.1.3"></plus></apply></interval></apply></apply><apply id="S3.E5.m1.6.6.6.cmml" xref="S3.E5.m1.6.6.6"><plus id="S3.E5.m1.6.6.6.5.cmml" xref="S3.E5.m1.6.6.6.5"></plus><apply id="S3.E5.m1.6.6.6.6.cmml" xref="S3.E5.m1.6.6.6.6"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.6.1.cmml" xref="S3.E5.m1.6.6.6.6">superscript</csymbol><ci id="S3.E5.m1.6.6.6.6.2.cmml" xref="S3.E5.m1.6.6.6.6.2">𝑒</ci><apply id="S3.E5.m1.4.4.4.2.2.cmml" xref="S3.E5.m1.4.4.4.2.2"><times id="S3.E5.m1.4.4.4.2.2.3.cmml" xref="S3.E5.m1.4.4.4.2.2.3"></times><ci id="S3.E5.m1.4.4.4.2.2.4.cmml" xref="S3.E5.m1.4.4.4.2.2.4">𝑠</ci><ci id="S3.E5.m1.4.4.4.2.2.5.cmml" xref="S3.E5.m1.4.4.4.2.2.5">𝑖</ci><ci id="S3.E5.m1.4.4.4.2.2.6.cmml" xref="S3.E5.m1.4.4.4.2.2.6">𝑚</ci><interval closure="open" id="S3.E5.m1.4.4.4.2.2.2.2.cmml" xref="S3.E5.m1.4.4.4.2.2.2.1"><ci id="S3.E5.m1.3.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.3.1.1.1">𝑞</ci><apply id="S3.E5.m1.4.4.4.2.2.2.1.1.cmml" xref="S3.E5.m1.4.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.2.2.2.1.1.1.cmml" xref="S3.E5.m1.4.4.4.2.2.2.1.1">superscript</csymbol><ci id="S3.E5.m1.4.4.4.2.2.2.1.1.2.cmml" xref="S3.E5.m1.4.4.4.2.2.2.1.1.2">𝐷</ci><plus id="S3.E5.m1.4.4.4.2.2.2.1.1.3.cmml" xref="S3.E5.m1.4.4.4.2.2.2.1.1.3"></plus></apply></interval></apply></apply><apply id="S3.E5.m1.6.6.6.7.cmml" xref="S3.E5.m1.6.6.6.7"><apply id="S3.E5.m1.6.6.6.7.1.cmml" xref="S3.E5.m1.6.6.6.7.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.7.1.1.cmml" xref="S3.E5.m1.6.6.6.7.1">subscript</csymbol><sum id="S3.E5.m1.6.6.6.7.1.2.cmml" xref="S3.E5.m1.6.6.6.7.1.2"></sum><apply id="S3.E5.m1.6.6.6.7.1.3.cmml" xref="S3.E5.m1.6.6.6.7.1.3"><in id="S3.E5.m1.6.6.6.7.1.3.1.cmml" xref="S3.E5.m1.6.6.6.7.1.3.1"></in><apply id="S3.E5.m1.6.6.6.7.1.3.2.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.7.1.3.2.1.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2">superscript</csymbol><apply id="S3.E5.m1.6.6.6.7.1.3.2.2.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.7.1.3.2.2.1.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2">subscript</csymbol><ci id="S3.E5.m1.6.6.6.7.1.3.2.2.2.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2.2.2">𝐷</ci><ci id="S3.E5.m1.6.6.6.7.1.3.2.2.3.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2.2.3">𝑖</ci></apply><minus id="S3.E5.m1.6.6.6.7.1.3.2.3.cmml" xref="S3.E5.m1.6.6.6.7.1.3.2.3"></minus></apply><apply id="S3.E5.m1.6.6.6.7.1.3.3.cmml" xref="S3.E5.m1.6.6.6.7.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.7.1.3.3.1.cmml" xref="S3.E5.m1.6.6.6.7.1.3.3">superscript</csymbol><ci id="S3.E5.m1.6.6.6.7.1.3.3.2.cmml" xref="S3.E5.m1.6.6.6.7.1.3.3.2">𝐷</ci><minus id="S3.E5.m1.6.6.6.7.1.3.3.3.cmml" xref="S3.E5.m1.6.6.6.7.1.3.3.3"></minus></apply></apply></apply><apply id="S3.E5.m1.6.6.6.7.2.cmml" xref="S3.E5.m1.6.6.6.7.2"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.7.2.1.cmml" xref="S3.E5.m1.6.6.6.7.2">superscript</csymbol><ci id="S3.E5.m1.6.6.6.7.2.2.cmml" xref="S3.E5.m1.6.6.6.7.2.2">𝑒</ci><apply id="S3.E5.m1.6.6.6.4.2.cmml" xref="S3.E5.m1.6.6.6.4.2"><times id="S3.E5.m1.6.6.6.4.2.3.cmml" xref="S3.E5.m1.6.6.6.4.2.3"></times><ci id="S3.E5.m1.6.6.6.4.2.4.cmml" xref="S3.E5.m1.6.6.6.4.2.4">𝑠</ci><ci id="S3.E5.m1.6.6.6.4.2.5.cmml" xref="S3.E5.m1.6.6.6.4.2.5">𝑖</ci><ci id="S3.E5.m1.6.6.6.4.2.6.cmml" xref="S3.E5.m1.6.6.6.4.2.6">𝑚</ci><interval closure="open" id="S3.E5.m1.6.6.6.4.2.2.2.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1"><ci id="S3.E5.m1.5.5.5.3.1.1.cmml" xref="S3.E5.m1.5.5.5.3.1.1">𝑞</ci><apply id="S3.E5.m1.6.6.6.4.2.2.1.1.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.4.2.2.1.1.1.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1">superscript</csymbol><apply id="S3.E5.m1.6.6.6.4.2.2.1.1.2.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.4.2.2.1.1.2.1.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1">subscript</csymbol><ci id="S3.E5.m1.6.6.6.4.2.2.1.1.2.2.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1.2.2">𝐷</ci><ci id="S3.E5.m1.6.6.6.4.2.2.1.1.2.3.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1.2.3">𝑖</ci></apply><minus id="S3.E5.m1.6.6.6.4.2.2.1.1.3.cmml" xref="S3.E5.m1.6.6.6.4.2.2.1.1.3"></minus></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.6c">\mathcal{L}_{r}=-\log\ \frac{e^{sim(q,D^{+})}}{e^{sim(q,D^{+})}+\sum\limits_{D%
_{i}^{-}\in{D^{-}}}e^{sim(q,D_{i}^{-})}}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.6d">caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = - roman_log divide start_ARG italic_e start_POSTSUPERSCRIPT italic_s italic_i italic_m ( italic_q , italic_D start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_s italic_i italic_m ( italic_q , italic_D start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ∈ italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_s italic_i italic_m ( italic_q , italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p4">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p4.2">Here, <math alttext="D^{+}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p4.1.m1.1a"><msup id="S3.SS2.SSS0.Px2.p4.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1.2.cmml">D</mi><mo id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p4.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1.2">𝐷</ci><plus id="S3.SS2.SSS0.Px2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p4.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p4.1.m1.1c">D^{+}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p4.1.m1.1d">italic_D start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math> represents the relevant document, and <math alttext="{D^{-}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p4.2.m2.1a"><msup id="S3.SS2.SSS0.Px2.p4.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1.2.cmml">D</mi><mo id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p4.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1.2">𝐷</ci><minus id="S3.SS2.SSS0.Px2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p4.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p4.2.m2.1c">{D^{-}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p4.2.m2.1d">italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT</annotation></semantics></math> denotes a set of irrelevant documents to the query. We employ the in-batch negatives method, where one positive document and several negative documents are included in one sample.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p5">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p5.1">We also propose a strategy for identifying hard negatives during contrastive learning. Specifically, we utilize a baseline retrieval model to retrieve a preliminary set of potential negatives, then the high-ranking yet erroneously matched documents, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p5.1.1">i.e.</span>, hard negatives, which helps refine the model’s discrimination capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p6">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p6.1">For our hard-negative mining, we first employ a base retrieval model that is fine-tuned on random negative samples to rank documents. Then we utilize the top 30 but incorrect results as hard-negative samples. Experimental results show that this strategy enhances the effectiveness of retrieval.LoRA is applied to all layers of our LLMs. The targeted weights for LoRA adaptation consist of the query, key, value, and output weights <math alttext="(W_{q},W_{k},W_{v},W_{o})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p6.1.m1.4"><semantics id="S3.SS2.SSS0.Px2.p6.1.m1.4a"><mrow id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml"><mo id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.5" stretchy="false" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml">(</mo><msub id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.6" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.3.cmml">k</mi></msub><mo id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.7" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.2" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.3" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.3.cmml">v</mi></msub><mo id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.8" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.cmml"><mi id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.2" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.3" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.3.cmml">o</mi></msub><mo id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.9" stretchy="false" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p6.1.m1.4b"><vector id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.5.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4"><apply id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.2">𝑊</ci><ci id="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.1.1.1.1.3">𝑞</ci></apply><apply id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.2">𝑊</ci><ci id="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.2.2.2.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.2">𝑊</ci><ci id="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.3.3.3.3.3">𝑣</ci></apply><apply id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.1.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.2.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.2">𝑊</ci><ci id="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.3.cmml" xref="S3.SS2.SSS0.Px2.p6.1.m1.4.4.4.4.3">𝑜</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p6.1.m1.4c">(W_{q},W_{k},W_{v},W_{o})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p6.1.m1.4d">( italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT )</annotation></semantics></math> within the attention modules, as well as the weights in the multi-layer perceptron (MLP) components.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Stage 3: Training Generator with Retrieval Augmented Instruction Tuning.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.4">To ensure that the generator can effectively utilize retrieved information, we augment the instruction-response pairs by incorporating the retrieved articles, forming triples <math alttext="(q,d,a)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.1.m1.3"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.3a"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml"><mo id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2.1" stretchy="false" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">q</mi><mo id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.2.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.3.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.3.cmml">a</mi><mo id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2.4" stretchy="false" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.3b"><vector id="S3.SS2.SSS0.Px3.p1.1.m1.3.4.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.4.2"><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">𝑞</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.2.2">𝑑</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.3.3">𝑎</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.3c">(q,d,a)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.1.m1.3d">( italic_q , italic_d , italic_a )</annotation></semantics></math>, where <math alttext="q" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.2.m2.1d">italic_q</annotation></semantics></math> denotes question tokens, <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px3.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.3.m3.1d">italic_d</annotation></semantics></math> denotes the retrieved document, and <math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.4.m4.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.4.m4.1d">italic_a</annotation></semantics></math> denotes the answer.
Specifically,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{g}=-\sum_{i=1}^{N}\mathrm{log}\ p(a_{i}|a_{&lt;i},q,d,\theta)" class="ltx_Math" display="block" id="S3.E6.m1.4"><semantics id="S3.E6.m1.4a"><mrow id="S3.E6.m1.4.4" xref="S3.E6.m1.4.4.cmml"><msub id="S3.E6.m1.4.4.3" xref="S3.E6.m1.4.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.4.4.3.2" xref="S3.E6.m1.4.4.3.2.cmml">ℒ</mi><mi id="S3.E6.m1.4.4.3.3" xref="S3.E6.m1.4.4.3.3.cmml">g</mi></msub><mo id="S3.E6.m1.4.4.2" xref="S3.E6.m1.4.4.2.cmml">=</mo><mrow id="S3.E6.m1.4.4.1" xref="S3.E6.m1.4.4.1.cmml"><mo id="S3.E6.m1.4.4.1a" xref="S3.E6.m1.4.4.1.cmml">−</mo><mrow id="S3.E6.m1.4.4.1.1" xref="S3.E6.m1.4.4.1.1.cmml"><munderover id="S3.E6.m1.4.4.1.1.2" xref="S3.E6.m1.4.4.1.1.2.cmml"><mo id="S3.E6.m1.4.4.1.1.2.2.2" movablelimits="false" xref="S3.E6.m1.4.4.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E6.m1.4.4.1.1.2.2.3" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml"><mi id="S3.E6.m1.4.4.1.1.2.2.3.2" xref="S3.E6.m1.4.4.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E6.m1.4.4.1.1.2.2.3.1" xref="S3.E6.m1.4.4.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6.m1.4.4.1.1.2.2.3.3" xref="S3.E6.m1.4.4.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.4.4.1.1.2.3" xref="S3.E6.m1.4.4.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E6.m1.4.4.1.1.1" xref="S3.E6.m1.4.4.1.1.1.cmml"><mi id="S3.E6.m1.4.4.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.3.cmml">log</mi><mo id="S3.E6.m1.4.4.1.1.1.2" lspace="0.500em" xref="S3.E6.m1.4.4.1.1.1.2.cmml">⁢</mo><mi id="S3.E6.m1.4.4.1.1.1.4" xref="S3.E6.m1.4.4.1.1.1.4.cmml">p</mi><mo id="S3.E6.m1.4.4.1.1.1.2a" xref="S3.E6.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.4.4.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.4.4.1.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.4.4.1.1.1.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3.2.cmml">a</mi><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.3.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.E6.m1.4.4.1.1.1.1.1.1.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">q</mi><mo id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">d</mi><mo id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.4" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml">θ</mi></mrow></mrow><mo id="S3.E6.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.4b"><apply id="S3.E6.m1.4.4.cmml" xref="S3.E6.m1.4.4"><eq id="S3.E6.m1.4.4.2.cmml" xref="S3.E6.m1.4.4.2"></eq><apply id="S3.E6.m1.4.4.3.cmml" xref="S3.E6.m1.4.4.3"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.3.1.cmml" xref="S3.E6.m1.4.4.3">subscript</csymbol><ci id="S3.E6.m1.4.4.3.2.cmml" xref="S3.E6.m1.4.4.3.2">ℒ</ci><ci id="S3.E6.m1.4.4.3.3.cmml" xref="S3.E6.m1.4.4.3.3">𝑔</ci></apply><apply id="S3.E6.m1.4.4.1.cmml" xref="S3.E6.m1.4.4.1"><minus id="S3.E6.m1.4.4.1.2.cmml" xref="S3.E6.m1.4.4.1"></minus><apply id="S3.E6.m1.4.4.1.1.cmml" xref="S3.E6.m1.4.4.1.1"><apply id="S3.E6.m1.4.4.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2">superscript</csymbol><apply id="S3.E6.m1.4.4.1.1.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2">subscript</csymbol><sum id="S3.E6.m1.4.4.1.1.2.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2"></sum><apply id="S3.E6.m1.4.4.1.1.2.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.2.3"><eq id="S3.E6.m1.4.4.1.1.2.2.3.1.cmml" xref="S3.E6.m1.4.4.1.1.2.2.3.1"></eq><ci id="S3.E6.m1.4.4.1.1.2.2.3.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.3.2">𝑖</ci><cn id="S3.E6.m1.4.4.1.1.2.2.3.3.cmml" type="integer" xref="S3.E6.m1.4.4.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.4.4.1.1.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.3">𝑁</ci></apply><apply id="S3.E6.m1.4.4.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1"><times id="S3.E6.m1.4.4.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.2"></times><ci id="S3.E6.m1.4.4.1.1.1.3.cmml" xref="S3.E6.m1.4.4.1.1.1.3">log</ci><ci id="S3.E6.m1.4.4.1.1.1.4.cmml" xref="S3.E6.m1.4.4.1.1.1.4">𝑝</ci><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3.2">𝑎</ci><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S3.E6.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1"><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑞</ci><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝑑</ci><ci id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3">𝜃</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.4c">\mathcal{L}_{g}=-\sum_{i=1}^{N}\mathrm{log}\ p(a_{i}|a_{&lt;i},q,d,\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.4d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_p ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_a start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT , italic_q , italic_d , italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.5">Similar to the embedding model, we employ the LoRA module to fine-tune only a subset of additional parameters <math alttext="\theta_{g_{s}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.5.m1.1"><semantics id="S3.SS2.SSS0.Px3.p1.5.m1.1a"><msub id="S3.SS2.SSS0.Px3.p1.5.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.2.cmml">θ</mi><msub id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.2.cmml">g</mi><mi id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.5.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.2">𝜃</ci><apply id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.2">𝑔</ci><ci id="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m1.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.5.m1.1c">\theta_{g_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.5.m1.1d">italic_θ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> on top of foundational LLM.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inference</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">During inference, the retriever is first employed to obtain documents, which are subsequently given to the generator for processing (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S2.F2" title="Figure 2 ‣ Retrieval Augmented Generation for LLMs. ‣ 2 Related Work ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">2</span></a>).
Specifically, we first prepare all embeddings for the documents in the domain-specific knowledge base by using the BSharedRAG retriever offline. In the online stage, we only extract the embedding of the question and use it to retrieve the most relevant <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_N</annotation></semantics></math> documents (<math alttext="N=3" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><eq id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></eq><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑁</ci><cn id="S3.SS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">N=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_N = 3</annotation></semantics></math>). Finally, we use the BSharedRAG generator to obtain an answer to the question by taking the top documents as input.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>WorthBuying Dataset</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Existing e-commerce datasets are mainly constructed from user-generated product reviews, which are brief and noisy, resulting in limited assistance to LLMs in generating rich content <cite class="ltx_cite ltx_citemacro_cite">Wan and McAuley (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib48" title="">2016</a>); Gupta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib16" title="">2019a</a>)</cite>.
To solve this issue, we build a WorthyBuying dataset, which includes an accurate knowledge base and high-quality QDA tuples for the e-commerce domain.
</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data Collection.</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">To obtain a high-quality corpus for the e-commerce domain,
we collect
1.1M professional product reviews from an e-commerce website<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://www.smzdm.com/" title="">https://www.smzdm.com/</a></span></span></span>.
Beginning from the raw corpus, we conduct several steps of data cleaning to ensure the quality, such as filtering contents with dirty words<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" title="">https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</a>.</span></span></span>, removing any HTML tags and images, and deleting those with the length beneath a threshold.
We then adopt the Minihash method<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://github.com/ChenghaoMou/text-dedup" title="">https://github.com/ChenghaoMou/text-dedup</a></span></span></span> to eliminate duplicate entries, which finally results in 735k cleaned documents.
More details are presented in the Appendix.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">QDA Tuples Generation.</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Users often ask LLMs to obtain information about products in the e-commerce domain.
However, the questions and answers in existing datasets are often derived from brief user QA comments, which are neither verified nor grounded in any relevant documents.
The informative review documents we collected can provide sufficient clues to answer product-related questions.
To facilitate documents lacking high-quality QDA tuples aligned with user needs, we leverage GPT-4 to generate questions and answers by prompt with these documents. We collect 50K such <math alttext="\langle\text{question},\text{document},\text{answer}\rangle" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.3"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.3a"><mrow id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2.1" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml">⟨</mo><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1a.cmml">question</mtext><mo id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml">,</mo><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.2.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.2.2a.cmml">document</mtext><mo id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2.3" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml">,</mo><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.3.3" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.3a.cmml">answer</mtext><mo id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2.4" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.3b"><list id="S4.SS0.SSS0.Px2.p1.1.m1.3.4.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.4.2"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1a.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1"><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">question</mtext></ci><ci id="S4.SS0.SSS0.Px2.p1.1.m1.2.2a.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.2.2"><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.2.2">document</mtext></ci><ci id="S4.SS0.SSS0.Px2.p1.1.m1.3.3a.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.3"><mtext id="S4.SS0.SSS0.Px2.p1.1.m1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.3.3">answer</mtext></ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.3c">\langle\text{question},\text{document},\text{answer}\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.3d">⟨ question , document , answer ⟩</annotation></semantics></math> tuples grounded on documents.
This dataset can support the training of the embedding model and generation model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Dataset Statistics and Comparisons.</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">As indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S3.T1" title="Table 1 ‣ Stage 1: Backbone Continual Pre-training. ‣ 3.2 Training Strategies ‣ 3 Method ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a>, previous datasets consist of undetailed short documents.
Their questions and answers are directly collected from brief user QA comments without verification, leading to noise and even incorrect answers.
Our WorthBuying dataset contains much longer documents with more than 50k QDA tuples grounded on 735k informative documents.
We also contain the most detailed product categories than existing datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiment</h2>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:556.0pt;height:182.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-78.4pt,25.7pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T2.1.1.1.1.2">CPR-Ecom</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T2.1.1.1.1.3">WorthBuying</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.1">nDCG@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.2">Hit@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.3">nDCG@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.4">Hit@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.5">nDCG@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.6">Hit@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.7">nDCG@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.2.8">Hit@5</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.3.3.1">Multi-E5-large <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib51" title="">2024b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.2">42.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.3">49.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.4">45.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.5">56.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.6">34.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.7">39.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.8">37.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.3.3.9">45.20</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.4.4.1">M3E-large <cite class="ltx_cite ltx_citemacro_cite">Wang Yuxin (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib53" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.2">44.89</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.3">51.70</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.4">47.65</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.5">58.40</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.6">43.02</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.7">49.40</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.8">45.64</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.4.9">55.40</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.5.5.1">BGE-large-zh <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib54" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.2">51.25</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.3">58.60</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.4">54.44</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.5">66.30</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.6">47.01</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.7">53.50</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.8">49.86</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5.5.9">56.30</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.6.6.1">BGE-large-zh + HN</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.2.1">52.38</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.3.1">60.35</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.4.1">56.23</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.5.1">68.50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.6.1">48.46</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.7.1">55.70</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.8.1">50.83</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.6.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.1.1.6.6.9.1">58.30</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.7.7.1">BGE-large-zh + CPT + HN</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.2">47.15</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.3">58.10</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.4">50.28</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.5">61.20</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.6">45.38</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.7">51.20</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.8">46.36</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7.7.9">55.40</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.8.8.1">FullyShared-RAG</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.2">38.67</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.3">44.80</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.4">43.24</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.5">48.80</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.6">33.28</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.7">36.70</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.8">36.56</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8.8.9">41.20</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.9.9.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.1.1">BShared-RAG Retriever (ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.2.1">61.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.3.1">68.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.4.1">64.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.5.1">75.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.6.1">51.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.7.1">58.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.8"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.8.1">54.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.9.9.9"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.9.1">63.90</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.10.10">
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.1">(+17%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.2">(+13%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.3">(+14%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.4">(+10%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.5">(+7%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.6">(+5%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.7">(+6%)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.10.10.8">(+10%)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.11.11.1">      w/o CPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.2">56.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.3">63.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.4">58.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.5">67.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.6">49.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.7">54.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.8">51.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.11.11.9">58.30</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.12.12.1">      w/o HN</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.2">58.35</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.3">64.70</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.4">61.23</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.5">70.60</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.6">49.34</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.7">56.30</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.8">52.03</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.12.12.9">60.90</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.1.13.13.1">      w/o CPT &amp; HN</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.2">53.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.3">61.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.4">57.76</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.5">66.30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.6">47.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.7">53.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.8">49.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.13.13.9">57.20</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparing retrievers of different RAG frameworks. CPT denotes continual pre-training and HN denotes using hard negative samples.
Our BShared-RAG Retriever outperforms all baselines by a large margin. CPT fails to help the BGE adapt to the e-commerce domain and even hurts the performance. FullShared-RAG performs the worst, showing that sharing all parameters between retrieval and generation leads to severe performance degradation.
</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experiment Setup</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Datasets and Evaluation Metrics</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">To evaluate the retrieval effectiveness, we use an existing dataset <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.1">CPR-Ecom</span> <cite class="ltx_cite ltx_citemacro_cite">Long et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib28" title="">2022</a>)</cite>, which contains a corpus of 1,002,822 Chinese documents, 100k human annotated query and relevant document pairs for training and 1k for testing in an e-commerce domain, and our constructed <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.2">WorthBuying</span> dataset, which contains a corpus of 735k Chinese documents, 50k GPT-4 generated questions from 50k documents for training and 1k GPT-4 generated questions with 10.2k human-annotated documents with 1 or 2 relevance ratings for testing, as benchmarks. We adopt two retrieval metrics for evaluating our models: nDCG (Normalized Discounted Cumulative Gain) and Hit Rate.
nDCG@n measures the ranking quality of top n retrieved documents, considering accumulated gains from relevant documents while discounting their positions. Hit@n assesses the presence of relevant documents within the top n results.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">To evaluate generation quality, we use our built WorthBuying dataset. We randomly sample 500 QA-pairs from the test set and hire human annotators to verify and correct the questions and answers.
This manual correction ensures the accuracy of the results and provides a more reliable evaluation.
We employ a comprehensive set of widely used automatic metrics for evaluation: n-gram based BLEU-3 <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib34" title="">2002</a>)</cite> and ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib24" title="">2004</a>)</cite>, BERTScore <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib58" title="">2020</a>)</cite> which calculates semantic similarity of ground-truth and generated answers based on a BERT model.
Accuracy <cite class="ltx_cite ltx_citemacro_cite">Es et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib12" title="">2023</a>)</cite> is calculated by GPT-4 to evaluate question and answer pairs.
More details are in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A3" title="Appendix C Generation Evaluation Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">C</span></a>.
</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">BLEU-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">ROUGE-L</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">BERTScore</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.5.1">Accuracy (%)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.2.2.1.1">FullySharedRAG as retriever</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.3.1"><span class="ltx_text" id="S5.T3.1.3.3.1.1">FullySharedRAG</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.2">2.35</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.3">2.58</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.4">62.37</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.5">68.34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.4.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.4.4.1.1">SeperateRAG as retriever</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.4.4.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.4.4.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.4.4.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.4.4.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.1"><span class="ltx_text" id="S5.T3.1.5.5.1.1">SeperateRAG (llama3 retriever)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.2">10.35</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.3">10.38</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.4">71.84</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.5">81.38</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.1"><span class="ltx_text" id="S5.T3.1.6.6.1.1">SeperateRAG (ecellm retriever)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.2">10.13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.3">11.23</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.4">71.98</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.5">82.41</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.7.7.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.7.7.1.1">BSharedRAG Retriever as retriever</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.1">GPT-3.5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.2"><span class="ltx_text" id="S5.T3.1.8.8.2.1" style="color:#808080;">16.07</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.3"><span class="ltx_text" id="S5.T3.1.8.8.3.1" style="color:#808080;">12.76</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.4"><span class="ltx_text" id="S5.T3.1.8.8.4.1" style="color:#808080;">74.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.5"><span class="ltx_text" id="S5.T3.1.8.8.5.1" style="color:#808080;">86.77</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.9.1">GPT-4 <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib1" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.2"><span class="ltx_text" id="S5.T3.1.9.9.2.1" style="color:#808080;">19.76</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.3"><span class="ltx_text" id="S5.T3.1.9.9.3.1" style="color:#808080;">15.92</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.4"><span class="ltx_text" id="S5.T3.1.9.9.4.1" style="color:#808080;">77.98</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.5"><span class="ltx_text" id="S5.T3.1.9.9.5.1" style="color:#808080;">89.86</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<td class="ltx_td ltx_align_left" id="S5.T3.1.10.10.1">EcomGPT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib23" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.2">6.91</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.3">8.12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.4">69.18</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.5">78.70</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<td class="ltx_td ltx_align_left" id="S5.T3.1.11.11.1">Baichuan2-7b-base</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.11.11.2">2.32</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.11.11.3">2.49</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.11.11.4">62.56</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.11.11.5">76.81</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.12">
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.1">Baichuan2-7b-base + CPT</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.12.12.2">2.46</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.12.12.3">3.24</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.12.12.4">62.58</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.12.12.5">80.85</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.13">
<td class="ltx_td ltx_align_left" id="S5.T3.1.13.13.1">Baichuan2-7b-base + RAG-IT</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.13.13.2">9.24</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.13.13.3">9.83</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.13.13.4">70.07</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.13.13.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.1.13.13.5.1">82.91</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.14.14">
<td class="ltx_td ltx_align_left" id="S5.T3.1.14.14.1">Baichuan2-7b-chat <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.14.14.2">8.82</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.14.14.3">11.21</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.14.14.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.1.14.14.4.1">72.45</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.14.14.5">79.84</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.15.15">
<td class="ltx_td ltx_align_left" id="S5.T3.1.15.15.1">Baichuan2-7b-chat + RAG-IT</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.15.15.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.1.15.15.2.1">10.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.15.15.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.1.15.15.3.1">12.34</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.15.15.4">72.12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.15.15.5">82.33</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.16.16">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.16.16.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.16.16.1.1">BSharedRAG Generator (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.16.16.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.16.16.2.1">12.63 (+23%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.16.16.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.16.16.3.1">12.85 (+4%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.16.16.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.16.16.4.1">74.75 (+3%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.16.16.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.16.16.5.1">84.26 (+2%)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation of generation results based on different retrievers on the WorthBuying-PQA test set. RAG-IT denotes retrieval augmented instruction tuning. The FullySharedRAG method performs worse because the generation objective may conflict with the retrieval objective. Compared with Baichuan2-7b series of baselines, our model achieves the best performance, demonstrating both CPT and RAG-IT contribute to the final performance.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Baselines</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">We compare our BSharedRAG Retriever with several widely-used baseline retrieval models:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.1">BGE</span> <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib54" title="">2023</a>)</cite> is the state-of-the-art BERT-like model trained with multi-stage contrastive learning using over 200M paired data.
To ensure a fair comparison, we fine-tune BGE through continual pre-training and in-domain contrastive learning with hard negative samples, resulting in two variants.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p3.1.1">M3E</span> <cite class="ltx_cite ltx_citemacro_cite">Wang Yuxin (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib53" title="">2023</a>)</cite> is an open-source text embedding model trained by a massive corpus with over 22M sentence pairs, supporting bilingual (Chinese-English) text retrieval.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p4.1.1">Multilingual-E5</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib51" title="">2024b</a>)</cite> transfers E5 embedding model (English only) <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib49" title="">2022</a>)</cite> to 100+ languages with a two-stage training and exhibits competitive performance across a broad range of languages.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p5">
<p class="ltx_p" id="S5.SS1.SSS2.p5.1">For generation, we choose the following representative LLMs from both open domains and e-commerce domain as baselines for comparison:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p6">
<p class="ltx_p" id="S5.SS1.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p6.1.1">GPT-3.5</span> and <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p6.1.2">GPT-4</span> <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib1" title="">2023</a>)</cite> are developed by OpenAI which are the most advanced LLMs. We feed them with retrieved passages in the prompt for a fair comparison.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p7">
<p class="ltx_p" id="S5.SS1.SSS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p7.1.1">EcomGPT</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib23" title="">2024</a>)</cite> is an e-commmerce domain-specific LLM trained on BLOOMZ-7B <cite class="ltx_cite ltx_citemacro_cite">Muennighoff et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib33" title="">2022</a>)</cite> with their close-source instruction dataset EcomInstruct.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p8">
<p class="ltx_p" id="S5.SS1.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p8.1.1">Baichuan2-7B-Chat</span> <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>)</cite> is one of the best open-source Chinese LLMs. It adopts 100k samples for supervised fine-tuning, after which is optimized using Reinforcement Learning from Human Feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">Ziegler et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib65" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p9">
<p class="ltx_p" id="S5.SS1.SSS2.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p9.1.1">Seperate RAG</span> has separate parts for retrieval and generation tasks, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a>(a). To make sure the same parameters as BSharedRAG in two stages, we train Llama3-8b <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib11" title="">2024</a>)</cite>, a powerful foundation LLM and eCeLLM-M(7B) <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib35" title="">2024</a>)</cite>, a powerful e-commerce domain LLM with the same data, and configuration as BSharedRAG-retriever in retrieval stage. For the generation, we use the BSharedRAG-generator.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p10">
<p class="ltx_p" id="S5.SS1.SSS2.p10.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p10.1.1">Fully Shared RAG</span> integrates retrieval and generation into a single model, as shown <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">1</span></a>(b).
Using the same base model, data, and configuration as BSharedRAG, it employs a single LoRA to fully share retrieval and generation parameters.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Results</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Retrieval Evaluation</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.T2" title="Table 2 ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">2</span></a> indicate that our <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.1">BShared-RAG Retriever</span> significantly outperforms all baselines in all metrics upon both datasets.
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.2">BGE-large-zh+HN</span>, which is fine-tuned on our dataset using hard negative contrastive learning, serves as the strongest baseline. However, our model still achieves improvements ranging from 5% to 17% over this baseline.
The <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.3">FullyShared-RAG</span> performs much worse than ours and most baselines in separate RAGs. This indicates that it is difficult to optimize two objectives with fully shared parameters. Compared to BERT-like retrievers, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.4">e.g.</span>, BGE-large-zh, our proposed method can effectively benefit from continual pre-training. In contrast, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p1.1.5">BGE-large-zh+CPT+HN</span> has significant drops in all metrics when applying continual pre-training.
In our analysis, there is evidence indicating that embedding models based on architectures like BGE require more fine-tuning training data after CPT, potentially reaching the terabyte (TB) level <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib50" title="">2024a</a>)</cite>. Fine-tuning with the same amount of data as BSharedRAG is insufficient for the BGE model, which could lead to a decline in performance.
We conduct ablation studies to analyze the detailed contributions of each module.
Both CPT and HN contribute to significant and consistent improvements, while CPT brings more gains.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Generation Evaluation</h4>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="S5.F3.g1" src="x3.png" width="823"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Evaluating the influence of different retrievers to generation effectiveness. CPT is continual pre-training, which benefits retrieval effectiveness a lot via sharing an LLM backbone. Accuracy is judged by GPT-4. Other metrics are not shown due to limited space, but we observe similar trends.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.T3" title="Table 3 ‣ 5.1.1 Datasets and Evaluation Metrics ‣ 5.1 Experiment Setup ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">3</span></a> show that our model performs the best among open-source models. In terms of ROUGE-L and BertScore, our model is even slightly better than GPT-3.5.
FullyShared-RAG performs the worst because it cannot well balance the retrieval and generation tasks.
Baichuan2-7b-base has no instruction following abilities and thus it performs lowest in QA testing. After RAG-IT, it can obtain instruction following abilities and thus its performance is significantly improved, <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">e.g.</span>, from 76.81% to 82.91% in Accuracy. A strong chat model, such as Baichuan2-7b-chat, can be also improved by RAG-IT because it brings domain-specific knowledge.
Our proposed BShared-RAG generator performs better than <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.2">Baichuan-7b-base + RAG-IT</span> by 37% in BLEU-3 and 31% in ROUGE-L.
These results demonstrate CPT and RAG-IT are both necessary and effective to improve generators.
Meanwhile, the Separate-RAG model, which employs Llama3 and EcellM as retrievers, maintains the same parameters as BSharedRAG but exhibits lower performance. This result underscores the importance of sharing a unified backbone, confirming its necessity for enhanced performance.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="494" id="S5.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A representative example to compare our BSharedRAG with a separate RAG. For the given question, our BSharedRAG Retriever favors the documents, in which some sentences are easy to be generated from the prompt of question. In contrast, the BERT-like BGE-large-zh model tends to retrieve some documents, in which some sentences match the question well. However, such document may be less suitable for generating answers due to some issues, <span class="ltx_text ltx_font_italic" id="S5.F4.2.1">e.g.</span>, important information missing or not easy to be used by generators. </figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:222.5pt;height:85.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.9pt,2.3pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.2" style="padding:2.5pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">Retrieval Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1" style="padding:2.5pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.1.m1.1a"><mi id="S5.T4.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.m1.1d">italic_τ</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3" style="padding:2.5pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Correlation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.2.1.1" style="padding:2.5pt 5.0pt;">BGE-large-zh</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.2" style="padding:2.5pt 5.0pt;">0.0498</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.3" style="padding:2.5pt 5.0pt;">0.045</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.3.2.1" style="padding:2.5pt 5.0pt;">Llama3-retriever</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.2" style="padding:2.5pt 5.0pt;">0.0937</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.3" style="padding:2.5pt 5.0pt;">0.085</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.4.3.1" style="padding:2.5pt 5.0pt;">Ecellm-retriever</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.2" style="padding:2.5pt 5.0pt;">0.1032</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.3" style="padding:2.5pt 5.0pt;">0.119</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.1.5.4.1" style="padding:2.5pt 5.0pt;">BSharedRAG Retriever</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.5.4.2" style="padding:2.5pt 5.0pt;">0.1433</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.5.4.3" style="padding:2.5pt 5.0pt;">0.155</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Measuring whether a retriever favors a sentence, which is easy to generate from a prompt of a question. We calculate Kendall’s tau and Pearson’s correlation between two ranked lists, one by a retrieval model and the other by the generation probability estimated by BSharedRAG Generator. The larger the better.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Influence of Retrieval to Generation</h4>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">We further conduct some experiments to analyze how retrievers influence generation. We compare the accuracy of generated results by models using BGE-large-zh, our BShared-RAG retriever and BShared-RAG w/o CPT. Results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.F3" title="Figure 3 ‣ 5.2.2 Generation Evaluation ‣ 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">3</span></a> indicate that our BShared-RAG Retriever can significantly improve generation accuracy no matter using GPT-3.5/GPT-4 or open-source models. Without CPT, the performance has a large drop. This underscores how important it is to bring continual pre-training into retrieval models.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Analysis of BSharedRAG Performance Gain</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Reasons for BSharedRAG’s outstanding performance can be attributed to following three factors:</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">BSharedRAG provides a way to scale up the retriever parameters appropriately</span>. Conventional BERT-based embedding models <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib10" title="">2019</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib49" title="">2022</a>); Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib54" title="">2023</a>)</cite> face inherent architectural limitations that hinder their ability to scale up parameters, thus preventing them from fully leveraging the benefits of scaling laws. While some approaches have employed LLM as retrievers, these methods often consume excessive memory. BSharedRAG effectively addresses these challenges by sharing a common backbone between the retriever and generator, allowing the retriever to benefit from larger parameter sizes without increasing the overall computational overhead.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">BSharedRAG enables both the retriever and generator to benefit from continued pre-training</span>. In SeparateRAG, the retriever and generator are trained independently, meaning the benefits of continued pre-training cannot be shared between them. On the other hand, in the FullySharedRAG structure, the retrieval loss and generation loss can interfere with each other, leading to degraded performance. In contrast, the BSharedRAG framework requires only continued pre-training (CPT) on the backbone LLM, allowing both the retriever LoRA and generator LoRA to benefit from domain-specific performance improvements achieved through CPT.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">BSharedRAG facilitates better preference alignment between the retriever and generator</span>. To verify this, we conducted an experiment. For each question, we gathered the ground-truth document title and top non-ground-truth titles returned by two methods. We then concatenated each question with a document title and calculated the generation probability using our BSharedRAG Generator. Next, we computed Kendall’s tau and Pearson correlation between ranked lists from either BGE-large-zh or our retriever, and from the generator. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.T4" title="Table 4 ‣ 5.2.2 Generation Evaluation ‣ 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">4</span></a>, our method achieves significantly higher correlations, indicating that the BSharedRAG Retriever favors texts with higher generation probabilities, helping it outperform separate RAG baselines.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.F4" title="Figure 4 ‣ 5.2.2 Generation Evaluation ‣ 5.2 Experimental Results ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">4</span></a>, BGE-large-zh prioritizes titles closely matching the question, whereas our BShared-RAG Retriever ranks titles that better answer the question.
For example, our method ranks “Equipped with a new 33 million pixel sensor, Sony officially released the full-frame mirrorless A7M4” higher, which is like a more relevant answer to the question.
In contrast, BGE-large-zh ranks “Sony A7M4 after buying user experience” without “pixel” information higher, or another document the second, which is hard for the generator to capture due to its isolation in the middle <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib27" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:259.9pt;height:55.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.8pt,8.3pt) scale(0.77,0.77) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T5.1.1.1.1.1" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.2.1">Parameters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.3" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.3.1">RAM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.4.1">VRAM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.5" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.5.1">Storage</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.1.2.1.1" style="padding:2.5pt 2.0pt;">SeperateRAG (bge-zh-large)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.2" style="padding:2.5pt 2.0pt;">7.87B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.3" style="padding:2.5pt 2.0pt;">30.07GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.4" style="padding:2.5pt 2.0pt;">58.63GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.5" style="padding:2.5pt 2.0pt;">15.19GB</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.3.2.1" style="padding:2.5pt 2.0pt;">SeperateRAG (llama3-retriever)</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.2" style="padding:2.5pt 2.0pt;">15.84B</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.3" style="padding:2.5pt 2.0pt;">61.19GB</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.4" style="padding:2.5pt 2.0pt;">120.38GB</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.5" style="padding:2.5pt 2.0pt;">29.05GB</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.1.4.3.1" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.4.3.1.1">BSharedRAG</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.4.3.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.4.3.2.1">7.54B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.4.3.3" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.4.3.3.1">29.02GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.4.3.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.4.3.4.1">56.61GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.4.3.5" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.4.3.5.1">14.09 GB</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparing the space costs of different RAG frameworks, SeperateRAG (bge-zh-large) and SeperateRAG (llama3-retriever) respectively utilize the bge-zh-large and finetuned llama3 models, as retrievers, while sharing the same generator as BSharedRAG.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Inference space cost and time</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We evaluate the inference space cost and time of our BSharedRAG framework.
As demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.T5" title="Table 5 ‣ 5.3 Analysis of BSharedRAG Performance Gain ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">5</span></a>, BSharedRAG has a lower space cost than SeperateRAG due to eliminating the need for an additional retrieval component.
In the retrieval part, BSharedRAG integrates a shared backbone model, which results in a marginal increase of retriever parameters compared to the BERT-based model. However, this does not lead to a significantly longer inference time. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#S5.T6" title="Table 6 ‣ 5.4 Inference space cost and time ‣ 5 Experiment ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">6</span></a>, the overall time increase is just 0.34 seconds, which remains within an acceptable range.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T6.1" style="width:253.4pt;height:51.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,1.4pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T6.1.1.1.1.1" style="padding:2.5pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.1.1">Framework</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.1.1.2" style="padding:2.5pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.2.1">Average Inference Time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.1.2.1.1" style="padding:2.5pt 8.0pt;">SeperateRAG (bge-zh-large)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.2.1.2" style="padding:2.5pt 8.0pt;">0.65s</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.1.1.3.2.1" style="padding:2.5pt 8.0pt;">BSharedRAG</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.3.2.2" style="padding:2.5pt 8.0pt;">0.91s</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>
We calculate the average inference time over 1000 questions randomly sampled from WorthyBuying Dataset. Both BSharedRAG retriever and generator are accelerated by vllm <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib21" title="">2023</a>)</cite>
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.T6.2">.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we construct a high-quality e-commerce dataset called WorthBuying, which contains an e-commerce knowledge base and QA pairs for training and evaluating retrieval and generation effectiveness.
We also design BSharedRAG, which can effectively adapt RAG models to a specific domain. In our framework, we propose sharing the backbone model that benefits from domain-specific continual pre-training and using hard negative mining and RAG instruction tuning to optimize two Low-rank Adaptation (LoRA) modules in the retriever and generator respectively. Compared to the fully shared RAG framework, our method can avoid the issue of negative transfer between tasks and the difficulty of loss balancing. Experimental results demonstrate that compared to existing separate RAG frameworks, both the retrieval performance and the generation performance have been significantly improved.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work is supported by the National Key R&amp;D Program of China (2023YFF0905402) and ZHI-TECH GROUP. We would like to thank <a class="ltx_ref ltx_href" href="https://www.zhidemai.com/" title="">Beijing Zhidemai Technology Co., Ltd.</a> for supporting their crucial technical and administrative support, along with the resources that greatly contributed to model development.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The limitations of this work are as follows: (1) Due to the limited budget, we currently use only Chinese models and datasets in e-commerce domain in our experiments. As the proposed method is language and domain independent, it shall be applicable to more. Thus we plan to extend our methods to more languages and domains. (2) We implement a fully shared RAG framework based on one LoRA in this work. We are interested to explore more possibilities to compare two frameworks.
(3) Our approach focuses on a basic retrieval and generation architecture. In the future, we plan to explore more advanced methodologies, such as after-retrieval Chain-of-Thought, re-ranking, and query rewriting, to enhance RAG’s capability and robustness in the e-commerce domain.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Ethical Statement</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Our work aims to adapt a general LLM to the E-commerce domain, but the models we train may have negative impacts. For example, they could be used inappropriately, although we have performed data cleaning to avoid offensive content. However,
this is a common issue currently faced in the LLM field, and it is not amplified by this work. In the future, we will consider more work on the safety of LLMs to optimize their security in the E-commerce domain.
To protect the intellectual property rights of the data, we will strictly limit the dissemination of this dataset to academic research purposes only.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:268232499" title="">The claude 3 model family: Opus, sonnet, haiku</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bjerva et al. (2020)</span>
<span class="ltx_bibblock">
Johannes Bjerva, Nikita Bhutani, Behzad Golshan, Wang-Chiew Tan, and Isabelle Augenstein. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.442" title="">Subjqa: A dataset for subjectivity and review comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">EMNLP 2020</em>, pages 5480–5494.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International conference on machine learning</em>, pages 2206–2240. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Preprint</em>, arXiv:2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, et al. 2024.

</span>
<span class="ltx_bibblock">Towards effective and efficient continual pre-training of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2407.18743</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019a)</span>
<span class="ltx_bibblock">
Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3289600.3290971" title="">Review-driven answer generation for product-related questions in e-commerce</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>, WSDM ’19, page 411–419, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019b)</span>
<span class="ltx_bibblock">
Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3289600.3290971" title="">Review-driven answer generation for product-related questions in e-commerce</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">WSDM 2019</em>, pages 411–419.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2023)</span>
<span class="ltx_bibblock">
Yang Deng, Wenxuan Zhang, Qian Yu, and Wai Lam. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.667" title="">Product question answering in E-commerce: A survey</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 11951–11964, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/n19-1423" title="">BERT: pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">NAACL-HLT 2019</em>, pages 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2407.21783</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et al. (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.15217" title="">Ragas: Automated evaluation of retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Preprint</em>, arXiv:2309.15217.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2019a)</span>
<span class="ltx_bibblock">
Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, and Rui Yan. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3289600.3290992" title="">Product-aware answer generation in e-commerce question-answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>, WSDM ’19, page 429–437, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2019b)</span>
<span class="ltx_bibblock">
Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, and Rui Yan. 2019b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3289600.3290992" title="">Product-aware answer generation in e-commerce question-answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">WSDM 2019</em>, pages 429–437.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Preprint</em>, arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2019a)</span>
<span class="ltx_bibblock">
Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.24963/ijcai.2019/694" title="">Amazonqa: A review-based question answering task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</em>, pages 4996–5002. International Joint Conferences on Artificial Intelligence Organization.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2019b)</span>
<span class="ltx_bibblock">
Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. 2019b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.24963/ijcai.2019/694" title="">Amazonqa: A review-based question answering task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IJCAI 2019</em>, pages 4996–5002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.09685" title="">Lora: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Preprint</em>, arXiv:2106.09685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.550" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769–6781, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. (2023)</span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.03241" title="">Continual pre-training of language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Preprint</em>, arXiv:2302.03241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, pages 611–626.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">langchain (2023)</span>
<span class="ltx_bibblock">
langchain. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/langchain-ai/langchain" title="">langchain</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2024.

</span>
<span class="ltx_bibblock">Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 18582–18590.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Text Summarization Branches Out</em>, pages 74–81.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.01352" title="">Ra-dit: Retrieval-augmented dual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Preprint</em>, arXiv:2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2022)</span>
<span class="ltx_bibblock">
Jerry Liu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.1234" title="">LlamaIndex</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Transactions of the Association for Computational Linguistics</em>, 12:157–173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. (2022)</span>
<span class="ltx_bibblock">
Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, and Ping Yang. 2022.

</span>
<span class="ltx_bibblock">Multi-cpr: A multi domain chinese dataset for passage retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 3046–3056.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023a)</span>
<span class="ltx_bibblock">
Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.15696" title="">Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Preprint</em>, arXiv:2312.15696.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023b)</span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.08319" title="">Fine-tuning llama for multi-stage text retrieval</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:2310.08319.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al. (2020)</span>
<span class="ltx_bibblock">
John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.14444" title="">The effect of natural distribution shift on question answering models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Preprint</em>, arXiv:2004.14444.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2024)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.09906" title="">Generative representational instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Preprint</em>, arXiv:2402.09906.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2211.01786</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P02-1040" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ACL 2002</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2024)</span>
<span class="ltx_bibblock">
Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.08831" title="">ecellm: Generalizing large language models for e-commerce from large-scale, high-quality instruction data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Preprint</em>, arXiv:2402.08831.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2024)</span>
<span class="ltx_bibblock">
Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Zhenxiang Li, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang Xu, Wei Li, Zhongying Tu, Lin Dahua, Yu Qiao, Hang Yan, and Conghui He. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.19282" title="">Wanjuan-cc: A safe and high-quality open-sourced english webtext dataset</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Preprint</em>, arXiv:2402.19282.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Que et al. (2024)</span>
<span class="ltx_bibblock">
Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, and Bo Zheng. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2406.01375" title="">D-cpt law: Domain-specific continual pre-training scaling law for large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Preprint</em>, arXiv:2406.01375.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.00083" title="">In-context retrieval-augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Preprint</em>, arXiv:2302.00083.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023a)</span>
<span class="ltx_bibblock">
Xiaoyu Shen, Akari Asai, Bill Byrne, and Adrià de Gispert. 2023a.

</span>
<span class="ltx_bibblock">xpqa: Cross-lingual product question answering across 12 languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2305.09249</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023b)</span>
<span class="ltx_bibblock">
Xiaoyu Shen, Akari Asai, Bill Byrne, and Adria De Gispert. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-industry.12" title="">xPQA: Cross-lingual product question answering in 12 languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)</em>, pages 103–115, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2022)</span>
<span class="ltx_bibblock">
Xiaoyu Shen, Gianni Barlacchi, Marco Del Tredici, Weiwei Cheng, and Adrià Gispert. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.ecnlp-1.14" title="">semiPQA: A study on product question answering over semi-structured data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5)</em>, pages 111–120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023a)</span>
<span class="ltx_bibblock">
Kaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, and Qing Li. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.04913" title="">Llama-e: Empowering e-commerce authoring with multi-aspect instruction following</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Preprint</em>, arXiv:2308.04913.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023b)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2301.12652" title="">Replug: Retrieval-augmented black-box language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Preprint</em>, arXiv:2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023c)</span>
<span class="ltx_bibblock">
Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, and Chengkai Li. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.770" title="">Hallucination mitigation in natural language generation from large-scale open-domain knowledge graphs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 12506–12521, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Preprint</em>, arXiv:2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan and McAuley (2016)</span>
<span class="ltx_bibblock">
Mengting Wan and Julian J. McAuley. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICDM.2016.0060" title="">Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ICDM 2016</em>, pages 489–498.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2212.03533</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.00368" title="">Improving text embeddings with large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Preprint</em>, arXiv:2401.00368.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.05672" title="">Multilingual e5 text embeddings: A technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Preprint</em>, arXiv:2402.05672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, and Xuanjing Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.949" title="">Hallucination detection for generative large language models by Bayesian sequential estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 15361–15371, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang Yuxin (2023)</span>
<span class="ltx_bibblock">
He sicheng Wang Yuxin, Sun Qingxuan. 2023.

</span>
<span class="ltx_bibblock">Moka massive mixed embedding.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.07597" title="">C-pack: Packaged resources to advance general chinese embedding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Preprint</em>, arXiv:2309.07597.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.08545" title="">Efficient continual pre-training for building domain specific large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Preprint</em>, arXiv:2311.08545.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open large-scale language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2309.10305</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.10131" title="">Raft: Adapting language model to domain specific rag</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Preprint</em>, arXiv:2403.10131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">Bertscore: Evaluating text generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ICLR 2020</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao Cao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.18397" title="">Iag: Induction-augmented generation framework for answering reasoning questions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Preprint</em>, arXiv:2311.18397.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023a)</span>
<span class="ltx_bibblock">
Lingjun Zhao, Khanh Nguyen, and Hal Daumé III. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.266" title="">Hallucination detection for grounded instruction generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 4044–4053, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.19473" title="">Retrieval-augmented generation for ai-generated content: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Preprint</em>, arXiv:2402.19473.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023b)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.18223" title="">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Preprint</em>, arXiv:2303.18223.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.13372" title="">Llamafactory: Unified efficient fine-tuning of 100+ language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Preprint</em>, arXiv:2403.13372.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.07107" title="">Large language models for information retrieval: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Preprint</em>, arXiv:2308.07107.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al. (2020)</span>
<span class="ltx_bibblock">
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1909.08593" title="">Fine-tuning language models from human preferences</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Preprint</em>, arXiv:1909.08593.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of WorthBuying Dataset</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Examples</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We show two examples of our constructed WorthBuying Dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<svg class="ltx_picture" height="668.14" id="A1.SS1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,668.14) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 662.23 C 0 665.49 2.64 668.14 5.91 668.14 L 594.09 668.14 C 597.36 668.14 600 665.49 600 662.23 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 662.23 C 1.97 664.4 3.73 666.17 5.91 666.17 L 594.09 666.17 C 596.27 666.17 598.03 664.4 598.03 662.23 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="640.58" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS1.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.1.1">title</span>: Upgraded from Xiaomi’s first robot vacuum to the Roborock G10S—here’s my honest take.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.2.1">contents</span>:</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.3">In today’s digital landscape, authentic product reviews are a rarity amidst the overwhelming amount of marketing content. Motivated by this, I’ve decided to share my genuine experience with the original Xiaomi and Roborock G10s vacuum robots to help potential buyers get a clearer picture.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.4">Roborock is praised online for its advanced algorithms and reliable quality. Conversely, Ecovacs is often criticized for poor algorithms and quality, despite being the top offline seller. Xiaomi and other similar brands are noted for their cost-effectiveness but lack distinctive features. I purchased the first-generation Xiaomi robot in 2017 for ¥1699, and it’s still running well without any major part replacements except for the filter. The Roborock G10s, bought during a sale in 2023 for ¥3999 without any freebies, was intended for my 101m² apartment with multiple rooms and balconies.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.5">Mapping issues occurred with the Roborock during the first two attempts, taking over 17 minutes each, but it succeeded on the third try in 11 minutes. Both robots struggle with certain thresholds and may require virtual barriers if doors are not closed. Cleaning efficiency seems stagnant over the years. The Roborock did not significantly outperform Xiaomi despite newer technology, spending 65 minutes to clean a slightly smaller area than Xiaomi’s 60 minutes for a similar space. Obstacle avoidance is one area where Roborock excels, identifying and navigating around small objects like cables and tissues, which Xiaomi tends to run over.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.6">The mopping function of the Roborock was adequate, though it left water streaks, and the overall cleaning process was lengthy, especially when using separate sweeping and mopping modes. The Roborock app is user-friendly, offering customizable cleaning modes and map management, although it occasionally fails to retrieve maps.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.7">In summary, while Roborock recognizes more obstacles and has a better mopping system, the difference in everyday use is minimal. Both require preliminary floor clearing, and neither has a significant edge in cleaning area or speed. For those with limited budgets prioritizing cleaning effectiveness, other models like the T7s Plus might be a viable, more affordable alternative.</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.8.1">type_one</span>: Home appliances</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.9.1">type_two</span>: Home devices</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.10.1">cnt_details</span>:</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.11">{ quality_score:2, share_count: 3, like_count: 8,, collect_count: 10, comment_count: 9 }</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.12.1">user_question</span>: What are the differences between my original Xiaomi robot vacuum and the Roborock G10S in terms of usage?</span>
<span class="ltx_p" id="A1.SS1.p2.pic1.1.1.1.1.1.13"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.pic1.1.1.1.1.1.13.1">answer</span>: The Xiaomi robot primarily focuses on sweeping and has less advanced obstacle detection, often running over small objects. In contrast, the Roborock G10S offers better obstacle avoidance, customizable cleaning through its app, and combines sweeping with more effective mopping, though it initially struggles with accurate mapping and leaves more water residue.</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p3">
<svg class="ltx_picture" height="550.37" id="A1.SS1.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,550.37) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 544.46 C 0 547.72 2.64 550.37 5.91 550.37 L 594.09 550.37 C 597.36 550.37 600 547.72 600 544.46 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 544.46 C 1.97 546.64 3.73 548.4 5.91 548.4 L 594.09 548.4 C 596.27 548.4 598.03 546.64 598.03 544.46 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="522.81" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS1.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.1.1">title</span>: Upgrading my new PS5 with a 2TB Fantom S790 SSD due to the price drop!</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.2.1">contents</span>:</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.3">I recently got a PS5 Digital Edition and quickly realized the storage was a bit tight after installing a few games. I took the opportunity to sell my idle PS4 to get some cash back. Meanwhile, with SSD prices plummeting, I decided to go all out and bought the 2TB Fantom S790 solid-state drive.</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.4">The drive comes in traditional Chinese black packaging with a highly recognizable Peking opera mask design. The back of the package lists the specific parameters and product information, offering a five-year warranty. Inside the box, there’s the SSD itself, a small screwdriver, a screw, and a user manual.
The drive has a graphene cooling sticker. If the heat sink uses a thermal silicone sheet, you’ll need to remove the sheet before installation. The back of the drive also has a sticker with basic parameter info for easy reference.</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.5">Installation steps:</span>
<span class="ltx_enumerate" id="A1.I1">
<span class="ltx_item" id="A1.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A1.I1.i1.p1">
<span class="ltx_p" id="A1.I1.i1.p1.1">Before opening your console, lay something underneath to avoid scratches, then gently open the side of the case without the logo.</span>
</span></span>
<span class="ltx_item" id="A1.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A1.I1.i2.p1">
<span class="ltx_p" id="A1.I1.i2.p1.1">Use a Phillips screwdriver to open the SSD compartment at the bottom, which shows different mounting positions depending on the motherboard.</span>
</span></span>
<span class="ltx_item" id="A1.I1.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A1.I1.i3.p1">
<span class="ltx_p" id="A1.I1.i3.p1.1">The screw and mount are fixed at the 110 position. Loosen the screw there.</span>
</span></span>
<span class="ltx_item" id="A1.I1.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A1.I1.i4.p1">
<span class="ltx_p" id="A1.I1.i4.p1.1">This SSD is of the 2280 standard size, so set the mount at the 80 position.</span>
</span></span>
<span class="ltx_item" id="A1.I1.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="A1.I1.i5.p1">
<span class="ltx_p" id="A1.I1.i5.p1.1">Install the SSD, tighten the screw, reset the console to test it, then put the case back on.</span>
</span></span>
</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.6">Testing:</span>
<span class="ltx_enumerate" id="A1.I2">
<span class="ltx_item" id="A1.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A1.I2.i1.p1">
<span class="ltx_p" id="A1.I2.i1.p1.1">Power on, and the system directly detects the SSD and prompts for formatting.</span>
</span></span>
<span class="ltx_item" id="A1.I2.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A1.I2.i2.p1">
<span class="ltx_p" id="A1.I2.i2.p1.1">I’m not sure if the speed is an issue.</span>
</span></span>
<span class="ltx_item" id="A1.I2.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A1.I2.i3.p1">
<span class="ltx_p" id="A1.I2.i3.p1.1">After formatting, go to settings to select the default installation location for games.</span>
</span></span>
<span class="ltx_item" id="A1.I2.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A1.I2.i4.p1">
<span class="ltx_p" id="A1.I2.i4.p1.1">You can choose the default installation location.</span>
</span></span>
<span class="ltx_item" id="A1.I2.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="A1.I2.i5.p1">
<span class="ltx_p" id="A1.I2.i5.p1.1">It shows a total of 2TB capacity, which is accurate
<br class="ltx_break"/></span>
</span></span>
</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.7.1">type_one</span>: Computer and Technology</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.8.1">type_two</span>:Computer Accessories</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.9.1">cnt_details</span>:</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.10">{ quality_score:3, share_count: 4, like_count: 5,, collect_count: 27, comment_count: 15 }</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.11.1">user_question</span>: Can I expand the storage on my Sony PlayStation 5 with a Fantom S790 SSD?</span>
<span class="ltx_p" id="A1.SS1.p3.pic1.1.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.pic1.1.1.1.1.1.12.1">answer</span>: Yes, you can use the Fanxiang S790 solid-state drive to expand the storage space of the Sony PlayStation 5 console. It should be noted that you need to follow the official tutorial and make sure that the solid-state drive used meets the requirements and standard specifications of the PS5.</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>QA Generation with the help of GPT-4 API</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this study, we employ a methodology based on an assessment of a product review article to simulate user queries from the perspective of GPT-4. Utilizing the in-context learning approach <cite class="ltx_cite ltx_citemacro_cite">Ram et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib39" title="">2023</a>)</cite>, GPT-4 generates questions that mimic potential user concerns and queries. Subsequently, we apply a similar technique to enable the model to respond to these generated questions using information derived from the original article.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Question Generation Prompt</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS1.p1">
<svg class="ltx_picture" height="356.88" id="A1.SS2.SSS1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,356.88) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 350.97 C 0 354.24 2.64 356.88 5.91 356.88 L 594.09 356.88 C 597.36 356.88 600 354.24 600 350.97 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 350.97 C 1.97 353.15 3.73 354.91 5.91 354.91 L 594.09 354.91 C 596.27 354.91 598.03 353.15 598.03 350.97 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="329.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.1">Assume you are a customer purchasing goods on an e-commerce platform, and you have a question about a product. After reading the article, your question is well answered. What is your question?</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.2">Requirements: The question should be concise and end with a question mark "?"</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.3">[Example 1]</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.4.1">Article</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.5">Practical test of the Xiaomi 65w gallium nitride charger charging a Samsung Note10+: Update modification (2020-03-06 17:53:12): Conclusion, the Xiaomi 65w gallium nitride does not support the 45w charging of Samsung Note10+. Among the several PPS-supporting PD chargers tested, the highest can reach the same 25w as the original charger.Xiaomi 65w Gallium NitrideWithout further ado, let’s directly look at the pictures for yourself; the phone was in a screen-off state with about 35% battery level, using the original charger cable. For a few that did not have the original cable, I used the Lenovo lipstick cable.Baseus 65w Gallium Nitride</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.6">Green Union 30w PD</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.7">Anker 18w nano PD</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.8">Lenovo lipstick 65w PD</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.9">Set as main image</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.10">Xiaomi 65w PD (actually the same as the ZMI black 65w)</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.11">Samsung Note10+ original 25wPD</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.12.1">Your question is</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.13">Can I use the Xiaomi 65w gallium nitride charger to charge my Samsung Note10+?</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.14">[Example 2]</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.15.1">Article</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.16">{passage}</span>
<span class="ltx_p" id="A1.SS2.SSS1.p1.pic1.1.1.1.1.1.17">Your question is:</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Answer Generation Prompt</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS2.p1">
<svg class="ltx_picture" height="456.51" id="A1.SS2.SSS2.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,456.51) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 450.6 C 0 453.86 2.64 456.51 5.91 456.51 L 594.09 456.51 C 597.36 456.51 600 453.86 600 450.6 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 450.6 C 1.97 452.77 3.73 454.54 5.91 454.54 L 594.09 454.54 C 596.27 454.54 598.03 452.77 598.03 450.6 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="428.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.1">As a question-and-answer AI specializing in the e-commerce sector, I am equipped to answer user inquiries strictly based on the reference material provided. There are no scenarios in which I am unable to provide an answer, as all questions directly originate from the supplied references. Please rely on the information provided for accurate responses.</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.2">[Example 1]</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.3.1">Article</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.4">Practical test of the Xiaomi 65w gallium nitride charger charging a Samsung Note10+: Update modification (2020-03-06 17:53:12): Conclusion, the Xiaomi 65w gallium nitride does not support the 45w charging of Samsung Note10+. Among the several PPS-supporting PD chargers tested, the highest can reach the same 25w as the original charger.Xiaomi 65w Gallium NitrideWithout further ado, let’s directly look at the pictures for yourself; the phone was in a screen-off state with about 35% battery level, using the original charger cable. For a few that did not have the original cable, I used the Lenovo lipstick cable.
Baseus 65w Gallium Nitride</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.5">Green Union 30w PD</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.6">Anker 18w nano PD</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.7">Lenovo lipstick 65w PD</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.8">Set as main image</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.9">Xiaomi 65w PD (actually the same as the ZMI black 65w)Samsung Note10+ original 25wPD.</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.10.1">question </span>:</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.11">Can I use the Xiaomi 65w gallium nitride charger to charge my Samsung Note10+?</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.12.1">Answer:</span></span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.13">You can use the Xiaomi 65w Gallium Nitride charger to charge your Samsung Note10+ smartphone; however, it is important to note that this charger does not support the 45w fast charging capability of the Samsung Note10+. According to test results, when utilizing the Xiaomi 65w Gallium Nitride charger, the maximum charging power attained is 25w, which is equivalent to the charging performance of the original Samsung Note10+ 25w PD charger. Therefore, although it cannot achieve the maximum charging speed of 45w, it can still safely charge at a rate of 25w.</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.14">[Example 2]</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.15.1">Article</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.16">{passage}</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.17"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.17.1">question</span>:</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.18">{question}</span>
<span class="ltx_p" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.19"><span class="ltx_text ltx_font_bold" id="A1.SS2.SSS2.p1.pic1.1.1.1.1.1.19.1">Answer:</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="539" id="A1.F5.g1" src="extracted/5889235/figs/pqa_pie.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Partial categories of WorthBuying dataset</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Model Training Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Domain-specific Continual Pretraining</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">In this section, we describe the continual pretraining process for the Baichuan2-7B-Base model <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib56" title="">2023</a>)</cite> and the recipe of training data. Our models are trained on Llama-Factory <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib63" title="">2024</a>)</cite> framework.</p>
</div>
<figure class="ltx_table" id="A2.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T7.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">DataType</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T7.1.1.1.2" style="padding-left:3.5pt;padding-right:3.5pt;">Language/Source</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T7.1.1.1.3" style="padding-left:3.5pt;padding-right:3.5pt;">Weight</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T7.1.1.1.4" style="padding-left:3.5pt;padding-right:3.5pt;">Number of Files</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T7.1.1.1.5" style="padding-left:3.5pt;padding-right:3.5pt;">Size(GB)</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.2.2.1" rowspan="3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A2.T7.1.2.2.1.1">E-commerce Domain dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.2.2.2" style="padding-left:3.5pt;padding-right:3.5pt;">WorthyBuying Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.2.2.3" style="padding-left:3.5pt;padding-right:3.5pt;">2.53%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.2.2.4" style="padding-left:3.5pt;padding-right:3.5pt;">735,975</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.2.2.5" style="padding-left:3.5pt;padding-right:3.5pt;">2.43GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.3.3">
<td class="ltx_td ltx_align_center" id="A2.T7.1.3.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">Xhs-ecommerce-note</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.3.3.2" style="padding-left:3.5pt;padding-right:3.5pt;">4.96%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.3.3.3" style="padding-left:3.5pt;padding-right:3.5pt;">2,841,686</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.3.3.4" style="padding-left:3.5pt;padding-right:3.5pt;">4.5GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.4.4">
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">Zhihu-ecommerce-note</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.4.2" style="padding-left:3.5pt;padding-right:3.5pt;">7.91%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.4.3" style="padding-left:3.5pt;padding-right:3.5pt;">2,996,802</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.4.4" style="padding-left:3.5pt;padding-right:3.5pt;">7.59GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.1.5.5.1" rowspan="6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A2.T7.1.5.5.1.1">General dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.5.5.2" style="padding-left:3.5pt;padding-right:3.5pt;">ZH-Wiki</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.5.5.3" style="padding-left:3.5pt;padding-right:3.5pt;">2.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.5.5.4" style="padding-left:3.5pt;padding-right:3.5pt;">1,348,766</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.5.5.5" style="padding-left:3.5pt;padding-right:3.5pt;">2.49GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.6.6">
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.6.1" style="padding-left:3.5pt;padding-right:3.5pt;">Common-crawl-web</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.6.2" style="padding-left:3.5pt;padding-right:3.5pt;">20.29%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.6.3" style="padding-left:3.5pt;padding-right:3.5pt;">6,741,652</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.6.4" style="padding-left:3.5pt;padding-right:3.5pt;">19.44GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.7.7">
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.7.1" style="padding-left:3.5pt;padding-right:3.5pt;">Chinese-new</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.7.2" style="padding-left:3.5pt;padding-right:3.5pt;">16.14%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.7.3" style="padding-left:3.5pt;padding-right:3.5pt;">5,208,071</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.7.4" style="padding-left:3.5pt;padding-right:3.5pt;">15.46GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.8.8">
<td class="ltx_td ltx_align_center" id="A2.T7.1.8.8.1" style="padding-left:3.5pt;padding-right:3.5pt;">Chinese-Law</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.8.8.2" style="padding-left:3.5pt;padding-right:3.5pt;">28.21%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.8.8.3" style="padding-left:3.5pt;padding-right:3.5pt;">5,363,805</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.8.8.4" style="padding-left:3.5pt;padding-right:3.5pt;">27.02GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.9.9">
<td class="ltx_td ltx_align_center" id="A2.T7.1.9.9.1" style="padding-left:3.5pt;padding-right:3.5pt;">Chinese-Patents</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.9.9.2" style="padding-left:3.5pt;padding-right:3.5pt;">15.79%</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.9.9.3" style="padding-left:3.5pt;padding-right:3.5pt;">1,127,932</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.9.9.4" style="padding-left:3.5pt;padding-right:3.5pt;">15.13GB</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.10.10.1" style="padding-left:3.5pt;padding-right:3.5pt;">Chinese-CLS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.10.10.2" style="padding-left:3.5pt;padding-right:3.5pt;">1.78%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.10.10.3" style="padding-left:3.5pt;padding-right:3.5pt;">2,310,165</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.10.10.4" style="padding-left:3.5pt;padding-right:3.5pt;">1.71GB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Training data of Continual pretraining</figcaption>
</figure>
<section class="ltx_subsubsection" id="A2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.1.1 </span>Training Dataset</h4>
<div class="ltx_para" id="A2.SS1.SSS1.p1">
<p class="ltx_p" id="A2.SS1.SSS1.p1.1">The detailed training datasets are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A2.T7" title="Table 7 ‣ B.1 Domain-specific Continual Pretraining ‣ Appendix B Model Training Details ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">7</span></a>. For the e-commerce domain dataset, besides our WorthBuying dataset we collect e-commerce relevant notes from Xiaohongshu and Zhihu websites. For the generation domain data, we use the open-source pertaining dataset from Linly-OpenLLaMA <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://github.com/CVI-SZU/Linly" title="">https://github.com/CVI-SZU/Linly</a></span></span></span> and WanJuan Corpus <cite class="ltx_cite ltx_citemacro_cite">Qiu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib36" title="">2024</a>)</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="A2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.1.2 </span>Training Configurations</h4>
<div class="ltx_para" id="A2.SS1.SSS2.p1">
<p class="ltx_p" id="A2.SS1.SSS2.p1.1">The model is trained using the DeepSpeed framework, which facilitates efficient distributed training. Key hyperparameters and configurations include:</p>
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">Precision:</span> Training was performed using bfloat16 mixed precision, which significantly reduces memory consumption while maintaining the model’s performance.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i2.p1.1.1">Optimizer:</span> We used the AdamW optimizer with a dynamically adjusted learning rate based on the training progression. This setup helps in stabilizing the training process in the initial phases and gradually fine-tuning the weights in later stages.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i3.p1.1.1">Learning Rate Scheduler:</span> A cosine decay scheduler with a warmup phase was implemented. The learning rate started at <math alttext="2\times 10^{-5}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.1.m1.1"><semantics id="A2.I1.i3.p1.1.m1.1a"><mrow id="A2.I1.i3.p1.1.m1.1.1" xref="A2.I1.i3.p1.1.m1.1.1.cmml"><mn id="A2.I1.i3.p1.1.m1.1.1.2" xref="A2.I1.i3.p1.1.m1.1.1.2.cmml">2</mn><mo id="A2.I1.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.I1.i3.p1.1.m1.1.1.1.cmml">×</mo><msup id="A2.I1.i3.p1.1.m1.1.1.3" xref="A2.I1.i3.p1.1.m1.1.1.3.cmml"><mn id="A2.I1.i3.p1.1.m1.1.1.3.2" xref="A2.I1.i3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I1.i3.p1.1.m1.1.1.3.3" xref="A2.I1.i3.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I1.i3.p1.1.m1.1.1.3.3a" xref="A2.I1.i3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.I1.i3.p1.1.m1.1.1.3.3.2" xref="A2.I1.i3.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I1.i3.p1.1.m1.1b"><apply id="A2.I1.i3.p1.1.m1.1.1.cmml" xref="A2.I1.i3.p1.1.m1.1.1"><times id="A2.I1.i3.p1.1.m1.1.1.1.cmml" xref="A2.I1.i3.p1.1.m1.1.1.1"></times><cn id="A2.I1.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.I1.i3.p1.1.m1.1.1.2">2</cn><apply id="A2.I1.i3.p1.1.m1.1.1.3.cmml" xref="A2.I1.i3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="A2.I1.i3.p1.1.m1.1.1.3">superscript</csymbol><cn id="A2.I1.i3.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.I1.i3.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="A2.I1.i3.p1.1.m1.1.1.3.3"><minus id="A2.I1.i3.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I1.i3.p1.1.m1.1.1.3.3"></minus><cn id="A2.I1.i3.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.I1.i3.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i3.p1.1.m1.1c">2\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A2.I1.i3.p1.1.m1.1d">2 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and was adjusted according to a predefined schedule to optimize convergence.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i4.p1.1.1">Model Parallelism:</span> We employed Zero Stage 2 optimization to manage GPU memory efficiently, offloading optimizer states and model parameters to CPU memory.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i5.p1.1.1">Hardware Configureation</span> The training was conducted on a cluster of four NVIDIA H100 80GB GPUs,</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i6.p1">
<p class="ltx_p" id="A2.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i6.p1.1.1">Other Configurations</span> We set 32 per device batch-size and use 100 step for warming up.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Contrastive Learning Training Details</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In our experiments, we utilize DeepSpeed for distributed training with four Nvidia-H100 80GB GPUs. Key hyperparameters included mixed precision training with bf16, a batch size of 4 per device with 4 gradient accumulation steps, and gradient checkpointing. The model is trained for one epoch with a learning rate of 1e-5, logging every 5 steps, and overwriting the output directory. Dataset processing uses 32 parallel processes, with cross-device negative sampling and a 100-step warmup phase to stabilize training.</p>
</div>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">In the training dataset, we integrate some open-source retrieval datasets with our WorthBuying dataset. For the open-source dataset, we use the train dataset of the retrieval part of C-MTEB <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib54" title="">2023</a>)</cite> Benchmark.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Generation Evaluation Details</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In the evaluation of generation, as mentioned in <cite class="ltx_cite ltx_citemacro_cite">Es et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#bib.bib12" title="">2023</a>)</cite>, due to the subjective nature of responses generated by models, it is challenging to directly compute accuracy. Therefore, we adopt the multi-turn evaluation method utilized in <span class="ltx_text ltx_font_italic" id="A3.p1.1.1">ragas</span> leveraging GPT-4. Below, we outline the prompts used in our evaluation to provide clarity and transparency in our assessment process.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span> Stage1: Extracting key points from answer</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<svg class="ltx_picture" height="568.35" id="A3.SS1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,568.35) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 562.45 C 0 565.71 2.64 568.35 5.91 568.35 L 594.09 568.35 C 597.36 568.35 600 565.71 600 562.45 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 562.45 C 1.97 564.62 3.73 566.39 5.91 566.39 L 594.09 566.39 C 596.27 566.39 598.03 564.62 598.03 562.45 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="540.8" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.SS1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.1">Below are questions and answers, from which key points have been extracted and presented as individual statements.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.2">======================</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.3">Question: What are the specifications of the Huawei X 14 smartphone?</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.4">Answer: Storage options include 256G and 512G; screen resolution is 2640x1080 FHD+ with a peak brightness of 1750 nits and a refresh rate of 120Hz; the internal screen measures 6.7 inches with a square aspect and 6.6 inches with rounded corners, while the external screen measures 3.4 inches square; it utilizes the Snapdragon 8 Gen 2 mobile platform with an octa-core CPU; the weight is a steady 187g; battery capacity is 3700mAh.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.5">Statements:</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.6">Storage options of 256G and 512G.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.7">Screen resolution of 2640x1080, refresh rate of 120Hz</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.8">Internal screen size of 6.7 inches, external screen of 3.4 inches square</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.9">Uses Snapdragon 8 Gen 2 mobile platform, CPU is octa-core</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.10">Weighs 187g.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.11">Battery capacity is 3700mAh.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.12">======================</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.13">Question: What oral supplements can help women restore their skin?</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.14">Answer: Vitamin C tablets from Northeast Pharmaceuticals, Kang’enbei’s Vitamin C chewable tablets, and Northerland’s compound Vitamin B and Vitamin C chewable tablets can help restore women’s skin. Vitamin C can whiten the skin and regulate the body, while Vitamin B can repair the skin and is effective in treating acne and mouth ulcers.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.15">Statements:</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.16">Vitamin C can whiten and help restore women’s skin.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.17">Vitamin B can repair the skin and is effective for acne and mouth ulcers.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.18">======================</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.19">Question: Can the refill in a Baixue ballpoint pen be replaced?</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.20">Answer: Sorry, I cannot answer this question based on the available information.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.21">Statement:</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.22">The model is unable to answer this question.</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.23">======================</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.24">Question: {question}</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.25">Answer: {answer}</span>
<span class="ltx_p" id="A3.SS1.p1.pic1.1.1.1.1.1.26">Statement:</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span> Stage2: Judging the key points</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<svg class="ltx_picture" height="670.83" id="A3.SS2.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,670.83) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 664.92 C 0 668.18 2.64 670.83 5.91 670.83 L 594.09 670.83 C 597.36 670.83 600 668.18 600 664.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 664.92 C 1.97 667.09 3.73 668.86 5.91 668.86 L 594.09 668.86 C 596.27 668.86 598.03 667.09 598.03 664.92 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="643.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.SS2.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.1">Below are a series of statements along with corresponding facts. Each statement is evaluated for honesty, determining if it is sufficiently supported by the facts provided.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.2">======================</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.3">Statement:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.4">1. Storage capacity of 256 and 512GB.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.5">2. Uses the Snapdragon 8 Gen 2 mobile platform, CPU has nine cores.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.6">3. Weighs 199g.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.7">4. Battery capacity is 3700mAh.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.8">Fact:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.9">This smartphone offers storage capacities of 256 and 512GB with an upgrade option available. Screen resolution is 2640x1080 FHD+ with a peak brightness of 1750 nits and a refresh rate of 120Hz; the internal screen sizes are 6.7 inches square and 6.6 inches rounded, external screen size is 3.4 inches square; it is equipped with the Snapdragon 8 Gen 2 mobile platform, and the CPU has eight cores; the weight is stable at 199g; battery capacity is 4600mAh.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.10">Analysis:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.11">1. Storage capacity of 256 and 512GB. This statement matches the facts, correct. [Yes]</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.12">2. Uses the Snapdragon 8 Gen 2 mobile platform, CPU has nine cores. According to the facts, the CPU should have eight cores, this statement is factually incorrect. [No]</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.13">3. Weighs 199g. This is consistent with the facts, correct. [Yes]</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.14">4. Battery capacity is 3700mAh. This does not match the facts, which state the battery capacity should be 4600mAh, incorrect. [No]</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.15">=====================</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.16">Statement:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.17">1. The model does not have enough information and cannot answer the question, the model’s response is unknown.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.18">Fact:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.19">Vitamin C tablets from Northeast Pharmaceuticals, Kang’enbei’s Vitamin C chewable tablets, and Northerland’s compound Vitamin B and Vitamin C chewable tablets can help restore women’s skin. Vitamin C has a whitening effect and can regulate the body, while Vitamin B can repair the skin and is effective in treating acne and mouth ulcers.</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.20">**Analysis:**</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.21">Although the model states it does not know, this makes it impossible for me to judge the response based on the provided facts. [Unknown]</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.22">=====================</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.23">Statement:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.24">{statement}</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.25">Fact:</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.26">{ground_truth}</span>
<span class="ltx_p" id="A3.SS2.p1.pic1.1.1.1.1.1.27">Analysis:</span>
</span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Analysis</h2>
<figure class="ltx_figure" id="A4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="A4.F6.g1" src="extracted/5889235/figs/delta.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Histograms of gap scores, calculated by the similarity score of ground-truth document minus the similarity score of top returned not-ground-truth document, for two retrievers. The more positive gap scores the better.</figcaption>
</figure>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">BShareRAG retriever can better distinguish hard negatives. We analyze the different behavior between two retrievers, <span class="ltx_text ltx_font_italic" id="A4.p1.1.1">i.e.</span>, BGE-large-zh and BShared-RAG Retriever. For each question, we first calculate the gap of similarity scores between a ground-truth document (it surely contains answers) and the top returned not-ground-truth document. The gap score ranges from -1 to 1. A larger gap means a retriever can better rank the ground-truth document among others. Then we draw histograms of the gap scores for the two retrievers, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20075v1#A4.F6" title="Figure 6 ‣ Appendix D Additional Analysis ‣ BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"><span class="ltx_text ltx_ref_tag">6</span></a>. It indicates that our BShared-RAG retriever can better distinguish the ground-truth document from other similar documents as the gap scores are positive for more questions. In contrast, <span class="ltx_text ltx_font_italic" id="A4.p1.1.2">BGE-large-zh</span> has a more flat distribution, which means it cannot estimate a larger similarity of the ground-truth document for many questions.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 08:25:58 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
