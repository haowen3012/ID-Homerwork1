<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation</title>
<!--Generated on Wed Jul  3 01:12:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Code Generation,  NL2CodeGen,  DSL,  NL2DSL,  RAG,  Fine-Tuning" lang="en" name="keywords"/>
<base href="/html/2407.02742v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S1" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.SS1" title="In 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Code Generation or Program Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.SS2" title="In 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Reasoning and Tool Integration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.SS3" title="In 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS1" title="In 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Fine-Tuned NL2DSL Generation Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2" title="In 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Grounding with dynamically selected few-shots</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2.SSS1" title="In 3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Pre-Trained Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2.SSS2" title="In 3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>TST based BERT Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS3" title="In 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Grounding with API Metadata</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS3.SSS1" title="In 3.3. Grounding with API Metadata ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>API Function Definitions for Few Shots</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS3.SSS2" title="In 3.3. Grounding with API Metadata ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Semantic Function Definitions</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment Design and Metrics Definition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS1" title="In 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS2" title="In 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>DSL Generation Quality Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS2.SSS1" title="In 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Average Similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS2.SSS2" title="In 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Unparsed rate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS2.SSS3" title="In 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Hallucination rate</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.SS1" title="In 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Impact of number of few-shots on RAG performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.SS2" title="In 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>TST vs Pre-trained Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.SS3" title="In 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Function Definition vs Semantic Function Definitions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S6" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#A1" title="In A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#A1.SS1" title="In Appendix A Appendix ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Sample with computed Average similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#A1.SS2" title="In Appendix A Appendix ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>An example of API metdata</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nastaran Bassamzadeh  Chhaya Methani
<br class="ltx_break"/>Microsoft Corporation
<br class="ltx_break"/>Redmond, USA
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id1.id1">Natural Language to Code Generation has made significant progress in recent years with the advent of Large Language Models (LLMs). While generation for general-purpose languages like C, C++, and Python has improved significantly, LLMs struggle with custom function names in Domain Specific Languages or DSLs. This leads to higher hallucination rates and syntax errors, specially for DSLs having a high number of custom function names. Additionally, constant updates to function names add to the challenge as LLMs need to stay up-to-date. In this paper, we present optimizations for using Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with an ablation study comparing these strategies. We generated a train as well as test dataset with a DSL to represent automation tasks across roughly 700 APIs in public domain. We used the training dataset to fine-tune a Codex model for this DSL. Our results showed that the fine-tuned model scored the best on code similarity metric. With our RAG optimizations, we achieved parity for similarity metric. The compilation rate, however, showed that both the models still got the syntax wrong many times, with RAG-based method being 2 pts better. Conversely, hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for API parameter keys. We conclude that an optimized RAG model can match the quality of fine-tuned models and offer advantages for new, unseen APIs.</p>
</div>
<div class="ltx_keywords">Code Generation, NL2CodeGen, DSL, NL2DSL, RAG, Fine-Tuning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">There has been significant progress made in improving and quantifying the quality of Natural Language to Code Generation or NL2Code (<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib3" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Nguyen and Nadi, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib19" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib6" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib3" title="">2021</a>)</cite>). Recent improvements in models for general-purpose languages like Python, C++ and Java can be attributed to larger LLMs (<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib20" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib21" title="">2023a</a>)</cite>) and the availability of pre-trained open-source models (<cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib6" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib17" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib2" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Mistral AI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib18" title="">2024</a>)</cite>) advancing the state-of-the-art. However, there hasn’t been a focus on improving quality of Natural Language to Domain Specific Languages or NL2DSL which a lot of enterprise applications rely on.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Domain Specific Languages (or DSLs) are custom Computer Languages designed and optimized for specific applications. Examples of DSLs include SQL and industry-specific languages for formalizing API calls, often using formats like JSON or YAML to represent API sequences. In this paper, we focus on the task of generating a DSL used for authoring high-level automation workflows across thousands of web-scale APIs. These workflows support a variety of customer scenarios like invoice processing, sales lead integration with forms/emails etc. The automation DSL represents API names as functions and codifies a sequence of API calls along with conditional logic over the invocation of APIs. We constrained the length of sequence to 5 APIs and hope to explore longer sequences in future work. An example of the DSL is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.F1" title="Figure 1 ‣ 2.3. Contributions ‣ 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing code generation methods are hard to adapt for this scenario due to the frequent hallucinations and syntax errors. This is largely due to the custom names, massive size and diversity of APIs in public as well private domain along with the ever-changing API landscape. Current NL2Code methods mainly use fine-tuning and do not focus on strategies for improving grounding LLMs to include new APIs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we outline an end to end system architecture for NL2DSL generation with high response rate using selective improvements to RAG techniques (<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib15" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>) using OpenAI models. We fine-tuned a Codex model for NL2DSL and show a comparative analysis of the impact of the approaches used to optimize RAG.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Along with metaprompt tuning for RAG, we also included additional grounding context in the form of API Function Definitions, like the approach used for Tool Selection (<cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib26" title="">2023</a>)</cite>,<cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib28" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib14" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>). This is motivated by the similarities between the code generation and task orchestration scenarios discussed in more detail in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2" title="2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The remainder of this study is structured as follows. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2" title="2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the NL2DSL problem formulation along with literature review. The focus is on comparing differences between Tool Selection of APIs as a framework compared to Code Generation over a set of APIs. This will help define the scope of the experiments in this study. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3" title="3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a> lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4" title="4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a> discusses Data Generation, Metric definition and Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5" title="5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">5</span></a> shares our results and discussion followed by Conclusion and Future Work in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S6" title="6. Conclusion ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Code Generation or Program Synthesis</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Program Synthesis is a hard research problem (<cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib9" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib4" title="">2017</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib6" title="">2020</a>)</cite>,<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib13" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib31" title="">2021</a>)</cite>). It has gained significant interest with many open-source models focusing on general programming languages since the release of Github Copilot (<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib3" title="">2021</a>)</cite>). These models include Code Llama <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib17" title="">2023</a>)</cite>, StarCoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib12" title="">2023</a>)</cite>, Codestral <cite class="ltx_cite ltx_citemacro_citep">(Mistral AI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib18" title="">2024</a>)</cite>, Phi-3 <cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib2" title="">2024</a>)</cite> and more. Many of these advancements have been achieved through pre-training language models for code generation with a focus on improving datasets((<cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib17" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib2" title="">2024</a>)</cite>)). However, for domain adaptation, <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">instruction fine-tuning</span> on top of a base model remains a popular approach (<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib3" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib11" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Prompting LLMs is an alternative technique for code generation (<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib15" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(White et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib30" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib29" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib10" title="">2023</a>)</cite>). Poesia et al. (<cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>) focused on improving response quality through grounding techniques. They fine-tuned a Sentence BERT model by changing the loss function to incorporate predicting similarity of the generated target programs. With this adapted similarity metric, better few shots are selected dynamically.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Reasoning and Tool Integration</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">When it comes to modeling the problem of selecting a sequence of API calls, we need to consider formulating it as a <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">planning</span> or <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.2">reasoning</span> task. LLMs show remarkable reasoning capability, however, they also have limitations when it comes to staying up-to-date with recent knowledge, performing mathematical calculations etc. A popular way to overcome this has been granting the LLMs access to external tools. This framework gained significant popularity with OpenAI Code Interpreter’s success (<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib22" title="">2023b</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">External Tool Integration has been studied since with a focus on including specific tools such as web search (<cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib26" title="">2023</a>)</cite>), python code interpreters (<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib22" title="">2023b</a>)</cite>), adding calculators (<cite class="ltx_cite ltx_citemacro_citep">(Parisi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib23" title="">2022</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>) and so on. Expanding the tool set to a generic list of tools has been explored (<cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib26" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>), but it remains limited and often predicts single tools instead of sequences needed for most enterprise scenarios. Tool Use has mostly been explored in the context of generating more accurate text outputs for Q&amp;A tasks with the help of external tools(<cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib26" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Parisi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib23" title="">2022</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">There is an increase in focus on incorporating LLM’s code generation capabilities to reasoning and task orchestration, this is an area of active research (<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib14" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>). However, most of the research either limits the tools to a set of small well-documented APIs ( (<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>,<cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib14" title="">2023</a>)</cite>), or limited their scope to predicting a single output API (<cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib26" title="">2023</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Posing the reasoning or orchestration task as a code generation problem is similar to the API sequence generation scenario highlighted in this paper. Representing a plan as a DSL, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S1" title="1. Introduction ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, aligns with our goal of generating DSL for workflow automation. Improving the quality of Natural Language to DSL generation, is thus beneficial for both reasoning and plan generation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Contributions</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In the previous section, we discussed formulating Task Orchestration as a Code Generation problem since it can be represented as yet another DSL. NL2DSL generation suffers from the hallucination and quality issues we discussed in <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S1" title="1. Introduction ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. Few studies address the challenges of end-to-end DSL generation, specifically over a large set of custom APIs.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">This paper presents improvements to known RAG techniques focusing on improving DSL generation quality for enterprise settings. Our DSL expands API or tool selection to a sequences of 5-6 API calls, also referred to as chain of tools, which is a first to the best of our knowledge. We also consider the real-world scenarios of adding conditional logic with API calls as shown with an example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.F1" title="Figure 1 ‣ 2.3. Contributions ‣ 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. Our contribution is outlining an end-to-end system as well as presenting an ablation study for NL2DSL generation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">We merged prompting and grounding approaches from code generation (<cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>,<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib15" title="">2023</a>)</cite>,<cite class="ltx_cite ltx_citemacro_citep">(White et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib30" title="">2023</a>)</cite>) and added API metadata as used in task orchestration area (<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib7" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib14" title="">2023</a>)</cite>) and studied their impact on reducing hallucination rate. We created a test set having 1000 NL-DSL pairs spanning over a set of approx. 700 API calls or functions using principles of synthetic dataset generation (similar to <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib8" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Schick and Schütze, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib27" title="">2021</a>)</cite>) and used manual approval to validate test set quality. Our fine-tuned DSL model is trained on a larger synthetic NL-DSL dataset (details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS1" title="3.1. Fine-Tuned NL2DSL Generation Model ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="332" id="S2.F1.g1" src="x1.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>System Architecture to show e2e working of &amp;
our DSL generation methodology using RAG. TST based semantic mapping &amp;
retrieves the relevant code snippet as shown. This helps get the right syntax. However, &amp; it gets the correct function name for approval from the API metadata&amp;</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S2.F1.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.F1.2">System Architecture along with NL and DSL pairs from our dataset. Note, function names are indicative to show API functionality for this illustration.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first provide an overview of the approaches used in our experiments. In the following sub-sections, we will delve deeper in the details of each of the approaches.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Details of fine-tuning are shared in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS1" title="3.1. Fine-Tuned NL2DSL Generation Model ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Fine-Tuning a base model, specifically, instruction fine-tuning is a preferred approach for domain adaptation. It’s limitations include inability to include newly added APIs on an ongoing basis, as well as the resource intensive data collection process for infrequently used APIs or the tail set.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We used RAG based approaches to overcome these limitations, and focused on improving grounding techniques for DSL generation (Details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2" title="3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We used dynamically generated few-shot examples approach (<cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>), and augmented it with API function definitions similar to the way it is used for Tool Selection (<cite class="ltx_cite ltx_citemacro_citep">(Patil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib24" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib28" title="">2023</a>)</cite>). These few-shots were selected from an expansive pool of synthetic NL-DSL pairs, empirically having 100s of variations of usage for each API (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS1" title="4.1. Dataset Generation ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">For computing semantic similarity of the few-shots with the input user query (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2" title="3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.2</span></a>), we fine-tuned a BERT model as highlighted in <cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite> with a modified loss function for predicting target DSL similarity. For selecting the API metadata for grounding (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS3" title="3.3. Grounding with API Metadata ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.3</span></a>), we created an index over API Function Definitions. We also tried metaprompt tuning, but limit the focus of this study to improving grounding techniques with a combination of dynamically selected few-shot samples as well as API metadata or tool description.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">We share the details of each approach and variation below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Fine-Tuned NL2DSL Generation Model</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We took the Codex base model from OpenAI due to it’s pre-training with code samples and used LoRA-based fine-tuning approach. The training set consists of NL-DSL pairs, NL refers to the user query and the DSL represents the workflow that the user is looking to automate. We used ¡START¿ and ¡END¿ token to indicate the end of code generation to the model.
The training set consists of a pool of 67k samples in the form of (prompt, flow) tuples generated synthetically ( details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS1" title="4.1. Dataset Generation ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>, and examples of NL-DSL are shared in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S2.F1" title="Figure 1 ‣ 2.3. Contributions ‣ 2. Related Work ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#A1" title="Appendix A Appendix ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We ran many iterations on this model to improve performance on the test set, specifically for the body and tail connectors, and went through multiple rounds of data augmentation. We found that predicting the parameter keys was very challenging with the fine-tuned model due to limitation of data generation. Even with synthetic models, it was hard to scale the NL-DSL sample variety needed for improving quality of parameters.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Grounding with dynamically selected few-shots</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">We tried two types of grounding information for RAG based DSL generation as described below. There are some variations of each technique described in the paragraph below as well. For each technique, we selected <math alttext="5" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn id="S3.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">5</annotation></semantics></math> and <math alttext="20" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn id="S3.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">20</annotation></semantics></math> shots dynamically, and saw performance impact driven by the approach used for sample selection.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Pre-Trained Model</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The first approach is using a vanilla Per-Trained model for determining the semantic similarity of NL-DSL samples based on the NL query. We computed the embeddings of NL queries using a Distil-RoBERTa Pre-Trained model. We created a Faiss Index (<cite class="ltx_cite ltx_citemacro_citep">(Douze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib5" title="">2024</a>)</cite>) for these embeddings to help with search over the dense embedding space.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>TST based BERT Fine-tuning</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In this approach, we fine-tuned the pre-trained model to improve the retrieval accuracy of the few-shots. This is similar to the approach used by Poesia et al. in <cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>. They show that if we fine-tune the pre-trained BERT model with a modified loss function to consider the similarity between the target DSL for each NL-DSL pair, the retrieved examples will have a higher quality and finally lead to better generation with LLM.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">To get positive and negative samples for fine-tuning, we compared cosine similarity between all pairs of Natural Language queries in our dataset (Dataset shared in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.SS1" title="4.1. Dataset Generation ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>). We used a Pre-Trained Tansformer model to generate embeddings for the purpose of similarity computation. A pair of tuples is considered a positive sample if the similarity between their corresponding NL prompts is greater than 0.7 and negative otherwise. We generated 100k pairs this way and leveraged them as training data for our fine-tuning experiment.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.3">The loss function used by TST (Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.E1" title="In 3.2.2. TST based BERT Fine-tuning ‣ 3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a> from <cite class="ltx_cite ltx_citemacro_citep">(Poesia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib25" title="">2022</a>)</cite>) is minimizing the Mean-Squared Error between the vanilla loss functions comparing the utterances (<math alttext="u_{i},u_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1.2"><semantics id="S3.SS2.SSS2.p3.1.m1.2a"><mrow id="S3.SS2.SSS2.p3.1.m1.2.2.2" xref="S3.SS2.SSS2.p3.1.m1.2.2.3.cmml"><msub id="S3.SS2.SSS2.p3.1.m1.1.1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1.2.cmml">u</mi><mi id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.SSS2.p3.1.m1.2.2.2.3" xref="S3.SS2.SSS2.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS2.SSS2.p3.1.m1.2.2.2.2" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.2" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2.2.cmml">u</mi><mi id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.3" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.2b"><list id="S3.SS2.SSS2.p3.1.m1.2.2.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.2.2.2"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1.2">𝑢</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2.2">𝑢</ci><ci id="S3.SS2.SSS2.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.2c">u_{i},u_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.1.m1.2d">italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>) and the target programs (<math alttext="p_{i},p_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.2.m2.2"><semantics id="S3.SS2.SSS2.p3.2.m2.2a"><mrow id="S3.SS2.SSS2.p3.2.m2.2.2.2" xref="S3.SS2.SSS2.p3.2.m2.2.2.3.cmml"><msub id="S3.SS2.SSS2.p3.2.m2.1.1.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1.2.cmml">p</mi><mi id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.SSS2.p3.2.m2.2.2.2.3" xref="S3.SS2.SSS2.p3.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS2.SSS2.p3.2.m2.2.2.2.2" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.2" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2.2.cmml">p</mi><mi id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.3" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.2b"><list id="S3.SS2.SSS2.p3.2.m2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.2.2.2"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1.2">𝑝</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2.2">𝑝</ci><ci id="S3.SS2.SSS2.p3.2.m2.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.2.2.2.2.3">𝑗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.2c">p_{i},p_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.2.m2.2d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>). Program similarity is denoted by <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.3.m3.1"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><mi id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><ci id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.3.m3.1d">italic_S</annotation></semantics></math>. They used AST to compute program similarity, however, we used a Jaccard score over lists of API function names to be consistent with our metrics definition (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4" title="4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{TST}(\theta):=E_{i,j~{}D}[f_{\theta}(u_{i},u_{j})-S(P_{i},p_{j})]^{2}" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><msub id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml"><mi id="S3.E1.m1.4.4.3.2.2" xref="S3.E1.m1.4.4.3.2.2.cmml">L</mi><mrow id="S3.E1.m1.4.4.3.2.3" xref="S3.E1.m1.4.4.3.2.3.cmml"><mi id="S3.E1.m1.4.4.3.2.3.2" xref="S3.E1.m1.4.4.3.2.3.2.cmml">T</mi><mo id="S3.E1.m1.4.4.3.2.3.1" xref="S3.E1.m1.4.4.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.4.3.2.3.3" xref="S3.E1.m1.4.4.3.2.3.3.cmml">S</mi><mo id="S3.E1.m1.4.4.3.2.3.1a" xref="S3.E1.m1.4.4.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.4.3.2.3.4" xref="S3.E1.m1.4.4.3.2.3.4.cmml">T</mi></mrow></msub><mo id="S3.E1.m1.4.4.3.1" xref="S3.E1.m1.4.4.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.3.3.2" xref="S3.E1.m1.4.4.3.cmml"><mo id="S3.E1.m1.4.4.3.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.3.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">θ</mi><mo id="S3.E1.m1.4.4.3.3.2.2" rspace="0.278em" stretchy="false" xref="S3.E1.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.2" rspace="0.278em" xref="S3.E1.m1.4.4.2.cmml">:=</mo><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml"><msub id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.4.4.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.4.4.1.3.2.cmml">E</mi><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">j</mi><mo id="S3.E1.m1.2.2.2.2.1.1" lspace="0.230em" xref="S3.E1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.3.cmml">D</mi></mrow></mrow></msub><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">⁢</mo><msup id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.2.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.4.4.1.1.1.1.1.2.4" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.4.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4.2.cmml">f</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.4.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4.3.cmml">θ</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.3.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">u</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.4" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.2.cmml">u</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.1.5" xref="S3.E1.m1.4.4.1.1.1.1.1.5.cmml">−</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.4" xref="S3.E1.m1.4.4.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.4.4" xref="S3.E1.m1.4.4.1.1.1.1.1.4.4.cmml">S</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.4.3" xref="S3.E1.m1.4.4.1.1.1.1.1.4.3.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.3.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.3" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.3.cmml">(</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.2.cmml">P</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.4" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.3.cmml">,</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.2.cmml">p</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.3.cmml">j</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.5" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.2.1.cmml">]</mo></mrow><mn id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><csymbol cd="latexml" id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2">assign</csymbol><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><times id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3.1"></times><apply id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.2.1.cmml" xref="S3.E1.m1.4.4.3.2">subscript</csymbol><ci id="S3.E1.m1.4.4.3.2.2.cmml" xref="S3.E1.m1.4.4.3.2.2">𝐿</ci><apply id="S3.E1.m1.4.4.3.2.3.cmml" xref="S3.E1.m1.4.4.3.2.3"><times id="S3.E1.m1.4.4.3.2.3.1.cmml" xref="S3.E1.m1.4.4.3.2.3.1"></times><ci id="S3.E1.m1.4.4.3.2.3.2.cmml" xref="S3.E1.m1.4.4.3.2.3.2">𝑇</ci><ci id="S3.E1.m1.4.4.3.2.3.3.cmml" xref="S3.E1.m1.4.4.3.2.3.3">𝑆</ci><ci id="S3.E1.m1.4.4.3.2.3.4.cmml" xref="S3.E1.m1.4.4.3.2.3.4">𝑇</ci></apply></apply><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝜃</ci></apply><apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1"><times id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2"></times><apply id="S3.E1.m1.4.4.1.3.cmml" xref="S3.E1.m1.4.4.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2">𝐸</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"></times><ci id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2">𝑗</ci><ci id="S3.E1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.3">𝐷</ci></apply></list></apply><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1">superscript</csymbol><apply id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><minus id="S3.E1.m1.4.4.1.1.1.1.1.5.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.5"></minus><apply id="S3.E1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2"><times id="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3"></times><apply id="S3.E1.m1.4.4.1.1.1.1.1.2.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.2.4.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.4.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4.2">𝑓</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.4.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.4.3">𝜃</ci></apply><interval closure="open" id="S3.E1.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2"><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑢</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.2">𝑢</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4"><times id="S3.E1.m1.4.4.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.3"></times><ci id="S3.E1.m1.4.4.1.1.1.1.1.4.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.4">𝑆</ci><interval closure="open" id="S3.E1.m1.4.4.1.1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2"><apply id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.2">𝑃</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.2">𝑝</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.2.2.3">𝑗</ci></apply></interval></apply></apply></apply><cn id="S3.E1.m1.4.4.1.1.3.cmml" type="integer" xref="S3.E1.m1.4.4.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">L_{TST}(\theta):=E_{i,j~{}D}[f_{\theta}(u_{i},u_{j})-S(P_{i},p_{j})]^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_L start_POSTSUBSCRIPT italic_T italic_S italic_T end_POSTSUBSCRIPT ( italic_θ ) := italic_E start_POSTSUBSCRIPT italic_i , italic_j italic_D end_POSTSUBSCRIPT [ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) - italic_S ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Grounding with API Metadata</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In addition to few-shots, we appended the API metadata in the metaprompt. This metadata includes Function Description along with the parameter keys and their description (See an example API Function Definition shared in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#A1" title="Appendix A Appendix ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">A</span></a>). We followed the below two approaches for selecting the metadata to be added.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>API Function Definitions for Few Shots</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">For the few-shots samples selected using the methods described above, we extracted the metadata for <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p1.1.1">each of</span> the functions present in those samples. This means that for the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mi id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">italic_n</annotation></semantics></math> few-shot samples dynamically added to the metaprompt, we iterated over all the API function names in each of these flows and added their function definitions to the metaprompt.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">We also modified the metaprompt to add instructions on how to use the Function Definitions. We want to explore how adding the metadata explaining the purpose of each function in the few-shot examples impacts LLM’s understanding of the task and map to user request.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Semantic Function Definitions</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Another approach for selecting the function definitions to be added to the metaprompt is to retrieve the semantically similar functions from a vector database created with API metadata. This approach is similar to the one followed by LlamaIndex (<cite class="ltx_cite ltx_citemacro_citep">(LlamaIndex, <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#bib.bib16" title="">2023</a>)</cite>) We created an index of all API definitions and retrieved the semantically similar functions by using the input NL query to search the index. Please note that this is different from the faiss index created for few-shot samples in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3.SS2" title="3.2. Grounding with dynamically selected few-shots ‣ 3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">We call this approach <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.1">Semantic Function Definition (SFD)</span> and will compare it with the <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.2">Regular FDs</span> described above. This approach can be specifically useful for tail-ish prompts where no few-shots might be retrieved. This helps us integrate the newly released web APIs in our DSL Generation framework making our approach scalable to the changing API landscape.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiment Design and Metrics Definition</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we outline the process of Dataset Generation and introduce the metrics we used for estimating the code quality. We then describe the experiments. Results and Discussion follows in the next section. We have used Azure AML pipelines to run our experiments. The GPT-4 (with 16k token limit) model is used as the LLM model. The metaprompt is kept consistent between experiments for the purpose of the ablation study.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Dataset Generation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We generated a total of 67k samples in the form of (prompt, flow) pairs from workflows created by users. We had many samples of workflow automations created by users across a large set of APIs. We sampled the automations containing <math alttext="700" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">700</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">700</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">700</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">700</annotation></semantics></math> publicly available APIs and synthetically generated the corresponding Natural Language prompts using GPT-4. For creating these NL descriptions for the workflows, we also provided API Function definitions to the metadata. This ensured the language of the description captured the functioanlity of the API.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">A subset of these synthetic samples were validated by human judges. We used these checks to improve the metaprompt used for synthetic data generation. For creating a test set, we used the same process with most of the test set evaluated by human judges to ensure quality. We followed the same distribution of APIs from users, to ensure that our metrics are not biased. The test data set consists of <math alttext="1000" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn id="S4.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">1000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">1000</annotation></semantics></math> samples that are verified by human judges.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>DSL Generation Quality Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We defined 3 key metrics to focus on code generation quality as well as syntactic accuracy and hallucination rate. We have a compiler to test the syntax and validate the functions against a database of API names as well as parameter keys.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Average Similarity</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Average Similarity measures the aggregated similarity between predicted flow and the ground truth flow. The average similarity between two flows is defined using the Longest Common Subsequence match (LCSS) metric. Each flow is reduced to a list of API call sequences and then the LCSS is computed. The final metric is reported as an average over all test samples. Hallucination and Parser failures lead to the sample being discarded and is assigned a similarity score of 0.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textrm{Similarity}=\frac{\mathrm{LCSS}(A,B)}{max(|\mathrm{Actions}_{A}|,|%
\mathrm{Actions}_{B}|)}" class="ltx_Math" display="block" id="S4.E2.m1.4"><semantics id="S4.E2.m1.4a"><mrow id="S4.E2.m1.4.5" xref="S4.E2.m1.4.5.cmml"><mtext id="S4.E2.m1.4.5.2" xref="S4.E2.m1.4.5.2a.cmml">Similarity</mtext><mo id="S4.E2.m1.4.5.1" xref="S4.E2.m1.4.5.1.cmml">=</mo><mfrac id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml"><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml"><mi id="S4.E2.m1.2.2.2.4" xref="S4.E2.m1.2.2.2.4.cmml">LCSS</mi><mo id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.2.3.cmml">⁢</mo><mrow id="S4.E2.m1.2.2.2.5.2" xref="S4.E2.m1.2.2.2.5.1.cmml"><mo id="S4.E2.m1.2.2.2.5.2.1" stretchy="false" xref="S4.E2.m1.2.2.2.5.1.cmml">(</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">A</mi><mo id="S4.E2.m1.2.2.2.5.2.2" xref="S4.E2.m1.2.2.2.5.1.cmml">,</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">B</mi><mo id="S4.E2.m1.2.2.2.5.2.3" stretchy="false" xref="S4.E2.m1.2.2.2.5.1.cmml">)</mo></mrow></mrow><mrow id="S4.E2.m1.4.4.4" xref="S4.E2.m1.4.4.4.cmml"><mi id="S4.E2.m1.4.4.4.4" xref="S4.E2.m1.4.4.4.4.cmml">m</mi><mo id="S4.E2.m1.4.4.4.3" xref="S4.E2.m1.4.4.4.3.cmml">⁢</mo><mi id="S4.E2.m1.4.4.4.5" xref="S4.E2.m1.4.4.4.5.cmml">a</mi><mo id="S4.E2.m1.4.4.4.3a" xref="S4.E2.m1.4.4.4.3.cmml">⁢</mo><mi id="S4.E2.m1.4.4.4.6" xref="S4.E2.m1.4.4.4.6.cmml">x</mi><mo id="S4.E2.m1.4.4.4.3b" xref="S4.E2.m1.4.4.4.3.cmml">⁢</mo><mrow id="S4.E2.m1.4.4.4.2.2" xref="S4.E2.m1.4.4.4.2.3.cmml"><mo id="S4.E2.m1.4.4.4.2.2.3" stretchy="false" xref="S4.E2.m1.4.4.4.2.3.cmml">(</mo><mrow id="S4.E2.m1.3.3.3.1.1.1.1" xref="S4.E2.m1.3.3.3.1.1.1.2.cmml"><mo id="S4.E2.m1.3.3.3.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.3.3.3.1.1.1.2.1.cmml">|</mo><msub id="S4.E2.m1.3.3.3.1.1.1.1.1" xref="S4.E2.m1.3.3.3.1.1.1.1.1.cmml"><mi id="S4.E2.m1.3.3.3.1.1.1.1.1.2" xref="S4.E2.m1.3.3.3.1.1.1.1.1.2.cmml">Actions</mi><mi id="S4.E2.m1.3.3.3.1.1.1.1.1.3" xref="S4.E2.m1.3.3.3.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S4.E2.m1.3.3.3.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.3.3.3.1.1.1.2.1.cmml">|</mo></mrow><mo id="S4.E2.m1.4.4.4.2.2.4" xref="S4.E2.m1.4.4.4.2.3.cmml">,</mo><mrow id="S4.E2.m1.4.4.4.2.2.2.1" xref="S4.E2.m1.4.4.4.2.2.2.2.cmml"><mo id="S4.E2.m1.4.4.4.2.2.2.1.2" stretchy="false" xref="S4.E2.m1.4.4.4.2.2.2.2.1.cmml">|</mo><msub id="S4.E2.m1.4.4.4.2.2.2.1.1" xref="S4.E2.m1.4.4.4.2.2.2.1.1.cmml"><mi id="S4.E2.m1.4.4.4.2.2.2.1.1.2" xref="S4.E2.m1.4.4.4.2.2.2.1.1.2.cmml">Actions</mi><mi id="S4.E2.m1.4.4.4.2.2.2.1.1.3" xref="S4.E2.m1.4.4.4.2.2.2.1.1.3.cmml">B</mi></msub><mo id="S4.E2.m1.4.4.4.2.2.2.1.3" stretchy="false" xref="S4.E2.m1.4.4.4.2.2.2.2.1.cmml">|</mo></mrow><mo id="S4.E2.m1.4.4.4.2.2.5" stretchy="false" xref="S4.E2.m1.4.4.4.2.3.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.4b"><apply id="S4.E2.m1.4.5.cmml" xref="S4.E2.m1.4.5"><eq id="S4.E2.m1.4.5.1.cmml" xref="S4.E2.m1.4.5.1"></eq><ci id="S4.E2.m1.4.5.2a.cmml" xref="S4.E2.m1.4.5.2"><mtext id="S4.E2.m1.4.5.2.cmml" xref="S4.E2.m1.4.5.2">Similarity</mtext></ci><apply id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4"><divide id="S4.E2.m1.4.4.5.cmml" xref="S4.E2.m1.4.4"></divide><apply id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"><times id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.3"></times><ci id="S4.E2.m1.2.2.2.4.cmml" xref="S4.E2.m1.2.2.2.4">LCSS</ci><interval closure="open" id="S4.E2.m1.2.2.2.5.1.cmml" xref="S4.E2.m1.2.2.2.5.2"><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">𝐴</ci><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">𝐵</ci></interval></apply><apply id="S4.E2.m1.4.4.4.cmml" xref="S4.E2.m1.4.4.4"><times id="S4.E2.m1.4.4.4.3.cmml" xref="S4.E2.m1.4.4.4.3"></times><ci id="S4.E2.m1.4.4.4.4.cmml" xref="S4.E2.m1.4.4.4.4">𝑚</ci><ci id="S4.E2.m1.4.4.4.5.cmml" xref="S4.E2.m1.4.4.4.5">𝑎</ci><ci id="S4.E2.m1.4.4.4.6.cmml" xref="S4.E2.m1.4.4.4.6">𝑥</ci><interval closure="open" id="S4.E2.m1.4.4.4.2.3.cmml" xref="S4.E2.m1.4.4.4.2.2"><apply id="S4.E2.m1.3.3.3.1.1.1.2.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1"><abs id="S4.E2.m1.3.3.3.1.1.1.2.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1.2"></abs><apply id="S4.E2.m1.3.3.3.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.3.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.3.3.3.1.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1.1.2">Actions</ci><ci id="S4.E2.m1.3.3.3.1.1.1.1.1.3.cmml" xref="S4.E2.m1.3.3.3.1.1.1.1.1.3">𝐴</ci></apply></apply><apply id="S4.E2.m1.4.4.4.2.2.2.2.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1"><abs id="S4.E2.m1.4.4.4.2.2.2.2.1.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1.2"></abs><apply id="S4.E2.m1.4.4.4.2.2.2.1.1.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.4.4.4.2.2.2.1.1.1.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1.1">subscript</csymbol><ci id="S4.E2.m1.4.4.4.2.2.2.1.1.2.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1.1.2">Actions</ci><ci id="S4.E2.m1.4.4.4.2.2.2.1.1.3.cmml" xref="S4.E2.m1.4.4.4.2.2.2.1.1.3">𝐵</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.4c">\textrm{Similarity}=\frac{\mathrm{LCSS}(A,B)}{max(|\mathrm{Actions}_{A}|,|%
\mathrm{Actions}_{B}|)}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.4d">Similarity = divide start_ARG roman_LCSS ( italic_A , italic_B ) end_ARG start_ARG italic_m italic_a italic_x ( | roman_Actions start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT | , | roman_Actions start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS1.p2.4">where <math alttext="|\textrm{Actions}_{A}|" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.1.m1.1"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml"><mo id="S4.SS2.SSS1.p2.1.m1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS1.p2.1.m1.1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.cmml"><mtext id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2a.cmml">Actions</mtext><mi id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml">A</mi></msub><mo id="S4.SS2.SSS1.p2.1.m1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><apply id="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1"><abs id="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.2"></abs><apply id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2a.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2"><mtext id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2">Actions</mtext></ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">|\textrm{Actions}_{A}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.1.m1.1d">| Actions start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of actions in flow <math alttext="A" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.2.m2.1"><semantics id="S4.SS2.SSS1.p2.2.m2.1a"><mi id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.1b"><ci id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.2.m2.1d">italic_A</annotation></semantics></math> and <math alttext="|\textrm{Actions}_{B}|" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.3.m3.1"><semantics id="S4.SS2.SSS1.p2.3.m3.1a"><mrow id="S4.SS2.SSS1.p2.3.m3.1.1.1" xref="S4.SS2.SSS1.p2.3.m3.1.1.2.cmml"><mo id="S4.SS2.SSS1.p2.3.m3.1.1.1.2" stretchy="false" xref="S4.SS2.SSS1.p2.3.m3.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS1.p2.3.m3.1.1.1.1" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.cmml"><mtext id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2a.cmml">Actions</mtext><mi id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.3" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.3.cmml">B</mi></msub><mo id="S4.SS2.SSS1.p2.3.m3.1.1.1.3" stretchy="false" xref="S4.SS2.SSS1.p2.3.m3.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.3.m3.1b"><apply id="S4.SS2.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1"><abs id="S4.SS2.SSS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.2"></abs><apply id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2a.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2"><mtext id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.2">Actions</mtext></ci><ci id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.3.m3.1c">|\textrm{Actions}_{B}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.3.m3.1d">| Actions start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of actions in flow <math alttext="B" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.4.m4.1"><semantics id="S4.SS2.SSS1.p2.4.m4.1a"><mi id="S4.SS2.SSS1.p2.4.m4.1.1" xref="S4.SS2.SSS1.p2.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.4.m4.1b"><ci id="S4.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p2.4.m4.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.4.m4.1d">italic_B</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">Please note that we are not using the commonly used AST metric for computing code similarity. AST drills down to compare similarity performance for parameters as well. As we wanted to focus on the problem of improving function name retrieval as well as it’s sequence, we chose to define the metric in this manner.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Compare impact of selecting <span class="ltx_text ltx_font_bold" id="S4.T1.16.1">5 vs 20 few shot</span> samples for both TST vs. Pre-trained Model without adding API function definitions using GPT-4. All results are shown as <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T1.2.m1.1"><semantics id="S4.T1.2.m1.1b"><mi id="S4.T1.2.m1.1.1" mathvariant="normal" xref="S4.T1.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><ci id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.m1.1e">roman_Δ</annotation></semantics></math> improvements compared to the baseline. The baseline uses Pre-Trained Transformer Model with 5 few-shot samples. For Avg. similarity, higher is better, and for the rest of metrics capturing failure rates, lower is better.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.14.13.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.14.13.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.13.1.2">Num of Few-Shots</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.13.1.3">Avg. Similarity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.13.1.4">%non-parsed flows</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.13.1.5">%made-up API names</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.14.13.1.6">%made-up parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.6.4.5"><span class="ltx_text ltx_font_typewriter" id="S4.T1.6.4.5.1">Pre-trained Model wo FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.6">20</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.1"><math alttext="+0.03" class="ltx_Math" display="inline" id="S4.T1.3.1.1.m1.1"><semantics id="S4.T1.3.1.1.m1.1a"><mrow id="S4.T1.3.1.1.m1.1.1" xref="S4.T1.3.1.1.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T1.3.1.1.m1.1.1a" mathvariant="bold" xref="S4.T1.3.1.1.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T1.3.1.1.m1.1.1.2" mathvariant="bold" xref="S4.T1.3.1.1.m1.1.1.2.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><apply id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1"><plus id="S4.T1.3.1.1.m1.1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1"></plus><cn id="S4.T1.3.1.1.m1.1.1.2.cmml" type="float" xref="S4.T1.3.1.1.m1.1.1.2">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">+0.03</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.1.1.m1.1d">bold_+ bold_0.03</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2"><math alttext="-3.37" class="ltx_Math" display="inline" id="S4.T1.4.2.2.m1.1"><semantics id="S4.T1.4.2.2.m1.1a"><mrow id="S4.T1.4.2.2.m1.1.1" xref="S4.T1.4.2.2.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T1.4.2.2.m1.1.1a" mathvariant="bold" xref="S4.T1.4.2.2.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T1.4.2.2.m1.1.1.2" mathvariant="bold" xref="S4.T1.4.2.2.m1.1.1.2.cmml">3.37</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.m1.1b"><apply id="S4.T1.4.2.2.m1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1"><minus id="S4.T1.4.2.2.m1.1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1"></minus><cn id="S4.T1.4.2.2.m1.1.1.2.cmml" type="float" xref="S4.T1.4.2.2.m1.1.1.2">3.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.m1.1c">-3.37</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.2.2.m1.1d">bold_- bold_3.37</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3"><math alttext="-7.34" class="ltx_Math" display="inline" id="S4.T1.5.3.3.m1.1"><semantics id="S4.T1.5.3.3.m1.1a"><mrow id="S4.T1.5.3.3.m1.1.1" xref="S4.T1.5.3.3.m1.1.1.cmml"><mo id="S4.T1.5.3.3.m1.1.1a" xref="S4.T1.5.3.3.m1.1.1.cmml">−</mo><mn id="S4.T1.5.3.3.m1.1.1.2" xref="S4.T1.5.3.3.m1.1.1.2.cmml">7.34</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.3.3.m1.1b"><apply id="S4.T1.5.3.3.m1.1.1.cmml" xref="S4.T1.5.3.3.m1.1.1"><minus id="S4.T1.5.3.3.m1.1.1.1.cmml" xref="S4.T1.5.3.3.m1.1.1"></minus><cn id="S4.T1.5.3.3.m1.1.1.2.cmml" type="float" xref="S4.T1.5.3.3.m1.1.1.2">7.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.3.3.m1.1c">-7.34</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.3.3.m1.1d">- 7.34</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4"><math alttext="-15.17" class="ltx_Math" display="inline" id="S4.T1.6.4.4.m1.1"><semantics id="S4.T1.6.4.4.m1.1a"><mrow id="S4.T1.6.4.4.m1.1.1" xref="S4.T1.6.4.4.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T1.6.4.4.m1.1.1a" mathvariant="bold" xref="S4.T1.6.4.4.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T1.6.4.4.m1.1.1.2" mathvariant="bold" xref="S4.T1.6.4.4.m1.1.1.2.cmml">15.17</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.4.m1.1b"><apply id="S4.T1.6.4.4.m1.1.1.cmml" xref="S4.T1.6.4.4.m1.1.1"><minus id="S4.T1.6.4.4.m1.1.1.1.cmml" xref="S4.T1.6.4.4.m1.1.1"></minus><cn id="S4.T1.6.4.4.m1.1.1.2.cmml" type="float" xref="S4.T1.6.4.4.m1.1.1.2">15.17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.4.m1.1c">-15.17</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.4.4.m1.1d">bold_- bold_15.17</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.10.8.5"><span class="ltx_text ltx_font_typewriter" id="S4.T1.10.8.5.1">TST wo FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.6">5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.1"><math alttext="+0.02" class="ltx_Math" display="inline" id="S4.T1.7.5.1.m1.1"><semantics id="S4.T1.7.5.1.m1.1a"><mrow id="S4.T1.7.5.1.m1.1.1" xref="S4.T1.7.5.1.m1.1.1.cmml"><mo id="S4.T1.7.5.1.m1.1.1a" xref="S4.T1.7.5.1.m1.1.1.cmml">+</mo><mn id="S4.T1.7.5.1.m1.1.1.2" xref="S4.T1.7.5.1.m1.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.1.m1.1b"><apply id="S4.T1.7.5.1.m1.1.1.cmml" xref="S4.T1.7.5.1.m1.1.1"><plus id="S4.T1.7.5.1.m1.1.1.1.cmml" xref="S4.T1.7.5.1.m1.1.1"></plus><cn id="S4.T1.7.5.1.m1.1.1.2.cmml" type="float" xref="S4.T1.7.5.1.m1.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.1.m1.1c">+0.02</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.5.1.m1.1d">+ 0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.2"><math alttext="-0.61" class="ltx_Math" display="inline" id="S4.T1.8.6.2.m1.1"><semantics id="S4.T1.8.6.2.m1.1a"><mrow id="S4.T1.8.6.2.m1.1.1" xref="S4.T1.8.6.2.m1.1.1.cmml"><mo id="S4.T1.8.6.2.m1.1.1a" xref="S4.T1.8.6.2.m1.1.1.cmml">−</mo><mn id="S4.T1.8.6.2.m1.1.1.2" xref="S4.T1.8.6.2.m1.1.1.2.cmml">0.61</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.6.2.m1.1b"><apply id="S4.T1.8.6.2.m1.1.1.cmml" xref="S4.T1.8.6.2.m1.1.1"><minus id="S4.T1.8.6.2.m1.1.1.1.cmml" xref="S4.T1.8.6.2.m1.1.1"></minus><cn id="S4.T1.8.6.2.m1.1.1.2.cmml" type="float" xref="S4.T1.8.6.2.m1.1.1.2">0.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.6.2.m1.1c">-0.61</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.6.2.m1.1d">- 0.61</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.3"><math alttext="-3.53" class="ltx_Math" display="inline" id="S4.T1.9.7.3.m1.1"><semantics id="S4.T1.9.7.3.m1.1a"><mrow id="S4.T1.9.7.3.m1.1.1" xref="S4.T1.9.7.3.m1.1.1.cmml"><mo id="S4.T1.9.7.3.m1.1.1a" xref="S4.T1.9.7.3.m1.1.1.cmml">−</mo><mn id="S4.T1.9.7.3.m1.1.1.2" xref="S4.T1.9.7.3.m1.1.1.2.cmml">3.53</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.9.7.3.m1.1b"><apply id="S4.T1.9.7.3.m1.1.1.cmml" xref="S4.T1.9.7.3.m1.1.1"><minus id="S4.T1.9.7.3.m1.1.1.1.cmml" xref="S4.T1.9.7.3.m1.1.1"></minus><cn id="S4.T1.9.7.3.m1.1.1.2.cmml" type="float" xref="S4.T1.9.7.3.m1.1.1.2">3.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.7.3.m1.1c">-3.53</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.7.3.m1.1d">- 3.53</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.4"><math alttext="-1.04" class="ltx_Math" display="inline" id="S4.T1.10.8.4.m1.1"><semantics id="S4.T1.10.8.4.m1.1a"><mrow id="S4.T1.10.8.4.m1.1.1" xref="S4.T1.10.8.4.m1.1.1.cmml"><mo id="S4.T1.10.8.4.m1.1.1a" xref="S4.T1.10.8.4.m1.1.1.cmml">−</mo><mn id="S4.T1.10.8.4.m1.1.1.2" xref="S4.T1.10.8.4.m1.1.1.2.cmml">1.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.8.4.m1.1b"><apply id="S4.T1.10.8.4.m1.1.1.cmml" xref="S4.T1.10.8.4.m1.1.1"><minus id="S4.T1.10.8.4.m1.1.1.1.cmml" xref="S4.T1.10.8.4.m1.1.1"></minus><cn id="S4.T1.10.8.4.m1.1.1.2.cmml" type="float" xref="S4.T1.10.8.4.m1.1.1.2">1.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.8.4.m1.1c">-1.04</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.8.4.m1.1d">- 1.04</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.14.12.5"><span class="ltx_text ltx_font_typewriter" id="S4.T1.14.12.5.1">TST wo FD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.14.12.6">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.11.9.1"><math alttext="+0.03" class="ltx_Math" display="inline" id="S4.T1.11.9.1.m1.1"><semantics id="S4.T1.11.9.1.m1.1a"><mrow id="S4.T1.11.9.1.m1.1.1" xref="S4.T1.11.9.1.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T1.11.9.1.m1.1.1a" mathvariant="bold" xref="S4.T1.11.9.1.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T1.11.9.1.m1.1.1.2" mathvariant="bold" xref="S4.T1.11.9.1.m1.1.1.2.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.11.9.1.m1.1b"><apply id="S4.T1.11.9.1.m1.1.1.cmml" xref="S4.T1.11.9.1.m1.1.1"><plus id="S4.T1.11.9.1.m1.1.1.1.cmml" xref="S4.T1.11.9.1.m1.1.1"></plus><cn id="S4.T1.11.9.1.m1.1.1.2.cmml" type="float" xref="S4.T1.11.9.1.m1.1.1.2">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.9.1.m1.1c">+0.03</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.9.1.m1.1d">bold_+ bold_0.03</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.12.10.2"><math alttext="-2.85" class="ltx_Math" display="inline" id="S4.T1.12.10.2.m1.1"><semantics id="S4.T1.12.10.2.m1.1a"><mrow id="S4.T1.12.10.2.m1.1.1" xref="S4.T1.12.10.2.m1.1.1.cmml"><mo id="S4.T1.12.10.2.m1.1.1a" xref="S4.T1.12.10.2.m1.1.1.cmml">−</mo><mn id="S4.T1.12.10.2.m1.1.1.2" xref="S4.T1.12.10.2.m1.1.1.2.cmml">2.85</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.12.10.2.m1.1b"><apply id="S4.T1.12.10.2.m1.1.1.cmml" xref="S4.T1.12.10.2.m1.1.1"><minus id="S4.T1.12.10.2.m1.1.1.1.cmml" xref="S4.T1.12.10.2.m1.1.1"></minus><cn id="S4.T1.12.10.2.m1.1.1.2.cmml" type="float" xref="S4.T1.12.10.2.m1.1.1.2">2.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.10.2.m1.1c">-2.85</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.10.2.m1.1d">- 2.85</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.11.3"><math alttext="-8.49" class="ltx_Math" display="inline" id="S4.T1.13.11.3.m1.1"><semantics id="S4.T1.13.11.3.m1.1a"><mrow id="S4.T1.13.11.3.m1.1.1" xref="S4.T1.13.11.3.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T1.13.11.3.m1.1.1a" mathvariant="bold" xref="S4.T1.13.11.3.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T1.13.11.3.m1.1.1.2" mathvariant="bold" xref="S4.T1.13.11.3.m1.1.1.2.cmml">8.49</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.13.11.3.m1.1b"><apply id="S4.T1.13.11.3.m1.1.1.cmml" xref="S4.T1.13.11.3.m1.1.1"><minus id="S4.T1.13.11.3.m1.1.1.1.cmml" xref="S4.T1.13.11.3.m1.1.1"></minus><cn id="S4.T1.13.11.3.m1.1.1.2.cmml" type="float" xref="S4.T1.13.11.3.m1.1.1.2">8.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.11.3.m1.1c">-8.49</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.11.3.m1.1d">bold_- bold_8.49</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.14.12.4"><math alttext="-14.58" class="ltx_Math" display="inline" id="S4.T1.14.12.4.m1.1"><semantics id="S4.T1.14.12.4.m1.1a"><mrow id="S4.T1.14.12.4.m1.1.1" xref="S4.T1.14.12.4.m1.1.1.cmml"><mo id="S4.T1.14.12.4.m1.1.1a" xref="S4.T1.14.12.4.m1.1.1.cmml">−</mo><mn id="S4.T1.14.12.4.m1.1.1.2" xref="S4.T1.14.12.4.m1.1.1.2.cmml">14.58</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.14.12.4.m1.1b"><apply id="S4.T1.14.12.4.m1.1.1.cmml" xref="S4.T1.14.12.4.m1.1.1"><minus id="S4.T1.14.12.4.m1.1.1.1.cmml" xref="S4.T1.14.12.4.m1.1.1"></minus><cn id="S4.T1.14.12.4.m1.1.1.2.cmml" type="float" xref="S4.T1.14.12.4.m1.1.1.2">14.58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.12.4.m1.1c">-14.58</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.12.4.m1.1d">- 14.58</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Impact of selecting <span class="ltx_text ltx_font_bold" id="S4.T2.16.1">5 few shot</span> samples using TST vs. Pre-trained Model with and without API Function Definitions using GPT4 model. All results are shown as <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mi id="S4.T2.2.m1.1.1" mathvariant="normal" xref="S4.T2.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><ci id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">roman_Δ</annotation></semantics></math> improvements compared to the baseline. The baseline uses Pre-Trained Transformer Model without API Function Definitions. For Avg. similarity, higher is better, and for the rest of metrics capturing failure rates, lower is better.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.14.13.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.14.13.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.14.13.1.2">Avg. Similarity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.14.13.1.3">%Unparsed flows</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.14.13.1.4">%made-up API names</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.14.13.1.5">%made-up API parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.6.4.5"><span class="ltx_text ltx_font_typewriter" id="S4.T2.6.4.5.1">Pre-trained Model + FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1"><math alttext="0" class="ltx_Math" display="inline" id="S4.T2.3.1.1.m1.1"><semantics id="S4.T2.3.1.1.m1.1a"><mn id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><cn id="S4.T2.3.1.1.m1.1.1.cmml" type="integer" xref="S4.T2.3.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.2.2"><math alttext="+2.75" class="ltx_Math" display="inline" id="S4.T2.4.2.2.m1.1"><semantics id="S4.T2.4.2.2.m1.1a"><mrow id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml"><mo id="S4.T2.4.2.2.m1.1.1a" xref="S4.T2.4.2.2.m1.1.1.cmml">+</mo><mn id="S4.T2.4.2.2.m1.1.1.2" xref="S4.T2.4.2.2.m1.1.1.2.cmml">2.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><apply id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1"><plus id="S4.T2.4.2.2.m1.1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1"></plus><cn id="S4.T2.4.2.2.m1.1.1.2.cmml" type="float" xref="S4.T2.4.2.2.m1.1.1.2">2.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">+2.75</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.2.2.m1.1d">+ 2.75</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.3.3"><math alttext="-4.3" class="ltx_Math" display="inline" id="S4.T2.5.3.3.m1.1"><semantics id="S4.T2.5.3.3.m1.1a"><mrow id="S4.T2.5.3.3.m1.1.1" xref="S4.T2.5.3.3.m1.1.1.cmml"><mo id="S4.T2.5.3.3.m1.1.1a" xref="S4.T2.5.3.3.m1.1.1.cmml">−</mo><mn id="S4.T2.5.3.3.m1.1.1.2" xref="S4.T2.5.3.3.m1.1.1.2.cmml">4.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><apply id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1"><minus id="S4.T2.5.3.3.m1.1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1"></minus><cn id="S4.T2.5.3.3.m1.1.1.2.cmml" type="float" xref="S4.T2.5.3.3.m1.1.1.2">4.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">-4.3</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.3.3.m1.1d">- 4.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.4"><math alttext="-20.16" class="ltx_Math" display="inline" id="S4.T2.6.4.4.m1.1"><semantics id="S4.T2.6.4.4.m1.1a"><mrow id="S4.T2.6.4.4.m1.1.1" xref="S4.T2.6.4.4.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T2.6.4.4.m1.1.1a" mathvariant="bold" xref="S4.T2.6.4.4.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T2.6.4.4.m1.1.1.2" mathvariant="bold" xref="S4.T2.6.4.4.m1.1.1.2.cmml">20.16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.4.m1.1b"><apply id="S4.T2.6.4.4.m1.1.1.cmml" xref="S4.T2.6.4.4.m1.1.1"><minus id="S4.T2.6.4.4.m1.1.1.1.cmml" xref="S4.T2.6.4.4.m1.1.1"></minus><cn id="S4.T2.6.4.4.m1.1.1.2.cmml" type="float" xref="S4.T2.6.4.4.m1.1.1.2">20.16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.4.m1.1c">-20.16</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.4.4.m1.1d">bold_- bold_20.16</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.10.8.5"><span class="ltx_text ltx_font_typewriter" id="S4.T2.10.8.5.1">TST wo FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.5.1"><math alttext="+0.02" class="ltx_Math" display="inline" id="S4.T2.7.5.1.m1.1"><semantics id="S4.T2.7.5.1.m1.1a"><mrow id="S4.T2.7.5.1.m1.1.1" xref="S4.T2.7.5.1.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T2.7.5.1.m1.1.1a" mathvariant="bold" xref="S4.T2.7.5.1.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T2.7.5.1.m1.1.1.2" mathvariant="bold" xref="S4.T2.7.5.1.m1.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.5.1.m1.1b"><apply id="S4.T2.7.5.1.m1.1.1.cmml" xref="S4.T2.7.5.1.m1.1.1"><plus id="S4.T2.7.5.1.m1.1.1.1.cmml" xref="S4.T2.7.5.1.m1.1.1"></plus><cn id="S4.T2.7.5.1.m1.1.1.2.cmml" type="float" xref="S4.T2.7.5.1.m1.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.5.1.m1.1c">+0.02</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.5.1.m1.1d">bold_+ bold_0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.6.2"><math alttext="-0.61" class="ltx_Math" display="inline" id="S4.T2.8.6.2.m1.1"><semantics id="S4.T2.8.6.2.m1.1a"><mrow id="S4.T2.8.6.2.m1.1.1" xref="S4.T2.8.6.2.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T2.8.6.2.m1.1.1a" mathvariant="bold" xref="S4.T2.8.6.2.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T2.8.6.2.m1.1.1.2" mathvariant="bold" xref="S4.T2.8.6.2.m1.1.1.2.cmml">0.61</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.6.2.m1.1b"><apply id="S4.T2.8.6.2.m1.1.1.cmml" xref="S4.T2.8.6.2.m1.1.1"><minus id="S4.T2.8.6.2.m1.1.1.1.cmml" xref="S4.T2.8.6.2.m1.1.1"></minus><cn id="S4.T2.8.6.2.m1.1.1.2.cmml" type="float" xref="S4.T2.8.6.2.m1.1.1.2">0.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.6.2.m1.1c">-0.61</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.6.2.m1.1d">bold_- bold_0.61</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.9.7.3"><math alttext="-3.53" class="ltx_Math" display="inline" id="S4.T2.9.7.3.m1.1"><semantics id="S4.T2.9.7.3.m1.1a"><mrow id="S4.T2.9.7.3.m1.1.1" xref="S4.T2.9.7.3.m1.1.1.cmml"><mo id="S4.T2.9.7.3.m1.1.1a" xref="S4.T2.9.7.3.m1.1.1.cmml">−</mo><mn id="S4.T2.9.7.3.m1.1.1.2" xref="S4.T2.9.7.3.m1.1.1.2.cmml">3.53</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.7.3.m1.1b"><apply id="S4.T2.9.7.3.m1.1.1.cmml" xref="S4.T2.9.7.3.m1.1.1"><minus id="S4.T2.9.7.3.m1.1.1.1.cmml" xref="S4.T2.9.7.3.m1.1.1"></minus><cn id="S4.T2.9.7.3.m1.1.1.2.cmml" type="float" xref="S4.T2.9.7.3.m1.1.1.2">3.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.7.3.m1.1c">-3.53</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.7.3.m1.1d">- 3.53</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.10.8.4"><math alttext="-1.04" class="ltx_Math" display="inline" id="S4.T2.10.8.4.m1.1"><semantics id="S4.T2.10.8.4.m1.1a"><mrow id="S4.T2.10.8.4.m1.1.1" xref="S4.T2.10.8.4.m1.1.1.cmml"><mo id="S4.T2.10.8.4.m1.1.1a" xref="S4.T2.10.8.4.m1.1.1.cmml">−</mo><mn id="S4.T2.10.8.4.m1.1.1.2" xref="S4.T2.10.8.4.m1.1.1.2.cmml">1.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.8.4.m1.1b"><apply id="S4.T2.10.8.4.m1.1.1.cmml" xref="S4.T2.10.8.4.m1.1.1"><minus id="S4.T2.10.8.4.m1.1.1.1.cmml" xref="S4.T2.10.8.4.m1.1.1"></minus><cn id="S4.T2.10.8.4.m1.1.1.2.cmml" type="float" xref="S4.T2.10.8.4.m1.1.1.2">1.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.8.4.m1.1c">-1.04</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.8.4.m1.1d">- 1.04</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.14.12.5"><span class="ltx_text ltx_font_typewriter" id="S4.T2.14.12.5.1">TST + FD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.11.9.1"><math alttext="+0.02" class="ltx_Math" display="inline" id="S4.T2.11.9.1.m1.1"><semantics id="S4.T2.11.9.1.m1.1a"><mrow id="S4.T2.11.9.1.m1.1.1" xref="S4.T2.11.9.1.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T2.11.9.1.m1.1.1a" mathvariant="bold" xref="S4.T2.11.9.1.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T2.11.9.1.m1.1.1.2" mathvariant="bold" xref="S4.T2.11.9.1.m1.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.9.1.m1.1b"><apply id="S4.T2.11.9.1.m1.1.1.cmml" xref="S4.T2.11.9.1.m1.1.1"><plus id="S4.T2.11.9.1.m1.1.1.1.cmml" xref="S4.T2.11.9.1.m1.1.1"></plus><cn id="S4.T2.11.9.1.m1.1.1.2.cmml" type="float" xref="S4.T2.11.9.1.m1.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.9.1.m1.1c">+0.02</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.9.1.m1.1d">bold_+ bold_0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.10.2"><math alttext="+0.68" class="ltx_Math" display="inline" id="S4.T2.12.10.2.m1.1"><semantics id="S4.T2.12.10.2.m1.1a"><mrow id="S4.T2.12.10.2.m1.1.1" xref="S4.T2.12.10.2.m1.1.1.cmml"><mo id="S4.T2.12.10.2.m1.1.1a" xref="S4.T2.12.10.2.m1.1.1.cmml">+</mo><mn id="S4.T2.12.10.2.m1.1.1.2" xref="S4.T2.12.10.2.m1.1.1.2.cmml">0.68</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.10.2.m1.1b"><apply id="S4.T2.12.10.2.m1.1.1.cmml" xref="S4.T2.12.10.2.m1.1.1"><plus id="S4.T2.12.10.2.m1.1.1.1.cmml" xref="S4.T2.12.10.2.m1.1.1"></plus><cn id="S4.T2.12.10.2.m1.1.1.2.cmml" type="float" xref="S4.T2.12.10.2.m1.1.1.2">0.68</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.10.2.m1.1c">+0.68</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.10.2.m1.1d">+ 0.68</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.13.11.3"><math alttext="-6.29" class="ltx_Math" display="inline" id="S4.T2.13.11.3.m1.1"><semantics id="S4.T2.13.11.3.m1.1a"><mrow id="S4.T2.13.11.3.m1.1.1" xref="S4.T2.13.11.3.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T2.13.11.3.m1.1.1a" mathvariant="bold" xref="S4.T2.13.11.3.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T2.13.11.3.m1.1.1.2" mathvariant="bold" xref="S4.T2.13.11.3.m1.1.1.2.cmml">6.29</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.13.11.3.m1.1b"><apply id="S4.T2.13.11.3.m1.1.1.cmml" xref="S4.T2.13.11.3.m1.1.1"><minus id="S4.T2.13.11.3.m1.1.1.1.cmml" xref="S4.T2.13.11.3.m1.1.1"></minus><cn id="S4.T2.13.11.3.m1.1.1.2.cmml" type="float" xref="S4.T2.13.11.3.m1.1.1.2">6.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.11.3.m1.1c">-6.29</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.11.3.m1.1d">bold_- bold_6.29</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.14.12.4"><math alttext="-19.99" class="ltx_Math" display="inline" id="S4.T2.14.12.4.m1.1"><semantics id="S4.T2.14.12.4.m1.1a"><mrow id="S4.T2.14.12.4.m1.1.1" xref="S4.T2.14.12.4.m1.1.1.cmml"><mo id="S4.T2.14.12.4.m1.1.1a" xref="S4.T2.14.12.4.m1.1.1.cmml">−</mo><mn id="S4.T2.14.12.4.m1.1.1.2" xref="S4.T2.14.12.4.m1.1.1.2.cmml">19.99</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.14.12.4.m1.1b"><apply id="S4.T2.14.12.4.m1.1.1.cmml" xref="S4.T2.14.12.4.m1.1.1"><minus id="S4.T2.14.12.4.m1.1.1.1.cmml" xref="S4.T2.14.12.4.m1.1.1"></minus><cn id="S4.T2.14.12.4.m1.1.1.2.cmml" type="float" xref="S4.T2.14.12.4.m1.1.1.2">19.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.12.4.m1.1c">-19.99</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.12.4.m1.1d">- 19.99</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Impact of selecting <span class="ltx_text ltx_font_bold" id="S4.T3.16.1">20 few shot</span> samples using TST vs. Pre-trained Model with and without function definitions using GPT4 model. All results are shown as <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mi id="S4.T3.2.m1.1.1" mathvariant="normal" xref="S4.T3.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><ci id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">roman_Δ</annotation></semantics></math> improvements compared to the baseline. The baseline uses Pre-Trained Transformer Model without API Function Definitions. For Avg. similarity, higher is better, and for the rest of metrics capturing failure rates, lower is better.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.14.13.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.14.13.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.14.13.1.2">Avg. Similarity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.14.13.1.3">%Unparsed flows</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.14.13.1.4">%made-up API names</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.14.13.1.5">%made-up API parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.6.4.5"><span class="ltx_text ltx_font_typewriter" id="S4.T3.6.4.5.1">Pre-trained Model + FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.1"><math alttext="-0.01" class="ltx_Math" display="inline" id="S4.T3.3.1.1.m1.1"><semantics id="S4.T3.3.1.1.m1.1a"><mrow id="S4.T3.3.1.1.m1.1.1" xref="S4.T3.3.1.1.m1.1.1.cmml"><mo id="S4.T3.3.1.1.m1.1.1a" xref="S4.T3.3.1.1.m1.1.1.cmml">−</mo><mn id="S4.T3.3.1.1.m1.1.1.2" xref="S4.T3.3.1.1.m1.1.1.2.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.m1.1b"><apply id="S4.T3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1"><minus id="S4.T3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1"></minus><cn id="S4.T3.3.1.1.m1.1.1.2.cmml" type="float" xref="S4.T3.3.1.1.m1.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.m1.1c">-0.01</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.1.1.m1.1d">- 0.01</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2"><math alttext="+2.29" class="ltx_Math" display="inline" id="S4.T3.4.2.2.m1.1"><semantics id="S4.T3.4.2.2.m1.1a"><mrow id="S4.T3.4.2.2.m1.1.1" xref="S4.T3.4.2.2.m1.1.1.cmml"><mo id="S4.T3.4.2.2.m1.1.1a" xref="S4.T3.4.2.2.m1.1.1.cmml">+</mo><mn id="S4.T3.4.2.2.m1.1.1.2" xref="S4.T3.4.2.2.m1.1.1.2.cmml">2.29</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.m1.1b"><apply id="S4.T3.4.2.2.m1.1.1.cmml" xref="S4.T3.4.2.2.m1.1.1"><plus id="S4.T3.4.2.2.m1.1.1.1.cmml" xref="S4.T3.4.2.2.m1.1.1"></plus><cn id="S4.T3.4.2.2.m1.1.1.2.cmml" type="float" xref="S4.T3.4.2.2.m1.1.1.2">2.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.m1.1c">+2.29</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.2.2.m1.1d">+ 2.29</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.3.3"><math alttext="-2.17" class="ltx_Math" display="inline" id="S4.T3.5.3.3.m1.1"><semantics id="S4.T3.5.3.3.m1.1a"><mrow id="S4.T3.5.3.3.m1.1.1" xref="S4.T3.5.3.3.m1.1.1.cmml"><mo id="S4.T3.5.3.3.m1.1.1a" xref="S4.T3.5.3.3.m1.1.1.cmml">−</mo><mn id="S4.T3.5.3.3.m1.1.1.2" xref="S4.T3.5.3.3.m1.1.1.2.cmml">2.17</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.m1.1b"><apply id="S4.T3.5.3.3.m1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1"><minus id="S4.T3.5.3.3.m1.1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1"></minus><cn id="S4.T3.5.3.3.m1.1.1.2.cmml" type="float" xref="S4.T3.5.3.3.m1.1.1.2">2.17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.m1.1c">-2.17</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.3.3.m1.1d">- 2.17</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.4.4"><math alttext="-6.93" class="ltx_Math" display="inline" id="S4.T3.6.4.4.m1.1"><semantics id="S4.T3.6.4.4.m1.1a"><mrow id="S4.T3.6.4.4.m1.1.1" xref="S4.T3.6.4.4.m1.1.1.cmml"><mo id="S4.T3.6.4.4.m1.1.1a" xref="S4.T3.6.4.4.m1.1.1.cmml">−</mo><mn id="S4.T3.6.4.4.m1.1.1.2" xref="S4.T3.6.4.4.m1.1.1.2.cmml">6.93</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.m1.1b"><apply id="S4.T3.6.4.4.m1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1"><minus id="S4.T3.6.4.4.m1.1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1"></minus><cn id="S4.T3.6.4.4.m1.1.1.2.cmml" type="float" xref="S4.T3.6.4.4.m1.1.1.2">6.93</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.m1.1c">-6.93</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.4.4.m1.1d">- 6.93</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.10.8.5"><span class="ltx_text ltx_font_typewriter" id="S4.T3.10.8.5.1">TST wo FD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.7.5.1"><math alttext="0" class="ltx_Math" display="inline" id="S4.T3.7.5.1.m1.1"><semantics id="S4.T3.7.5.1.m1.1a"><mn id="S4.T3.7.5.1.m1.1.1" xref="S4.T3.7.5.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.1.m1.1b"><cn id="S4.T3.7.5.1.m1.1.1.cmml" type="integer" xref="S4.T3.7.5.1.m1.1.1">0</cn></annotation-xml></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.6.2"><math alttext="+0.52" class="ltx_Math" display="inline" id="S4.T3.8.6.2.m1.1"><semantics id="S4.T3.8.6.2.m1.1a"><mrow id="S4.T3.8.6.2.m1.1.1" xref="S4.T3.8.6.2.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T3.8.6.2.m1.1.1a" mathvariant="bold" xref="S4.T3.8.6.2.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T3.8.6.2.m1.1.1.2" mathvariant="bold" xref="S4.T3.8.6.2.m1.1.1.2.cmml">0.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.2.m1.1b"><apply id="S4.T3.8.6.2.m1.1.1.cmml" xref="S4.T3.8.6.2.m1.1.1"><plus id="S4.T3.8.6.2.m1.1.1.1.cmml" xref="S4.T3.8.6.2.m1.1.1"></plus><cn id="S4.T3.8.6.2.m1.1.1.2.cmml" type="float" xref="S4.T3.8.6.2.m1.1.1.2">0.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.2.m1.1c">+0.52</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.6.2.m1.1d">bold_+ bold_0.52</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.7.3"><math alttext="-1.15" class="ltx_Math" display="inline" id="S4.T3.9.7.3.m1.1"><semantics id="S4.T3.9.7.3.m1.1a"><mrow id="S4.T3.9.7.3.m1.1.1" xref="S4.T3.9.7.3.m1.1.1.cmml"><mo id="S4.T3.9.7.3.m1.1.1a" xref="S4.T3.9.7.3.m1.1.1.cmml">−</mo><mn id="S4.T3.9.7.3.m1.1.1.2" xref="S4.T3.9.7.3.m1.1.1.2.cmml">1.15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.3.m1.1b"><apply id="S4.T3.9.7.3.m1.1.1.cmml" xref="S4.T3.9.7.3.m1.1.1"><minus id="S4.T3.9.7.3.m1.1.1.1.cmml" xref="S4.T3.9.7.3.m1.1.1"></minus><cn id="S4.T3.9.7.3.m1.1.1.2.cmml" type="float" xref="S4.T3.9.7.3.m1.1.1.2">1.15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.3.m1.1c">-1.15</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.7.3.m1.1d">- 1.15</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.8.4"><math alttext="+0.52" class="ltx_Math" display="inline" id="S4.T3.10.8.4.m1.1"><semantics id="S4.T3.10.8.4.m1.1a"><mrow id="S4.T3.10.8.4.m1.1.1" xref="S4.T3.10.8.4.m1.1.1.cmml"><mo id="S4.T3.10.8.4.m1.1.1a" xref="S4.T3.10.8.4.m1.1.1.cmml">+</mo><mn id="S4.T3.10.8.4.m1.1.1.2" xref="S4.T3.10.8.4.m1.1.1.2.cmml">0.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.8.4.m1.1b"><apply id="S4.T3.10.8.4.m1.1.1.cmml" xref="S4.T3.10.8.4.m1.1.1"><plus id="S4.T3.10.8.4.m1.1.1.1.cmml" xref="S4.T3.10.8.4.m1.1.1"></plus><cn id="S4.T3.10.8.4.m1.1.1.2.cmml" type="float" xref="S4.T3.10.8.4.m1.1.1.2">0.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.8.4.m1.1c">+0.52</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.8.4.m1.1d">+ 0.52</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.14.12.5"><span class="ltx_text ltx_font_typewriter" id="S4.T3.14.12.5.1">TST + FD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.11.9.1"><math alttext="+0.02" class="ltx_Math" display="inline" id="S4.T3.11.9.1.m1.1"><semantics id="S4.T3.11.9.1.m1.1a"><mrow id="S4.T3.11.9.1.m1.1.1" xref="S4.T3.11.9.1.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T3.11.9.1.m1.1.1a" mathvariant="bold" xref="S4.T3.11.9.1.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S4.T3.11.9.1.m1.1.1.2" mathvariant="bold" xref="S4.T3.11.9.1.m1.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.11.9.1.m1.1b"><apply id="S4.T3.11.9.1.m1.1.1.cmml" xref="S4.T3.11.9.1.m1.1.1"><plus id="S4.T3.11.9.1.m1.1.1.1.cmml" xref="S4.T3.11.9.1.m1.1.1"></plus><cn id="S4.T3.11.9.1.m1.1.1.2.cmml" type="float" xref="S4.T3.11.9.1.m1.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.9.1.m1.1c">+0.02</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.9.1.m1.1d">bold_+ bold_0.02</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.12.10.2"><math alttext="+0.83" class="ltx_Math" display="inline" id="S4.T3.12.10.2.m1.1"><semantics id="S4.T3.12.10.2.m1.1a"><mrow id="S4.T3.12.10.2.m1.1.1" xref="S4.T3.12.10.2.m1.1.1.cmml"><mo id="S4.T3.12.10.2.m1.1.1a" xref="S4.T3.12.10.2.m1.1.1.cmml">+</mo><mn id="S4.T3.12.10.2.m1.1.1.2" xref="S4.T3.12.10.2.m1.1.1.2.cmml">0.83</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.12.10.2.m1.1b"><apply id="S4.T3.12.10.2.m1.1.1.cmml" xref="S4.T3.12.10.2.m1.1.1"><plus id="S4.T3.12.10.2.m1.1.1.1.cmml" xref="S4.T3.12.10.2.m1.1.1"></plus><cn id="S4.T3.12.10.2.m1.1.1.2.cmml" type="float" xref="S4.T3.12.10.2.m1.1.1.2">0.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.10.2.m1.1c">+0.83</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.10.2.m1.1d">+ 0.83</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.13.11.3"><math alttext="-2.7" class="ltx_Math" display="inline" id="S4.T3.13.11.3.m1.1"><semantics id="S4.T3.13.11.3.m1.1a"><mrow id="S4.T3.13.11.3.m1.1.1" xref="S4.T3.13.11.3.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T3.13.11.3.m1.1.1a" mathvariant="bold" xref="S4.T3.13.11.3.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T3.13.11.3.m1.1.1.2" mathvariant="bold" xref="S4.T3.13.11.3.m1.1.1.2.cmml">2.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.13.11.3.m1.1b"><apply id="S4.T3.13.11.3.m1.1.1.cmml" xref="S4.T3.13.11.3.m1.1.1"><minus id="S4.T3.13.11.3.m1.1.1.1.cmml" xref="S4.T3.13.11.3.m1.1.1"></minus><cn id="S4.T3.13.11.3.m1.1.1.2.cmml" type="float" xref="S4.T3.13.11.3.m1.1.1.2">2.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.11.3.m1.1c">-2.7</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.11.3.m1.1d">bold_- bold_2.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.14.12.4"><math alttext="-7.06" class="ltx_Math" display="inline" id="S4.T3.14.12.4.m1.1"><semantics id="S4.T3.14.12.4.m1.1a"><mrow id="S4.T3.14.12.4.m1.1.1" xref="S4.T3.14.12.4.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S4.T3.14.12.4.m1.1.1a" mathvariant="bold" xref="S4.T3.14.12.4.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S4.T3.14.12.4.m1.1.1.2" mathvariant="bold" xref="S4.T3.14.12.4.m1.1.1.2.cmml">7.06</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.14.12.4.m1.1b"><apply id="S4.T3.14.12.4.m1.1.1.cmml" xref="S4.T3.14.12.4.m1.1.1"><minus id="S4.T3.14.12.4.m1.1.1.1.cmml" xref="S4.T3.14.12.4.m1.1.1"></minus><cn id="S4.T3.14.12.4.m1.1.1.2.cmml" type="float" xref="S4.T3.14.12.4.m1.1.1.2">7.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.12.4.m1.1c">-7.06</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.12.4.m1.1d">bold_- bold_7.06</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Unparsed rate</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">This metric captures the rate of syntactic errors. A flow that cannot be parsed by the parser is considered not usable for the purpose of similarity metric computation. Unparsed rate is computed as follow:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\%\mathrm{unparsed\ flows}=\frac{|\mathrm{Flows}_{\mathrm{unparsed}}|}{|%
\mathrm{Flows}_{\mathrm{total}}|}" class="ltx_math_unparsed" display="block" id="S4.E3.m1.2"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2b"><mo id="S4.E3.m1.2.3">%</mo><mi id="S4.E3.m1.2.4">unparsed</mi><mi id="S4.E3.m1.2.5">flows</mi><mo id="S4.E3.m1.2.6">=</mo><mfrac id="S4.E3.m1.2.2"><mrow id="S4.E3.m1.1.1.1.1"><mo id="S4.E3.m1.1.1.1.1.2" stretchy="false">|</mo><msub id="S4.E3.m1.1.1.1.1.1"><mi id="S4.E3.m1.1.1.1.1.1.2">Flows</mi><mi id="S4.E3.m1.1.1.1.1.1.3">unparsed</mi></msub><mo id="S4.E3.m1.1.1.1.1.3" stretchy="false">|</mo></mrow><mrow id="S4.E3.m1.2.2.2.1"><mo id="S4.E3.m1.2.2.2.1.2" stretchy="false">|</mo><msub id="S4.E3.m1.2.2.2.1.1"><mi id="S4.E3.m1.2.2.2.1.1.2">Flows</mi><mi id="S4.E3.m1.2.2.2.1.1.3">total</mi></msub><mo id="S4.E3.m1.2.2.2.1.3" stretchy="false">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex" id="S4.E3.m1.2c">\%\mathrm{unparsed\ flows}=\frac{|\mathrm{Flows}_{\mathrm{unparsed}}|}{|%
\mathrm{Flows}_{\mathrm{total}}|}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.2d">% roman_unparsed roman_flows = divide start_ARG | roman_Flows start_POSTSUBSCRIPT roman_unparsed end_POSTSUBSCRIPT | end_ARG start_ARG | roman_Flows start_POSTSUBSCRIPT roman_total end_POSTSUBSCRIPT | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS2.p2.2">where, <math alttext="|\mathrm{Flows}_{\mathrm{unparsed}}|" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.1.m1.1"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mrow id="S4.SS2.SSS2.p2.1.m1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml"><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.2.cmml">Flows</mi><mi id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.3" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.3.cmml">unparsed</mi></msub><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><apply id="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1"><abs id="S4.SS2.SSS2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.2"></abs><apply id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.2">Flows</ci><ci id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.3">unparsed</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">|\mathrm{Flows}_{\mathrm{unparsed}}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.1.m1.1d">| roman_Flows start_POSTSUBSCRIPT roman_unparsed end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of flows that were not parsed and <math alttext="|\mathrm{Flows}_{\mathrm{total}}|" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.2.m2.1"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mrow id="S4.SS2.SSS2.p2.2.m2.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml"><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1.2" stretchy="false" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS2.p2.2.m2.1.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.2.cmml">Flows</mi><mi id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.3" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.3.cmml">total</mi></msub><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1.3" stretchy="false" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><apply id="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1"><abs id="S4.SS2.SSS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.2"></abs><apply id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.2">Flows</ci><ci id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.3">total</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">|\mathrm{Flows}_{\mathrm{total}}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.2.m2.1d">| roman_Flows start_POSTSUBSCRIPT roman_total end_POSTSUBSCRIPT |</annotation></semantics></math>is the total number of flows in the sample set.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Hallucination rate</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">This metric captures the rate of made-up APIs (or function names) and made-up parameter keys in the generated code. Predicting a flow with a hallucinated API name is counted as a failure and leads to the code being considered invalid.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">We compute this by counting the number of flows that have at least one hallucinated function name and divide it by the total number of flows in the sample set.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\%\mathrm{made-up\ APIs}=\frac{|\mathrm{Flows}_{h}|}{|\mathrm{Flows}_{\mathrm{%
parsed}}|}*100" class="ltx_math_unparsed" display="block" id="S4.E4.m1.2"><semantics id="S4.E4.m1.2a"><mrow id="S4.E4.m1.2b"><mo id="S4.E4.m1.2.3">%</mo><mi id="S4.E4.m1.2.4">made</mi><mo id="S4.E4.m1.2.5">−</mo><mi id="S4.E4.m1.2.6">up</mi><mi id="S4.E4.m1.2.7">APIs</mi><mo id="S4.E4.m1.2.8">=</mo><mfrac id="S4.E4.m1.2.2"><mrow id="S4.E4.m1.1.1.1.1"><mo id="S4.E4.m1.1.1.1.1.2" stretchy="false">|</mo><msub id="S4.E4.m1.1.1.1.1.1"><mi id="S4.E4.m1.1.1.1.1.1.2">Flows</mi><mi id="S4.E4.m1.1.1.1.1.1.3">h</mi></msub><mo id="S4.E4.m1.1.1.1.1.3" stretchy="false">|</mo></mrow><mrow id="S4.E4.m1.2.2.2.1"><mo id="S4.E4.m1.2.2.2.1.2" stretchy="false">|</mo><msub id="S4.E4.m1.2.2.2.1.1"><mi id="S4.E4.m1.2.2.2.1.1.2">Flows</mi><mi id="S4.E4.m1.2.2.2.1.1.3">parsed</mi></msub><mo id="S4.E4.m1.2.2.2.1.3" stretchy="false">|</mo></mrow></mfrac><mo id="S4.E4.m1.2.9" lspace="0.222em" rspace="0.222em">∗</mo><mn id="S4.E4.m1.2.10">100</mn></mrow><annotation encoding="application/x-tex" id="S4.E4.m1.2c">\%\mathrm{made-up\ APIs}=\frac{|\mathrm{Flows}_{h}|}{|\mathrm{Flows}_{\mathrm{%
parsed}}|}*100</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.2d">% roman_made - roman_up roman_APIs = divide start_ARG | roman_Flows start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | end_ARG start_ARG | roman_Flows start_POSTSUBSCRIPT roman_parsed end_POSTSUBSCRIPT | end_ARG ∗ 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.2">where <math alttext="|\mathrm{Flows}_{h}|" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p4.1.m1.1"><semantics id="S4.SS2.SSS3.p4.1.m1.1a"><mrow id="S4.SS2.SSS3.p4.1.m1.1.1.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.2.cmml"><mo id="S4.SS2.SSS3.p4.1.m1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p4.1.m1.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS3.p4.1.m1.1.1.1.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.2" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1.2.cmml">Flows</mi><mi id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.3" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1.3.cmml">h</mi></msub><mo id="S4.SS2.SSS3.p4.1.m1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p4.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.1.m1.1b"><apply id="S4.SS2.SSS3.p4.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1"><abs id="S4.SS2.SSS3.p4.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.2"></abs><apply id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1.2">Flows</ci><ci id="S4.SS2.SSS3.p4.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.1.1.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.1.m1.1c">|\mathrm{Flows}_{h}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p4.1.m1.1d">| roman_Flows start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of flows with hallucinated API names and <math alttext="|\mathrm{Flows}_{\mathrm{parsed}}|" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p4.2.m2.1"><semantics id="S4.SS2.SSS3.p4.2.m2.1a"><mrow id="S4.SS2.SSS3.p4.2.m2.1.1.1" xref="S4.SS2.SSS3.p4.2.m2.1.1.2.cmml"><mo id="S4.SS2.SSS3.p4.2.m2.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p4.2.m2.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS3.p4.2.m2.1.1.1.1" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.2" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1.2.cmml">Flows</mi><mi id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.3" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1.3.cmml">parsed</mi></msub><mo id="S4.SS2.SSS3.p4.2.m2.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p4.2.m2.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.2.m2.1b"><apply id="S4.SS2.SSS3.p4.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1"><abs id="S4.SS2.SSS3.p4.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.2"></abs><apply id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1.2">Flows</ci><ci id="S4.SS2.SSS3.p4.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.1.3">parsed</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.2.m2.1c">|\mathrm{Flows}_{\mathrm{parsed}}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p4.2.m2.1d">| roman_Flows start_POSTSUBSCRIPT roman_parsed end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of flows that were parsed correctly.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1">Similarly, we compute the rate at which parameters were not parsed. Failure to parse parameters does not result in the flow being discounted from average similarity computation. However, it shows up as run-time errors. Fixing these run-time errors is beyond the scope of this paper.</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\%\mathrm{made-up\ parameters}=\frac{|\mathrm{Flows}_{hp}|}{|\mathrm{Flows}_{%
\mathrm{parsed}}|}*100" class="ltx_math_unparsed" display="block" id="S4.E5.m1.2"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2b"><mo id="S4.E5.m1.2.3">%</mo><mi id="S4.E5.m1.2.4">made</mi><mo id="S4.E5.m1.2.5">−</mo><mi id="S4.E5.m1.2.6">up</mi><mi id="S4.E5.m1.2.7">parameters</mi><mo id="S4.E5.m1.2.8">=</mo><mfrac id="S4.E5.m1.2.2"><mrow id="S4.E5.m1.1.1.1.1"><mo id="S4.E5.m1.1.1.1.1.2" stretchy="false">|</mo><msub id="S4.E5.m1.1.1.1.1.1"><mi id="S4.E5.m1.1.1.1.1.1.2">Flows</mi><mrow id="S4.E5.m1.1.1.1.1.1.3"><mi id="S4.E5.m1.1.1.1.1.1.3.2">h</mi><mo id="S4.E5.m1.1.1.1.1.1.3.1">⁢</mo><mi id="S4.E5.m1.1.1.1.1.1.3.3">p</mi></mrow></msub><mo id="S4.E5.m1.1.1.1.1.3" stretchy="false">|</mo></mrow><mrow id="S4.E5.m1.2.2.2.1"><mo id="S4.E5.m1.2.2.2.1.2" stretchy="false">|</mo><msub id="S4.E5.m1.2.2.2.1.1"><mi id="S4.E5.m1.2.2.2.1.1.2">Flows</mi><mi id="S4.E5.m1.2.2.2.1.1.3">parsed</mi></msub><mo id="S4.E5.m1.2.2.2.1.3" stretchy="false">|</mo></mrow></mfrac><mo id="S4.E5.m1.2.9" lspace="0.222em" rspace="0.222em">∗</mo><mn id="S4.E5.m1.2.10">100</mn></mrow><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\%\mathrm{made-up\ parameters}=\frac{|\mathrm{Flows}_{hp}|}{|\mathrm{Flows}_{%
\mathrm{parsed}}|}*100</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.2d">% roman_made - roman_up roman_parameters = divide start_ARG | roman_Flows start_POSTSUBSCRIPT italic_h italic_p end_POSTSUBSCRIPT | end_ARG start_ARG | roman_Flows start_POSTSUBSCRIPT roman_parsed end_POSTSUBSCRIPT | end_ARG ∗ 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p6">
<p class="ltx_p" id="S4.SS2.SSS3.p6.2">where, <math alttext="|\mathrm{Flows}_{hp}|" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p6.1.m1.1"><semantics id="S4.SS2.SSS3.p6.1.m1.1a"><mrow id="S4.SS2.SSS3.p6.1.m1.1.1.1" xref="S4.SS2.SSS3.p6.1.m1.1.1.2.cmml"><mo id="S4.SS2.SSS3.p6.1.m1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p6.1.m1.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS3.p6.1.m1.1.1.1.1" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.2" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.2.cmml">Flows</mi><mrow id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.cmml"><mi id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.2" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.2.cmml">h</mi><mo id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.1" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.3" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.3.cmml">p</mi></mrow></msub><mo id="S4.SS2.SSS3.p6.1.m1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p6.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p6.1.m1.1b"><apply id="S4.SS2.SSS3.p6.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1"><abs id="S4.SS2.SSS3.p6.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.2"></abs><apply id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.2">Flows</ci><apply id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3"><times id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.1.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.1"></times><ci id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.2.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.2">ℎ</ci><ci id="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.3.cmml" xref="S4.SS2.SSS3.p6.1.m1.1.1.1.1.3.3">𝑝</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p6.1.m1.1c">|\mathrm{Flows}_{hp}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p6.1.m1.1d">| roman_Flows start_POSTSUBSCRIPT italic_h italic_p end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of flows with hallucinated parameter key names and <math alttext="|\mathrm{Flows}_{\mathrm{parsed}}|" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p6.2.m2.1"><semantics id="S4.SS2.SSS3.p6.2.m2.1a"><mrow id="S4.SS2.SSS3.p6.2.m2.1.1.1" xref="S4.SS2.SSS3.p6.2.m2.1.1.2.cmml"><mo id="S4.SS2.SSS3.p6.2.m2.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p6.2.m2.1.1.2.1.cmml">|</mo><msub id="S4.SS2.SSS3.p6.2.m2.1.1.1.1" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.2" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1.2.cmml">Flows</mi><mi id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.3" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1.3.cmml">parsed</mi></msub><mo id="S4.SS2.SSS3.p6.2.m2.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p6.2.m2.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p6.2.m2.1b"><apply id="S4.SS2.SSS3.p6.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1"><abs id="S4.SS2.SSS3.p6.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.2"></abs><apply id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1.2">Flows</ci><ci id="S4.SS2.SSS3.p6.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p6.2.m2.1.1.1.1.3">parsed</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p6.2.m2.1c">|\mathrm{Flows}_{\mathrm{parsed}}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p6.2.m2.1d">| roman_Flows start_POSTSUBSCRIPT roman_parsed end_POSTSUBSCRIPT |</annotation></semantics></math> is the number of flows that were parsed correctly.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present the results of the above approaches on a test set of 1000 NL-DSL pairs. These samples, while generated synthetically, have been evaluated by human judges for quality. They were also sampled to represent the distribution of APIs in actual product usage.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We compare the impact of each ablation in sections below.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Impact of number of few-shots on RAG performance</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We compare the impact of number of code samples added to the meta prompt with two different settings i.e. 5 few-shots vs 20 few-shots. We measured the results for both Pre-Trained model as well as TST model. Results are shared in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.T1" title="Table 1 ‣ 4.2.1. Average Similarity ‣ 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a> and show the <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S5.SS1.p1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">roman_Δ</annotation></semantics></math> change compared to that Baseline model. The baseline setting here is Pre-Trained model with 5 few-shots.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Looking at row 1 and comparing rows 2 and 3 with respect to the baseline , we can see that adding more few-shots improves the performance of both the Pre-Trained as well as the TST model on all metrics. The gain is particularly pronounced for reducing the number of made-up API names as well as reducing the number of made-up API parameter keys. We saw the gain plateau beyond this, and we intend to run more experiments in the future to study this effect better.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Impact of adding API or tool related metadata on performance (with GPT-4 model and 20 few shots). FD refers to including only metadata for APIs present in few-shots. SFD refers to extracting APIs similar to the input query (Refer to Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3" title="3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a>) for details. The baseline uses fine-tuned Codex model. For Avg. similarity, higher value is better, and for the rest of metrics capturing failure rates, lower is better. </figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.12.13.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.12.13.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.12.13.1.2">Avg. Similarity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.12.13.1.3">%Unparsed flows</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.12.13.1.4">%made-up API names</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.12.13.1.5">%made-up API parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.4.5"><span class="ltx_text ltx_font_typewriter" id="S5.T4.4.4.5.1">TST + FD</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.1"><math alttext="0" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mn id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">𝟎</mn><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><cn id="S5.T4.1.1.1.m1.1.1.cmml" type="integer" xref="S5.T4.1.1.1.m1.1.1">0</cn></annotation-xml><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1c">bold_0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.2"><math alttext="-5.3" class="ltx_Math" display="inline" id="S5.T4.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.m1.1a"><mrow id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S5.T4.2.2.2.m1.1.1a" mathvariant="bold" xref="S5.T4.2.2.2.m1.1.1.cmml">−</mo><mn class="ltx_mathvariant_bold" id="S5.T4.2.2.2.m1.1.1.2" mathvariant="bold" xref="S5.T4.2.2.2.m1.1.1.2.cmml">5.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><apply id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"><minus id="S5.T4.2.2.2.m1.1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"></minus><cn id="S5.T4.2.2.2.m1.1.1.2.cmml" type="float" xref="S5.T4.2.2.2.m1.1.1.2">5.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">-5.3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.m1.1d">bold_- bold_5.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3"><math alttext="+1.7" class="ltx_Math" display="inline" id="S5.T4.3.3.3.m1.1"><semantics id="S5.T4.3.3.3.m1.1a"><mrow id="S5.T4.3.3.3.m1.1.1" xref="S5.T4.3.3.3.m1.1.1.cmml"><mo id="S5.T4.3.3.3.m1.1.1a" xref="S5.T4.3.3.3.m1.1.1.cmml">+</mo><mn id="S5.T4.3.3.3.m1.1.1.2" xref="S5.T4.3.3.3.m1.1.1.2.cmml">1.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.m1.1b"><apply id="S5.T4.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.m1.1.1"><plus id="S5.T4.3.3.3.m1.1.1.1.cmml" xref="S5.T4.3.3.3.m1.1.1"></plus><cn id="S5.T4.3.3.3.m1.1.1.2.cmml" type="float" xref="S5.T4.3.3.3.m1.1.1.2">1.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.m1.1c">+1.7</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.m1.1d">+ 1.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.4"><math alttext="+1.11" class="ltx_Math" display="inline" id="S5.T4.4.4.4.m1.1"><semantics id="S5.T4.4.4.4.m1.1a"><mrow id="S5.T4.4.4.4.m1.1.1" xref="S5.T4.4.4.4.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S5.T4.4.4.4.m1.1.1a" mathvariant="bold" xref="S5.T4.4.4.4.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S5.T4.4.4.4.m1.1.1.2" mathvariant="bold" xref="S5.T4.4.4.4.m1.1.1.2.cmml">1.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.m1.1b"><apply id="S5.T4.4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.4.m1.1.1"><plus id="S5.T4.4.4.4.m1.1.1.1.cmml" xref="S5.T4.4.4.4.m1.1.1"></plus><cn id="S5.T4.4.4.4.m1.1.1.2.cmml" type="float" xref="S5.T4.4.4.4.m1.1.1.2">1.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.m1.1c">+1.11</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.m1.1d">bold_+ bold_1.11</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.8.8.5"><span class="ltx_text ltx_font_typewriter" id="S5.T4.8.8.5.1">TST + SFD</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.1"><math alttext="-0.01" class="ltx_Math" display="inline" id="S5.T4.5.5.1.m1.1"><semantics id="S5.T4.5.5.1.m1.1a"><mrow id="S5.T4.5.5.1.m1.1.1" xref="S5.T4.5.5.1.m1.1.1.cmml"><mo id="S5.T4.5.5.1.m1.1.1a" xref="S5.T4.5.5.1.m1.1.1.cmml">−</mo><mn id="S5.T4.5.5.1.m1.1.1.2" xref="S5.T4.5.5.1.m1.1.1.2.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.1.m1.1b"><apply id="S5.T4.5.5.1.m1.1.1.cmml" xref="S5.T4.5.5.1.m1.1.1"><minus id="S5.T4.5.5.1.m1.1.1.1.cmml" xref="S5.T4.5.5.1.m1.1.1"></minus><cn id="S5.T4.5.5.1.m1.1.1.2.cmml" type="float" xref="S5.T4.5.5.1.m1.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.1.m1.1c">-0.01</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.1.m1.1d">- 0.01</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.6.6.2"><math alttext="-1.43" class="ltx_Math" display="inline" id="S5.T4.6.6.2.m1.1"><semantics id="S5.T4.6.6.2.m1.1a"><mrow id="S5.T4.6.6.2.m1.1.1" xref="S5.T4.6.6.2.m1.1.1.cmml"><mo id="S5.T4.6.6.2.m1.1.1a" xref="S5.T4.6.6.2.m1.1.1.cmml">−</mo><mn id="S5.T4.6.6.2.m1.1.1.2" xref="S5.T4.6.6.2.m1.1.1.2.cmml">1.43</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.2.m1.1b"><apply id="S5.T4.6.6.2.m1.1.1.cmml" xref="S5.T4.6.6.2.m1.1.1"><minus id="S5.T4.6.6.2.m1.1.1.1.cmml" xref="S5.T4.6.6.2.m1.1.1"></minus><cn id="S5.T4.6.6.2.m1.1.1.2.cmml" type="float" xref="S5.T4.6.6.2.m1.1.1.2">1.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.2.m1.1c">-1.43</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.2.m1.1d">- 1.43</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.7.7.3"><math alttext="+1.21" class="ltx_Math" display="inline" id="S5.T4.7.7.3.m1.1"><semantics id="S5.T4.7.7.3.m1.1a"><mrow id="S5.T4.7.7.3.m1.1.1" xref="S5.T4.7.7.3.m1.1.1.cmml"><mo id="S5.T4.7.7.3.m1.1.1a" xref="S5.T4.7.7.3.m1.1.1.cmml">+</mo><mn id="S5.T4.7.7.3.m1.1.1.2" xref="S5.T4.7.7.3.m1.1.1.2.cmml">1.21</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.3.m1.1b"><apply id="S5.T4.7.7.3.m1.1.1.cmml" xref="S5.T4.7.7.3.m1.1.1"><plus id="S5.T4.7.7.3.m1.1.1.1.cmml" xref="S5.T4.7.7.3.m1.1.1"></plus><cn id="S5.T4.7.7.3.m1.1.1.2.cmml" type="float" xref="S5.T4.7.7.3.m1.1.1.2">1.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.3.m1.1c">+1.21</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.3.m1.1d">+ 1.21</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.4"><math alttext="+6.76" class="ltx_Math" display="inline" id="S5.T4.8.8.4.m1.1"><semantics id="S5.T4.8.8.4.m1.1a"><mrow id="S5.T4.8.8.4.m1.1.1" xref="S5.T4.8.8.4.m1.1.1.cmml"><mo id="S5.T4.8.8.4.m1.1.1a" xref="S5.T4.8.8.4.m1.1.1.cmml">+</mo><mn id="S5.T4.8.8.4.m1.1.1.2" xref="S5.T4.8.8.4.m1.1.1.2.cmml">6.76</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.4.m1.1b"><apply id="S5.T4.8.8.4.m1.1.1.cmml" xref="S5.T4.8.8.4.m1.1.1"><plus id="S5.T4.8.8.4.m1.1.1.1.cmml" xref="S5.T4.8.8.4.m1.1.1"></plus><cn id="S5.T4.8.8.4.m1.1.1.2.cmml" type="float" xref="S5.T4.8.8.4.m1.1.1.2">6.76</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.4.m1.1c">+6.76</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.4.m1.1d">+ 6.76</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T4.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.12.12.5"><span class="ltx_text ltx_font_typewriter" id="S5.T4.12.12.5.1">TST + FD + SFD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.9.9.1"><math alttext="0" class="ltx_Math" display="inline" id="S5.T4.9.9.1.m1.1"><semantics id="S5.T4.9.9.1.m1.1a"><mn id="S5.T4.9.9.1.m1.1.1" xref="S5.T4.9.9.1.m1.1.1.cmml">𝟎</mn><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.1.m1.1b"><cn id="S5.T4.9.9.1.m1.1.1.cmml" type="integer" xref="S5.T4.9.9.1.m1.1.1">0</cn></annotation-xml><annotation encoding="application/x-llamapun" id="S5.T4.9.9.1.m1.1c">bold_0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.2"><math alttext="-2.74" class="ltx_Math" display="inline" id="S5.T4.10.10.2.m1.1"><semantics id="S5.T4.10.10.2.m1.1a"><mrow id="S5.T4.10.10.2.m1.1.1" xref="S5.T4.10.10.2.m1.1.1.cmml"><mo id="S5.T4.10.10.2.m1.1.1a" xref="S5.T4.10.10.2.m1.1.1.cmml">−</mo><mn id="S5.T4.10.10.2.m1.1.1.2" xref="S5.T4.10.10.2.m1.1.1.2.cmml">2.74</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.10.10.2.m1.1b"><apply id="S5.T4.10.10.2.m1.1.1.cmml" xref="S5.T4.10.10.2.m1.1.1"><minus id="S5.T4.10.10.2.m1.1.1.1.cmml" xref="S5.T4.10.10.2.m1.1.1"></minus><cn id="S5.T4.10.10.2.m1.1.1.2.cmml" type="float" xref="S5.T4.10.10.2.m1.1.1.2">2.74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.10.2.m1.1c">-2.74</annotation><annotation encoding="application/x-llamapun" id="S5.T4.10.10.2.m1.1d">- 2.74</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.11.11.3"><math alttext="+0.94" class="ltx_Math" display="inline" id="S5.T4.11.11.3.m1.1"><semantics id="S5.T4.11.11.3.m1.1a"><mrow id="S5.T4.11.11.3.m1.1.1" xref="S5.T4.11.11.3.m1.1.1.cmml"><mo class="ltx_mathvariant_bold" id="S5.T4.11.11.3.m1.1.1a" mathvariant="bold" xref="S5.T4.11.11.3.m1.1.1.cmml">+</mo><mn class="ltx_mathvariant_bold" id="S5.T4.11.11.3.m1.1.1.2" mathvariant="bold" xref="S5.T4.11.11.3.m1.1.1.2.cmml">0.94</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.11.11.3.m1.1b"><apply id="S5.T4.11.11.3.m1.1.1.cmml" xref="S5.T4.11.11.3.m1.1.1"><plus id="S5.T4.11.11.3.m1.1.1.1.cmml" xref="S5.T4.11.11.3.m1.1.1"></plus><cn id="S5.T4.11.11.3.m1.1.1.2.cmml" type="float" xref="S5.T4.11.11.3.m1.1.1.2">0.94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.11.11.3.m1.1c">+0.94</annotation><annotation encoding="application/x-llamapun" id="S5.T4.11.11.3.m1.1d">bold_+ bold_0.94</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.12.12.4"><math alttext="+2.03" class="ltx_Math" display="inline" id="S5.T4.12.12.4.m1.1"><semantics id="S5.T4.12.12.4.m1.1a"><mrow id="S5.T4.12.12.4.m1.1.1" xref="S5.T4.12.12.4.m1.1.1.cmml"><mo id="S5.T4.12.12.4.m1.1.1a" xref="S5.T4.12.12.4.m1.1.1.cmml">+</mo><mn id="S5.T4.12.12.4.m1.1.1.2" xref="S5.T4.12.12.4.m1.1.1.2.cmml">2.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.12.12.4.m1.1b"><apply id="S5.T4.12.12.4.m1.1.1.cmml" xref="S5.T4.12.12.4.m1.1.1"><plus id="S5.T4.12.12.4.m1.1.1.1.cmml" xref="S5.T4.12.12.4.m1.1.1"></plus><cn id="S5.T4.12.12.4.m1.1.1.2.cmml" type="float" xref="S5.T4.12.12.4.m1.1.1.2">2.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.12.12.4.m1.1c">+2.03</annotation><annotation encoding="application/x-llamapun" id="S5.T4.12.12.4.m1.1d">+ 2.03</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>TST vs Pre-trained Model</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Comparing the rows in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.T1" title="Table 1 ‣ 4.2.1. Average Similarity ‣ 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, both Pre-Trained and TST with 20 samples look comparable for computing the Average Similarity but have slight variations in Unparsed flow rate as well as Hallucinations rates. TST model performs better in reducing the <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">%</annotation></semantics></math> made-up API names, while the Pre-trained model has a slight edge in the other two metrics.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.2">So, we additionally look at the impact of including API Function Definitions to both the models (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.T2" title="Table 2 ‣ 4.2.1. Average Similarity ‣ 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>). Here, we have used GPT4 model with 5 few shots. The results are represented as <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S5.SS2.p2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">roman_Δ</annotation></semantics></math> changes compared to the Baseline setting i.e. using the Pre-Trained model to choose <math alttext="5" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn id="S5.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS2.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">5</annotation></semantics></math> few-shot NL-DSL code samples. TST with FD setting performs overall better than all other options with values close to the best in every metric.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We see a similar trend in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.T3" title="Table 3 ‣ 4.2.1. Average Similarity ‣ 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a> where we captured the results for <math alttext="20" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn id="S5.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p3.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">20</annotation></semantics></math> few-shots. This leads us to conclude that the presence of few-shot examples is supported by adding the API functions definitions of these functions (as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S3" title="3. Methodology ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a>). The addition predominantly helps reducing the hallucination rate for API names and parameters, which improves the overall response rate of NL2DSL generation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Function Definition vs Semantic Function Definitions</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">As the next step, we will compare the impact of Semantic Function Definitions (SFD) vs adding the API Function Definitions for selected examples only. We used a Fine-Tuned model as baseline for this experiment. Based on the insights from the previous step, we used 20 few-shots for TST along with including FDs. All results in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.T4" title="Table 4 ‣ 5.1. Impact of number of few-shots on RAG performance ‣ 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a> are shown as <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" mathvariant="normal" xref="S5.SS3.p1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">roman_Δ</annotation></semantics></math> improvements compared to the baseline.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">Looking at metrics in columns for <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mo id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">%</annotation></semantics></math> made-up API names and <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><mo id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">%</annotation></semantics></math> made-up parameter keys, we see that the hallucination rate is in general increasing for RAG based approach. However, we need to keep in mind that a fine-tuned model on the function names is hard to beat as it has been trained on 67,000 samples compared to only 20 few-shots that have been added to the RAG model.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Within the RAG approaches, comparing rows 1 and 2 (”TST + FD” vs ”TST + SFD”), SFD in general results in a slight drop in average similarity and an increase in the Unparse rate as well as hallucination rate for parameter keys. This indicates that the approach to simply add semantically similar API metadata for a query is not useful for DSL generation. We get better similarity, as well as reduced Hallucination Rate when we include the API Function Definitions for the samples selected by TST (as shown in Row 1).</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">The addition of Semantically matching Function Definitions tends to reduce the hallucination of API names indicating that it could have potential of adding FDs that are not a part of the code sample set. This could have implications for improving the performance for newly added APIs in the public cloud, that will help keep the performance of the system updated. We will explore this topic in a future study.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Concluding from the ablations study shared in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5" title="5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity ((Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.T4" title="Table 4 ‣ 5.1. Impact of number of few-shots on RAG performance ‣ 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>)).</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Fine-Tuning still outperforms the RAG based model in terms of lower hallucinations (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.T4" title="Table 4 ‣ 5.1. Impact of number of few-shots on RAG performance ‣ 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a> where fine-tuned model is the baseline). However, the parsing errors are more common in the fine-tuned model. This could be due to the fact that few shot examples have been successfully teaching the correct syntax to the LLM model. It is, however, surprising that the syntax correctness for RAG is better than that of the fine-tuned model which was trained on a much larger sample set.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">It is also interesting to note that this benefit does not transfer to hallucinated API names and their parameters keys where the fine-tuned model holds the advantage. The increase of 6.76 pts in hallucination rate for parameters due to adding Semantic Function definitions indicates that adding too many API descriptions can confuse rather than help the LLM (Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.T4" title="Table 4 ‣ 5.1. Impact of number of few-shots on RAG performance ‣ 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>). It also signifies the higher impact of the few shot samples for the scenario of DSL Generation or API selection compared to simply providing the API description. This learning can be used to inform the Tool Selection or orchestration scenario. Providing high quality examples of sample orchestration will reduce the failure rate more.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Overall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts. and that of parameter keys dropped by approx. 20 pts (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S4.T2" title="Table 2 ‣ 4.2.1. Average Similarity ‣ 4.2. DSL Generation Quality Metrics ‣ 4. Experiment Design and Metrics Definition ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>). The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02742v1#S5.T4" title="Table 4 ‣ 5.1. Impact of number of few-shots on RAG performance ‣ 5. Results ‣ A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>), with the potential to bootstrap quickly. Optimized RAG can also allow extending the benefits of metaprompt tuning to include unseen APIs, reducing the need to fine-tune the model frequently. This will be the focus of our future work.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan,
Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan,
Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp
Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024.

</span>
<span class="ltx_bibblock">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.14219

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2107.03374 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.03374" title="">https://arxiv.org/abs/2107.03374</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel rahman Mohamed, and Pushmeet Kohli. 2017.

</span>
<span class="ltx_bibblock">RobustFill: Neural Program Learning under Noisy I/O.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1703.07469

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.

</span>
<span class="ltx_bibblock">The Faiss library.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.08281 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.08281" title="">https://arxiv.org/abs/2401.08281</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 1536–1547.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.findings-emnlp.139" title="">https://doi.org/10.18653/v1/2020.findings-emnlp.139</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">PAL: Program-aided Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2211.10435

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.

</span>
<span class="ltx_bibblock">Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.09689 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2021.

</span>
<span class="ltx_bibblock">Jigsaw: Large Language Models meet Program Synthesis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.02969 [cs.SE]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.02969" title="">https://arxiv.org/abs/2112.02969</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023.

</span>
<span class="ltx_bibblock">Large Language Models are Zero-Shot Reasoners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.11916

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022.

</span>
<span class="ltx_bibblock">Solving Quantitative Reasoning Problems with Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2206.14858

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason
Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau,
Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023.

</span>
<span class="ltx_bibblock">StarCoder: may the source be with you!

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.06161

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022.

</span>
<span class="ltx_bibblock">Competition-level code generation with AlphaCode.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Science</em> 378, 6624 (Dec. 2022), 1092–1097.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1126/science.abq1158" title="">https://doi.org/10.1126/science.abq1158</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.16434 [cs.AI]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu, Xiaohong Zhang, and Meng Yan. 2023.

</span>
<span class="ltx_bibblock">Improving ChatGPT Prompt for Code Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.08360 [cs.SE]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.08360" title="">https://arxiv.org/abs/2305.08360</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LlamaIndex (2023)</span>
<span class="ltx_bibblock">
LlamaIndex 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">LlamaIndex</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llama.meta.com/docs/integration-guides/llamaindex/" title="">https://llama.meta.com/docs/integration-guides/llamaindex/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2023)</span>
<span class="ltx_bibblock">
Meta 2023.

</span>
<span class="ltx_bibblock">Code Llama: Open Foundation Models for Code.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.12950" title="">https://doi.org/10.48550/arXiv.2308.12950</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mistral AI (2024)</span>
<span class="ltx_bibblock">
Mistral AI 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Codestral</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/codestral/" title="">https://mistral.ai/news/codestral/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Nadi (2022)</span>
<span class="ltx_bibblock">
Nhan Nguyen and Sarah Nadi. 2022.

</span>
<span class="ltx_bibblock">An Empirical Evaluation of GitHub Copilot’s Code Suggestions. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)</em>. 1–5.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3524842.3528470" title="">https://doi.org/10.1145/3524842.3528470</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI 2022.

</span>
<span class="ltx_bibblock">ChatGPT.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI 2023a.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Gpt-4 technical report</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/papers/gpt-4.pdf" title="">https://cdn.openai.com/papers/gpt-4.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI 2023b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">OpenAI Code Interpretor</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/assistants/tools/code-interpreter" title="">https://platform.openai.com/docs/assistants/tools/code-interpreter</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parisi et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock">TALM: Tool Augmented Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.12255

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023.

</span>
<span class="ltx_bibblock">Gorilla: Large Language Model Connected with Massive APIs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.15334 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.15334" title="">https://arxiv.org/abs/2305.15334</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poesia et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022.

</span>
<span class="ltx_bibblock">Synchromesh: Reliable code generation from pre-trained language models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11227 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11227" title="">https://arxiv.org/abs/2201.11227</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock">Toolformer: Language Models Can Teach Themselves to Use Tools.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2302.04761

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze (2021)</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze. 2021.

</span>
<span class="ltx_bibblock">Generating Datasets with Pretrained Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6943–6951.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.emnlp-main.555" title="">https://doi.org/10.18653/v1/2021.emnlp-main.555</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.

</span>
<span class="ltx_bibblock">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.17580 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.17580" title="">https://arxiv.org/abs/2303.17580</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11903

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023.

</span>
<span class="ltx_bibblock">ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.07839 [cs.SE]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.07839" title="">https://arxiv.org/abs/2303.07839</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock">In-IDE Code Generation from Natural Language: Promise and Challenges.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2101.11149 [cs.SE]

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Sample with computed Average similarity</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Sample showing how flow similarity is computed for two flows Flow A and Flow B.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p2.1">

Query = "Post a message in the channel of teams,
when a new form is created in the forms"

Ground Truth = "triggerOutputs=
await shared\_microsoftforms.CreateFormWebhook({});
outputs_shared_teams_PostMessageToConversation=
shared_teams.PostMessageToConversation(
{ \"poster\": \"User\"});"

prediction:"triggerOutputs=
awaitshared_microsoftforms.CreateFormWebhook({});
outputs_Get_my_profile_V2 = shared_office365users.
MyProfile_V2({}); outputs_shared_teams_PostMessage
= shared_teams.PostMessageToConversation(
{\"poster\": \"User\",\"location\": \"Channel\"});"

API Functions list in ground_truth =
[shared_microsoftforms.CreateFormWebhook,
shared_teams.PostMessageToConversation]


API function list in model generation=
[shared_microsoftforms.CreateFormWebhook,
shared_office365users.MyProfile_V2,
shared_teams.PostMessageToConversation]

Similarity Score = 2/3 = 0.666

Since the functions shared_microsoftforms.
CreateFormWebhook and shared_teams.
PostMessageToConversation are found
in the ground truth.
</pre>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>An example of API metdata</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We share a sample of API metadata to highlight the details included in the API description provided to the metaprompt.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS2.p2.1">

"shared_outlook.SendEmailV2": {
    "FunctionName": "shared_outlook.SendEmailV2",
    "Description": "This operation sends an email message.",
    "IsInTrainingSet": false,
    "DisplayName": "Send an email (V2)",
        "ParametersInfo": [
            {
                "Key": "emailMessage/To",
                "Type": "String",
                "Summary": "To",
                "Format": "email",
                "Description": "Specify email addresses
                    separated by semicolons like
                    someone@contoso.com"
            }, .
                        ],
        "ResponseSchema": [],
        "IsTrigger": false
    }

</pre>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul  3 01:12:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
