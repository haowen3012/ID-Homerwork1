<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs</title>
<!--Generated on Thu Aug  1 16:28:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.10251v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S1" title="In The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2" title="In The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2.SS1" title="In 2 Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2.SS2" title="In 2 Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tasks and Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2.SS2.SSS1" title="In 2.2 Tasks and Datasets ‣ 2 Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2.SS3" title="In 2 Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3" title="In The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.SS1" title="In 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.SS2" title="In 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.SS3" title="In 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Number of retrieved documents</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.SS4" title="In 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Retrievers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.SS5" title="In 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Benchmark comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S4" title="In The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S5" title="In The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\copyrightclause</span>
<p class="ltx_p" id="p1.2">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0).</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\conference</span>
<p class="ltx_p" id="p2.2">IR-RAG@SIGIR’ 24: The 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 14–18, 2024, Washington D.C., USA</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[orcid=0009-0004-3866-597X,
email=m.yazan@hva.nl,
]
<span class="ltx_ERROR undefined" id="p3.1.1">\cormark</span>[1]</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">[orcid=0000-0002-9609-9505,
email=s.verberne@liacs.leidenuniv.nl,
]</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">[orcid=0000-0002-2156-2083,
email=f.b.i.situmeang@uva.nl,
]</p>
</div>
<div class="ltx_para" id="p6">
<span class="ltx_ERROR undefined" id="p6.1">\cortext</span>
<p class="ltx_p" id="p6.2">[1]Corresponding author.</p>
</div>
<h1 class="ltx_title ltx_title_document">The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mert Yazan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suzan Verberne
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frederik Situmeang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Amsterdam University of Applied Sciences, Fraijlemaborg 133, 1102 CV Amsterdam, Netherlands
</span>
<span class="ltx_contact ltx_role_address">Leiden University, Einsteinweg 55, 2333 CC Leiden, Netherlands
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Post-training quantization reduces the computational demand of Large Language Models (LLMs) but can weaken some of their capabilities. Since LLM abilities emerge with scale, smaller LLMs are more sensitive to quantization. In this paper, we explore how quantization affects smaller LLMs’ ability to perform retrieval-augmented generation (RAG), specifically in longer contexts. We chose personalization for evaluation because it is a challenging domain to perform using RAG as it requires long-context reasoning over multiple documents. We compare the original FP16 and the quantized INT4 performance of multiple 7B and 8B LLMs on two tasks while progressively increasing the number of retrieved documents to test how quantized models fare against longer contexts. To better understand the effect of retrieval, we evaluate three retrieval models in our experiments. Our findings reveal that if a 7B LLM performs the task well, quantization does not impair its performance and long-context reasoning capabilities. We conclude that it is possible to utilize RAG with quantized smaller LLMs.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Retrieval Augmented Generation <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>Quantization, <span class="ltx_ERROR undefined" id="id3.id2">\sep</span>Efficiency <span class="ltx_ERROR undefined" id="id4.id3">\sep</span>Large Language Models <span class="ltx_ERROR undefined" id="id5.id4">\sep</span>Personalization

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Model (LLM) outputs can be enhanced by fetching relevant documents via a retriever and adding them as context for the prompt. The LLM can generate an output grounded with relevant information with the added context. This process is called Retrieval Augmented Generation (RAG). RAG has many benefits such as improving effectiveness in downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib4" title="">4</a>]</cite>, reducing hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib5" title="">5</a>]</cite>, increasing factuality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib6" title="">6</a>]</cite>, by-passing knowledge cut-offs, and presenting proprietary data that is not available to the LLMs.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The performance of RAG depends on the number, quality, and relevance of the retrieved documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib7" title="">7</a>]</cite>. To perform RAG, many tasks demand a lot of passages extracted from multiple, unstructured documents: For question-answering tasks, the answer might be scattered around many documents because of ambiguity or the time-series nature of the question (eg. price change of a stock). For more open-ended tasks like personalization, many documents from different sources might be needed to capture the characteristics of the individual. Therefore to handle RAG in these tasks, an LLM needs to look at multiple sources, identify the relevant parts, and compose the most plausible answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">LLMs do not pay the same attention to their whole context windows, meaning the placement of documents in the prompt directly affects the final output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib8" title="">8</a>]</cite>. On top of that, some of the retrieved documents may be unrelated to the task, or they may contain contradictory information compared to the parametric knowledge of the LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib9" title="">9</a>]</cite>. An LLM has to overcome these challenges to leverage RAG to its advantage. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib4" title="">4</a>]</cite> have shown that an open-source 70B LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib10" title="">10</a>]</cite> equipped with RAG can beat proprietary models, meaning it is not necessary to use an LLM in the caliber of GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib11" title="">11</a>]</cite> to implement RAG. Still, for many use cases, it might not be feasible to deploy a 70B LLM as it is computationally demanding. To decrease the computational demand of LLMs, post-training quantization can be used. Quantization drastically reduces the required amount of RAM to load a model and can increase the inference speed by more than 3 times <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib13" title="">13</a>]</cite>. Despite the benefits, quantization affects LLMs differently depending on their size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib14" title="">14</a>]</cite>. For capabilities that are important to RAG, such as long-context reasoning, smaller LLMs (&lt;13B) are found to be more sensitive to quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S1.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Prompts used for both datasets. The ones on the top represent <math alttext="k=0" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mrow id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml"><mi id="S1.F1.3.m1.1.1.2" xref="S1.F1.3.m1.1.1.2.cmml">k</mi><mo id="S1.F1.3.m1.1.1.1" xref="S1.F1.3.m1.1.1.1.cmml">=</mo><mn id="S1.F1.3.m1.1.1.3" xref="S1.F1.3.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><apply id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1"><eq id="S1.F1.3.m1.1.1.1.cmml" xref="S1.F1.3.m1.1.1.1"></eq><ci id="S1.F1.3.m1.1.1.2.cmml" xref="S1.F1.3.m1.1.1.2">𝑘</ci><cn id="S1.F1.3.m1.1.1.3.cmml" type="integer" xref="S1.F1.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">k=0</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">italic_k = 0</annotation></semantics></math> (zero-shot, no retrieved documents) and the ones on the bottom are for <math alttext="k&gt;0" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><mrow id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml"><mi id="S1.F1.4.m2.1.1.2" xref="S1.F1.4.m2.1.1.2.cmml">k</mi><mo id="S1.F1.4.m2.1.1.1" xref="S1.F1.4.m2.1.1.1.cmml">&gt;</mo><mn id="S1.F1.4.m2.1.1.3" xref="S1.F1.4.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><apply id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1"><gt id="S1.F1.4.m2.1.1.1.cmml" xref="S1.F1.4.m2.1.1.1"></gt><ci id="S1.F1.4.m2.1.1.2.cmml" xref="S1.F1.4.m2.1.1.2">𝑘</ci><cn id="S1.F1.4.m2.1.1.3.cmml" type="integer" xref="S1.F1.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">k&gt;0</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">italic_k &gt; 0</annotation></semantics></math> settings (RAG). The green text is the model output. Line endings are not shown for space reasons.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we investigate the effectiveness of quantization on RAG-enhanced 7B and 8B LLMs. We evaluate the full (FP16) and quantized (INT4) versions of multiple LLMs on two personalization tasks taken from the LaMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite> benchmark. To better study how quantized LLMs perform in longer contexts, we compared the performance gap between FP16 and INT4 models with an increasing number of retrieved documents. We chose personalization because it is a challenging task to perform with RAG as it demands long-context reasoning over many documents. Contrary to question-answering where the LLM has to find the correct answer from a couple of documents, personalization requires the LLM to carefully study a person’s style from all the provided documents. Our findings show that the effect of quantization depends on the model and the task: we find almost no drop in performance for OpenChat while LLaMA2 seems to be more sensitive. Our experiments show that quantized smaller LLMs can be good candidates for RAG pipelines, especially if efficiency is essential.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approach</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Starting with LLaMA2-7B (Chat-hf) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib10" title="">10</a>]</cite> to have a baseline, we experiment with the following LLMs: LLaMA3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib16" title="">16</a>]</cite>, Zephyr (Beta) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib17" title="">17</a>]</cite>, OpenChat (3.5) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib18" title="">18</a>]</cite>, and Starling (LM-alpha) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib19" title="">19</a>]</cite>. These models were chosen because they were the highest-ranked 7B and 8B LLMs in the Chatbot Arena Leaderboard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib20" title="">20</a>]</cite> according to the Elo ratings at the time of writing. Since all models except LLaMA are finetuned variants of Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib21" title="">21</a>]</cite>, we add Mistral-7B (Instruct-0.1)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Although there is an updated v0.2 version of Mistral-7B, we used v0.1 to match the other LLMs that are finetuned on it</span></span></span> to our experiments too. We use Activation-aware Weight Quantization (AWQ) as it outperforms other methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib22" title="">22</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tasks and Datasets</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">We use the LaMP benchmark that offers 7 personalization datasets with either a classification or a generation task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite>. To represent both types of tasks, we chose one dataset from each: LaMP-3 (“Personalized Product Rating”) and LaMP-5 (“Personalized Scholarly Title Generation”). LaMP-3 is composed of product reviews and their corresponding scores. For each user, one of the review–score pairs is chosen as the target and other pairs become the user profile. The LLM’s task, in this case, is to predict the score given a review using the other review–score pairs of the same user. LaMP-5 aims to generate a title for an academic paper based on the abstract. In this case, the user profile consists of abstract–title pairs that demonstrate the writing style of the user (scholar). The task of the LLM is to generate a title for the given abstract by incorporating the writing style of the scholar. Those datasets were chosen because compared to the other ones, on average, they had more samples in their user profiles, and the samples were longer. Therefore, they represented a better opportunity to evaluate RAG effectiveness as the retrieval part would be trickier.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">We work with the user-based splits (LaMP-3U, LaMP-5U) where the user appears only in one of the data splits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite>. The labels for the test sets are not publicly available (results can be obtained by submitting the predictions to the leaderboard) and since we did not fine-tune our models, we chose to use the validation sets for evaluation. For both datasets, we noticed that some samples do not fit in the context windows. After analyzing the overall length of the samples, we concluded that those cases only represent a tiny minority and removed data points that are not in the 0.995th percentile. For LaMP-5U, we also removed abstracts that consisted only of the text “no abstract available”. There are 2500 samples in the validation sets, and we have 2487 samples left after the preprocessing steps for both datasets.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Evaluation</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">We used mean absolute error (MAE) for LaMP-3 and Rouge-L <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib23" title="">23</a>]</cite> for LaMP-5, following the LaMP paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite>. Their experiments also include root mean square error (RMSE) and Rouge-1 scores, but we found that the correlation between MAE and RMSE is 0.94, and between Rouge-1 and Rouge-L is 0.99. Therefore, we do not include those metrics in our results. The prompts we use are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>. Even though the LLMs are instructed to output only the score or the title, we notice that some are prone to give lengthy answers such as “Sure, here is the title for the given abstract, Title: (generated title)”. We apply a post-processing step on the LLM outputs to extract only the score or the title before evaluation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Retrieval</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.9">We conduct the experiments with the following number of retrieved documents: <math alttext="k\in\{0,1,3,5,max\_4K,max\_8K\}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.6"><semantics id="S2.SS3.p1.1.m1.6a"><mrow id="S2.SS3.p1.1.m1.6.6" xref="S2.SS3.p1.1.m1.6.6.cmml"><mi id="S2.SS3.p1.1.m1.6.6.4" xref="S2.SS3.p1.1.m1.6.6.4.cmml">k</mi><mo id="S2.SS3.p1.1.m1.6.6.3" xref="S2.SS3.p1.1.m1.6.6.3.cmml">∈</mo><mrow id="S2.SS3.p1.1.m1.6.6.2.2" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml"><mo id="S2.SS3.p1.1.m1.6.6.2.2.3" stretchy="false" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">{</mo><mn id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">0</mn><mo id="S2.SS3.p1.1.m1.6.6.2.2.4" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mn id="S2.SS3.p1.1.m1.2.2" xref="S2.SS3.p1.1.m1.2.2.cmml">1</mn><mo id="S2.SS3.p1.1.m1.6.6.2.2.5" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mn id="S2.SS3.p1.1.m1.3.3" xref="S2.SS3.p1.1.m1.3.3.cmml">3</mn><mo id="S2.SS3.p1.1.m1.6.6.2.2.6" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mn id="S2.SS3.p1.1.m1.4.4" xref="S2.SS3.p1.1.m1.4.4.cmml">5</mn><mo id="S2.SS3.p1.1.m1.6.6.2.2.7" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mrow id="S2.SS3.p1.1.m1.5.5.1.1.1" xref="S2.SS3.p1.1.m1.5.5.1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.5.5.1.1.1.2" xref="S2.SS3.p1.1.m1.5.5.1.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.1.m1.5.5.1.1.1.1" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.5.5.1.1.1.3" xref="S2.SS3.p1.1.m1.5.5.1.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.1.m1.5.5.1.1.1.1a" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.5.5.1.1.1.4" xref="S2.SS3.p1.1.m1.5.5.1.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.1.m1.5.5.1.1.1.1b" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.5.5.1.1.1.5" mathvariant="normal" xref="S2.SS3.p1.1.m1.5.5.1.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.1.m1.5.5.1.1.1.1c" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.1.m1.5.5.1.1.1.6" xref="S2.SS3.p1.1.m1.5.5.1.1.1.6.cmml">4</mn><mo id="S2.SS3.p1.1.m1.5.5.1.1.1.1d" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.5.5.1.1.1.7" xref="S2.SS3.p1.1.m1.5.5.1.1.1.7.cmml">K</mi></mrow><mo id="S2.SS3.p1.1.m1.6.6.2.2.8" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mrow id="S2.SS3.p1.1.m1.6.6.2.2.2" xref="S2.SS3.p1.1.m1.6.6.2.2.2.cmml"><mi id="S2.SS3.p1.1.m1.6.6.2.2.2.2" xref="S2.SS3.p1.1.m1.6.6.2.2.2.2.cmml">m</mi><mo id="S2.SS3.p1.1.m1.6.6.2.2.2.1" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.6.6.2.2.2.3" xref="S2.SS3.p1.1.m1.6.6.2.2.2.3.cmml">a</mi><mo id="S2.SS3.p1.1.m1.6.6.2.2.2.1a" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.6.6.2.2.2.4" xref="S2.SS3.p1.1.m1.6.6.2.2.2.4.cmml">x</mi><mo id="S2.SS3.p1.1.m1.6.6.2.2.2.1b" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.6.6.2.2.2.5" mathvariant="normal" xref="S2.SS3.p1.1.m1.6.6.2.2.2.5.cmml">_</mi><mo id="S2.SS3.p1.1.m1.6.6.2.2.2.1c" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mn id="S2.SS3.p1.1.m1.6.6.2.2.2.6" xref="S2.SS3.p1.1.m1.6.6.2.2.2.6.cmml">8</mn><mo id="S2.SS3.p1.1.m1.6.6.2.2.2.1d" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.6.6.2.2.2.7" xref="S2.SS3.p1.1.m1.6.6.2.2.2.7.cmml">K</mi></mrow><mo id="S2.SS3.p1.1.m1.6.6.2.2.9" stretchy="false" xref="S2.SS3.p1.1.m1.6.6.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.6b"><apply id="S2.SS3.p1.1.m1.6.6.cmml" xref="S2.SS3.p1.1.m1.6.6"><in id="S2.SS3.p1.1.m1.6.6.3.cmml" xref="S2.SS3.p1.1.m1.6.6.3"></in><ci id="S2.SS3.p1.1.m1.6.6.4.cmml" xref="S2.SS3.p1.1.m1.6.6.4">𝑘</ci><set id="S2.SS3.p1.1.m1.6.6.2.3.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2"><cn id="S2.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1">0</cn><cn id="S2.SS3.p1.1.m1.2.2.cmml" type="integer" xref="S2.SS3.p1.1.m1.2.2">1</cn><cn id="S2.SS3.p1.1.m1.3.3.cmml" type="integer" xref="S2.SS3.p1.1.m1.3.3">3</cn><cn id="S2.SS3.p1.1.m1.4.4.cmml" type="integer" xref="S2.SS3.p1.1.m1.4.4">5</cn><apply id="S2.SS3.p1.1.m1.5.5.1.1.1.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1"><times id="S2.SS3.p1.1.m1.5.5.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.5.5.1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.2">𝑚</ci><ci id="S2.SS3.p1.1.m1.5.5.1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.3">𝑎</ci><ci id="S2.SS3.p1.1.m1.5.5.1.1.1.4.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.4">𝑥</ci><ci id="S2.SS3.p1.1.m1.5.5.1.1.1.5.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.5">_</ci><cn id="S2.SS3.p1.1.m1.5.5.1.1.1.6.cmml" type="integer" xref="S2.SS3.p1.1.m1.5.5.1.1.1.6">4</cn><ci id="S2.SS3.p1.1.m1.5.5.1.1.1.7.cmml" xref="S2.SS3.p1.1.m1.5.5.1.1.1.7">𝐾</ci></apply><apply id="S2.SS3.p1.1.m1.6.6.2.2.2.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2"><times id="S2.SS3.p1.1.m1.6.6.2.2.2.1.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.1"></times><ci id="S2.SS3.p1.1.m1.6.6.2.2.2.2.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.2">𝑚</ci><ci id="S2.SS3.p1.1.m1.6.6.2.2.2.3.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.3">𝑎</ci><ci id="S2.SS3.p1.1.m1.6.6.2.2.2.4.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.4">𝑥</ci><ci id="S2.SS3.p1.1.m1.6.6.2.2.2.5.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.5">_</ci><cn id="S2.SS3.p1.1.m1.6.6.2.2.2.6.cmml" type="integer" xref="S2.SS3.p1.1.m1.6.6.2.2.2.6">8</cn><ci id="S2.SS3.p1.1.m1.6.6.2.2.2.7.cmml" xref="S2.SS3.p1.1.m1.6.6.2.2.2.7">𝐾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.6c">k\in\{0,1,3,5,max\_4K,max\_8K\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.6d">italic_k ∈ { 0 , 1 , 3 , 5 , italic_m italic_a italic_x _ 4 italic_K , italic_m italic_a italic_x _ 8 italic_K }</annotation></semantics></math>. <math alttext="0" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mn id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><cn id="S2.SS3.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS3.p1.2.m2.1.1">0</cn></annotation-xml></semantics></math> refers to zero-shot without any retrieval and <math alttext="max" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.3.m3.1.1.1" xref="S2.SS3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.3.m3.1.1.1a" xref="S2.SS3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.3.m3.1.1.4" xref="S2.SS3.p1.3.m3.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><times id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></times><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">𝑚</ci><ci id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">𝑎</ci><ci id="S2.SS3.p1.3.m3.1.1.4.cmml" xref="S2.SS3.p1.3.m3.1.1.4">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">max</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">italic_m italic_a italic_x</annotation></semantics></math> is the maximum number of documents that can be put into the prompt, given the context window of the LLM. LLaMA2 has a context window of 4096 tokens while other models have 8192 tokens. To make it fair, we include two options for the <math alttext="max" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m4.1"><semantics id="S2.SS3.p1.4.m4.1a"><mrow id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml"><mi id="S2.SS3.p1.4.m4.1.1.2" xref="S2.SS3.p1.4.m4.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.4.m4.1.1.1" xref="S2.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.4.m4.1.1.3" xref="S2.SS3.p1.4.m4.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.4.m4.1.1.1a" xref="S2.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.4.m4.1.1.4" xref="S2.SS3.p1.4.m4.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><apply id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1"><times id="S2.SS3.p1.4.m4.1.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1.1"></times><ci id="S2.SS3.p1.4.m4.1.1.2.cmml" xref="S2.SS3.p1.4.m4.1.1.2">𝑚</ci><ci id="S2.SS3.p1.4.m4.1.1.3.cmml" xref="S2.SS3.p1.4.m4.1.1.3">𝑎</ci><ci id="S2.SS3.p1.4.m4.1.1.4.cmml" xref="S2.SS3.p1.4.m4.1.1.4">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">max</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.4.m4.1d">italic_m italic_a italic_x</annotation></semantics></math> setting: 4K and 8K. For <math alttext="max\_4K" class="ltx_Math" display="inline" id="S2.SS3.p1.5.m5.1"><semantics id="S2.SS3.p1.5.m5.1a"><mrow id="S2.SS3.p1.5.m5.1.1" xref="S2.SS3.p1.5.m5.1.1.cmml"><mi id="S2.SS3.p1.5.m5.1.1.2" xref="S2.SS3.p1.5.m5.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.5.m5.1.1.1" xref="S2.SS3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.5.m5.1.1.3" xref="S2.SS3.p1.5.m5.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.5.m5.1.1.1a" xref="S2.SS3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.5.m5.1.1.4" xref="S2.SS3.p1.5.m5.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.5.m5.1.1.1b" xref="S2.SS3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.5.m5.1.1.5" mathvariant="normal" xref="S2.SS3.p1.5.m5.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.5.m5.1.1.1c" xref="S2.SS3.p1.5.m5.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.5.m5.1.1.6" xref="S2.SS3.p1.5.m5.1.1.6.cmml">4</mn><mo id="S2.SS3.p1.5.m5.1.1.1d" xref="S2.SS3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.5.m5.1.1.7" xref="S2.SS3.p1.5.m5.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m5.1b"><apply id="S2.SS3.p1.5.m5.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1"><times id="S2.SS3.p1.5.m5.1.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1.1"></times><ci id="S2.SS3.p1.5.m5.1.1.2.cmml" xref="S2.SS3.p1.5.m5.1.1.2">𝑚</ci><ci id="S2.SS3.p1.5.m5.1.1.3.cmml" xref="S2.SS3.p1.5.m5.1.1.3">𝑎</ci><ci id="S2.SS3.p1.5.m5.1.1.4.cmml" xref="S2.SS3.p1.5.m5.1.1.4">𝑥</ci><ci id="S2.SS3.p1.5.m5.1.1.5.cmml" xref="S2.SS3.p1.5.m5.1.1.5">_</ci><cn id="S2.SS3.p1.5.m5.1.1.6.cmml" type="integer" xref="S2.SS3.p1.5.m5.1.1.6">4</cn><ci id="S2.SS3.p1.5.m5.1.1.7.cmml" xref="S2.SS3.p1.5.m5.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m5.1c">max\_4K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.5.m5.1d">italic_m italic_a italic_x _ 4 italic_K</annotation></semantics></math>, we assume that all models have a 4096 token context window, we use the original 8192 token context windows for <math alttext="max\_8K" class="ltx_Math" display="inline" id="S2.SS3.p1.6.m6.1"><semantics id="S2.SS3.p1.6.m6.1a"><mrow id="S2.SS3.p1.6.m6.1.1" xref="S2.SS3.p1.6.m6.1.1.cmml"><mi id="S2.SS3.p1.6.m6.1.1.2" xref="S2.SS3.p1.6.m6.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.6.m6.1.1.1" xref="S2.SS3.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.6.m6.1.1.3" xref="S2.SS3.p1.6.m6.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.6.m6.1.1.1a" xref="S2.SS3.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.6.m6.1.1.4" xref="S2.SS3.p1.6.m6.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.6.m6.1.1.1b" xref="S2.SS3.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.6.m6.1.1.5" mathvariant="normal" xref="S2.SS3.p1.6.m6.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.6.m6.1.1.1c" xref="S2.SS3.p1.6.m6.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.6.m6.1.1.6" xref="S2.SS3.p1.6.m6.1.1.6.cmml">8</mn><mo id="S2.SS3.p1.6.m6.1.1.1d" xref="S2.SS3.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.6.m6.1.1.7" xref="S2.SS3.p1.6.m6.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m6.1b"><apply id="S2.SS3.p1.6.m6.1.1.cmml" xref="S2.SS3.p1.6.m6.1.1"><times id="S2.SS3.p1.6.m6.1.1.1.cmml" xref="S2.SS3.p1.6.m6.1.1.1"></times><ci id="S2.SS3.p1.6.m6.1.1.2.cmml" xref="S2.SS3.p1.6.m6.1.1.2">𝑚</ci><ci id="S2.SS3.p1.6.m6.1.1.3.cmml" xref="S2.SS3.p1.6.m6.1.1.3">𝑎</ci><ci id="S2.SS3.p1.6.m6.1.1.4.cmml" xref="S2.SS3.p1.6.m6.1.1.4">𝑥</ci><ci id="S2.SS3.p1.6.m6.1.1.5.cmml" xref="S2.SS3.p1.6.m6.1.1.5">_</ci><cn id="S2.SS3.p1.6.m6.1.1.6.cmml" type="integer" xref="S2.SS3.p1.6.m6.1.1.6">8</cn><ci id="S2.SS3.p1.6.m6.1.1.7.cmml" xref="S2.SS3.p1.6.m6.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m6.1c">max\_8K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.6.m6.1d">italic_m italic_a italic_x _ 8 italic_K</annotation></semantics></math>. Consequently, LLaMA2-7B is not included in the <math alttext="max\_8K" class="ltx_Math" display="inline" id="S2.SS3.p1.7.m7.1"><semantics id="S2.SS3.p1.7.m7.1a"><mrow id="S2.SS3.p1.7.m7.1.1" xref="S2.SS3.p1.7.m7.1.1.cmml"><mi id="S2.SS3.p1.7.m7.1.1.2" xref="S2.SS3.p1.7.m7.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.7.m7.1.1.1" xref="S2.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.7.m7.1.1.3" xref="S2.SS3.p1.7.m7.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.7.m7.1.1.1a" xref="S2.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.7.m7.1.1.4" xref="S2.SS3.p1.7.m7.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.7.m7.1.1.1b" xref="S2.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.7.m7.1.1.5" mathvariant="normal" xref="S2.SS3.p1.7.m7.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.7.m7.1.1.1c" xref="S2.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.7.m7.1.1.6" xref="S2.SS3.p1.7.m7.1.1.6.cmml">8</mn><mo id="S2.SS3.p1.7.m7.1.1.1d" xref="S2.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.7.m7.1.1.7" xref="S2.SS3.p1.7.m7.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.7.m7.1b"><apply id="S2.SS3.p1.7.m7.1.1.cmml" xref="S2.SS3.p1.7.m7.1.1"><times id="S2.SS3.p1.7.m7.1.1.1.cmml" xref="S2.SS3.p1.7.m7.1.1.1"></times><ci id="S2.SS3.p1.7.m7.1.1.2.cmml" xref="S2.SS3.p1.7.m7.1.1.2">𝑚</ci><ci id="S2.SS3.p1.7.m7.1.1.3.cmml" xref="S2.SS3.p1.7.m7.1.1.3">𝑎</ci><ci id="S2.SS3.p1.7.m7.1.1.4.cmml" xref="S2.SS3.p1.7.m7.1.1.4">𝑥</ci><ci id="S2.SS3.p1.7.m7.1.1.5.cmml" xref="S2.SS3.p1.7.m7.1.1.5">_</ci><cn id="S2.SS3.p1.7.m7.1.1.6.cmml" type="integer" xref="S2.SS3.p1.7.m7.1.1.6">8</cn><ci id="S2.SS3.p1.7.m7.1.1.7.cmml" xref="S2.SS3.p1.7.m7.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.7.m7.1c">max\_8K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.7.m7.1d">italic_m italic_a italic_x _ 8 italic_K</annotation></semantics></math> experiments. To put it into perspective, the number of retrieved documents varies between 15-18 for <math alttext="max\_4K" class="ltx_Math" display="inline" id="S2.SS3.p1.8.m8.1"><semantics id="S2.SS3.p1.8.m8.1a"><mrow id="S2.SS3.p1.8.m8.1.1" xref="S2.SS3.p1.8.m8.1.1.cmml"><mi id="S2.SS3.p1.8.m8.1.1.2" xref="S2.SS3.p1.8.m8.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.8.m8.1.1.1" xref="S2.SS3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.8.m8.1.1.3" xref="S2.SS3.p1.8.m8.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.8.m8.1.1.1a" xref="S2.SS3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.8.m8.1.1.4" xref="S2.SS3.p1.8.m8.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.8.m8.1.1.1b" xref="S2.SS3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.8.m8.1.1.5" mathvariant="normal" xref="S2.SS3.p1.8.m8.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.8.m8.1.1.1c" xref="S2.SS3.p1.8.m8.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.8.m8.1.1.6" xref="S2.SS3.p1.8.m8.1.1.6.cmml">4</mn><mo id="S2.SS3.p1.8.m8.1.1.1d" xref="S2.SS3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.8.m8.1.1.7" xref="S2.SS3.p1.8.m8.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.8.m8.1b"><apply id="S2.SS3.p1.8.m8.1.1.cmml" xref="S2.SS3.p1.8.m8.1.1"><times id="S2.SS3.p1.8.m8.1.1.1.cmml" xref="S2.SS3.p1.8.m8.1.1.1"></times><ci id="S2.SS3.p1.8.m8.1.1.2.cmml" xref="S2.SS3.p1.8.m8.1.1.2">𝑚</ci><ci id="S2.SS3.p1.8.m8.1.1.3.cmml" xref="S2.SS3.p1.8.m8.1.1.3">𝑎</ci><ci id="S2.SS3.p1.8.m8.1.1.4.cmml" xref="S2.SS3.p1.8.m8.1.1.4">𝑥</ci><ci id="S2.SS3.p1.8.m8.1.1.5.cmml" xref="S2.SS3.p1.8.m8.1.1.5">_</ci><cn id="S2.SS3.p1.8.m8.1.1.6.cmml" type="integer" xref="S2.SS3.p1.8.m8.1.1.6">4</cn><ci id="S2.SS3.p1.8.m8.1.1.7.cmml" xref="S2.SS3.p1.8.m8.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.8.m8.1c">max\_4K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.8.m8.1d">italic_m italic_a italic_x _ 4 italic_K</annotation></semantics></math>, and between 25-28 for <math alttext="max\_8K" class="ltx_Math" display="inline" id="S2.SS3.p1.9.m9.1"><semantics id="S2.SS3.p1.9.m9.1a"><mrow id="S2.SS3.p1.9.m9.1.1" xref="S2.SS3.p1.9.m9.1.1.cmml"><mi id="S2.SS3.p1.9.m9.1.1.2" xref="S2.SS3.p1.9.m9.1.1.2.cmml">m</mi><mo id="S2.SS3.p1.9.m9.1.1.1" xref="S2.SS3.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.9.m9.1.1.3" xref="S2.SS3.p1.9.m9.1.1.3.cmml">a</mi><mo id="S2.SS3.p1.9.m9.1.1.1a" xref="S2.SS3.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.9.m9.1.1.4" xref="S2.SS3.p1.9.m9.1.1.4.cmml">x</mi><mo id="S2.SS3.p1.9.m9.1.1.1b" xref="S2.SS3.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.9.m9.1.1.5" mathvariant="normal" xref="S2.SS3.p1.9.m9.1.1.5.cmml">_</mi><mo id="S2.SS3.p1.9.m9.1.1.1c" xref="S2.SS3.p1.9.m9.1.1.1.cmml">⁢</mo><mn id="S2.SS3.p1.9.m9.1.1.6" xref="S2.SS3.p1.9.m9.1.1.6.cmml">8</mn><mo id="S2.SS3.p1.9.m9.1.1.1d" xref="S2.SS3.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.9.m9.1.1.7" xref="S2.SS3.p1.9.m9.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.9.m9.1b"><apply id="S2.SS3.p1.9.m9.1.1.cmml" xref="S2.SS3.p1.9.m9.1.1"><times id="S2.SS3.p1.9.m9.1.1.1.cmml" xref="S2.SS3.p1.9.m9.1.1.1"></times><ci id="S2.SS3.p1.9.m9.1.1.2.cmml" xref="S2.SS3.p1.9.m9.1.1.2">𝑚</ci><ci id="S2.SS3.p1.9.m9.1.1.3.cmml" xref="S2.SS3.p1.9.m9.1.1.3">𝑎</ci><ci id="S2.SS3.p1.9.m9.1.1.4.cmml" xref="S2.SS3.p1.9.m9.1.1.4">𝑥</ci><ci id="S2.SS3.p1.9.m9.1.1.5.cmml" xref="S2.SS3.p1.9.m9.1.1.5">_</ci><cn id="S2.SS3.p1.9.m9.1.1.6.cmml" type="integer" xref="S2.SS3.p1.9.m9.1.1.6">8</cn><ci id="S2.SS3.p1.9.m9.1.1.7.cmml" xref="S2.SS3.p1.9.m9.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.9.m9.1c">max\_8K</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.9.m9.1d">italic_m italic_a italic_x _ 8 italic_K</annotation></semantics></math> in LaMP-5U, depending on the average length of documents in the user profile. As retrievers, we evaluate BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib24" title="">24</a>]</cite> (BM25 Okapi) <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org/project/rank-bm25/" title="">https://pypi.org/project/rank-bm25/</a></span></span></span>, Contriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib25" title="">25</a>]</cite> (finetuned on MS-Marco), and DPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib26" title="">26</a>]</cite> (finetuned on Natural Questions)<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/facebook/dpr-question_encoder-single-nq-base" title="">https://huggingface.co/facebook/dpr-question_encoder-single-nq-base</a></span></span></span>. Since we focus on efficiency by reducing the computational load, the retrievers are not finetuned on the datasets.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>The absolute percentage change between FP16 and INT4 scores, using Contriever. More than a 5% drop in performance is highlighted in red. For MAE, the lower is better while the inverse is true for Rouge-L.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S2.T1.2">
<tr class="ltx_tr" id="S2.T1.2.3">
<td class="ltx_td" colspan="3" id="S2.T1.2.3.1"></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.2.1" style="font-size:70%;">LLaMA2</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.3.1" style="font-size:70%;">OpenChat</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.4.1" style="font-size:70%;">Starling</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.5.1" style="font-size:70%;">Zephyr</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.6.1" style="font-size:70%;">Mistral</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S2.T1.2.3.7"><span class="ltx_text ltx_font_bold" id="S2.T1.2.3.7.1" style="font-size:70%;">LLaMA3</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.4.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.4.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.2.1" style="font-size:70%;">Metric</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.4.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.3.1" style="font-size:70%;">k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.4.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.5.1" style="font-size:70%;">INT4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.6.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.7"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.7.1" style="font-size:70%;">INT4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.8"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.8.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.9"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.9.1" style="font-size:70%;">INT4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.10"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.10.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.11"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.11.1" style="font-size:70%;">INT4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.12"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.12.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.13"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.13.1" style="font-size:70%;">INT4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.14"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.14.1" style="font-size:70%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.15"><span class="ltx_text ltx_font_bold" id="S2.T1.2.4.15.1" style="font-size:70%;">INT4</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.2" rowspan="6"><span class="ltx_text" id="S2.T1.1.1.2.1" style="font-size:70%;">LaMP-3U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1" rowspan="6"><span class="ltx_text" id="S2.T1.1.1.1.1" style="font-size:70%;">MAE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3"><span class="ltx_text" id="S2.T1.1.1.3.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.4"><span class="ltx_text" id="S2.T1.1.1.4.1" style="font-size:70%;">0.684</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.5"><span class="ltx_text" id="S2.T1.1.1.5.1" style="font-size:70%;">+2.9%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.6"><span class="ltx_text" id="S2.T1.1.1.6.1" style="font-size:70%;">0.440</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.7"><span class="ltx_text" id="S2.T1.1.1.7.1" style="font-size:70%;color:#FF0000;">-7.8%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.8"><span class="ltx_text" id="S2.T1.1.1.8.1" style="font-size:70%;">1.603</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.9"><span class="ltx_text" id="S2.T1.1.1.9.1" style="font-size:70%;">+45%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.10"><span class="ltx_text" id="S2.T1.1.1.10.1" style="font-size:70%;">0.435</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.11"><span class="ltx_text" id="S2.T1.1.1.11.1" style="font-size:70%;color:#FF0000;">-14.7%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.12"><span class="ltx_text" id="S2.T1.1.1.12.1" style="font-size:70%;">0.569</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.13"><span class="ltx_text" id="S2.T1.1.1.13.1" style="font-size:70%;">-2.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.14"><span class="ltx_text" id="S2.T1.1.1.14.1" style="font-size:70%;">0.481</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.15"><span class="ltx_text" id="S2.T1.1.1.15.1" style="font-size:70%;color:#FF0000;">-5.9%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.5.1"><span class="ltx_text" id="S2.T1.2.5.1.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.2"><span class="ltx_text" id="S2.T1.2.5.2.1" style="font-size:70%;">0.453</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.3"><span class="ltx_text" id="S2.T1.2.5.3.1" style="font-size:70%;">-1.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.4"><span class="ltx_text" id="S2.T1.2.5.4.1" style="font-size:70%;">0.312</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.5"><span class="ltx_text" id="S2.T1.2.5.5.1" style="font-size:70%;">+5.5%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.6"><span class="ltx_text" id="S2.T1.2.5.6.1" style="font-size:70%;">0.800</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.7"><span class="ltx_text" id="S2.T1.2.5.7.1" style="font-size:70%;">+7.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.8"><span class="ltx_text" id="S2.T1.2.5.8.1" style="font-size:70%;">0.300</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.9"><span class="ltx_text" id="S2.T1.2.5.9.1" style="font-size:70%;">+1.9%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.10"><span class="ltx_text" id="S2.T1.2.5.10.1" style="font-size:70%;">0.461</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.11"><span class="ltx_text" id="S2.T1.2.5.11.1" style="font-size:70%;color:#FF0000;">-9.3%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.12"><span class="ltx_text" id="S2.T1.2.5.12.1" style="font-size:70%;">0.364</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.5.13"><span class="ltx_text" id="S2.T1.2.5.13.1" style="font-size:70%;color:#FF0000;">-10.8%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.6.1"><span class="ltx_text" id="S2.T1.2.6.1.1" style="font-size:70%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.2"><span class="ltx_text" id="S2.T1.2.6.2.1" style="font-size:70%;">0.637</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.3"><span class="ltx_text" id="S2.T1.2.6.3.1" style="font-size:70%;color:#FF0000;">-7.6%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.4"><span class="ltx_text" id="S2.T1.2.6.4.1" style="font-size:70%;">0.256</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.5"><span class="ltx_text" id="S2.T1.2.6.5.1" style="font-size:70%;">+2.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.6"><span class="ltx_text" id="S2.T1.2.6.6.1" style="font-size:70%;">0.718</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.7"><span class="ltx_text" id="S2.T1.2.6.7.1" style="font-size:70%;color:#FF0000;">-30.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.8"><span class="ltx_text" id="S2.T1.2.6.8.1" style="font-size:70%;">0.273</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.9"><span class="ltx_text" id="S2.T1.2.6.9.1" style="font-size:70%;">+2.6%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.10"><span class="ltx_text" id="S2.T1.2.6.10.1" style="font-size:70%;">0.404</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.11"><span class="ltx_text" id="S2.T1.2.6.11.1" style="font-size:70%;color:#FF0000;">-8.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.12"><span class="ltx_text" id="S2.T1.2.6.12.1" style="font-size:70%;">0.320</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.6.13"><span class="ltx_text" id="S2.T1.2.6.13.1" style="font-size:70%;color:#FF0000;">-9.2%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.7.1"><span class="ltx_text" id="S2.T1.2.7.1.1" style="font-size:70%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.2"><span class="ltx_text" id="S2.T1.2.7.2.1" style="font-size:70%;">0.724</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.3"><span class="ltx_text" id="S2.T1.2.7.3.1" style="font-size:70%;color:#FF0000;">-23.3%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.4"><span class="ltx_text" id="S2.T1.2.7.4.1" style="font-size:70%;">0.238</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.5"><span class="ltx_text" id="S2.T1.2.7.5.1" style="font-size:70%;">+1.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.6"><span class="ltx_text" id="S2.T1.2.7.6.1" style="font-size:70%;">0.797</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.7"><span class="ltx_text" id="S2.T1.2.7.7.1" style="font-size:70%;color:#FF0000;">-32.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.8"><span class="ltx_text" id="S2.T1.2.7.8.1" style="font-size:70%;">0.266</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.9"><span class="ltx_text" id="S2.T1.2.7.9.1" style="font-size:70%;">+0.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.10"><span class="ltx_text" id="S2.T1.2.7.10.1" style="font-size:70%;">0.380</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.11"><span class="ltx_text" id="S2.T1.2.7.11.1" style="font-size:70%;color:#FF0000;">-8.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.12"><span class="ltx_text" id="S2.T1.2.7.12.1" style="font-size:70%;">0.305</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.7.13"><span class="ltx_text" id="S2.T1.2.7.13.1" style="font-size:70%;color:#FF0000;">-13.0%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.8.1"><span class="ltx_text" id="S2.T1.2.8.1.1" style="font-size:70%;">max_4K</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.2"><span class="ltx_text" id="S2.T1.2.8.2.1" style="font-size:70%;">0.508</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.3"><span class="ltx_text" id="S2.T1.2.8.3.1" style="font-size:70%;color:#FF0000;">-80.2%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.8.4.1" style="font-size:70%;">0.224</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.5"><span class="ltx_text" id="S2.T1.2.8.5.1" style="font-size:70%;">+1.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.6"><span class="ltx_text" id="S2.T1.2.8.6.1" style="font-size:70%;">0.985</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.7"><span class="ltx_text" id="S2.T1.2.8.7.1" style="font-size:70%;color:#FF0000;">-57.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.8"><span class="ltx_text" id="S2.T1.2.8.8.1" style="font-size:70%;">0.237</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.9"><span class="ltx_text" id="S2.T1.2.8.9.1" style="font-size:70%;">-4.4%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.10"><span class="ltx_text" id="S2.T1.2.8.10.1" style="font-size:70%;">0.346</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.11"><span class="ltx_text" id="S2.T1.2.8.11.1" style="font-size:70%;color:#FF0000;">-14.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.12"><span class="ltx_text" id="S2.T1.2.8.12.1" style="font-size:70%;">0.285</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.8.13"><span class="ltx_text" id="S2.T1.2.8.13.1" style="font-size:70%;color:#FF0000;">-23.1%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.9.1"><span class="ltx_text" id="S2.T1.2.9.1.1" style="font-size:70%;">max_8K</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.2"><span class="ltx_text" id="S2.T1.2.9.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.3"><span class="ltx_text" id="S2.T1.2.9.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.4"><span class="ltx_text" id="S2.T1.2.9.4.1" style="font-size:70%;">0.257</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.5"><span class="ltx_text" id="S2.T1.2.9.5.1" style="font-size:70%;">-3.9%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.6"><span class="ltx_text" id="S2.T1.2.9.6.1" style="font-size:70%;">1.352</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.7"><span class="ltx_text" id="S2.T1.2.9.7.1" style="font-size:70%;">-1.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.8"><span class="ltx_text" id="S2.T1.2.9.8.1" style="font-size:70%;">0.392</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.9"><span class="ltx_text" id="S2.T1.2.9.9.1" style="font-size:70%;color:#FF0000;">-6.3%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.10"><span class="ltx_text" id="S2.T1.2.9.10.1" style="font-size:70%;">0.368</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.11"><span class="ltx_text" id="S2.T1.2.9.11.1" style="font-size:70%;color:#FF0000;">-16.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.12"><span class="ltx_text" id="S2.T1.2.9.12.1" style="font-size:70%;">0.288</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.9.13"><span class="ltx_text" id="S2.T1.2.9.13.1" style="font-size:70%;color:#FF0000;">-23.4%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.2.2" rowspan="6"><span class="ltx_text" id="S2.T1.2.2.2.1" style="font-size:70%;">LaMP-5U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.2.1" rowspan="6"><span class="ltx_text" id="S2.T1.2.2.1.1" style="font-size:70%;">Rouge-L <math alttext="\uparrow" class="ltx_Math" display="inline" id="S2.T1.2.2.1.1.m1.1"><semantics id="S2.T1.2.2.1.1.m1.1a"><mo id="S2.T1.2.2.1.1.m1.1.1" stretchy="false" xref="S2.T1.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.1.m1.1b"><ci id="S2.T1.2.2.1.1.m1.1.1.cmml" xref="S2.T1.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.2.3"><span class="ltx_text" id="S2.T1.2.2.3.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4"><span class="ltx_text" id="S2.T1.2.2.4.1" style="font-size:70%;">0.338</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.5"><span class="ltx_text" id="S2.T1.2.2.5.1" style="font-size:70%;">-0.6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.6"><span class="ltx_text" id="S2.T1.2.2.6.1" style="font-size:70%;">0.361</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.7"><span class="ltx_text" id="S2.T1.2.2.7.1" style="font-size:70%;">-0.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.8"><span class="ltx_text" id="S2.T1.2.2.8.1" style="font-size:70%;">0.359</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.9"><span class="ltx_text" id="S2.T1.2.2.9.1" style="font-size:70%;">-2.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.10"><span class="ltx_text" id="S2.T1.2.2.10.1" style="font-size:70%;">0.335</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.11"><span class="ltx_text" id="S2.T1.2.2.11.1" style="font-size:70%;">-0.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.12"><span class="ltx_text" id="S2.T1.2.2.12.1" style="font-size:70%;">0.361</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.13"><span class="ltx_text" id="S2.T1.2.2.13.1" style="font-size:70%;">+1.2%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.14"><span class="ltx_text" id="S2.T1.2.2.14.1" style="font-size:70%;">0.384</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.15"><span class="ltx_text" id="S2.T1.2.2.15.1" style="font-size:70%;">-2.5%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.10.1"><span class="ltx_text" id="S2.T1.2.10.1.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.2"><span class="ltx_text" id="S2.T1.2.10.2.1" style="font-size:70%;">0.380</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.3"><span class="ltx_text" id="S2.T1.2.10.3.1" style="font-size:70%;color:#FF0000;">-9.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.4"><span class="ltx_text" id="S2.T1.2.10.4.1" style="font-size:70%;">0.404</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.5"><span class="ltx_text" id="S2.T1.2.10.5.1" style="font-size:70%;">-1.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.6"><span class="ltx_text" id="S2.T1.2.10.6.1" style="font-size:70%;">0.397</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.7"><span class="ltx_text" id="S2.T1.2.10.7.1" style="font-size:70%;">-1.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.8"><span class="ltx_text" id="S2.T1.2.10.8.1" style="font-size:70%;">0.360</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.9"><span class="ltx_text" id="S2.T1.2.10.9.1" style="font-size:70%;">+0.9%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.10"><span class="ltx_text" id="S2.T1.2.10.10.1" style="font-size:70%;">0.400</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.11"><span class="ltx_text" id="S2.T1.2.10.11.1" style="font-size:70%;">-0.9%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.12"><span class="ltx_text" id="S2.T1.2.10.12.1" style="font-size:70%;">0.402</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.10.13"><span class="ltx_text" id="S2.T1.2.10.13.1" style="font-size:70%;color:#FF0000;">-5.6%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.11.1"><span class="ltx_text" id="S2.T1.2.11.1.1" style="font-size:70%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.2"><span class="ltx_text" id="S2.T1.2.11.2.1" style="font-size:70%;">0.385</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.3"><span class="ltx_text" id="S2.T1.2.11.3.1" style="font-size:70%;color:#FF0000;">-11.6%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.4"><span class="ltx_text" id="S2.T1.2.11.4.1" style="font-size:70%;">0.415</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.5"><span class="ltx_text" id="S2.T1.2.11.5.1" style="font-size:70%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.6"><span class="ltx_text" id="S2.T1.2.11.6.1" style="font-size:70%;">0.412</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.7"><span class="ltx_text" id="S2.T1.2.11.7.1" style="font-size:70%;">-0.2%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.8"><span class="ltx_text" id="S2.T1.2.11.8.1" style="font-size:70%;">0.360</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.9"><span class="ltx_text" id="S2.T1.2.11.9.1" style="font-size:70%;">-0.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.10"><span class="ltx_text" id="S2.T1.2.11.10.1" style="font-size:70%;">0.410</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.11"><span class="ltx_text" id="S2.T1.2.11.11.1" style="font-size:70%;">-0.5%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.12"><span class="ltx_text" id="S2.T1.2.11.12.1" style="font-size:70%;">0.404</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.11.13"><span class="ltx_text" id="S2.T1.2.11.13.1" style="font-size:70%;color:#FF0000;">-5.2%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.12.1"><span class="ltx_text" id="S2.T1.2.12.1.1" style="font-size:70%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.2"><span class="ltx_text" id="S2.T1.2.12.2.1" style="font-size:70%;">0.374</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.3"><span class="ltx_text" id="S2.T1.2.12.3.1" style="font-size:70%;color:#FF0000;">-10.3%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.12.4.1" style="font-size:70%;">0.422</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.5"><span class="ltx_text" id="S2.T1.2.12.5.1" style="font-size:70%;">-0.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.6"><span class="ltx_text" id="S2.T1.2.12.6.1" style="font-size:70%;">0.419</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.7"><span class="ltx_text" id="S2.T1.2.12.7.1" style="font-size:70%;">-1.4%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.8"><span class="ltx_text" id="S2.T1.2.12.8.1" style="font-size:70%;">0.365</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.9"><span class="ltx_text" id="S2.T1.2.12.9.1" style="font-size:70%;">-2.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.10"><span class="ltx_text" id="S2.T1.2.12.10.1" style="font-size:70%;">0.415</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.11"><span class="ltx_text" id="S2.T1.2.12.11.1" style="font-size:70%;">-0.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.12"><span class="ltx_text" id="S2.T1.2.12.12.1" style="font-size:70%;">0.397</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.12.13"><span class="ltx_text" id="S2.T1.2.12.13.1" style="font-size:70%;">-4.5%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.13.1"><span class="ltx_text" id="S2.T1.2.13.1.1" style="font-size:70%;">max_4K</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.2"><span class="ltx_text" id="S2.T1.2.13.2.1" style="font-size:70%;">0.337</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.3"><span class="ltx_text" id="S2.T1.2.13.3.1" style="font-size:70%;color:#FF0000;">-16.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.4"><span class="ltx_text" id="S2.T1.2.13.4.1" style="font-size:70%;">0.419</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.5"><span class="ltx_text" id="S2.T1.2.13.5.1" style="font-size:70%;">-1.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.6"><span class="ltx_text" id="S2.T1.2.13.6.1" style="font-size:70%;">0.402</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.7"><span class="ltx_text" id="S2.T1.2.13.7.1" style="font-size:70%;">-0.5%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.8"><span class="ltx_text" id="S2.T1.2.13.8.1" style="font-size:70%;">0.357</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.9"><span class="ltx_text" id="S2.T1.2.13.9.1" style="font-size:70%;color:#FF0000;">-7.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.10"><span class="ltx_text" id="S2.T1.2.13.10.1" style="font-size:70%;">0.410</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.11"><span class="ltx_text" id="S2.T1.2.13.11.1" style="font-size:70%;">-1.1%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.12"><span class="ltx_text" id="S2.T1.2.13.12.1" style="font-size:70%;">0.376</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.13.13"><span class="ltx_text" id="S2.T1.2.13.13.1" style="font-size:70%;">-2.6%</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.14.1"><span class="ltx_text" id="S2.T1.2.14.1.1" style="font-size:70%;">max_8K</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.2"><span class="ltx_text" id="S2.T1.2.14.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.3"><span class="ltx_text" id="S2.T1.2.14.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.4"><span class="ltx_text" id="S2.T1.2.14.4.1" style="font-size:70%;">0.395</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.5"><span class="ltx_text" id="S2.T1.2.14.5.1" style="font-size:70%;">-1.0%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.6"><span class="ltx_text" id="S2.T1.2.14.6.1" style="font-size:70%;">0.379</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.7"><span class="ltx_text" id="S2.T1.2.14.7.1" style="font-size:70%;">-1.9%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.8"><span class="ltx_text" id="S2.T1.2.14.8.1" style="font-size:70%;">0.326</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.9"><span class="ltx_text" id="S2.T1.2.14.9.1" style="font-size:70%;color:#FF0000;">-18.7%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.10"><span class="ltx_text" id="S2.T1.2.14.10.1" style="font-size:70%;">0.387</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.11"><span class="ltx_text" id="S2.T1.2.14.11.1" style="font-size:70%;">-0.8%</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.12"><span class="ltx_text" id="S2.T1.2.14.12.1" style="font-size:70%;">0.384</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.14.13"><span class="ltx_text" id="S2.T1.2.14.13.1" style="font-size:70%;color:#FF0000;">-7.2%</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S3.F2.sf1.g1" src="extracted/5769092/lamp3_res_tight.png" width="586"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>MAE results for LaMP-3U, the lower the better</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S3.F2.sf2.g1" src="extracted/5769092/lamp5_res_tight.png" width="586"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Rouge-L results for LaMP-5U, the higher the better</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Results for both datasets. The upper and lower borders of each colored area represent the quantized and not-quantized performances of the models, and the corresponding lines are the mean of both.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>LLMs</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S2.T1" title="Table 1 ‣ 2.3 Retrieval ‣ 2 Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>. We see the dominance of OpenChat in both datasets. Zephyr performs very close to OpenChat in LaMP-3U but falls far behind in LaMP-5U. The same can be said for Starling but reversed. Mistral-7B performs stable in both datasets albeit being slightly behind OpenChat. Overall, LLaMA2 performs the worst as it is below average in both datasets. Despite being the dominant small LLM currently, LLaMA3 is not the best for both tasks, despite performing reasonably well in LaMP-3U. Interestingly, LLaMA3 has the best zero-shot score in LaMP-5U but it struggles to improve itself with retrieval.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Quantization</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">How much an LLM gets affected by quantization seems to be related to how well it performs the task. OpenChat suffers almost no performance degradation from quantization. On the contrary, LLaMA2 seems very sensitive, especially when the number of retrieved documents is increased. Starling suffers no significant consequence from quantization in LaMP-5U where it performs well, but it does suffer in LaMP-3U. There also seems to be a disparity between the tasks as quantized LLMs perform much worse in LaMP-3U than in LaMP-5U.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Number of retrieved documents</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.F2" title="Figure 2 ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> shows that LLM performance is saturated with a couple of documents, and the improvement obtained from more is marginal. In LaMP-5U, adding more than 5 documents starts to hurt the performance: Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.F2.sf2" title="In Figure 2 ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2b</span></a> shows an inverse-U-shaped distribution for all LLMs except LLaMA3. For some models, performance even drops below the zero-shot setting when all the available context window is filled with retrieved documents. The LaMP-3U results (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.F2.sf1" title="In Figure 2 ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2a</span></a>) continue to improve after adding more than 5 documents as can be observed from <math alttext="max\_4K" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">m</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">a</mi><mo id="S3.SS3.p1.1.m1.1.1.1a" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.4" xref="S3.SS3.p1.1.m1.1.1.4.cmml">x</mi><mo id="S3.SS3.p1.1.m1.1.1.1b" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.5" mathvariant="normal" xref="S3.SS3.p1.1.m1.1.1.5.cmml">_</mi><mo id="S3.SS3.p1.1.m1.1.1.1c" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mn id="S3.SS3.p1.1.m1.1.1.6" xref="S3.SS3.p1.1.m1.1.1.6.cmml">4</mn><mo id="S3.SS3.p1.1.m1.1.1.1d" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.7" xref="S3.SS3.p1.1.m1.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑚</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑎</ci><ci id="S3.SS3.p1.1.m1.1.1.4.cmml" xref="S3.SS3.p1.1.m1.1.1.4">𝑥</ci><ci id="S3.SS3.p1.1.m1.1.1.5.cmml" xref="S3.SS3.p1.1.m1.1.1.5">_</ci><cn id="S3.SS3.p1.1.m1.1.1.6.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.6">4</cn><ci id="S3.SS3.p1.1.m1.1.1.7.cmml" xref="S3.SS3.p1.1.m1.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">max\_4K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_m italic_a italic_x _ 4 italic_K</annotation></semantics></math> scores, but MAE also starts to get worse for <math alttext="max\_8K" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">m</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">a</mi><mo id="S3.SS3.p1.2.m2.1.1.1a" xref="S3.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.2.m2.1.1.4" xref="S3.SS3.p1.2.m2.1.1.4.cmml">x</mi><mo id="S3.SS3.p1.2.m2.1.1.1b" xref="S3.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.2.m2.1.1.5" mathvariant="normal" xref="S3.SS3.p1.2.m2.1.1.5.cmml">_</mi><mo id="S3.SS3.p1.2.m2.1.1.1c" xref="S3.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mn id="S3.SS3.p1.2.m2.1.1.6" xref="S3.SS3.p1.2.m2.1.1.6.cmml">8</mn><mo id="S3.SS3.p1.2.m2.1.1.1d" xref="S3.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.2.m2.1.1.7" xref="S3.SS3.p1.2.m2.1.1.7.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑚</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑎</ci><ci id="S3.SS3.p1.2.m2.1.1.4.cmml" xref="S3.SS3.p1.2.m2.1.1.4">𝑥</ci><ci id="S3.SS3.p1.2.m2.1.1.5.cmml" xref="S3.SS3.p1.2.m2.1.1.5">_</ci><cn id="S3.SS3.p1.2.m2.1.1.6.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.6">8</cn><ci id="S3.SS3.p1.2.m2.1.1.7.cmml" xref="S3.SS3.p1.2.m2.1.1.7">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">max\_8K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_m italic_a italic_x _ 8 italic_K</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We analyze whether a longer context window hurts the quantized variants more and find that there seems to be a peculiar relationship. INT4 LLaMA2 suffers from longer contexts, while INT4 OpenChat performs well and acts almost the same as its FP16 counterpart. INT4 Mistral and LLaMA3 act very similar to their FP16 counterparts in LaMP-5U but in LaMP-3U, they get progressively worse with more documents. Overall, quantization can increase the risk of worsened long-context capabilities but there is not a direct relationship as it is highly task and context-dependent.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Retrievers</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.F2" title="Figure 2 ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the three retrievers gave almost identical results, albeit BM25 being marginally behind the others. Also in LaMP-5U, the gap between the FP16 and INT4 LLaMA2 varies slightly. Other than that, the retriever model does not have a noticeable impact on the personalization tasks we experimented with. The patterns we found regarding LLMs, quantization, and the number of retrieved documents are the same for all the retrievers.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Our results compared with LaMP. Results indicated with * are not significantly lower than the reported best result (FlanT5-XXL). A quantized 7B LLM can perform on par with a larger model while being much more efficient.
</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3">
<tr class="ltx_tr" id="S3.T2.3.3">
<td class="ltx_td ltx_border_r" id="S3.T2.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.1">
<span class="ltx_text" id="S3.T2.1.1.1.2"></span><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1"> <span class="ltx_text" id="S3.T2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.1.1.1">
<span class="ltx_tr" id="S3.T2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.1.1.1.2.1">LaMP-3U</span></span>
<span class="ltx_tr" id="S3.T2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.1.1.1.1.1">(MAE) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.1.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.2">
<span class="ltx_text" id="S3.T2.2.2.2.2"></span><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.1"> <span class="ltx_text" id="S3.T2.2.2.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.2.2.2.1.1.1">
<span class="ltx_tr" id="S3.T2.2.2.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.2.2.1.1.1.2.1">LaMP-5U</span></span>
<span class="ltx_tr" id="S3.T2.2.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.2.2.1.1.1.1.1">(Rouge-L) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.2.2.2.1.1.1.1.1.m1.1"><semantics id="S3.T2.2.2.2.1.1.1.1.1.m1.1a"><mo id="S3.T2.2.2.2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.2.2.2.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.1.1.1.1.m1.1b"><ci id="S3.T2.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S3.T2.2.2.2.1.2"></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.3">
<span class="ltx_text" id="S3.T2.3.3.3.2"></span><span class="ltx_text ltx_font_bold" id="S3.T2.3.3.3.1"> <span class="ltx_text" id="S3.T2.3.3.3.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.3.1.1.1">
<span class="ltx_tr" id="S3.T2.3.3.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.3.1.1.1.2.1">Required</span></span>
<span class="ltx_tr" id="S3.T2.3.3.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.3.1.1.1.1.1">VRAM <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.3.3.3.1.1.1.1.1.m1.1"><semantics id="S3.T2.3.3.3.1.1.1.1.1.m1.1a"><mo id="S3.T2.3.3.3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.3.3.3.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.1.1.1.1.m1.1b"><ci id="S3.T2.3.3.3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S3.T2.3.3.3.1.2"></span></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.3.4.1">
<span class="ltx_text" id="S3.T2.3.4.1.1"></span> <span class="ltx_text" id="S3.T2.3.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.3.4.1.2.1">
<span class="ltx_tr" id="S3.T2.3.4.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.4.1.2.1.1.1">ChatGPT</span></span>
<span class="ltx_tr" id="S3.T2.3.4.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.4.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S3.T2.3.4.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.4.2">0.658</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.4.3">0.336</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.4.4">?</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.3.5.1">
<span class="ltx_text" id="S3.T2.3.5.1.1"></span> <span class="ltx_text" id="S3.T2.3.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.3.5.1.2.1">
<span class="ltx_tr" id="S3.T2.3.5.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.5.1.2.1.1.1">FlanT5-XXL</span></span>
<span class="ltx_tr" id="S3.T2.3.5.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.5.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite></span></span>
</span></span><span class="ltx_text" id="S3.T2.3.5.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.5.2">0.282</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.5.3"><span class="ltx_text ltx_font_bold" id="S3.T2.3.5.3.1">0.424</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.4">43 GB</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.3.6.1">
<span class="ltx_text" id="S3.T2.3.6.1.1"></span> <span class="ltx_text" id="S3.T2.3.6.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.3.6.1.2.1">
<span class="ltx_tr" id="S3.T2.3.6.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.6.1.2.1.1.1">OpenChat</span></span>
<span class="ltx_tr" id="S3.T2.3.6.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.6.1.2.1.2.1">(FP16)</span></span>
</span></span><span class="ltx_text" id="S3.T2.3.6.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.6.2">0.238</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.6.3">0.423*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.4">28 GB</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.3.7.1">
<span class="ltx_text" id="S3.T2.3.7.1.1"></span> <span class="ltx_text" id="S3.T2.3.7.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.3.7.1.2.1">
<span class="ltx_tr" id="S3.T2.3.7.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.7.1.2.1.1.1">OpenChat</span></span>
<span class="ltx_tr" id="S3.T2.3.7.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.7.1.2.1.2.1">(INT4)</span></span>
</span></span><span class="ltx_text" id="S3.T2.3.7.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.7.2"><span class="ltx_text ltx_font_bold" id="S3.T2.3.7.2.1">0.234</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.7.3">0.419*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.7.4"><span class="ltx_text ltx_font_bold" id="S3.T2.3.7.4.1">4.2 GB</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Benchmark comparison</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.2">Finally, we compared our findings with the RAG results from LaMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib15" title="">15</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.T2" title="Table 2 ‣ 3.4 Retrievers ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> shows that OpenChat (Contriever, <math alttext="k=5" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mrow id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">k</mi><mo id="S3.SS5.p1.1.m1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><eq id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1"></eq><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝑘</ci><cn id="S3.SS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS5.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_k = 5</annotation></semantics></math>) can beat FlanT5-XXL in LaMP-3U and performs very close in LaMP-5U. More importantly, its quantized counterpart has very similar results. Since we do not have the per-sample scores for the baseline models from LaMP, we perform a one-sample t-test on the Rouge-L scores. The corresponding <math alttext="p" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_p</annotation></semantics></math> value of 0.29 shows a non-significant difference between the results. Moreover, the results from the LaMP paper are with finetuned retrievers while our results are with non-finetuned retrievers. This indicates that a quantized-7B LLM can compete and even outperform a bigger model on personalization with RAG.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#S3.T2" title="Table 2 ‣ 3.4 Retrievers ‣ 3 Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> shows how much GPU VRAM is needed to deploy each model. With this comparison, the benefit of quantization becomes more pronounced: multiple high-level consumer GPUs or an A100 is necessary for running even a 7B LLM while a mid-level consumer GPU (eg. RTX 3060) would be enough to run it with quantization. According to the scores taken from LaMP, both FlanT5-XXL and OpenChat decisively beat ChatGPT, but the authors warn that the prompts used for ChatGPT may not be ideal and may contribute to a sub-optimal performance. Therefore, our results should not be used to make a comparison with ChatGPT.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our results show that some LLMs (in particular OpenChat) can be successful in RAG pipelines, even after quantization, but the performance is LLM- and task-dependent. The method of quantization affects LLMs differently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib14" title="">14</a>]</cite>. Thus, the relationship between quantization and RAG performance is not straightforward and can be studied more extensively. Still, our results indicate that when a small LLM performs the task well, its AWQ-quantized counterpart performs on par.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The differing performance of some LLMs between the datasets may be partly due to prompting. LLMs are sensitive to prompts, and a prompt that works for one LLM may not work for another one <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib27" title="">27</a>]</cite>. The most peculiar result is the lackluster performance of LLaMA3 in LaMP-5U. LLaMA3 is a recently released model trained with an extensive pretraining corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib16" title="">16</a>]</cite>. It has a higher chance of seeing the abstracts presented in the LaMP-5U in its pretraining data. This may explain its superior zero-shot performance. LLMs suffer from a knowledge conflict between their parametric information and the contextual information presented through retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib9" title="">9</a>]</cite>. If LLaMA3 had already memorized some of the titles of the abstracts in LaMP-5U, it might result in a knowledge conflict when similar abstract-title pairs of the same author are presented. This may explain the reduced improvement in its performance with retrieval.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">LLMs have been shown to struggle with too many retrieved documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib8" title="">8</a>]</cite>, and our findings are in accordance. Our results indicate that more than <math alttext="&gt;5" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><gt id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">absent</csymbol><cn id="S4.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">&gt;5</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">&gt; 5</annotation></semantics></math> documents do not help and can even hurt performance. From prior studies, we know that LLMs focus more on the bottom and the top of their context window <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib8" title="">8</a>]</cite>. We progressively put the most relevant documents starting from the bottom to the top. Therefore especially for <math alttext="k=max" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">k</mi><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">=</mo><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mi id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">m</mi><mo id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml">a</mi><mo id="S4.p3.2.m2.1.1.3.1a" xref="S4.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.p3.2.m2.1.1.3.4" xref="S4.p3.2.m2.1.1.3.4.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><eq id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></eq><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝑘</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><times id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></times><ci id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">𝑚</ci><ci id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3">𝑎</ci><ci id="S4.p3.2.m2.1.1.3.4.cmml" xref="S4.p3.2.m2.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">k=max</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_k = italic_m italic_a italic_x</annotation></semantics></math> settings, the less relevant documents are put on the top. This situation might hurt the LLM performance as it would focus on the most and the least related information in this case. That being said, state-of-the-art LLMs with more than 7B parameters also suffer from the same phenomenon even when not quantized <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10251v3#bib.bib8" title="">8</a>]</cite>. Although quantization increases the risk of worsened long-context performance, we cannot conclude that it is the sole perpetrator, as this is an inherent problem for all LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">We have shown that quantized smaller LLMs can use RAG to perform complex tasks such as personalization. Even though quantization might decrease the ability of LLMs to analyze long contexts, it is task- and LLM-dependent. An LLM that performs well on a task does not lose much of its long-context abilities when quantized. Thus, we conclude that quantized 7B LLMs can be the backbones of RAG with long contexts. The reduced computational load obtained from quantization would make it possible to run RAG applications with more affordable and accessible hardware. For future work, more quantization methods can be included in the experiments to see if the findings can be replicated across different methods. We can also extend the number set of k, especially between <math alttext="k=5" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><eq id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></eq><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝑘</ci><cn id="S5.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_k = 5</annotation></semantics></math> and <math alttext="k=max\_4K" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">k</mi><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml"><mi id="S5.p1.2.m2.1.1.3.2" xref="S5.p1.2.m2.1.1.3.2.cmml">m</mi><mo id="S5.p1.2.m2.1.1.3.1" xref="S5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.p1.2.m2.1.1.3.3" xref="S5.p1.2.m2.1.1.3.3.cmml">a</mi><mo id="S5.p1.2.m2.1.1.3.1a" xref="S5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.p1.2.m2.1.1.3.4" xref="S5.p1.2.m2.1.1.3.4.cmml">x</mi><mo id="S5.p1.2.m2.1.1.3.1b" xref="S5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.p1.2.m2.1.1.3.5" mathvariant="normal" xref="S5.p1.2.m2.1.1.3.5.cmml">_</mi><mo id="S5.p1.2.m2.1.1.3.1c" xref="S5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mn id="S5.p1.2.m2.1.1.3.6" xref="S5.p1.2.m2.1.1.3.6.cmml">4</mn><mo id="S5.p1.2.m2.1.1.3.1d" xref="S5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.p1.2.m2.1.1.3.7" xref="S5.p1.2.m2.1.1.3.7.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><eq id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></eq><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">𝑘</ci><apply id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3"><times id="S5.p1.2.m2.1.1.3.1.cmml" xref="S5.p1.2.m2.1.1.3.1"></times><ci id="S5.p1.2.m2.1.1.3.2.cmml" xref="S5.p1.2.m2.1.1.3.2">𝑚</ci><ci id="S5.p1.2.m2.1.1.3.3.cmml" xref="S5.p1.2.m2.1.1.3.3">𝑎</ci><ci id="S5.p1.2.m2.1.1.3.4.cmml" xref="S5.p1.2.m2.1.1.3.4">𝑥</ci><ci id="S5.p1.2.m2.1.1.3.5.cmml" xref="S5.p1.2.m2.1.1.3.5">_</ci><cn id="S5.p1.2.m2.1.1.3.6.cmml" type="integer" xref="S5.p1.2.m2.1.1.3.6">4</cn><ci id="S5.p1.2.m2.1.1.3.7.cmml" xref="S5.p1.2.m2.1.1.3.7">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">k=max\_4K</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_k = italic_m italic_a italic_x _ 4 italic_K</annotation></semantics></math>, and change the order of the documents to better understand how quantized LLMs use their context windows.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research is part of the project LESSEN with project number NWA.1389.20.183 of the research program NWA_ORC 202021, which is (partly) funded by the Dutch Research Council (NWO).

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023]</span>
<span class="ltx_bibblock">
J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, B. Catanzaro, Raven: In-context learning with retrieval augmented encoder-decoder language models, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2308.07922" title="">arXiv:2308.07922</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023]</span>
<span class="ltx_bibblock">
X. Ma, Y. Gong, P. He, H. Zhao, N. Duan, Query rewriting for retrieval-augmented large language models, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2305.14283" title="">arXiv:2305.14283</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2023]</span>
<span class="ltx_bibblock">
W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, W. tau Yih, Replug: Retrieval-augmented black-box language models, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2301.12652" title="">arXiv:2301.12652</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, B. Catanzaro, Retrieval meets long context large language models, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2310.03025" title="">arXiv:2310.03025</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Proser [2023]</span>
<span class="ltx_bibblock">
Z. Proser, Retrieval augmented generation (rag): Reducing hallucinations in genai applications, 2023. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pinecone.io/learn/retrieval-augmented-generation/" title="">https://www.pinecone.io/learn/retrieval-augmented-generation/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. [2022]</span>
<span class="ltx_bibblock">
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, J. Schulman, Webgpt: Browser-assisted question-answering with human feedback, 2022. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2112.09332" title="">arXiv:2112.09332</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2024]</span>
<span class="ltx_bibblock">
Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, H. Wang, Retrieval-augmented generation for large language models: A survey, 2024. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2312.10997" title="">arXiv:2312.10997</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang, Lost in the middle: How language models use long contexts, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2307.03172" title="">arXiv:2307.03172</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2024]</span>
<span class="ltx_bibblock">
R. Xu, Z. Qi, C. Wang, H. Wang, Y. Zhang, W. Xu, Knowledge conflicts for llms: A survey, 2024. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2403.08319" title="">arXiv:2403.08319</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux,
T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, T. Scialom, Llama 2: Open
foundation and fine-tuned chat models, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2307.09288" title="">arXiv:2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI, Gpt-4 technical report, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" title="">arXiv:2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. [2023]</span>
<span class="ltx_bibblock">
T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2305.14314" title="">arXiv:2305.14314</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et al. [2023]</span>
<span class="ltx_bibblock">
E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2210.17323" title="">arXiv:2210.17323</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, Y. Wang, Evaluating quantized large language models, 2024. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2402.18158" title="">arXiv:2402.18158</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi et al. [2023]</span>
<span class="ltx_bibblock">
A. Salemi, S. Mysore, M. Bendersky, H. Zamani, Lamp: When large language models meet personalization, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2304.11406" title="">arXiv:2304.11406</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta [2024]</span>
<span class="ltx_bibblock">
Meta, Introducing meta llama 3: The most capable openly available llm to date, 2024. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et al. [2023]</span>
<span class="ltx_bibblock">
L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib, N. Sarrazin, O. Sanseviero, A. M. Rush, T. Wolf, Zephyr: Direct distillation of lm alignment, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2310.16944" title="">arXiv:2310.16944</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023]</span>
<span class="ltx_bibblock">
G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, Y. Liu, Openchat: Advancing open-source language models with mixed-quality data, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2309.11235" title="">arXiv:2309.11235</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
B. Zhu, E. Frick, T. Wu, H. Zhu, J. Jiao, Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023]</span>
<span class="ltx_bibblock">
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, I. Stoica, Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2306.05685" title="">arXiv:2306.05685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, W. E. Sayed, Mistral 7b, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2310.06825" title="">arXiv:2310.06825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023]</span>
<span class="ltx_bibblock">
J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, C. Gan, S. Han, Awq: Activation-aware weight quantization for llm compression and acceleration, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2306.00978" title="">arXiv:2306.00978</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin [2004]</span>
<span class="ltx_bibblock">
C.-Y. Lin,

</span>
<span class="ltx_bibblock">ROUGE: A package for automatic evaluation of summaries,

</span>
<span class="ltx_bibblock">in: Text Summarization Branches Out, Association for Computational Linguistics, Barcelona, Spain, 2004, pp. 74–81. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W04-1013" title="">https://aclanthology.org/W04-1013</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. [1994]</span>
<span class="ltx_bibblock">
S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, M. Gatford,

</span>
<span class="ltx_bibblock">Okapi at trec-3.,

</span>
<span class="ltx_bibblock">1994, pp. 0–.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. [2022]</span>
<span class="ltx_bibblock">
G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, E. Grave, Unsupervised dense information retrieval with contrastive learning, 2022. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2112.09118" title="">arXiv:2112.09118</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. [2020]</span>
<span class="ltx_bibblock">
V. Karpukhin, B. Oğuz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, W. tau Yih, Dense passage retrieval for open-domain question answering, 2020. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2004.04906" title="">arXiv:2004.04906</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sclar et al. [2023]</span>
<span class="ltx_bibblock">
M. Sclar, Y. Choi, Y. Tsvetkov, A. Suhr, Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting, 2023. <a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2310.11324" title="">arXiv:2310.11324</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug  1 16:28:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
