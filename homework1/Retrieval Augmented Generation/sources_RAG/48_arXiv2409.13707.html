<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support</title>
<!--Generated on Fri Sep  6 12:59:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13707v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S1" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S2" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S3" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Development and Validation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.SSx1" title="In 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Case Turn Classifier</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.SSx2" title="In 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Query Generator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.SSx3" title="In 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.SSx4" title="In 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Answer Generator</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S5" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Deployment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S6" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Lessons Learned</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S6.SSx1" title="In 6 Lessons Learned ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Use Case Formation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S6.SSx2" title="In 6 Lessons Learned ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Inter-Annotator Agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S6.SSx3" title="In 6 Lessons Learned ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">RAG Bottlenecks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S7" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S7.SSx1" title="In 7 Related Work ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">LLM-Based AIOps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S7.SSx2" title="In 7 Related Work ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title">Retrieval and Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S8" title="In Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Retrieval Augmented Generation-Based 
<br class="ltx_break"/>Incident Resolution Recommendation System for IT Support</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Paulina Toro Isaza,
Michael Nidd,
Noah Zheutlin,
Jae-wook Ahn,
Chidansh Amitkumar Bhatt,
<br class="ltx_break"/>Yu Deng,
Ruchi Mahindru,
Martin Franz,
Hans Florian,
Salim Roukos
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Clients wishing to implement generative AI in the domain of IT Support and AIOps face two critical issues: domain coverage and model size constraints due to model choice limitations. Clients might choose to not use larger proprietary models such as GPT-4 due to cost and privacy concerns and so are limited to smaller models with potentially less domain coverage that do not generalize to the client’s domain. Retrieval augmented generation is a common solution that addresses both of these issues: a retrieval system first retrieves the necessary domain knowledge which a smaller generative model leverages as context for generation. We present a system developed for a client in the IT Support domain for support case solution recommendation that combines retrieval augmented generation (RAG) for answer generation with an encoder-only model for classification and a generative large language model for query generation. We cover architecture details, data collection and annotation, development journey and preliminary validations, expected final deployment process and evaluation plans, and finally lessons learned.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The recent boost in performance and popularization of generative models has resulted in clients across various domains requesting generative AI powered question-answering and recommendation systems. However, there are two critical issues facing many clients wishing to implement generative AI: domain coverage and model size constraints due to model choice limitations. Much of the focus for generative models has been on the general domain and only some specific tasks such as coding. Models that work on these domains might not necessarily work for a client’s targeted domain. Additionally, clients might choose not to leverage larger proprietary models such as GPT-4 because of cost and privacy concerns so clients are limited to smaller models with less domain coverage and likely lower out-of-the-box performance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We have faced both of these issues when building a solution recommendation system for resolving IT support cases. Models and tasks within the domain of IT support and Artificial Intelligence for IT Operations (AIOps) are under-researched. No IT support specific fine-tuned generative AI model exists and there are limited publicly available datasets on IT support tasks like question-answering (QA) or retrieval over a corpus of IT support documents. Thus, it is non-trivial to evaluate out-of-the-box AI models on IT support tasks as well as train and evaluate custom AI solutions in this domain.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In specific, the IT support
use case involved IT product support tickets that are opened by a customer and answered by a support agent after a series of interactions.
It required a system that would respond only to cases that did not need any additional information or clarification from the customer. That is, the support case could be resolved based only on the initial information present in the case subject and description when the ticket was first opened. Additionally, the use case required that solutions be grounded in an official support document that would be presented to the customer as a link along with the summarized solution.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Given these use case requirements, retrieval augmented generation (RAG) was a natural fit <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib11" title="">2020</a>)</cite>. It is a common solution that addresses the two main issues of inadequate generalizability to more niche domains and model size limits. It does so by using a retrieval system to first retrieves the necessary domain knowledge which a smaller generative model then leverages as context for answer generation.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We present the resulting system for IT support case solution recommendation that combines an encoder-only model for classification, two dense embedding models for retrieval, and generative large language models for query and answer generation. The solution brings several novel contributions to the field of AIOps:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The first reported evaluation of a RAG system for IT support incident resolution recommendation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The first reported use of a classifier for determining single vs multi-turn IT support cases.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Evidence of substantial retrieval improvement using re-ranking based on a new model, IBM Slate 125m <cite class="ltx_cite ltx_citemacro_citep">(IBM Research <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib9" title="">2024</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">A comparison of answer generation performance across diverse model sizes that shows smaller models can match or even beat the performance of very large models in the RAG incident remediation use case.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Architecture</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="456" id="S2.F1.g1" src="extracted/5836367/figures/architecture.png" width="250"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>System architecture</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Given a support case subject, description, and product name, our system generates recommended solutions based on corpora of support documents. Our system consists of four major components as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S2.F1" title="Figure 1 ‣ 2 Architecture ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">1</span></a>: an encoder-only transformer <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">classifier</span>, a <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">query generation</span> system, a <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">retriever</span> system, and an <span class="ltx_text ltx_font_italic" id="S2.p1.1.4">answer generator</span> system.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.1">Preprocessing:</span> Support cases are ingested with unstructured text data fields of case subject, case description, product name, and product version number. The escape and non-ASCII characters are removed from the case subject and description, and the two fields are concatenated. We preprocess the product name by matching it to a dictionary of known product acronyms or alternative names to append to the query used in retrieval.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p3.1.1">Case Turn Classifier:</span> The cleaned and concatenated case subject and description are fed into the classifier which determines if the support case is a single turn. ingle-turn cases are defined as those that can be resolved using only the information present in the case subject and description, without requiring any additional information or clarification from the customer. The classifier is an encoder-only transformer model IBM Slate 125m <cite class="ltx_cite ltx_citemacro_citep">(IBM Research <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib9" title="">2024</a>)</cite> that was fine-tuned on almost 14,000 examples labeled by subject matter experts. If the case is predicted to be single turn, the case continues to the next step in the pipeline.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p4.1.1">Question Generator:</span> The question generator summarizes the often vague case subjects and verbose, convoluted descriptions into concise text queries suitable for the retrieval system. The system prompts a large language model, Mixtral 8x7B Instruct <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib10" title="">2024</a>)</cite>, to generate a concise question based on the provided case subject and description. Additional post-processing keeps only the first generated sentence in case the generative model does not follow instructions and generates additional sentences beyond the first question.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p5.1.1">Retriever:</span> Our documentation is retrieved from multiple data collections in a Milvus vector database, all indexed with the standard Slate-30M embedding. If the search stage requires top-3 documents, we search for 3
from each of these indexes, and then merge-sort based on the score before retaining the top-<math alttext="k" class="ltx_Math" display="inline" id="S2.p5.1.m1.1"><semantics id="S2.p5.1.m1.1a"><mi id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><ci id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p5.1.m1.1d">italic_k</annotation></semantics></math> from the combined set.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Having retrieved the top-3 documents, as ranked by a
general-purpose embedding, we re-rank them using a Bi-Encoder model
that has been fine-tuned using application-specific training data.
Re-ranker scores are computed as cosine similarities of a combination of the original case and the derived question, compared with the same passages (with recalculated vectorizations) that were matched in the first pass.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p7.1.1">Answer Generator: </span>For each of the top three documents retrieved, the answer generator produces an answer to the previously generated query by leveraging the top three re-ranked passages from the document. First, we split the retrieved document content into 2500-token chunks with 250-token overlaps and use cosine similarity to pick the three most relevant chunk contexts. The answer generator prompts a large language model, Mixtral, to answer the query using the three contexts. Finally, the system returns the three retrieved links with their corresponding generated answers. The results can then be displayed to the support agent in a graphic interface.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We collected almost 19,000 real support cases across nine software products: six to serve as training and three to serve as validation. Each case included a case subject and description originally drafted by a customer when opening the case. Additionally, the three indices for documentation leveraged for retrieval had a total corpus of over 5 million documents.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">For each product, we asked five support agents who were singled out as product subject matter experts to carry out the following tasks for each of the nine products: 1) annotate single-turn vs. multi-turn label, 2) validate silver ground truth query based on case subject and description and provide updated query as necessary, 3) provide link to relevant document in support corpus, and finally 4) copy and paste solution to query as found in the relevant support document.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Tasks #2 through #4 were carried out only for cases that had been labeled as single-turn. After cleaning and removing missing annotations, we created a dataset of almost 19,000 support cases, with almost 3,000 cases for each training product and over 400 cases for each evaluation product.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Development and Validation</h2>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Case Turn Classifier</h3>
<div class="ltx_para" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">As our solution is meant to supply answers before involving a support agent, we needed to develop a method for classifying incoming cases as single-turn vs. multi-turn. The classifier model is a binary encoder-only IBM Slate 125m model fine-tuned on the single-turn/multi-turn labels of almost 7,000 unique cases across six software products. The final training data is around 14,000 cases as it includes two copies of a given case: one with tokens from both the case subject and description fields, and another with tokens only from the case subject..</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Positive</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.5" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.6" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.7" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.2.1.1" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.2.1">Train</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.3.1">Eval</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.4.1">Class %</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.5.1">F1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.6.1">P</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.2.1.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.7.1">R</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">In-Domain</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.2" style="padding-left:2.8pt;padding-right:2.8pt;">13964</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.3" style="padding-left:2.8pt;padding-right:2.8pt;">3512</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.4" style="padding-left:2.8pt;padding-right:2.8pt;">25%</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.5" style="padding-left:2.8pt;padding-right:2.8pt;">0.46</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.6" style="padding-left:2.8pt;padding-right:2.8pt;">0.31</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.7" style="padding-left:2.8pt;padding-right:2.8pt;">0.89</th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">Out-of-Domain</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.2" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.3" style="padding-left:2.8pt;padding-right:2.8pt;">1375</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.4" style="padding-left:2.8pt;padding-right:2.8pt;">53%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.5" style="padding-left:2.8pt;padding-right:2.8pt;">0.65</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.6" style="padding-left:2.8pt;padding-right:2.8pt;">0.54</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.7" style="padding-left:2.8pt;padding-right:2.8pt;">0.80</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Single-Turn classifier model performance on six in-domain training products and three out-of-domain evaluation products</figcaption>
</figure>
<div class="ltx_para" id="S4.SSx1.p2">
<p class="ltx_p" id="S4.SSx1.p2.1">For our application, we prioritized correctly predicting single-turn cases (positive class) versus multi-turn cases (negative) emphasizing recall over precision. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T1" title="Table 1 ‣ Case Turn Classifier ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">1</span></a> presents the final fine-tuned model performance on the held-out evaluation set of the six products when using a classification threshold of 0.1. We found that performance varies widely depending on the product, ranging from F1 of 0.27 to 0.62 and recall from 0.75 to 0.98. The lower performance can be explained in part by the varying class imbalance across products (positive class proportion from 11% to 44%) as well as the products’ differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate, and dropout were determined based on a small grid search.</p>
</div>
<div class="ltx_para" id="S4.SSx1.p3">
<p class="ltx_p" id="S4.SSx1.p3.1">We then evaluated the fine-tuned classifier on about 1,400 cases from three additional products that were not in the training set to validate if the model generalized to other products. The resulting F1 of 0.65, precision of 0.54, and recall of 0.80 for the three products suggests that classifier model generalizes well to products not seen during training even with substantially different class balance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Query Generator</h3>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:151.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.7pt,-30.5pt) scale(1.6786164963825,1.6786164963825) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1">BertScore F1</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1">ROUGE-L F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.2.1.1">Falcon-40B*</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.2.1.2">0.91</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.2.1.3">0.40</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.3.2.1">Mistral-Large-2</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.3.2.2">0.91</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.3.2.3">0.38</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.4.3.1">Mixtral-8x7B-Instruct</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4.3.2">0.91</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4.3.3">0.36</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.5.4.1">Granite-13B-Chat-v2</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.5.4.2">0.89</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.5.4.3">0.28</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of BertScore F1 and ROUGE-L F1 for different models on query generation task. BertScore is based on roberta-large embeddings. 
<br class="ltx_break"/>*Falcon-40B scores are not comparable to the other models because the prompt used was different, and it was used to create the initial questions that were validated or edited by SMEs to create the ground truth.</figcaption>
</figure>
<div class="ltx_para" id="S4.SSx2.p1">
<p class="ltx_p" id="S4.SSx2.p1.1">In order to create a concise query that could be used by the retriever, we generated a single sentence question based on the case subject and description. Our experiments over various open-source generative models (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T2" title="Table 2 ‣ Query Generator ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">2</span></a>) and model availability in the client’s services led us to choose Mixtral-8x7b-Instruct as the model for query generation which reliably reproduced the ground truth queries despite being a relatively small model with no domain knowledge. Note that the results are skewed for Falcon-40B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib3" title="">2023</a>)</cite> as Falcon-40B generated the first pass of silver ground truth queries that were then edited by subject matter experts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Retriever</h3>
<div class="ltx_para" id="S4.SSx3.p1">
<p class="ltx_p" id="S4.SSx3.p1.2">Support experts supplied us with a collection of cases with one ground truth link each that a “correct” solution should reference; our evaluation is based on whether this link is contained in the top <math alttext="n" class="ltx_Math" display="inline" id="S4.SSx3.p1.1.m1.1"><semantics id="S4.SSx3.p1.1.m1.1a"><mi id="S4.SSx3.p1.1.m1.1.1" xref="S4.SSx3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SSx3.p1.1.m1.1b"><ci id="S4.SSx3.p1.1.m1.1.1.cmml" xref="S4.SSx3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SSx3.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SSx3.p1.1.m1.1d">italic_n</annotation></semantics></math> links returned (for various values of <math alttext="n" class="ltx_Math" display="inline" id="S4.SSx3.p1.2.m2.1"><semantics id="S4.SSx3.p1.2.m2.1a"><mi id="S4.SSx3.p1.2.m2.1.1" xref="S4.SSx3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SSx3.p1.2.m2.1b"><ci id="S4.SSx3.p1.2.m2.1.1.cmml" xref="S4.SSx3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SSx3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SSx3.p1.2.m2.1d">italic_n</annotation></semantics></math>). Implementing this evaluation presented several challenges:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">URL Duplication:</span> A single page of documentation often has several different URLs to identify it.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Subtle Content Variations:</span> Documentation for the same topic in different versions of a product may have subtly different titles, like “How to update a list” and “Lists: Updating”.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Identical Content Across Different Documents:</span> Documentation for the same topic in different versions may be identical in which case results from different versions are still valid.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SSx3.p2">
<p class="ltx_p" id="S4.SSx3.p2.1">Mitigating the first of these challenges, many of our documentation pages include a “canonical link” in their metadata.
In many cases, this allows us to identify identical links.
The two issues with documentation evolution between versions are addressed with Rouge-1 scores, using a threshold of 0.90 as sufficiently similar to count as identical.</p>
</div>
<div class="ltx_para" id="S4.SSx3.p3">
<p class="ltx_p" id="S4.SSx3.p3.1">For retrieval, a dedicated team is already responsible for maintaining indexed collections of the software product documentation.
This saves our project from gathering, maintaining, and indexing all of these documents, and its base Slate-30M embedding returns a good first set of results.</p>
</div>
<div class="ltx_para" id="S4.SSx3.p4">
<p class="ltx_p" id="S4.SSx3.p4.1">Re-ranking this first set allows us to use fine-tuning to improve performance without maintaining a parallel set of indexes.
For this final task-specific fine-tuning stage, we used training data based on 1,430 questions with up to three matching passages per question extracted from documents identified in user interactions, together with negative examples found using BM25 search. The resulting IBM Slate-125M model was then distilled into the deployed IBM Slate-30M model. To evaluate its effectiveness, we present recall before and after re-ranking with Google Search as a baseline in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.F2" title="Figure 2 ‣ Retriever ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S4.F2.g1" src="extracted/5836367/figures/retriever_results.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Retriever recall for top <math alttext="x" class="ltx_Math" display="inline" id="S4.F2.2.m1.1"><semantics id="S4.F2.2.m1.1b"><mi id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><ci id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.m1.1e">italic_x</annotation></semantics></math> (log scale), comparing with (yellow) and without (orange) re-ranking vs. direct Google search via SerpApi (grey) for 1729 customer issues over six products</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">0 - Completely</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">1 - somewhat</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">2 - Solution</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.2.2.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.1.1">Rating</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.2.1">irrelevant</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.3.1">relevant/helpful</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.2.2.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.4.1">in link</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.3.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">Product A</th>
<td class="ltx_td ltx_border_t" id="S4.T3.1.3.3.2" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.3.3.3" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.3.3.4" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">SME</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.2" style="padding-left:2.8pt;padding-right:2.8pt;">58% (11)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.3" style="padding-left:2.8pt;padding-right:2.8pt;">0% (0)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.4" style="padding-left:2.8pt;padding-right:2.8pt;">42% (8)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.5.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">Tool</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.2" style="padding-left:2.8pt;padding-right:2.8pt;">26% (5)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.3" style="padding-left:2.8pt;padding-right:2.8pt;">5% (1)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.4" style="padding-left:2.8pt;padding-right:2.8pt;">68% (13)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.6.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">Product B</th>
<td class="ltx_td ltx_border_t" id="S4.T3.1.6.6.2" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.6.6.3" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.6.6.4" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.7.7.1" style="padding-left:2.8pt;padding-right:2.8pt;">SME</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.2" style="padding-left:2.8pt;padding-right:2.8pt;">20% (12)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.3" style="padding-left:2.8pt;padding-right:2.8pt;">48% (28)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7.4" style="padding-left:2.8pt;padding-right:2.8pt;">28% (17)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.8.8.1" style="padding-left:2.8pt;padding-right:2.8pt;">Tool</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.8.2" style="padding-left:2.8pt;padding-right:2.8pt;">48% (29)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.8.3" style="padding-left:2.8pt;padding-right:2.8pt;">37% (22)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.8.4" style="padding-left:2.8pt;padding-right:2.8pt;">12% (7)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>AB testing human evaluation of retrieved links</figcaption>
</figure>
<div class="ltx_para" id="S4.SSx3.p5">
<p class="ltx_p" id="S4.SSx3.p5.1">We also conducted an AB test in which support agents of two products were provided with a link retrieved by the tool and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The support agents were asked to rate each link as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T3" title="Table 3 ‣ Retriever ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">3</span></a> and to pick the better of the two solutions.</p>
</div>
<div class="ltx_para" id="S4.SSx3.p6">
<p class="ltx_p" id="S4.SSx3.p6.1">The results for Product A show that support agents gave higher ratings to and preferred the links suggested by the tool over those from a SME. The results for Product B however show higher scores for links provided by the SME but about half of the cases were still rated as having a somewhat helpful or complete link provided by our tool. Additionally, when directly asked to compare the recommendations, support agents reported that the tool was more helpful or just as helpful as the SME link 69% of the time for Product A and 35% of the time for Product B.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx4">
<h3 class="ltx_title ltx_title_subsection">Answer Generator</h3>
<div class="ltx_para" id="S4.SSx4.p1">
<p class="ltx_p" id="S4.SSx4.p1.1">The final step of our solution takes in the generated query and top three most relevant retrieved passages as context to prompt the answer generator. In particular, the prompt asks the model to use the information within the provided contexts to generate an answer. Additionally, if the context is insufficient, then the model is instructed to state that an accurate answer cannot be provided.</p>
</div>
<div class="ltx_para" id="S4.SSx4.p2">
<p class="ltx_p" id="S4.SSx4.p2.1">To evaluate the answers, we used the subject matter expert’s annotated ground truth answers and ground truth documents verified to contain the answer to the question. We compared the answers generated by the answer extractor using the ground truth document to the ground truth answer using BertScore <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib21" title="">2020</a>)</cite> and ROUGE-L F1. We evaluated different models and prompts to find the optimal combination and present the results of the models assessed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T4" title="Table 4 ‣ Answer Generator ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">4</span></a>. While BertScore (roberta-large) F1 is rather low (in practice it ranges between 0.85-0.95), ROUGE-L F1, traditionally a rather strict metric, shows promising results for Mixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b-Instruct’s outperforms of GPT-4o, included as a baseline for larger models, in all three metrics, despite having substantially less parameters. Likewise, Granite-13B-Chat-v2 is not far behind GPT-4o despite its merely 13 billion parameters compared to GPT-4o’s rumored hundreds of billions or even trillions of parameters. This suggests that the RAG approach of smaller models leveraging retrieved context is a viable solution for IT incident resolution recommendation systems.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">BertScore (roberta-large) F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">BertScore (deberta-xlarge-mnli) F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.4.1">ROUGE-L F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.2.1.1">GPT-4o (2024-08-06)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4">0.34</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.3.2.1">Mistral-Large-2</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4">0.37</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.3.1">Mixtral-8x7B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.2">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.3">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.4">0.41</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.5.4.1">Granite-13B-Chat-v2</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.2">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.3">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.4">0.32</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of BertScore F1 and ROUGE-L F1 for different models performing the answer generation task. BertScore based on roberta-large embeddings</figcaption>
</figure>
<div class="ltx_para" id="S4.SSx4.p3">
<p class="ltx_p" id="S4.SSx4.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SSx4.p3.1.1">Knowledge Infusion for Answer Generation</span></p>
</div>
<div class="ltx_para" id="S4.SSx4.p4">
<p class="ltx_p" id="S4.SSx4.p4.1">Directly applying general foundational models to AIOps for answer generation tasks often does not yield optimal results. The knowledge infusion approach involves adapting these pre-trained models to specific tasks through additional training on task-specific data.</p>
</div>
<div class="ltx_para" id="S4.SSx4.p5">
<p class="ltx_p" id="S4.SSx4.p5.1">To further boost the results of our action generation task for AIOps domain, we employ the knowledge infusion methodology described in <cite class="ltx_cite ltx_citemacro_citep">(Sudalairaj et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib18" title="">2024</a>)</cite>.
First, we manually created a seed dataset with six tuples, each containing context and four related question-answer pairs. Then, we randomly selected fifteen documents from the corpus to guide synthetic data generation, using the seed dataset to replicate similar artifacts for each document. Using this seed dataset, we created 14,000 synthetic samples with the Mixtral-8x7B-Instruct model as the teacher. IBM’s Granite 7B IL-Internal-Granite-7B-Base, a much smaller model, was fine-tuned with IT domain data to cater to the specific task of answer generation for the IT Support use case resulting in the InstructLab-IT model with domain knowledge infusion.</p>
</div>
<div class="ltx_para" id="S4.SSx4.p6">
<p class="ltx_p" id="S4.SSx4.p6.1">To evaluate the quality improvement, we conducted a user study with 6 technical experts and forty test question-answer pairs per model. For each question, we retrieved context from a 1200-document corpus of six software products and used it to prompt each model separately for an answer. Our user study used a 0 to 1 rubric to evaluate answer correctness with clear descriptions for each level:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">0 = Incorrect: irrelevant or fails to answer the question.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">0.25 = Mostly Incorrect: Some details are correct, but key details are missing, fabricated, or mostly irrelevant.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">0.5 = Partially Correct: Most details are correct, but some key details are missing, fabricated, or include a lot of irrelevant information.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">0.75 = Mostly Correct: Most details are accurate, with only minor gaps or irrelevant information.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">1 = Correct: Includes only relevant details.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SSx4.p7">
<p class="ltx_p" id="S4.SSx4.p7.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T5" title="Table 5 ‣ Answer Generator ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">5</span></a>, we present the final score for each model as the average of human annotators’ scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the improvement in results with InstructLab-IT was very noticeable over both models. This is especially significant considering the model sizes: GPT-4o (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b-Instruct (8 billion parameters and 16 GB), and InstructLab-IT (7 billion parameters and 28 GB). These results signal that a smaller, domain-specific model tuned for a specific set of use cases may better meet client requirements.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.1.2.1.1">GPT-4o</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.1.2.1.2">0.68</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.3.2.1">Llama-3.1-8b-Instruct</th>
<td class="ltx_td ltx_align_right" id="S4.T5.1.3.2.2">0.70</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.4.3.1">InstructLab-IT</th>
<td class="ltx_td ltx_align_right" id="S4.T5.1.4.3.2">0.76</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of analytic rubric scores for different models on answer generation task.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Deployment</h2>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="152" id="S5.F3.g1" src="extracted/5836367/figures/ui_feedback.png" width="698"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Working mockup of online deployment UI of single system result with feedback items.</figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The tool is currently integrated into the ticketing system but silently deployed (not visible to agents) for testing purposes. We are currently working on refinements and integration and plan to deploy the system before the end of the year. We will incorporate feedback buttons for the tool once it is deployed online and visible to agents. In the customer support portal user interface, for a given case, support agents will see a suggested solution and link. This will include five-star ratings for accuracy and readability as well as a drop-down menu for feedback including the options: “useful”, “somewhat useful”, “no useful suggestion”, and “need more client info”. See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S5.F3" title="Figure 3 ‣ 5 Deployment ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">3</span></a> for a working mock-up of the user interface.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Lessons Learned</h2>
<section class="ltx_subsection" id="S6.SSx1">
<h3 class="ltx_title ltx_title_subsection">Use Case Formation</h3>
<div class="ltx_para" id="S6.SSx1.p1">
<p class="ltx_p" id="S6.SSx1.p1.1">Defining the proper use case is probably the most critical step in developing a proper RAG recommendation application. Because of the expense of data collection, annotation, and development, any confusion or change in the exact use case and capabilities of the model can result in substantial delays and costs.</p>
</div>
<div class="ltx_para" id="S6.SSx1.p2">
<p class="ltx_p" id="S6.SSx1.p2.1">For example, the first dataset that we considered for this use case was synthetic data for which subject matter experts crafted questions based on support document titles, and then provided corresponding answers. When we compared this data to actual customer cases, we found the genuine questions to be more verbose and to contain more off-topic “noise.” Thus we decided to use the more challenging actual support ticket data for training and validation, as it appeared better suited to our final deployment than the cleaner synthetic data.</p>
</div>
<div class="ltx_para" id="S6.SSx1.p3">
<p class="ltx_p" id="S6.SSx1.p3.1">We recommend spending time early on to understand how stakeholders will interact with the system, knowing that changes and evolution in the actual workflow may cause a decrease in system performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx2">
<h3 class="ltx_title ltx_title_subsection">Inter-Annotator Agreement</h3>
<figure class="ltx_table" id="S6.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.1.1">Product</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.2.1">Classifier</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.3.1">Question</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.4.1">Link</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.1.2.1.1">Product 1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.2.1.2">0.80 (20)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.2.1.3">0.75 (16)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.2.1.4">0.50 (16)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.3.2.1">Product 2</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.3.2.2">0.50 (20)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.3.2.3">0.50 (10)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.3.2.4">0.70 (10)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.4.3.1">Product 3</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.4.3.2">0.15 (20)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.4.3.3">1.00 (3)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.4.3.4">1.00 (3)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.5.4.1">Product 4</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.5.4.2">0.65 (20)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.5.4.3">0.85 (13)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.5.4.4">0.69 (13)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.6.5.1">Product 5</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.6.5.2">0.65 (20)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.6.5.3">0.54 (13)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.6.5.4">0.69 (13)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.7.6.1">Product 6</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.7.6.2">0.25 (20)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.7.6.3">1.00 (4)</td>
<td class="ltx_td ltx_align_center" id="S6.T6.1.7.6.4">1.00 (4)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.1.8.7.1">Total</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.8.7.2">0.50 (120)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.8.7.3">0.72 (59)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.8.7.4">0.67 (59)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Inter-Annotator Agreement: Proportion of labels that 3 annotators agreed on. Total N in parenthesis. For question and link labels, proportions only calculated based on cases for which all 3 annotators labeled as single turn and evaluated quality of corresponding question and link.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx2.p1">
<p class="ltx_p" id="S6.SSx2.p1.1">Three SMEs labeled a subset of twenty cases to determine inter-annotator agreement. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S6.T6" title="Table 6 ‣ Inter-Annotator Agreement ‣ 6 Lessons Learned ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">6</span></a> show that labeling cases as single vs. multi-turn is not a trivial task and for most products, SMEs disagreed widely. Of the cases in which all three annotators agreed to be single-turn, agreement on the question and link quality was better but still raises questions about the validity of the training and evaluation data. In particular, the low agreement of the provided links can be explained by the fact that more than one link can potentially solve the same question and so neither annotator is necessarily wrong. This suggests that for ground truth data, we should consider a list of correct links instead of a single ground truth link for each question. The low agreement of single-turn vs. multi-turn labels also potentially explains the lower performance of the classifier model if the model is attempting to learn from potentially conflicting information.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx3">
<h3 class="ltx_title ltx_title_subsection">RAG Bottlenecks</h3>
<div class="ltx_para" id="S6.SSx3.p1">
<p class="ltx_p" id="S6.SSx3.p1.1">The major bottleneck in RAG systems is the retrieval component. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.T4" title="Table 4 ‣ Answer Generator ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">4</span></a>, when given the correct context, LLMs can typically generate responses that match the ground truth answers. However, we cannot expect to generate the correct answer if given the wrong contexts which happens for around 60% of the cases (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.F2" title="Figure 2 ‣ Retriever ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">2</span></a>). For comparison, Google search limited to the corresponding domains indexed by the Milvus database performed worse at 30% R@3 compared to our method at 43% R@3 (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#S4.F2" title="Figure 2 ‣ Retriever ‣ 4 Development and Validation ‣ Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support"><span class="ltx_text ltx_ref_tag">2</span></a>). This implies, as other researchers have suggested, that the retrieval component in RAG is not a solved problem by any means. <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib16" title="">2024</a>; Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib5" title="">2024</a>)</cite></p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Related Work</h2>
<section class="ltx_subsection" id="S7.SSx1">
<h3 class="ltx_title ltx_title_subsection">LLM-Based AIOps</h3>
<div class="ltx_para" id="S7.SSx1.p1">
<p class="ltx_p" id="S7.SSx1.p1.1">As software systems become more complex, Artificial Intelligence for IT Operations (AIOps) methods are widely used to manage software system failures and ensure the high availability and reliability of large-scale distributed software systems <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib20" title="">2024</a>)</cite>. Machine learning and natural language processing methods such as LLMs have been used in AIOps for incident triage, data pre-processing, failure perception, root cause analysis, and auto remediation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib20" title="">2024</a>)</cite>. Historically and currently, many of these tasks including both incident triage and auto remediation have been treated as classification problems: for example, <cite class="ltx_cite ltx_citemacro_citet">Ahmed et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib1" title="">2023a</a>)</cite> treats incident resolution as a classification task matching incident tickets to a relatively small number of possible resolutions using the BERT model and embeddings. With the rise of better performing generative AI models, researchers have moved towards using these models to generate solutions in the auto remediation task using prompting strategies <cite class="ltx_cite ltx_citemacro_citep">(Ahmed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib2" title="">2023b</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib13" title="">2024</a>)</cite> or creating a model fine-tuned for a variety of IT tasks such as question-answering <cite class="ltx_cite ltx_citemacro_citep">(Guo et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib7" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SSx1.p2">
<p class="ltx_p" id="S7.SSx1.p2.1">Our use case can be considered an example of <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib20" title="">2024</a>)</cite>’s ”Assisted Questioning”, an auto remediation task that involves utilizing LLMs to aid operations personnel in answering system-related queries. As far as we are aware, no current work exists that leverages a RAG-based approach to solve this task, although one does exist for a similar IT task of root cause analysis <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib4" title="">2024</a>)</cite>. The RAG-based approach was taken in lieu of fine-tuning such as in <cite class="ltx_cite ltx_citemacro_citet">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib7" title="">2023</a>)</cite>’s OWL model because of issues in real-world deployment due to its resource-intensive nature which requires significant computational resources and the interpretation of model decisions. Likewise, we discounted using a simple prompting approach without retrieval because of client limitations in model choice that prevented us from using larger models.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx2">
<h3 class="ltx_title ltx_title_subsection">Retrieval and Retrieval Augmented Generation</h3>
<div class="ltx_para" id="S7.SSx2.p1">
<p class="ltx_p" id="S7.SSx2.p1.1">Methods for finding the relevant documents or passages to answer a user query are typically divided into sparse <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib17" title="">2009</a>)</cite> and dense retrieval systems <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib22" title="">2024</a>)</cite>. Our retrieval starts with a Milvus <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib19" title="">2021</a>)</cite> vector database that has indexed the software support documentation with a general purpose <span class="ltx_text ltx_font_italic" id="S7.SSx2.p1.1.1">dense</span> embedding that serves multiple services.
In our solution, we then make use of a popular optimization by re-ranking the first-pass result to obtain a more appropriate ranking for our particular application <cite class="ltx_cite ltx_citemacro_citep">(Nogueira and Cho <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib15" title="">2020</a>; Han et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib8" title="">2020</a>)</cite>, giving us the results of a special-purpose index while still retaining the benefits of a central indexing service. Other popular improvement methods include combining sparse and dense embeddings into a hybrid system <cite class="ltx_cite ltx_citemacro_citep">(Luan et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib14" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SSx2.p2">
<p class="ltx_p" id="S7.SSx2.p2.1">Retrieval augmented generation was developed to address cases in which large language models have not learned and stored domain knowledge through pre-training. Originally implemented by combining dense retrieval and a fine-tuned BART model, the method generalizes well to larger generative models <cite class="ltx_cite ltx_citemacro_citep">(Fan et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib6" title="">2024</a>)</cite>. The quality of the answer generation can be improved through prompt engineering, refining how the specific generative model is prompted with the context, question, and other instructions. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib12" title="">2023</a>)</cite>. However, it has been noted that the retrieval component in RAG has been understudied in comparison to the generation component despite its substantial impact on the final performance of such hybrid systems. <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib16" title="">2024</a>; Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib5" title="">2024</a>)</cite></p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We were able to deliver a first working version of an IT product solution recommendation system employing RAG. To our knowledge, this is the first published architecture and performance metrics of such a RAG system in this domain. Our system also differentiates itself with a few innovations including a component for classifying support cases as single-turn and a component for distilling verbose case descriptions into a query suitable for retrieval. We demonstrate that smaller models leveraging retrieved domain context can match or out-perform substantially larger models both with and without context <cite class="ltx_cite ltx_citemacro_citep">(Ahmed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib2" title="">2023b</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13707v1#bib.bib13" title="">2024</a>)</cite> particularly with knowledge infusion through fine-tuning. However, there are still many challenges in implementing RAG for IT support incident resolution including improving retrieval performance. We are collecting feedback from support specialists using our current deployment, and intend to incorporate their advice into future improvements.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmed et al. (2023a)</span>
<span class="ltx_bibblock">
Ahmed, S.; Singh, M.; Doherty, B.; Ramlan, E.; Harkin, K.; Bucholc, M.; and Coyle, D. 2023a.

</span>
<span class="ltx_bibblock">Knowledge-based Intelligent System for IT Incident DevOps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2023 IEEE/ACM International Workshop on Cloud Intelligence &amp; AIOps (AIOps)</em>, 1–7. Los Alamitos, CA, USA: IEEE Computer Society.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmed et al. (2023b)</span>
<span class="ltx_bibblock">
Ahmed, T.; Ghosh, S.; Bansal, C.; Zimmermann, T.; Zhang, X.; and Rajmohan, S. 2023b.

</span>
<span class="ltx_bibblock">Recommending Root-Cause and Mitigation Steps for Cloud Incidents Using Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 45th International Conference on Software Engineering</em>, ICSE ’23, 1737–1749. IEEE Press.

</span>
<span class="ltx_bibblock">ISBN 9781665457019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et al. (2023)</span>
<span class="ltx_bibblock">
Almazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.; Cojocaru, R.; Debbah, M.; Goffinet, E.; Heslow, D.; Launay, J.; Malartic, Q.; Noune, B.; Pannier, B.; and Penedo, G. 2023.

</span>
<span class="ltx_bibblock">Falcon-40B: an open large language model with state-of-the-art performance.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chen, Y.; Xie, H.; Ma, M.; Kang, Y.; Gao, X.; Shi, L.; Cao, Y.; Gao, X.; Fan, H.; Wen, M.; Zeng, J.; Ghosh, S.; Zhang, X.; Zhang, C.; Lin, Q.; Rajmohan, S.; Zhang, D.; and Xu, T. 2024.

</span>
<span class="ltx_bibblock">Automatic Root Cause Analysis via Large Language Models for Cloud Incidents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Nineteenth European Conference on Computer Systems</em>, EuroSys ’24, 674–688. New York, NY, USA: Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400704376.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al. (2024)</span>
<span class="ltx_bibblock">
Cuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Campagnano, C.; Maarek, Y.; Tonellotto, N.; and Silvestri, F. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR ’24, 719–729. New York, NY, USA: Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400704314.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2024)</span>
<span class="ltx_bibblock">
Fan, W.; Ding, Y.; Ning, L.; Wang, S.; Li, H.; Yin, D.; Chua, T.-S.; and Li, Q. 2024.

</span>
<span class="ltx_bibblock">A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2405.06211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Guo, H.; Yang, J.; Liu, J.; Yang, L.; Chai, L.; Bai, J.; Peng, J.; Hu, X.; Chen, C.; Zhang, D.; Shi, X.; Zheng, T.; Zheng, L.; Zhang, B.; Xu, K.; and Li, Z. 2023.

</span>
<span class="ltx_bibblock">OWL: A Large Language Model for IT Operations.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2020)</span>
<span class="ltx_bibblock">
Han, S.; Wang, X.; Bendersky, M.; and Najork, M. 2020.

</span>
<span class="ltx_bibblock">Learning-to-Rank with BERT in TF-Ranking.

</span>
<span class="ltx_bibblock">arXiv:2004.08476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IBM Research (2024)</span>
<span class="ltx_bibblock">
IBM Research, I. W. 2024.

</span>
<span class="ltx_bibblock">IBM slate-125m-english-rtrvr-v2 model card.

</span>
<span class="ltx_bibblock">https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-slate-125m-english-rtrvr-v2-model-card.html?context=wx&amp;audience=wdp.

</span>
<span class="ltx_bibblock">Accessed: 2024-08-16.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Jiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.; Savary, B.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Hanna, E. B.; Bressand, F.; Lengyel, G.; Bour, G.; Lample, G.; Lavaud, L. R.; Saulnier, L.; Lachaux, M.-A.; Stock, P.; Subramanian, S.; Yang, S.; Antoniak, S.; Scao, T. L.; Gervet, T.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2024.

</span>
<span class="ltx_bibblock">Mixtral of Experts.

</span>
<span class="ltx_bibblock">arXiv:2401.04088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS ’20. Red Hook, NY, USA: Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781713829546.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2023.

</span>
<span class="ltx_bibblock">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ACM Comput. Surv.</em>, 55(9).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Liu, Y.; Pei, C.; Xu, L.; Chen, B.; Sun, M.; Zhang, Z.; Sun, Y.; Zhang, S.; Wang, K.; Zhang, H.; Li, J.; Xie, G.; Wen, X.; Nie, X.; Ma, M.; and Pei, D. 2024.

</span>
<span class="ltx_bibblock">OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2310.07637.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luan et al. (2021)</span>
<span class="ltx_bibblock">
Luan, Y.; Eisenstein, J.; Toutanova, K.; and Collins, M. 2021.

</span>
<span class="ltx_bibblock">Sparse, Dense, and Attentional Representations for Text Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Transactions of the Association for Computational Linguistics</em>, 9: 329–345.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira and Cho (2020)</span>
<span class="ltx_bibblock">
Nogueira, R.; and Cho, K. 2020.

</span>
<span class="ltx_bibblock">Passage Re-ranking with BERT.

</span>
<span class="ltx_bibblock">arXiv:1901.04085.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2024)</span>
<span class="ltx_bibblock">
Petroni, F.; Siciliano, F.; Silvestri, F.; and Trappolini, G. 2024.

</span>
<span class="ltx_bibblock">IR-RAG @ SIGIR24: Information Retrieval’s Role in RAG Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR ’24, 3036–3039. New York, NY, USA: Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400704314.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
Robertson, S.; and Zaragoza, H. 2009.

</span>
<span class="ltx_bibblock">The Probabilistic Relevance Framework: BM25 and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Found. Trends Inf. Retr.</em>, 3(4): 333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sudalairaj et al. (2024)</span>
<span class="ltx_bibblock">
Sudalairaj, S.; Bhandwaldar, A.; Pareja, A.; Xu, K.; Cox, D. D.; and Srivastava, A. 2024.

</span>
<span class="ltx_bibblock">LAB: Large-Scale Alignment for ChatBots.

</span>
<span class="ltx_bibblock">arXiv:2403.01081.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, J.; Yi, X.; Guo, R.; Jin, H.; Xu, P.; Li, S.; Wang, X.; Guo, X.; Li, C.; Xu, X.; Yu, K.; Yuan, Y.; Zou, Y.; Long, J.; Cai, Y.; Li, Z.; Zhang, Z.; Mo, Y.; Gu, J.; Jiang, R.; Wei, Y.; and Xie, C. 2021.

</span>
<span class="ltx_bibblock">Milvus: A Purpose-Built Vector Data Management System.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2021 International Conference on Management of Data</em>, SIGMOD ’21, 2614–2627. New York, NY, USA: Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Zhang, L.; Jia, T.; Jia, M.; Wu, Y.; Liu, A.; Yang, Y.; Wu, Z.; Hu, X.; Yu, P. S.; and Li, Y. 2024.

</span>
<span class="ltx_bibblock">A Survey of AIOps for Failure Management in the Era of Large Language Models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020.

</span>
<span class="ltx_bibblock">BERTScore: Evaluating Text Generation with BERT.

</span>
<span class="ltx_bibblock">arXiv:1904.09675.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Zhao, W. X.; Liu, J.; Ren, R.; and Wen, J.-R. 2024.

</span>
<span class="ltx_bibblock">Dense Text Retrieval Based on Pretrained Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ACM Trans. Inf. Syst.</em>, 42(4).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep  6 12:59:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
