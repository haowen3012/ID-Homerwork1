<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation</title>
<!--Generated on Thu Sep 19 17:52:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12941v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S1" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_typewriter ltx_font_bold">FRAMES</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.SS0.SSS0.Px1" title="In 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Synthetic Data Generation Attempts.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.SS0.SSS0.Px2" title="In 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Human annotation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.SS0.SSS0.Px3" title="In 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Dataset Statistics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.SS0.SSS0.Px4" title="In 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Quality Checks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Empirical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS1" title="In 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Single-Step Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS1.SSS0.Px1" title="In 3.1 Single-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Experiment Setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS1.SSS0.Px2" title="In 3.1 Single-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">LLMs perform poorly in single-step evaluations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS2" title="In 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Multi-Step Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS2.SSS0.Px1" title="In 3.2 Multi-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Multi-Step retrievals significantly improve model performance.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S4" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S5" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S5.SS0.SSS0.Px1" title="In 5 Conclusion ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Future Work.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S5.SS0.SSS0.Px2" title="In 5 Conclusion ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Limitations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A1" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Synthetic Data Generation Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A2" title="In Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Autorater Prompt</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Satyapriya Krishna 
<br class="ltx_break"/>Harvard University 
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id1.1.id1">Kalpesh Krishna<sup class="ltx_sup" id="id1.1.id1.1">†</sup>, Anhad Mohananey<sup class="ltx_sup" id="id1.1.id1.2">†</sup>, Steven Schwarcz, Adam Stambler</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id2.2.id2">Shyam Upadhyay </span>&amp;<span class="ltx_text ltx_font_bold" id="id3.3.id3"> Manaal Faruqui</span>
<br class="ltx_break"/>Google, Inc
</span><span class="ltx_author_notes">Work done as an intern at Google. Corresponding Author: skrishna@g.harvard.edu<span class="ltx_text ltx_font_bold" id="id4.4.id1">Equal contribution</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.1">FRAMES</span> (<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.2">F</span>actuality, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.3">R</span>etrieval, And reasoning <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.4">ME</span>asurement <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.5">S</span>et), a high-quality evaluation dataset designed to test LLMs’ ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id5.id1.6">FRAMES</span> offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (&gt;50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advancements in Large Language Models (LLMs) have significantly enhanced their capabilities across various natural language processing tasks, especially in systems that demand both factual accuracy and sophisticated reasoning for complex queries <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib33" title="">2023</a>)</cite>. Retrieval-augmented generation (RAG) techniques <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib16" title="">2020</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib3" title="">2019</a>; Guu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib9" title="">2020</a>)</cite> have become a powerful approach by leveraging the strengths of retrieval systems and the generative capabilities of LLMs. These techniques are particularly effective for tasks requiring multi-hop reasoning, factual grounding, and synthesizing information from diverse knowledge domains <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib5" title="">2023</a>)</cite>. However, despite this progress, the evaluation of RAG systems remains fragmented and insufficient, as existing benchmarks typically assess components like retrieval, factual correctness, and reasoning in isolation <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib31" title="">2024a</a>)</cite>. This piecemeal approach fails to capture the holistic performance of these systems in real-world applications <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib32" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To bridge this gap, we introduce a novel evaluation framework, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.1">FRAMES</span> <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Dataset link : <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/google/frames-benchmark" title="">https://huggingface.co/datasets/google/frames-benchmark</a></span></span></span> (<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.2">F</span>actuality, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.3">R</span>etrieval, And reasoning <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.4">ME</span>asurement <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.5">S</span>et), designed to rigorously test LLMs on all three core capabilities—fact retrieval, reasoning across multiple constraints, and accurate synthesis of information into coherent responses. Unlike existing datasets such as TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib18" title="">2021</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib30" title="">2018b</a>)</cite>, or GSM8k <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib2" title="">2021</a>)</cite>, which focus on isolated aspects of LLM performance, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p2.1.6">FRAMES</span> provides an integrated evaluation that challenges models across these dimensions simultaneously. This approach offers a more accurate reflection of how these systems perform as end-to-end reasoning solutions, especially in scenarios requiring multi-document retrieval and complex reasoning. For instance, a sample from our dataset is <span class="ltx_text ltx_font_italic" id="S1.p2.1.7">"How many years earlier would Punxsutawney Phil have to be canonically alive to have made a Groundhog Day prediction in the same state as the US capitol?"</span> This requires the model to perform temporal and numerical reasoning after retrieving the critical articles needed to answer the question.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our work addresses a critical void in the current landscape by offering a challenging evaluation benchmark that not only tests the individual components of LLMs but also evaluates their performance in an end-to-end context. Through our dataset, we simulate realistic, multi-document queries to assess the ability of LLMs to retrieve relevant facts, reason accurately, and synthesize information into coherent responses. Additionally, we present empirical results on the performance of state-of-the-art models, highlighting both their strengths and the limitations in their reasoning capabilities. These findings pave the way for further research and development of more robust and efficient retrieval-augmented generation systems. Our key contributions are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.I1.i1.p1.1.1">FRAMES</span>, a novel dataset of 824 test samples designed to evaluate LLMs’ ability to retrieve and reason across multiple documents in a unified framework.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We provide a comprehensive evaluation of state-of-the-art LLMs, highlighting their performance on factuality, retrieval, and reasoning tasks across diverse domains.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We present new empirical insights into the limitations of existing LLMs in handling multi-hop and temporal reasoning tasks, offering avenues for future research to improve these systems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We propose a multi-step retrieval and reasoning framework that compels models to iteratively retrieve and reason, significantly enhancing their performance on complex queries.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.1.1">
<span class="ltx_p" id="S1.T1.1.1.1.1.1.1" style="width:156.5pt;"><span class="ltx_text" id="S1.T1.1.1.1.1.1.1.1">Dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.2"><span class="ltx_text" id="S1.T1.1.1.1.2.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S1.T1.1.1.1.2.1.1" style="width:31.3pt;">
<span class="ltx_p" id="S1.T1.1.1.1.2.1.1.1">Factuality</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.3"><span class="ltx_text" id="S1.T1.1.1.1.3.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S1.T1.1.1.1.3.1.1" style="width:31.3pt;">
<span class="ltx_p" id="S1.T1.1.1.1.3.1.1.1">Retrieval</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.4"><span class="ltx_text" id="S1.T1.1.1.1.4.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S1.T1.1.1.1.4.1.1" style="width:31.3pt;">
<span class="ltx_p" id="S1.T1.1.1.1.4.1.1.1">Reasoning</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.5"><span class="ltx_text" id="S1.T1.1.1.1.5.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S1.T1.1.1.1.5.1.1" style="width:28.5pt;">
<span class="ltx_p" id="S1.T1.1.1.1.5.1.1.1">Multi-Hop/Step</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.6"><span class="ltx_text" id="S1.T1.1.1.1.6.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S1.T1.1.1.1.6.1.1" style="width:51.2pt;">
<span class="ltx_p" id="S1.T1.1.1.1.6.1.1.1">Temporal Disambiguation</span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.2.1.1">
<span class="ltx_p" id="S1.T1.1.2.2.1.1.1" style="width:156.5pt;"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.T1.1.2.2.1.1.1.1">FRAMES</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.2"><span class="ltx_text" id="S1.T1.1.2.2.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.3"><span class="ltx_text" id="S1.T1.1.2.2.3.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.4"><span class="ltx_text" id="S1.T1.1.2.2.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.5"><span class="ltx_text" id="S1.T1.1.2.2.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.6"><span class="ltx_text" id="S1.T1.1.2.2.6.1" style="color:#00FF00;">✔</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.3.1.1">
<span class="ltx_p" id="S1.T1.1.3.3.1.1.1" style="width:156.5pt;">TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib18" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.2"><span class="ltx_text" id="S1.T1.1.3.3.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.3"><span class="ltx_text" id="S1.T1.1.3.3.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.4"><span class="ltx_text" id="S1.T1.1.3.3.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.5"><span class="ltx_text" id="S1.T1.1.3.3.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.6"><span class="ltx_text" id="S1.T1.1.3.3.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.4.1.1">
<span class="ltx_p" id="S1.T1.1.4.4.1.1.1" style="width:156.5pt;">OpenbookQA <cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib21" title="">2018</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.4.4.2"><span class="ltx_text" id="S1.T1.1.4.4.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.4.4.3"><span class="ltx_text" id="S1.T1.1.4.4.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.4.4.4"><span class="ltx_text" id="S1.T1.1.4.4.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.4.4.5"><span class="ltx_text" id="S1.T1.1.4.4.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.4.4.6"><span class="ltx_text" id="S1.T1.1.4.4.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.5.1.1">
<span class="ltx_p" id="S1.T1.1.5.5.1.1.1" style="width:156.5pt;">HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib30" title="">2018b</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.5.5.2"><span class="ltx_text" id="S1.T1.1.5.5.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.5.5.3"><span class="ltx_text" id="S1.T1.1.5.5.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.5.5.4"><span class="ltx_text" id="S1.T1.1.5.5.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.5.5.5"><span class="ltx_text" id="S1.T1.1.5.5.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.5.5.6"><span class="ltx_text" id="S1.T1.1.5.5.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.6.1.1">
<span class="ltx_p" id="S1.T1.1.6.6.1.1.1" style="width:156.5pt;">HybridQA <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib1" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.6.6.2"><span class="ltx_text" id="S1.T1.1.6.6.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.6.6.3"><span class="ltx_text" id="S1.T1.1.6.6.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.6.6.4"><span class="ltx_text" id="S1.T1.1.6.6.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.6.6.5"><span class="ltx_text" id="S1.T1.1.6.6.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.6.6.6"><span class="ltx_text" id="S1.T1.1.6.6.6.1" style="color:#00FF00;">✔</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.7.1.1">
<span class="ltx_p" id="S1.T1.1.7.7.1.1.1" style="width:156.5pt;">GSM8k <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib2" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.7.7.2"><span class="ltx_text" id="S1.T1.1.7.7.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.7.7.3"><span class="ltx_text" id="S1.T1.1.7.7.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.7.7.4"><span class="ltx_text" id="S1.T1.1.7.7.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.7.7.5"><span class="ltx_text" id="S1.T1.1.7.7.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.7.7.6"><span class="ltx_text" id="S1.T1.1.7.7.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.8.8.1.1">
<span class="ltx_p" id="S1.T1.1.8.8.1.1.1" style="width:156.5pt;">Multihop-RAG<cite class="ltx_cite ltx_citemacro_citep">(Tang &amp; Yang, <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib25" title="">2024</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.8.8.2"><span class="ltx_text" id="S1.T1.1.8.8.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.8.8.3"><span class="ltx_text" id="S1.T1.1.8.8.3.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.8.8.4"><span class="ltx_text" id="S1.T1.1.8.8.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.8.8.5"><span class="ltx_text" id="S1.T1.1.8.8.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.8.8.6"><span class="ltx_text" id="S1.T1.1.8.8.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.9.9.1.1">
<span class="ltx_p" id="S1.T1.1.9.9.1.1.1" style="width:156.5pt;">MoreHopQA <cite class="ltx_cite ltx_citemacro_citep">(Schnitzler et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib24" title="">2024</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.9.9.2"><span class="ltx_text" id="S1.T1.1.9.9.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.9.9.3"><span class="ltx_text" id="S1.T1.1.9.9.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.9.9.4"><span class="ltx_text" id="S1.T1.1.9.9.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.9.9.5"><span class="ltx_text" id="S1.T1.1.9.9.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.9.9.6"><span class="ltx_text" id="S1.T1.1.9.9.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.10.10.1.1">
<span class="ltx_p" id="S1.T1.1.10.10.1.1.1" style="width:156.5pt;">MuSiQue <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib26" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.10.10.2"><span class="ltx_text" id="S1.T1.1.10.10.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.10.10.3"><span class="ltx_text" id="S1.T1.1.10.10.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.10.10.4"><span class="ltx_text" id="S1.T1.1.10.10.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.10.10.5"><span class="ltx_text" id="S1.T1.1.10.10.5.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.10.10.6"><span class="ltx_text" id="S1.T1.1.10.10.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.11.11.1.1">
<span class="ltx_p" id="S1.T1.1.11.11.1.1.1" style="width:156.5pt;">NaturalQuestions <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib15" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.11.11.2"><span class="ltx_text" id="S1.T1.1.11.11.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.11.11.3"><span class="ltx_text" id="S1.T1.1.11.11.3.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.11.11.4"><span class="ltx_text" id="S1.T1.1.11.11.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.11.11.5"><span class="ltx_text" id="S1.T1.1.11.11.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.11.11.6"><span class="ltx_text" id="S1.T1.1.11.11.6.1" style="color:#00FF00;">✔</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.12.12.1.1">
<span class="ltx_p" id="S1.T1.1.12.12.1.1.1" style="width:156.5pt;">TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib10" title="">2017</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.12.12.2"><span class="ltx_text" id="S1.T1.1.12.12.2.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.12.12.3"><span class="ltx_text" id="S1.T1.1.12.12.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.12.12.4"><span class="ltx_text" id="S1.T1.1.12.12.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.12.12.5"><span class="ltx_text" id="S1.T1.1.12.12.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.12.12.6"><span class="ltx_text" id="S1.T1.1.12.12.6.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_l ltx_border_r" id="S1.T1.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.13.13.1.1">
<span class="ltx_p" id="S1.T1.1.13.13.1.1.1" style="width:156.5pt;">ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib3" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.1.13.13.2"><span class="ltx_text" id="S1.T1.1.13.13.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.1.13.13.3"><span class="ltx_text" id="S1.T1.1.13.13.3.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.1.13.13.4"><span class="ltx_text" id="S1.T1.1.13.13.4.1" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.1.13.13.5"><span class="ltx_text" id="S1.T1.1.13.13.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.1.13.13.6"><span class="ltx_text" id="S1.T1.1.13.13.6.1" style="color:#FF0000;">✗</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.T1.4.1">FRAMES</span> against other datasets. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.T1.5.2">FRAMES</span> provides a combination of evaluation samples to test the factuality, retrieval, and reasoning of RAG systems. The dataset also covers multi-hop/step questions along with temporal disambiguation.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.1.1">FRAMES</span>
</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.1">FRAMES</span> (<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.2">F</span>actuality, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.3">R</span>etrieval, And reasoning <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.4">ME</span>asurement <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.5">S</span>et) is an evaluation set of 824 questions designed to provide an end-to-end evaluation of Retrieval Augmented Generation (RAG) systems. It assesses three key components of a RAG system: Factuality, Retrieval, and Reasoning. Unlike most existing datasets and benchmarks that evaluate each of these RAG components in isolation, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.6">FRAMES</span> offers a comprehensive test bed to gain a clear understanding of the overall quality of RAG systems<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib19" title="">2022</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib29" title="">2018a</a>; Welbl et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib28" title="">2017</a>)</cite>. This holistic approach allows for a more accurate reflection of how these systems perform in real-world scenarios. In this section, we first detail our data collection process, which involved both synthetic data generation attempts and human annotation. Next, we present the dataset statistics, showcasing the diversity of topics and reasoning types covered. Finally, we outline the rigorous quality checks implemented to ensure the dataset’s reliability and challenging nature. By providing this end-to-end evaluation framework, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.7">FRAMES</span> aims to bridge the gap in existing benchmarks and foster the development of more robust and efficient RAG systems.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><svg class="ltx_picture" height="402.54" id="S2.F1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,402.54) matrix(1 0 0 -1 0 0)"><g fill="#333399" fill-opacity="1.0"><path d="M 0 5.91 L 0 396.64 C 0 399.9 2.64 402.54 5.91 402.54 L 594.09 402.54 C 597.36 402.54 600 399.9 600 396.64 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 378.43 L 598.03 378.43 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 384.34)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.F1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.F1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.F1.pic1.1.1.1.1.1.1.1">Task Instruction Prompt for Human Annotation</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="352.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.F1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S2.F1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.F1.pic1.2.2.2.1.1.1.1">Task Description</span></span>
<span class="ltx_p" id="S2.F1.pic1.2.2.2.1.1.2">Create <span class="ltx_text ltx_font_typewriter" id="S2.F1.pic1.2.2.2.1.1.2.1">n</span> factoid questions that demand multi-hop reasoning based on information found across multiple Wikipedia articles. These questions should ideally have a single, unambiguous answer and may optionally incorporate elements of challenging reasoning.</span>
<span class="ltx_p" id="S2.F1.pic1.2.2.2.1.1.3">Here’s a breakdown of the key terms and their relationships:</span>
<span class="ltx_itemize" id="S2.I1">
<span class="ltx_item" id="S2.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S2.I1.i1.p1">
<span class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Factoid Questions:</span> These are trivia-style questions with a single, clearly defined, and factually correct answer.</span>
</span></span>
<span class="ltx_item" id="S2.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S2.I1.i2.p1">
<span class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Multi-hop Reasoning:</span> This refers to the core requirement that answering the questions necessitates combining information from different sections within multiple Wikipedia articles. The example provided ("What is the name of the river that flows through the city where the Eiffel Tower is located?") highlights how this differs from a simple factoid question ("What is the capital of France?").</span>
</span></span>
<span class="ltx_item" id="S2.I1.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<span class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Challenging Reasoning:</span> This aspect encourages the inclusion of questions that go beyond simple information retrieval and demand critical thinking. This can be achieved through various question types like:</span>
<span class="ltx_itemize" id="S2.I1.i3.I1">
<span class="ltx_item" id="S2.I1.i3.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.I1.i1.1.1.1">–</span></span>
<span class="ltx_para" id="S2.I1.i3.I1.i1.p1">
<span class="ltx_p" id="S2.I1.i3.I1.i1.p1.1">Numerical Reasoning: Involving counting, comparisons, or calculations. <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.I1.i1.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
<span class="ltx_item" id="S2.I1.i3.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.I1.i2.1.1.1">–</span></span>
<span class="ltx_para" id="S2.I1.i3.I1.i2.p1">
<span class="ltx_p" id="S2.I1.i3.I1.i2.p1.1">Tabular Reasoning: Involving statistics found in tables / info boxes in wikipedia. <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.I1.i2.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
<span class="ltx_item" id="S2.I1.i3.I1.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.I1.i3.1.1.1">–</span></span>
<span class="ltx_para" id="S2.I1.i3.I1.i3.p1">
<span class="ltx_p" id="S2.I1.i3.I1.i3.p1.1">Multiple Constraints: Questions involving multiple constraints, whose intersection points towards a unique answer. <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.I1.i3.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
<span class="ltx_item" id="S2.I1.i3.I1.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.I1.i4.1.1.1">–</span></span>
<span class="ltx_para" id="S2.I1.i3.I1.i4.p1">
<span class="ltx_p" id="S2.I1.i3.I1.i4.p1.1">Temporal Reasoning : Questions involving reasoning through timelines. <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.I1.i4.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
<span class="ltx_item" id="S2.I1.i3.I1.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.I1.i5.1.1.1">–</span></span>
<span class="ltx_para ltx_noindent" id="S2.I1.i3.I1.i5.p1">
<span class="ltx_p" id="S2.I1.i3.I1.i5.p1.1">Post processing: This involves requiring the answerer to perform specific post-processing steps after all necessary facts have been gathered. <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.I1.i5.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
</span>
</span></span>
</span>
<span class="ltx_p" id="S2.F1.pic1.2.2.2.1.1.4">Additional Requirements:</span>
<span class="ltx_itemize" id="S2.I2">
<span class="ltx_item" id="S2.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S2.I2.i1.p1">
<span class="ltx_p" id="S2.I2.i1.p1.1">Wikipedia Articles: The information used to answer the questions must be sourced from Wikipedia articles.</span>
</span></span>
<span class="ltx_item" id="S2.I2.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S2.I2.i2.p1">
<span class="ltx_p" id="S2.I2.i2.p1.1">Standalone and Context-Independent: Questions should be understandable without requiring additional information or context.</span>
</span></span>
<span class="ltx_item" id="S2.I2.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S2.I2.i3.p1">
<span class="ltx_p" id="S2.I2.i3.p1.1">Single, Unambiguous Answer: Each question should have only one correct answer, leaving no room for ambiguity.</span>
</span></span>
<span class="ltx_item" id="S2.I2.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para ltx_noindent" id="S2.I2.i4.p1">
<span class="ltx_p" id="S2.I2.i4.p1.1">Avoid boolean questions (yes/no questions) that can be answered with a simple "yes" or "no." <span class="ltx_text ltx_font_typewriter" id="S2.I2.i4.p1.1.1">&lt;examples&gt;</span></span>
</span></span>
</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Task instruction provided to human annotators to generate samples for <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.F1.2.1">FRAMES</span>.</figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Data Generation Attempts. </h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">We start our data collection process with synthetic dataset generation to explore a potentially cost-effective alternative to expensive human annotation. We prompt a state-of-the-art LLM with instructions to use multiple articles to generate questions that would require information from multiple articles to answer. The prompt (shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A1.F6" title="Figure 6 ‣ Appendix A Synthetic Data Generation Prompt ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">6</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A1" title="Appendix A Synthetic Data Generation Prompt ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">A</span></a>) takes as input the number of articles provided to generate questions. However, we observed significant issues with this approach. While the LLMs were able to generate coherent questions, there was a high proportion of hallucinated questions and answers (&gt;30%). Additionally, the LLM struggled to generate questions that strictly required more than four articles. To evaluate the potential of this approach, we manually cleaned the hallucinated questions and answers from the obtained set. We then evaluated the same LLM on these cleaned questions and obtained an accuracy of <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.1d">∼</annotation></semantics></math>32%, suggesting that the legitimate questions generated by LLMs were indeed challenging for state-of-the-art models. There are two key takeaways from our experimentation with synthetic data generation: (1) Synthetic test data requires heavy manual cleaning before usage, which suggests that we will need to rely on human annotations instead of LLMs to generate the final evaluation set; and (2) models performed significantly poorly on the correct test samples we tested on, suggesting that the instruction to create questions can be used to generate a challenging evaluation set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Human annotation.</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Given these findings, we decided to use the core instruction for generating questions that combine information from multiple articles as a guide for human annotation, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.F1" title="Figure 1 ‣ 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. This approach aimed to leverage the challenging nature of the synthetic questions while also mitigating the issues of hallucination present in LLM-generated content. Human annotators were tasked with creating questions that required information from multiple Wikipedia articles, following a similar structure to the synthetic prompts but with greater reliability and accuracy. The outcome of this human annotation resulted in 824 questions with their correct responses along with the list of Wikipedia articles needed to answer the questions. We also ask the human annotators to label each question based on five reasoning types, i.e, Numerical Reasoning, Tabular Reasoning, Multiple Constraints, Temporal Reasoning, and Post-Processing, described in more details in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.T2" title="Table 2 ‣ Human annotation. ‣ 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>. Please note that a question can belong to multiple reasoning types. To ensure the highest quality annotations, we engaged a team of carefully vetted experts with extensive experience in question generation and complex reasoning tasks.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.1.1">
<span class="ltx_p" id="S2.T2.1.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1.1.1">Reasoning Type</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.1.1">
<span class="ltx_p" id="S2.T2.1.2.1.1.1.1" style="width:85.4pt;">Numerical Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.2.1">
<span class="ltx_p" id="S2.T2.1.2.1.2.1.1" style="width:284.5pt;">This involves counting, comparisons, or calculations. For example, the question <span class="ltx_text ltx_font_italic" id="S2.T2.1.2.1.2.1.1.1">"How many times faster is the second fastest bird in the Americas compared to the fastest human in the world? Round to the nearest integer."</span> asks for a calculation comparing the speeds of two objects.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.1.1">
<span class="ltx_p" id="S2.T2.1.3.2.1.1.1" style="width:85.4pt;">Tabular Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.2.1">
<span class="ltx_p" id="S2.T2.1.3.2.2.1.1" style="width:284.5pt;">This involves statistics found in tables or infoboxes in Wikipedia. For example, the question <span class="ltx_text ltx_font_italic" id="S2.T2.1.3.2.2.1.1.1">"How many runs did the West Indies vice-captain score in the 1983 World Cup?"</span> requires the answerer to analyze tabular data of top run scorers and extract the relevant information.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.1.1">
<span class="ltx_p" id="S2.T2.1.4.3.1.1.1" style="width:85.4pt;">Multiple Constraints</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.2.1">
<span class="ltx_p" id="S2.T2.1.4.3.2.1.1" style="width:284.5pt;">This involves questions with multiple constraints whose intersection points towards a unique answer. For example, <span class="ltx_text ltx_font_italic" id="S2.T2.1.4.3.2.1.1.1">"I’m thinking of an airport near the intersection of US-52 and I-95. Can you remind me which one it is?"</span> This query has two constraints: first, to locate an airport, and second, that it should be near the intersection of US-52 and I-95.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.1.1">
<span class="ltx_p" id="S2.T2.1.5.4.1.1.1" style="width:85.4pt;">Temporal Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.2.1">
<span class="ltx_p" id="S2.T2.1.5.4.2.1.1" style="width:284.5pt;">This involves reasoning through timelines. For example, <span class="ltx_text ltx_font_italic" id="S2.T2.1.5.4.2.1.1.1">"Leonardo DiCaprio once won an Oscar for best actor. Who won the award for best costume design sixteen years earlier?"</span>.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T2.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.1.1">
<span class="ltx_p" id="S2.T2.1.6.5.1.1.1" style="width:85.4pt;">Post-Processing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S2.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.2.1">
<span class="ltx_p" id="S2.T2.1.6.5.2.1.1" style="width:284.5pt;">This requires the answerer to perform specific post-processing steps after all necessary facts have been gathered. For example, consider the question: <span class="ltx_text ltx_font_italic" id="S2.T2.1.6.5.2.1.1.1">"What is five years after the founding of the largest country in North America in Roman numerals?"</span>. This question requires the following sub-instructions: (1) Numerical reasoning: Add five years to the founding date, and (2) Post-processing: Convert the resulting year into Roman numerals.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>This table provides descriptions of the different reasoning types to which each question in <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.T2.3.1">FRAMES</span> belongs. The distribution of samples belonging to each reasoning type is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.F2" title="Figure 2 ‣ Human annotation. ‣ 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.1" style="width:216.8pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="456" id="S2.F2.1.g1" src="x1.png" width="761"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.2" style="width:216.8pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="456" id="S2.F2.2.g1" src="x2.png" width="761"/>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The figure shows the distribution of questions in the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.F2.4.1">FRAMES</span>, with the percentage of questions requiring different numbers of Wikipedia articles (left) and the percentage of the dataset belonging to each reasoning type (right). Please note that the percentage bar for 11 denotes the percentage of questions requiring 11 or more Wikipedia articles. </figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dataset Statistics. </h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.4">The dataset comprises questions related to a diverse set of topics from Wikipedia, involving subjects such as history, sports, science, animals, health, etc. Each question in our dataset require 2-15 Wikipedia articles to answer, with the distribution of the percentage of dataset requiring different numbers of Wikipedia articles shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.F2" title="Figure 2 ‣ Human annotation. ‣ 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> (left). Approximately 36% of questions require two articles to answer, <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px3.p1.1.m1.1a"><mo id="S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.1.m1.1d">∼</annotation></semantics></math>35% require three articles, <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px3.p1.2.m2.1a"><mo id="S2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.2.m2.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.2.m2.1d">∼</annotation></semantics></math>16% require four articles, and so on. This distribution also represents the general trend of queries asked from LLMs in the real world <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib20" title="">2009</a>)</cite>, since the proportion of questions requiring two articles is higher than more complicated questions requiring a greater number of articles. Additionally, we have a healthy distribution of questions belonging to different reasoning types, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S2.F2" title="Figure 2 ‣ Human annotation. ‣ 2 FRAMES ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> (right). Questions requiring reasoning over multiple constraints hold the highest percentage of data samples in the test (<math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px3.p1.3.m3.1a"><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.3.m3.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.3.m3.1d">∼</annotation></semantics></math>36%), followed by questions requiring numerical reasoning (<math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px3.p1.4.m4.1a"><mo id="S2.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.4.m4.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.4.m4.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.4.m4.1d">∼</annotation></semantics></math>20%). Please note that many questions in the dataset also require a combination of different reasoning abilities to find an answer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Quality Checks. </h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">Other than the data collection process described in the section above, human annotators also implemented several quality checks to ensure the dataset’s high quality and effectiveness in evaluating RAG capabilities:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px4.p2">
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Ensuring correctness and groundedness to Wikipedia:</span>
We verified the correctness of questions and their corresponding answers by re-annotating the questions. Human annotators were asked to confirm if the provided answer was correct and could be answered using the associated Wikipedia pages. This annotation process was conducted three months after collecting the initial data, filtering out 5.5% of samples where the answer was no longer true after that period.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Removing ambiguity due to freshness (temporal disambiguation):</span>
Annotators added extra context to disambiguate answers that could change over time. For example, a question like <span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.2">"Which country were holders of the FIFA World Cup the last time the UEFA Champions League was won by a club from London?"</span> was revised to <span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.3">"As of August 1, 2024, which country were holders of the FIFA World Cup the last time the UEFA Champions League was won by a club from London?"</span>. This approach mitigates issues with frequent manual updates required for maintaining previous datasets <cite class="ltx_cite ltx_citemacro_citep">(Vu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib27" title="">2023</a>; Kasai et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib11" title="">2024</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">Preventing guesswork by ensuring a large output space:</span>
We removed questions with binary answers ("yes" or "no") to prevent LLMs from achieving 50% accuracy through random guessing. This ensures the dataset is challenging enough to clearly evaluate LLM capabilities.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I3.i4.p1">
<p class="ltx_p" id="S2.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i4.p1.1.1">Ensuring dataset interpretability and reliability:</span>
We limited the articles to those from Wikipedia, which has a lower chance of containing unreliable information compared to other sources.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I3.i5.p1">
<p class="ltx_p" id="S2.I3.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i5.p1.1.1">Addressing contamination issues:</span>
To mitigate concerns about potential contamination due to Wikipedia articles being in LLM training sets, we designed questions that require additional reasoning and operations beyond simple fact retrieval. For instance, the question <span class="ltx_text ltx_font_italic" id="S2.I3.i5.p1.1.2">"How many years earlier would Punxsutawney Phil have to be canonically alive to have made a Groundhog Day prediction in the same state as the US capitol?"</span> requires not only fact extraction but also additional calculations.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Empirical Analysis</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">After obtaining a high-quality test set, we evaluate state-of-the-art LLMs on their ability to answer questions that require proficiency in factuality, retrieval, and reasoning. Our analysis is divided into two sections: (1) Single-step Evaluations (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS1" title="3.1 Single-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3.1</span></a>): Here, we evaluate the LLMs based on a single-shot inference, where the idea is to ask the question and assess the response after a single inference call. This evaluation is further divided into cases with and without retrieval to analyze the impact of retrieval on performance. (2) Multi-Step Evaluations (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.SS2" title="3.2 Multi-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3.2</span></a>): In this case, we evaluate the models after making more than a single inference step, focusing on scenarios where retrieval is explicitly required. The motivation for multi-step evaluations is to determine whether forcing the model to retrieve and reason across multiple steps could lead to performance improvements. Next, we describe the details of the two sets of experiments.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Single-Step Evaluations</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this set of experiments, we evaluate the model using several baseline prompting methods on our test set to understand how well existing LLMs perform. Specifically, we experiment with three baseline approaches: <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">(1) Naive Prompt:</span> This is a straightforward approach where we simply ask the question to the model and evaluate if the model’s response without search retrieval contains the correct answer. <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.2">(2) BM25-Retrieved Prompt (n_docs):</span> This approach augments the question with the top n_docs documents having the highest BM25 score <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib22" title="">1995</a>)</cite> retrieved from a Wikipedia data dump<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20230601en_default_config" title="">https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20230601en_default_config</a></span></span></span>. The BM25 score is computed between the question and every article in the Wikipedia dump, after which the top n_docs with the highest scores are added to the prompt. The motivation behind this approach is to observe improvements in model performance when relevant articles are added to the context. This is denoted as BM25-R (n_doc) in the results. <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.3">(3) Oracle Prompt:</span> This prompt includes the question along with all the ground truth Wikipedia articles used by the human annotators to generate the question. The performance of the Oracle Prompt provides the upper bound of model performance in the case of a perfect retrieval system that is able to extract all the relevant articles.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Experiment Setup. </h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">For the experiments, we use <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.1">Gemini-Pro-1.5-0514</span> <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib8" title="">2024b</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.2">Gemini-Flash-1.5-0514</span> <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib7" title="">2024a</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.3">Gemma2-27b</span> <cite class="ltx_cite ltx_citemacro_citep">(Gemma et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib6" title="">2024</a>)</cite>, and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.4">Gemma2-9b</span> <cite class="ltx_cite ltx_citemacro_citep">(Gemma et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib6" title="">2024</a>)</cite> as the state-of-the-art LLM since they have shown great performance on several public benchmarks. Since the gold answers to questions in the dataset are free-form tokens instead of choices from multiple-choice answers, we use an LLM to evaluate if the outcome from the LLM under evaluation matches the gold answer, using the prompt shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A2.F7" title="Figure 7 ‣ Appendix B Autorater Prompt ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">7</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#A2" title="Appendix B Autorater Prompt ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">B</span></a>. This auto-rating mechanism was tested against human evaluations, in which the LLM-based evaluation showed strong alignment with human annotations (accuracy: 0.96 and Cohen’s Kappa: 0.889 for <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.5">Gemini-Pro-1.5-0514</span> as autorating LLM), making LLM-based evaluation a suitable approach to evaluate the correctness of model responses.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="461" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy of <span class="ltx_text ltx_font_typewriter" id="S3.F3.2.1">Gemini-Pro-1.5-0514</span> across different reasoning types in our test set. The results indicate superior performance on logical and temporal reasoning tasks, with notable deficiencies in numerical, tabular, and post-processing reasoning. The substantial performance improvements observed with oracle information underscore the critical role of relevant contextual information in enhancing model accuracy across all reasoning categories.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">LLMs perform poorly in single-step evaluations. </h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.8">Based on results shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.T3" title="Table 3 ‣ 3.2 Multi-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that naive prompting attains a performance of <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">∼</annotation></semantics></math>40% with gradual increases when including BM25 retrieved articles for <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.8.1">Gemini-Pro-1.5-0514</span>. The model achieves an accuracy of <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><mo id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.2.m2.1d">∼</annotation></semantics></math>45% when the number of documents in the context is 2, and <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><mo id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.3.m3.1d">∼</annotation></semantics></math>47% when double the number of articles are added to the context. These improvements demonstrate the room for enhancement when the model is able to retrieve relevant articles required to answer the question. The core reason behind these improvements is the improvement in recall in the articles present in context which increased from 0.12 (BM25-R(n_docs = 2) to 0.15 (BM25-R (n_docs = 4)). In addition to these approaches, we observe an accuracy of <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><mo id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.4.m4.1d">∼</annotation></semantics></math>72% for <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.8.2">Gemini-Pro-1.5-0514</span> when all the gold Wikipedia articles are provided in the context, which we call Oracle Prompt. Out of <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.1a"><mo id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.5.m5.1d">∼</annotation></semantics></math>28% samples where the model made errors, <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS1.SSS0.Px2.p1.6.m6.1a"><mo id="S3.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.6.m6.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.6.m6.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.6.m6.1d">∼</annotation></semantics></math>80% of those misclassifications belong to numerical, tabular, and post-processing categories. Hence, these misclassifications show the reasoning gaps in model performance where even after providing all the relevant facts, the model failed to reason through the different facts to provide a correct answer to the question. The accuracies obtained by the Naive Prompt and Oracle Prompt can be considered as the lower bound (when no relevant articles were provided to the model) and upper bound (when all relevant articles were provided to the model) of model performances on <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.SS1.SSS0.Px2.p1.8.3">FRAMES</span>. This pattern can also be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.F3" title="Figure 3 ‣ Experiment Setup. ‣ 3.1 Single-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> where we plotted accuracy for each reasoning type and observe that the model performed the lowest in numerical, post-processing, and tabular reasoning tasks. We also observe that adding BM25 retrieved articles primarily helped with questions requiring reasoning through multiple constraints (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS1.SSS0.Px2.p1.7.m7.1a"><mo id="S3.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.7.m7.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.7.m7.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.7.m7.1d">∼</annotation></semantics></math>8% improvement) and post-processing (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.8.m8.1"><semantics id="S3.SS1.SSS0.Px2.p1.8.m8.1a"><mo id="S3.SS1.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.8.m8.1b"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.8.m8.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.8.m8.1d">∼</annotation></semantics></math>10% improvement). This aligns well with the fact that providing more relevant articles helps in obtaining facts for each constraint, leading to improvements in performance. We take these learnings and experiment with a more complicated setup where the model is asked to find answer to questions through multiple iterations instead of a single step.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-Step Evaluations</h3>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.3.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.3.1.1.1">
<span class="ltx_p" id="S3.T3.2.3.1.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S3.T3.2.3.1.1.1.1.1">Baselines</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.3.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.3.1.2.1">G-Pro-1.5</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.3.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.3.1.3.1">G-Flash-1.5</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.3.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.3.1.4.1">Gemma2-27b</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.3.1.5"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.3.1.5.1">Gemma2-9b</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.4.1.1.1">
<span class="ltx_p" id="S3.T3.2.4.1.1.1.1" style="width:85.4pt;">Naive Prompt</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.4.1.2.1">
<span class="ltx_p" id="S3.T3.2.4.1.2.1.1" style="width:14.2pt;">0.408</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.4.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.4.1.3.1">
<span class="ltx_p" id="S3.T3.2.4.1.3.1.1" style="width:14.2pt;">0.263</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.4.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.4.1.4.1">
<span class="ltx_p" id="S3.T3.2.4.1.4.1.1" style="width:14.2pt;">0.308</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.4.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.4.1.5.1">
<span class="ltx_p" id="S3.T3.2.4.1.5.1.1" style="width:14.2pt;">0.051</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1" style="width:85.4pt;">BM25-R (n<math alttext="\_" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.m1.1a"><mi id="S3.T3.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.m1.1c">\_</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.m1.1d">_</annotation></semantics></math>doc=2)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.2.1">
<span class="ltx_p" id="S3.T3.1.1.2.1.1" style="width:14.2pt;">0.452</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.3.1">
<span class="ltx_p" id="S3.T3.1.1.3.1.1" style="width:14.2pt;">0.288</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.4.1">
<span class="ltx_p" id="S3.T3.1.1.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.5.1">
<span class="ltx_p" id="S3.T3.1.1.5.1.1" style="width:14.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.1.1">
<span class="ltx_p" id="S3.T3.2.2.1.1.1" style="width:85.4pt;">BM25-R (n<math alttext="\_" class="ltx_Math" display="inline" id="S3.T3.2.2.1.1.1.m1.1"><semantics id="S3.T3.2.2.1.1.1.m1.1a"><mi id="S3.T3.2.2.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T3.2.2.1.1.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.1.1.m1.1b"><ci id="S3.T3.2.2.1.1.1.m1.1.1.cmml" xref="S3.T3.2.2.1.1.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.1.1.m1.1c">\_</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.1.1.1.m1.1d">_</annotation></semantics></math>doc=4)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.2.1">
<span class="ltx_p" id="S3.T3.2.2.2.1.1" style="width:14.2pt;">0.474</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.3.1">
<span class="ltx_p" id="S3.T3.2.2.3.1.1" style="width:14.2pt;">0.315</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.4.1">
<span class="ltx_p" id="S3.T3.2.2.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.5.1">
<span class="ltx_p" id="S3.T3.2.2.5.1.1" style="width:14.2pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.2.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.5.2.1.1">
<span class="ltx_p" id="S3.T3.2.5.2.1.1.1" style="width:85.4pt;">Oracle Prompt</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.2.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.5.2.2.1">
<span class="ltx_p" id="S3.T3.2.5.2.2.1.1" style="width:14.2pt;">0.729</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.2.5.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.5.2.3.1">
<span class="ltx_p" id="S3.T3.2.5.2.3.1.1" style="width:14.2pt;">0.665</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.2.5.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.5.2.4.1">
<span class="ltx_p" id="S3.T3.2.5.2.4.1.1" style="width:14.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.2.5.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.5.2.5.1">
<span class="ltx_p" id="S3.T3.2.5.2.5.1.1" style="width:14.2pt;">-</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>This table presents the accuracy performance of <span class="ltx_text ltx_font_typewriter" id="S3.T3.7.1">Gemini-Pro-1.5-0514</span> (G-Pro-1.5), <span class="ltx_text ltx_font_typewriter" id="S3.T3.8.2">Gemini-Flash-1.5-0514</span> (G-Flash-1.5), <span class="ltx_text ltx_font_typewriter" id="S3.T3.9.3">Gemma2-27b</span>, and <span class="ltx_text ltx_font_typewriter" id="S3.T3.10.4">Gemma2-9b</span> on our proposed evaluation dataset. Please note that the performance of Gemma models is not reported for cases requiring longer context due to the small maximum context length of the model.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.SS2.fig1">
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Multi-Step Evaluation with BM25 Retrieval</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>  <span class="ltx_text ltx_font_bold" id="alg1.l1.1">Input:</span> Initial question <math alttext="Q" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_Q</annotation></semantics></math>, number of iterations <math alttext="n" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_n</annotation></semantics></math>, number of queries <math alttext="k" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">italic_k</annotation></semantics></math>, number of documents <math alttext="n\_docs" class="ltx_Math" display="inline" id="alg1.l1.m4.1"><semantics id="alg1.l1.m4.1a"><mrow id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml"><mi id="alg1.l1.m4.1.1.2" xref="alg1.l1.m4.1.1.2.cmml">n</mi><mo id="alg1.l1.m4.1.1.1" xref="alg1.l1.m4.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m4.1.1.3" mathvariant="normal" xref="alg1.l1.m4.1.1.3.cmml">_</mi><mo id="alg1.l1.m4.1.1.1a" xref="alg1.l1.m4.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m4.1.1.4" xref="alg1.l1.m4.1.1.4.cmml">d</mi><mo id="alg1.l1.m4.1.1.1b" xref="alg1.l1.m4.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m4.1.1.5" xref="alg1.l1.m4.1.1.5.cmml">o</mi><mo id="alg1.l1.m4.1.1.1c" xref="alg1.l1.m4.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m4.1.1.6" xref="alg1.l1.m4.1.1.6.cmml">c</mi><mo id="alg1.l1.m4.1.1.1d" xref="alg1.l1.m4.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m4.1.1.7" xref="alg1.l1.m4.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><apply id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1"><times id="alg1.l1.m4.1.1.1.cmml" xref="alg1.l1.m4.1.1.1"></times><ci id="alg1.l1.m4.1.1.2.cmml" xref="alg1.l1.m4.1.1.2">𝑛</ci><ci id="alg1.l1.m4.1.1.3.cmml" xref="alg1.l1.m4.1.1.3">_</ci><ci id="alg1.l1.m4.1.1.4.cmml" xref="alg1.l1.m4.1.1.4">𝑑</ci><ci id="alg1.l1.m4.1.1.5.cmml" xref="alg1.l1.m4.1.1.5">𝑜</ci><ci id="alg1.l1.m4.1.1.6.cmml" xref="alg1.l1.m4.1.1.6">𝑐</ci><ci id="alg1.l1.m4.1.1.7.cmml" xref="alg1.l1.m4.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">n\_docs</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m4.1d">italic_n _ italic_d italic_o italic_c italic_s</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>  <span class="ltx_text ltx_font_bold" id="alg1.l2.1">Output:</span> Final response <math alttext="R" class="ltx_Math" display="inline" id="alg1.l2.m1.1"><semantics id="alg1.l2.m1.1a"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="alg1.l2.m1.1d">italic_R</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>  Initialize context <math alttext="C\leftarrow\{Q\}" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.2" xref="alg1.l3.m1.1.2.cmml"><mi id="alg1.l3.m1.1.2.2" xref="alg1.l3.m1.1.2.2.cmml">C</mi><mo id="alg1.l3.m1.1.2.1" stretchy="false" xref="alg1.l3.m1.1.2.1.cmml">←</mo><mrow id="alg1.l3.m1.1.2.3.2" xref="alg1.l3.m1.1.2.3.1.cmml"><mo id="alg1.l3.m1.1.2.3.2.1" stretchy="false" xref="alg1.l3.m1.1.2.3.1.cmml">{</mo><mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">Q</mi><mo id="alg1.l3.m1.1.2.3.2.2" stretchy="false" xref="alg1.l3.m1.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.2.cmml" xref="alg1.l3.m1.1.2"><ci id="alg1.l3.m1.1.2.1.cmml" xref="alg1.l3.m1.1.2.1">←</ci><ci id="alg1.l3.m1.1.2.2.cmml" xref="alg1.l3.m1.1.2.2">𝐶</ci><set id="alg1.l3.m1.1.2.3.1.cmml" xref="alg1.l3.m1.1.2.3.2"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝑄</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">C\leftarrow\{Q\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_C ← { italic_Q }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>  <span class="ltx_text ltx_font_bold" id="alg1.l4.1">for</span> iteration <math alttext="i=1" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">i</mi><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">=</mo><mn id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><eq id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></eq><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝑖</ci><cn id="alg1.l4.m1.1.1.3.cmml" type="integer" xref="alg1.l4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">i=1</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">italic_i = 1</annotation></semantics></math> to <math alttext="n" class="ltx_Math" display="inline" id="alg1.l4.m2.1"><semantics id="alg1.l4.m2.1a"><mi id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b"><ci id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m2.1d">italic_n</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l4.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l5.1">for</span> query <math alttext="j=1" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">j</mi><mo id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">=</mo><mn id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><eq id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"></eq><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">𝑗</ci><cn id="alg1.l5.m1.1.1.3.cmml" type="integer" xref="alg1.l5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">j=1</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">italic_j = 1</annotation></semantics></math> to <math alttext="k" class="ltx_Math" display="inline" id="alg1.l5.m2.1"><semantics id="alg1.l5.m2.1a"><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m2.1d">italic_k</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l5.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>        Generate search query <math alttext="Q_{ij}" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><msub id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">Q</mi><mrow id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">i</mi><mo id="alg1.l6.m1.1.1.3.1" xref="alg1.l6.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1">subscript</csymbol><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑄</ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><times id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3.1"></times><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝑖</ci><ci id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">Q_{ij}</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_Q start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> based on context <math alttext="C" class="ltx_Math" display="inline" id="alg1.l6.m2.1"><semantics id="alg1.l6.m2.1a"><mi id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.1d">italic_C</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>        Retrieve top <math alttext="n\_docs" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">n</mi><mo id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3" mathvariant="normal" xref="alg1.l7.m1.1.1.3.cmml">_</mi><mo id="alg1.l7.m1.1.1.1a" xref="alg1.l7.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.4" xref="alg1.l7.m1.1.1.4.cmml">d</mi><mo id="alg1.l7.m1.1.1.1b" xref="alg1.l7.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.5" xref="alg1.l7.m1.1.1.5.cmml">o</mi><mo id="alg1.l7.m1.1.1.1c" xref="alg1.l7.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.6" xref="alg1.l7.m1.1.1.6.cmml">c</mi><mo id="alg1.l7.m1.1.1.1d" xref="alg1.l7.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.7" xref="alg1.l7.m1.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><times id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1"></times><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">𝑛</ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">_</ci><ci id="alg1.l7.m1.1.1.4.cmml" xref="alg1.l7.m1.1.1.4">𝑑</ci><ci id="alg1.l7.m1.1.1.5.cmml" xref="alg1.l7.m1.1.1.5">𝑜</ci><ci id="alg1.l7.m1.1.1.6.cmml" xref="alg1.l7.m1.1.1.6">𝑐</ci><ci id="alg1.l7.m1.1.1.7.cmml" xref="alg1.l7.m1.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">n\_docs</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">italic_n _ italic_d italic_o italic_c italic_s</annotation></semantics></math> documents <math alttext="D_{ij}" class="ltx_Math" display="inline" id="alg1.l7.m2.1"><semantics id="alg1.l7.m2.1a"><msub id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml"><mi id="alg1.l7.m2.1.1.2" xref="alg1.l7.m2.1.1.2.cmml">D</mi><mrow id="alg1.l7.m2.1.1.3" xref="alg1.l7.m2.1.1.3.cmml"><mi id="alg1.l7.m2.1.1.3.2" xref="alg1.l7.m2.1.1.3.2.cmml">i</mi><mo id="alg1.l7.m2.1.1.3.1" xref="alg1.l7.m2.1.1.3.1.cmml">⁢</mo><mi id="alg1.l7.m2.1.1.3.3" xref="alg1.l7.m2.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><apply id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.1.cmml" xref="alg1.l7.m2.1.1">subscript</csymbol><ci id="alg1.l7.m2.1.1.2.cmml" xref="alg1.l7.m2.1.1.2">𝐷</ci><apply id="alg1.l7.m2.1.1.3.cmml" xref="alg1.l7.m2.1.1.3"><times id="alg1.l7.m2.1.1.3.1.cmml" xref="alg1.l7.m2.1.1.3.1"></times><ci id="alg1.l7.m2.1.1.3.2.cmml" xref="alg1.l7.m2.1.1.3.2">𝑖</ci><ci id="alg1.l7.m2.1.1.3.3.cmml" xref="alg1.l7.m2.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">D_{ij}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m2.1d">italic_D start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> using BM25 based on <math alttext="Q_{ij}" class="ltx_Math" display="inline" id="alg1.l7.m3.1"><semantics id="alg1.l7.m3.1a"><msub id="alg1.l7.m3.1.1" xref="alg1.l7.m3.1.1.cmml"><mi id="alg1.l7.m3.1.1.2" xref="alg1.l7.m3.1.1.2.cmml">Q</mi><mrow id="alg1.l7.m3.1.1.3" xref="alg1.l7.m3.1.1.3.cmml"><mi id="alg1.l7.m3.1.1.3.2" xref="alg1.l7.m3.1.1.3.2.cmml">i</mi><mo id="alg1.l7.m3.1.1.3.1" xref="alg1.l7.m3.1.1.3.1.cmml">⁢</mo><mi id="alg1.l7.m3.1.1.3.3" xref="alg1.l7.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m3.1b"><apply id="alg1.l7.m3.1.1.cmml" xref="alg1.l7.m3.1.1"><csymbol cd="ambiguous" id="alg1.l7.m3.1.1.1.cmml" xref="alg1.l7.m3.1.1">subscript</csymbol><ci id="alg1.l7.m3.1.1.2.cmml" xref="alg1.l7.m3.1.1.2">𝑄</ci><apply id="alg1.l7.m3.1.1.3.cmml" xref="alg1.l7.m3.1.1.3"><times id="alg1.l7.m3.1.1.3.1.cmml" xref="alg1.l7.m3.1.1.3.1"></times><ci id="alg1.l7.m3.1.1.3.2.cmml" xref="alg1.l7.m3.1.1.3.2">𝑖</ci><ci id="alg1.l7.m3.1.1.3.3.cmml" xref="alg1.l7.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m3.1c">Q_{ij}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m3.1d">italic_Q start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>        <math alttext="C\leftarrow C\cup(D_{ij}\setminus C)" class="ltx_Math" display="inline" id="alg1.l8.m1.1"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml">C</mi><mo id="alg1.l8.m1.1.1.2" stretchy="false" xref="alg1.l8.m1.1.1.2.cmml">←</mo><mrow id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml"><mi id="alg1.l8.m1.1.1.1.3" xref="alg1.l8.m1.1.1.1.3.cmml">C</mi><mo id="alg1.l8.m1.1.1.1.2" xref="alg1.l8.m1.1.1.1.2.cmml">∪</mo><mrow id="alg1.l8.m1.1.1.1.1.1" xref="alg1.l8.m1.1.1.1.1.1.1.cmml"><mo id="alg1.l8.m1.1.1.1.1.1.2" stretchy="false" xref="alg1.l8.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l8.m1.1.1.1.1.1.1" xref="alg1.l8.m1.1.1.1.1.1.1.cmml"><msub id="alg1.l8.m1.1.1.1.1.1.1.2" xref="alg1.l8.m1.1.1.1.1.1.1.2.cmml"><mi id="alg1.l8.m1.1.1.1.1.1.1.2.2" xref="alg1.l8.m1.1.1.1.1.1.1.2.2.cmml">D</mi><mrow id="alg1.l8.m1.1.1.1.1.1.1.2.3" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.cmml"><mi id="alg1.l8.m1.1.1.1.1.1.1.2.3.2" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="alg1.l8.m1.1.1.1.1.1.1.2.3.1" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="alg1.l8.m1.1.1.1.1.1.1.2.3.3" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="alg1.l8.m1.1.1.1.1.1.1.1" xref="alg1.l8.m1.1.1.1.1.1.1.1.cmml">∖</mo><mi id="alg1.l8.m1.1.1.1.1.1.1.3" xref="alg1.l8.m1.1.1.1.1.1.1.3.cmml">C</mi></mrow><mo id="alg1.l8.m1.1.1.1.1.1.3" stretchy="false" xref="alg1.l8.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">←</ci><ci id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3">𝐶</ci><apply id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1"><union id="alg1.l8.m1.1.1.1.2.cmml" xref="alg1.l8.m1.1.1.1.2"></union><ci id="alg1.l8.m1.1.1.1.3.cmml" xref="alg1.l8.m1.1.1.1.3">𝐶</ci><apply id="alg1.l8.m1.1.1.1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1.1.1"><setdiff id="alg1.l8.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.1"></setdiff><apply id="alg1.l8.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.1.1.1.1.2.1.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.1.1.1.1.2.2.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2.2">𝐷</ci><apply id="alg1.l8.m1.1.1.1.1.1.1.2.3.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2.3"><times id="alg1.l8.m1.1.1.1.1.1.1.2.3.1.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.1"></times><ci id="alg1.l8.m1.1.1.1.1.1.1.2.3.2.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="alg1.l8.m1.1.1.1.1.1.1.2.3.3.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><ci id="alg1.l8.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l8.m1.1.1.1.1.1.1.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">C\leftarrow C\cup(D_{ij}\setminus C)</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.m1.1d">italic_C ← italic_C ∪ ( italic_D start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ∖ italic_C )</annotation></semantics></math> {Add only new documents to context}

</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l9.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l9.2">for</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>  <span class="ltx_text ltx_font_bold" id="alg1.l10.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l10.2">for</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>  Generate final response <math alttext="R" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">italic_R</annotation></semantics></math> using context <math alttext="C" class="ltx_Math" display="inline" id="alg1.l11.m2.1"><semantics id="alg1.l11.m2.1a"><mi id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><ci id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m2.1d">italic_C</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>  <span class="ltx_text ltx_font_bold" id="alg1.l12.1">return</span> <math alttext="R" class="ltx_Math" display="inline" id="alg1.l12.m1.1"><semantics id="alg1.l12.m1.1a"><mi id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><ci id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m1.1d">italic_R</annotation></semantics></math>
</div>
</div>
</figure>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">Based on the findings from the previous experiment with single-step evaluations, where we observed an increase in performance when related articles are added to the context, wew were led to explore a setting where the model is compelled to plan its search for relevant Wikipedia articles in order to find answers. More specifically, we design a pipeline where the model is asked a question along with the instruction to generate <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> search queries which are then used to extract the top-n_docs Wikipedia articles with the highest BM25 scores. These documents are then appended to the context. This process of query generation and retrieved article augmentation is carried forward for <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_n</annotation></semantics></math> steps. Once the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_n</annotation></semantics></math> steps of retrieval are completed, the model is asked to answer the question based on the articles appended in the context, as shown in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#alg1" title="Algorithm 1 ‣ 3.2 Multi-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. We conduct two sets of experiments here: (1) Vanilla with no explicit planning instructions, and (2) With search planning instructions to help the model navigate the search process efficiently. To implement this pipeline, we used the simplest document retrieval component which is essentially an index of Wikipedia pages, where the articles with the highest BM25 scores for each query are returned to the LLM and added to the context. This retrieval component is used instead of making direct calls to an online search engine for two reasons: (1) We would like to keep the retrieval system constant to clearly evaluate the search planning capability of the LLMs instead of the retrieval system’s capability in returning the most relevant articles, and (2) The BM25-based retrieval system makes our pipeline reproducible and limits the search space to Wikipedia pages only, as the questions were generated from Wikipedia articles only.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S3.F4.1" style="width:208.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="548" id="S3.F4.1.g1" src="extracted/5866557/results/multi_step_search_plan_k.png" width="548"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S3.F4.2" style="width:208.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="548" id="S3.F4.2.g1" src="extracted/5866557/results/multi_step_search_plan_n.png" width="548"/>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The figure shows the performance improvements when the number of steps (n) and number of queries per step (k) is changed. We achieved the best performance of 0.66 with the combination of (<math alttext="k" class="ltx_Math" display="inline" id="S3.F4.6.m1.1"><semantics id="S3.F4.6.m1.1b"><mi id="S3.F4.6.m1.1.1" xref="S3.F4.6.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.F4.6.m1.1c"><ci id="S3.F4.6.m1.1.1.cmml" xref="S3.F4.6.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S3.F4.6.m1.1e">italic_k</annotation></semantics></math>, <math alttext="n" class="ltx_Math" display="inline" id="S3.F4.7.m2.1"><semantics id="S3.F4.7.m2.1b"><mi id="S3.F4.7.m2.1.1" xref="S3.F4.7.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F4.7.m2.1c"><ci id="S3.F4.7.m2.1.1.cmml" xref="S3.F4.7.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.7.m2.1d">n</annotation><annotation encoding="application/x-llamapun" id="S3.F4.7.m2.1e">italic_n</annotation></semantics></math>, <math alttext="n\_docs" class="ltx_Math" display="inline" id="S3.F4.8.m3.1"><semantics id="S3.F4.8.m3.1b"><mrow id="S3.F4.8.m3.1.1" xref="S3.F4.8.m3.1.1.cmml"><mi id="S3.F4.8.m3.1.1.2" xref="S3.F4.8.m3.1.1.2.cmml">n</mi><mo id="S3.F4.8.m3.1.1.1" xref="S3.F4.8.m3.1.1.1.cmml">⁢</mo><mi id="S3.F4.8.m3.1.1.3" mathvariant="normal" xref="S3.F4.8.m3.1.1.3.cmml">_</mi><mo id="S3.F4.8.m3.1.1.1b" xref="S3.F4.8.m3.1.1.1.cmml">⁢</mo><mi id="S3.F4.8.m3.1.1.4" xref="S3.F4.8.m3.1.1.4.cmml">d</mi><mo id="S3.F4.8.m3.1.1.1c" xref="S3.F4.8.m3.1.1.1.cmml">⁢</mo><mi id="S3.F4.8.m3.1.1.5" xref="S3.F4.8.m3.1.1.5.cmml">o</mi><mo id="S3.F4.8.m3.1.1.1d" xref="S3.F4.8.m3.1.1.1.cmml">⁢</mo><mi id="S3.F4.8.m3.1.1.6" xref="S3.F4.8.m3.1.1.6.cmml">c</mi><mo id="S3.F4.8.m3.1.1.1e" xref="S3.F4.8.m3.1.1.1.cmml">⁢</mo><mi id="S3.F4.8.m3.1.1.7" xref="S3.F4.8.m3.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.8.m3.1c"><apply id="S3.F4.8.m3.1.1.cmml" xref="S3.F4.8.m3.1.1"><times id="S3.F4.8.m3.1.1.1.cmml" xref="S3.F4.8.m3.1.1.1"></times><ci id="S3.F4.8.m3.1.1.2.cmml" xref="S3.F4.8.m3.1.1.2">𝑛</ci><ci id="S3.F4.8.m3.1.1.3.cmml" xref="S3.F4.8.m3.1.1.3">_</ci><ci id="S3.F4.8.m3.1.1.4.cmml" xref="S3.F4.8.m3.1.1.4">𝑑</ci><ci id="S3.F4.8.m3.1.1.5.cmml" xref="S3.F4.8.m3.1.1.5">𝑜</ci><ci id="S3.F4.8.m3.1.1.6.cmml" xref="S3.F4.8.m3.1.1.6">𝑐</ci><ci id="S3.F4.8.m3.1.1.7.cmml" xref="S3.F4.8.m3.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.8.m3.1d">n\_docs</annotation><annotation encoding="application/x-llamapun" id="S3.F4.8.m3.1e">italic_n _ italic_d italic_o italic_c italic_s</annotation></semantics></math>) = (5,5,10)</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="604" id="S3.F5.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>This plot shows the accuracy of <span class="ltx_text ltx_font_typewriter" id="S3.F5.2.1">Gemini-Pro-1.5-0514</span> on each reasoning type in our test set. We observe a significant increase in performance for all reasoning types when we use multi-step search planning, with the performance on numerical reasoning even exceeding oracle performance.</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Multi-Step retrievals significantly improve model performance. </h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.8">We plot model performance on different combinations of (<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math>, <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_n</annotation></semantics></math>, <math alttext="n\_docs" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" mathvariant="normal" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">_</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.4" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.4.cmml">d</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1b" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.5" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.5.cmml">o</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1c" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.6" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.6.cmml">c</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1d" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.7" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><times id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝑛</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">_</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.4">𝑑</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.5.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.5">𝑜</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.6.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.6">𝑐</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.7.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">n\_docs</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.1d">italic_n _ italic_d italic_o italic_c italic_s</annotation></semantics></math>) in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S3.F5" title="Figure 5 ‣ 3.2 Multi-Step Evaluations ‣ 3 Empirical Analysis ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a>. Based on these results, we observe a steady increase in performance as the number of steps and queries are increased with accuracy increasing from <math alttext="\sim 0.45" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml"></mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">∼</mo><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">0.45</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">absent</csymbol><cn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" type="float" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">0.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\sim 0.45</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.1d">∼ 0.45</annotation></semantics></math> to <math alttext="\sim 0.52" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml"></mi><mo id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml">∼</mo><mn id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml">0.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2">absent</csymbol><cn id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml" type="float" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3">0.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.1c">\sim 0.52</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m5.1d">∼ 0.52</annotation></semantics></math> for the case of (<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px1.p1.6.m6.1a"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m6.1b"><ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.6.m6.1d">italic_k</annotation></semantics></math>, <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px1.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.7.m7.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.7.m7.1d">italic_n</annotation></semantics></math>, <math alttext="n\_docs" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.8.m8.1"><semantics id="S3.SS2.SSS0.Px1.p1.8.m8.1a"><mrow id="S3.SS2.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml">n</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3" mathvariant="normal" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml">_</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.4" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.4.cmml">d</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1b" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.5" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.5.cmml">o</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1c" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.6" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.6.cmml">c</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1d" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.7" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.8.m8.1b"><apply id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1"><times id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2">𝑛</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3">_</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.4">𝑑</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.5.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.5">𝑜</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.6.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.6">𝑐</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.7.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.8.m8.1c">n\_docs</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.8.m8.1d">italic_n _ italic_d italic_o italic_c italic_s</annotation></semantics></math>)=(5,5,2) for vanilla setting where the model is not provided with any specific planning instructions. This is expected, as more steps and queries allow the model to add more relevant documents to its context, leading the model to improve recall, which translates to better accuracy. However, the performance still remains quite low even after five iterations of search retrievals, which is computationally expensive since this requires six non-parallelizable inference calls (five for retrieval + one for final answering) to answer each question. One of the reasons we found behind this slow progress in performance is the lack of diversity in the queries generated by the model; it seemed like the model goes in the wrong direction in search retrievals and never corrects itself. To mitigate this problem, we experiment with two changes to the instructions: (1) We provide a few examples of how an ideal best-case search query sequence should look, and (2) We provide instructions not to repeat queries and force the model to "think step-by-step" <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib14" title="">2022</a>)</cite>. We observe a very promising trend with these changes, where the model performance (0.66) through iterations reaches close to the oracle performance (0.73) by the end of five iterations of retrievals. We hope our benchmark will be useful for the community to further reduce the number of search calls and improve model accuracy.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Evaluating Retrieval-Augmented Generation (RAG) systems has become increasingly important as these models integrate retrieval mechanisms with generative capabilities to enhance factual accuracy and reasoning<cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib32" title="">2024b</a>)</cite>. Existing benchmarks, such as NaturalQuestions <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib15" title="">2019</a>)</cite>, TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib10" title="">2017</a>)</cite>, and ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib3" title="">2019</a>)</cite>, have been used to evaluate RAG models, but they often focus on specific aspects like retrieval accuracy or single-turn question answering without considering the full complexity of real-world applications. For instance, NaturalQuestions primarily tests retrieval precision, while TriviaQA emphasizes factual correctness in trivia-style questions. ELI5, on the other hand, is designed for explainability but does not rigorously assess the multi-hop reasoning necessary for synthesizing information from multiple sources. These benchmarks, while valuable, tend to evaluate RAG systems in a piecemeal fashion, missing the comprehensive assessment needed to truly measure their end-to-end capabilities. We provide additional comparisons against other datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.p2.1.1">FRAMES</span> addresses these limitations by offering a unified and more holistic evaluation framework for RAG systems. Unlike existing datasets, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.p2.1.2">FRAMES</span> tests models across three critical dimensions: factual retrieval, reasoning, and synthesis. It incorporates complex multi-hop queries that require models to retrieve and integrate information from various sources while also handling temporal disambiguation—a challenge not adequately covered by benchmarks like NaturalQuestions or ELI5. Additionally, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.p2.1.3">FRAMES</span> includes tasks that assess the synthesis of information into coherent and contextually accurate responses, ensuring that RAG systems are evaluated on their ability to perform in realistic, multifaceted scenarios. This makes <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.p2.1.4">FRAMES</span> a more rigorous and comprehensive benchmark, well suited for guiding the development of next-generation RAG systems.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we introduced <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.p1.1.1">FRAMES</span>, a comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning. Our experiments with state-of-the-art LLMs highlight the existing gaps in their ability to handle complex, multi-hop reasoning tasks. The baseline results showed that even advanced models struggle significantly with the challenging scenarios presented in <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.p1.1.2">FRAMES</span>, achieving only moderate improvements when multi-step retrieval and reasoning strategies were employed. The <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.p1.1.3">FRAMES</span> dataset addresses a critical need in the evaluation of RAG systems by offering an integrated framework that tests these systems in a more holistic manner compared to existing benchmarks. By simulating realistic, multi-document queries, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.p1.1.4">FRAMES</span> provides a clearer picture of the current capabilities and limitations of LLMs in real-world applications. Our findings underscore the importance of further enhancing both the retrieval mechanisms and the reasoning capabilities of these models to improve their overall performance.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Future Work.</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Moving forward, there are several promising avenues for future research. First, the development of more sophisticated retrieval strategies is essential. This includes exploring dense retrievers trained directly on the multihop retrieval task, such as those based on ColBERT <cite class="ltx_cite ltx_citemacro_citep">(Khattab &amp; Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib12" title="">2020</a>)</cite>, or SimCSE <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib4" title="">2021</a>)</cite> architectures. These approaches could better handle diverse and complex queries by adapting to the context iteratively. Second, improving the reasoning capabilities of LLMs remains a significant challenge. We can explore process supervision methods like those used in PRM-800K <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib17" title="">2023</a>)</cite>, or investigate distillation techniques on successful trajectories, similar to approaches in ToolFormer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib23" title="">2024</a>)</cite> and DSPy<cite class="ltx_cite ltx_citemacro_citep">(Khattab et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12941v1#bib.bib13" title="">2023</a>)</cite>. These methods could enhance numerical, temporal, and post-processing reasoning. Additionally, we can explore modeling approaches such as context reduction of wiki articles to improve planning capabilities and training query generators for more effective information retrieval. Lastly, expanding the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.SS0.SSS0.Px1.p1.1.1">FRAMES</span> dataset to include more diverse and domain-specific questions, as well as incorporating more dynamic elements such as real-time information retrieval, could further enhance its utility as a benchmark for next-generation RAG systems. It is important to note that future work should also address the potential limitations of our current approach, including the risk of pretraining data contamination, which may affect the generalizability and reliability of the results, particularly when using Wikipedia articles that could overlap with LLM training data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Limitations.</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">While <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.SS0.SSS0.Px2.p1.1.1">FRAMES</span> provides a comprehensive evaluation framework for RAG systems, it is important to acknowledge certain limitations. One significant concern is the potential for pretraining data contamination. As large language models are trained on vast amounts of internet data, there is a risk that some of the information in our dataset may have been seen by these models during their pretraining phase. This could lead to artificially inflated performance metrics and reduce the dataset’s effectiveness in measuring true generalization capabilities. Future iterations of <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S5.SS0.SSS0.Px2.p1.1.2">FRAMES</span> should explore techniques to mitigate this issue, such as using more recent or synthetic data, or developing methods to quantify and account for potential contamination. Additionally, while we have strived for diversity in our dataset, it may not fully represent the entire spectrum of real-world queries and scenarios, potentially limiting its applicability to certain domains or use cases. Addressing these limitations will be crucial for improving the robustness and reliability of RAG system evaluations.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang.

</span>
<span class="ltx_bibblock">Hybridqa: A dataset of multi-hop question answering over tabular and
textual data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2004.07347</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2019)</span>
<span class="ltx_bibblock">
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
Michael Auli.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1907.09190</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2021)</span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2104.08821</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma et al. (2024)</span>
<span class="ltx_bibblock">
Team Gemma, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy
Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak
Shahriari, Alexandre Ramé, et al.

</span>
<span class="ltx_bibblock">Gemma 2: Improving open language models at a practical size.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2408.00118</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2024a)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini 1.5 flash.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://deepmind.google/technologies/gemini/flash/" title="">https://deepmind.google/technologies/gemini/flash/</a>,
2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2024b)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini 1.5 pro.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/" title="">https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</a>,
2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International conference on machine learning</em>, pp. 3929–3938. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai et al. (2024)</span>
<span class="ltx_bibblock">
Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, Kentaro Inui, et al.

</span>
<span class="ltx_bibblock">Realtime qa: what’s the answer right now?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab &amp; Zaharia (2020)</span>
<span class="ltx_bibblock">
Omar Khattab and Matei Zaharia.

</span>
<span class="ltx_bibblock">Colbert: Efficient and effective passage search via contextualized
late interaction over bert.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval</em>, pp.  39–48, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et al. (2023)</span>
<span class="ltx_bibblock">
Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav
Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi,
Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts.

</span>
<span class="ltx_bibblock">Dspy: Compiling declarative language model calls into self-improving
pipelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2310.03714</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in neural information processing systems</em>,
35:22199–22213, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Transactions of the Association for Computational Linguistics</em>,
7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems</em>,
33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al. (2023)</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2305.20050</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2109.07958</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),
<em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>, pp.  3214–3252. Association for Computational
Linguistics, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/V1/2022.ACL-LONG.229</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.acl-long.229" title="">https://doi.org/10.18653/v1/2022.acl-long.229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu et al.

</span>
<span class="ltx_bibblock">Learning to rank for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Foundations and Trends® in Information
Retrieval</em>, 3(3):225–331, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. (1995)</span>
<span class="ltx_bibblock">
Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,
Mike Gatford, et al.

</span>
<span class="ltx_bibblock">Okapi at trec-3.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Nist Special Publication Sp</em>, 109:109, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2024)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria
Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schnitzler et al. (2024)</span>
<span class="ltx_bibblock">
Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, and
Akiko Aizawa.

</span>
<span class="ltx_bibblock">Morehopqa: More than multi-hop reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2406.13397</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang &amp; Yang (2024)</span>
<span class="ltx_bibblock">
Yixuan Tang and Yi Yang.

</span>
<span class="ltx_bibblock">Multihop-rag: Benchmarking retrieval-augmented generation for
multi-hop queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2401.15391</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2022)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Musique: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Trans. Assoc. Comput. Linguistics</em>, 10:539–554,
2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/TACL\_A\_00475</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00475" title="">https://doi.org/10.1162/tacl_a_00475</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2023)</span>
<span class="ltx_bibblock">
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris
Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al.

</span>
<span class="ltx_bibblock">Freshllms: Refreshing large language models with search engine
augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2310.03214</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. (2017)</span>
<span class="ltx_bibblock">
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel.

</span>
<span class="ltx_bibblock">Constructing datasets for multi-hop reading comprehension across
documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CoRR</em>, abs/1710.06481, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1710.06481" title="">http://arxiv.org/abs/1710.06481</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018a)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock">In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii
(eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, Brussels, Belgium, October 31 - November 4,
2018</em>, pp.  2369–2380. Association for Computational Linguistics,
2018a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/V1/D18-1259</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/d18-1259" title="">https://doi.org/10.18653/v1/d18-1259</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018b)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan
Salakhutdinov, and Christopher D Manning.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:1809.09600</em>, 2018b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024a)</span>
<span class="ltx_bibblock">
Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.

</span>
<span class="ltx_bibblock">Evaluation of retrieval-augmented generation: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ArXiv</em>, abs/2405.07437, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:269758033" title="">https://api.semanticscholar.org/CorpusID:269758033</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024b)</span>
<span class="ltx_bibblock">
Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.

</span>
<span class="ltx_bibblock">Evaluation of retrieval-augmented generation: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2405.07437</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Synthetic Data Generation Prompt</h2>
<figure class="ltx_figure" id="A1.F6"><svg class="ltx_picture" height="545.07" id="A1.F6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,545.07) matrix(1 0 0 -1 0 0)"><g fill="#333399" fill-opacity="1.0"><path d="M 0 5.91 L 0 539.16 C 0 542.42 2.64 545.07 5.91 545.07 L 594.09 545.07 C 597.36 545.07 600 542.42 600 539.16 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 520.96 L 598.03 520.96 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 526.86)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F6.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.F6.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.F6.pic1.1.1.1.1.1.1.1">Synthetic Data Generation</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="495.36" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F6.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.1.1">System:</span> You are a helpful assistant.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.2.1">User:</span> """<span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.2.2">TASK:</span> You will be provided with <span class="ltx_text ltx_font_typewriter" id="A1.F6.pic1.2.2.2.1.1.2.3">{k_context}</span> Wikipedia article extracts. Based on these extracts, generate <span class="ltx_text ltx_font_typewriter" id="A1.F6.pic1.2.2.2.1.1.2.4">{n_questions}</span> challenging factoid questions that meet the following criteria:</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.3">1. <span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.3.1">Standalone &amp; Context-Independent:</span> Questions should not contain any references to "Article 1", "Article 2", etc. They should be understandable without any additional context.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.4">2. <span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.4.1">Unambiguous Answer:</span> Each question should have a single, clear, and factual answer.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.5">3. <span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.5.1">Multi-hop Reasoning:</span> Answering each question should require combining information from ALL <span class="ltx_text ltx_font_typewriter" id="A1.F6.pic1.2.2.2.1.1.5.2">{k_context}</span> provided Wikipedia articles.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.6">4. <span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.6.1">Grounded in Context &amp; Conceptual Format:</span> Each question must conceptually follow this format, seamlessly integrating information from each article:</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.7">**Start with a clear question word (What/How/Where/When).**</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.8">**Introduce information from each article step-by-step, using connectors to link them logically.**</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.9">**Example connectors: ’in relation to’, ’compared to’, ’as a result of’, ’which also’, ’in addition to’.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.10">** For each question: *</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.11">**Provide the single-word answer in parentheses after the question mark.** *</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.12">**On a new line, clearly explain the reasoning process.** *</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.13">**For each article, bullet point the specific piece of information used to formulate the question.**</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.14"><span class="ltx_text ltx_font_bold" id="A1.F6.pic1.2.2.2.1.1.14.1">Example:</span></span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.15">**Question:** What type of bird, belonging to the Ardeidae family, went extinct around 1690 and was known for its terrestrial abilities? (Dodo)</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.16">**Reasoning:** *
**Article 1:** Provides information about the Dodo belonging to the Ardeidae family. *
**Article 2:** Mentions the extinction of the Dodo around 1690. *
**Article 3:** Highlights the Dodo’s adaptation to terrestrial life.</span>
<span class="ltx_p" id="A1.F6.pic1.2.2.2.1.1.17"><span class="ltx_text ltx_font_typewriter" id="A1.F6.pic1.2.2.2.1.1.17.1">{WIKI ARTICLES}</span>
"""</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Prompt used to generate questions synthetically using <span class="ltx_text ltx_font_typewriter" id="A1.F6.4.1">Gemini-Pro-1.5-0514</span>. <span class="ltx_text ltx_font_typewriter" id="A1.F6.5.2">k_context</span> and <span class="ltx_text ltx_font_typewriter" id="A1.F6.6.3">n_questions</span> are placeholders for the number of articles provided and the number of questions to generate per inference.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Autorater Prompt</h2>
<figure class="ltx_figure" id="A2.F7"><svg class="ltx_picture" height="410.69" id="A2.F7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,410.69) matrix(1 0 0 -1 0 0)"><g fill="#333399" fill-opacity="1.0"><path d="M 0 5.91 L 0 404.79 C 0 408.05 2.64 410.69 5.91 410.69 L 594.09 410.69 C 597.36 410.69 600 408.05 600 404.79 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 386.74 L 598.03 386.74 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 392.64)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F7.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F7.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.F7.pic1.1.1.1.1.1.1.1">Auto-rating Prompt</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="361.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F7.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A2.F7.pic1.2.2.2.1.1.1.1">System:</span> You are a helpful assistant.</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A2.F7.pic1.2.2.2.1.1.2.1">User:</span> """===Task===</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.3">I need your help in evaluating an answer provided by an LLM against a ground truth answer. Your task is to determine if the ground truth answer is present in the LLM’s response. Please analyze the provided data and make a decision.</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.4">===Instructions===</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.5">1. Carefully compare the "Predicted Answer" with the "Ground Truth Answer".</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.6">2. Consider the substance of the answers – look for equivalent information or correct answers. Do not focus on exact wording unless the exact wording is crucial to the meaning.</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.7">3. Your final decision should be based on whether the meaning and the vital facts of the "Ground Truth Answer" are present in the "Predicted Answer:"</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.8">===Input Data===</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.9">- Question: <span class="ltx_text ltx_font_typewriter" id="A2.F7.pic1.2.2.2.1.1.9.1">&lt;&lt;question&gt;&gt;</span></span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.10">- Predicted Answer: <span class="ltx_text ltx_font_typewriter" id="A2.F7.pic1.2.2.2.1.1.10.1">&lt;&lt;LLM_response&gt;&gt;</span></span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.11">- Ground Truth Answer: <span class="ltx_text ltx_font_typewriter" id="A2.F7.pic1.2.2.2.1.1.11.1">&lt;&lt;ground_truth_answer&gt;&gt;
<br class="ltx_break"/></span></span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.12">===Output Format===</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.13">Provide your final evaluation in the following format:</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.14">"Explanation:" (How you made the decision?)</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.15">"Decision:" ("TRUE" or "FALSE" )</span>
<span class="ltx_p" id="A2.F7.pic1.2.2.2.1.1.16">Please proceed with the evaluation."""
"""</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Prompt used to auto-rate the responses of LLM in the experiments. The LLM is provided with questions, model responses, and ground truth answers, along with instructions to check if the model response contains the gold answer.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 17:52:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
