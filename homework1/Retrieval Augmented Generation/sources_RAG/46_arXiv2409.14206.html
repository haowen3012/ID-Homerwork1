<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues</title>
<!--Generated on Sat Sep 21 17:18:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14206v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14206v1#S1" title="In AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14206v1#S2" title="In AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>IPAs for Future Space Missions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14206v1#S3" title="In AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results - CORE Prototype</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14206v1#S4" title="In AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oliver Bensch
</span><span class="ltx_author_notes">Corresponding author.
<span class="ltx_contact ltx_role_affiliation">German Aerospace Center, Institute for Software Technology
</span>
<span class="ltx_contact ltx_role_affiliation">AI Institute in Dynamic Systems, University of Washington, Seattle, WA USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leonie Bensch
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">German Aerospace Center, Institute for Software Technology
</span>
<span class="ltx_contact ltx_role_affiliation">European Space Agency, European Astronaut Center (ESA-EAC)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tommy Nilsson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">European Space Agency, European Astronaut Center (ESA-EAC)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Florian Saling
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">European Space Agency, European Astronaut Center (ESA-EAC)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bernd Bewer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">European Space Agency, European Astronaut Center (ESA-EAC)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sophie Jentzsch
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">German Aerospace Center, Institute for Software Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tobias Hecking
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">German Aerospace Center, Institute for Software Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">J. Nathan Kutz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Applied Mathematics, University of Washington, Seattle, WA USA
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This paper describes the capabilities and potential of the intelligent personal assistant (IPA) CORE (Checklist Organizer for Research and Exploration), designed to support astronauts during procedures onboard the International Space Station (ISS), the Lunar Gateway station, and beyond. We reflect on the importance of a reliable and flexible assistant capable of offline operation and highlight the usefulness of audiovisual interaction using augmented reality elements to intuitively display checklist information. We argue that current approaches to the design of IPAs in space operations fall short of meeting these criteria. Therefore, we propose CORE as an assistant that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements to ensure an intuitive understanding of procedure steps, reliability, offline availability, and flexibility in terms of response style and procedure updates.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\makeCustomtitle</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Amidst a surging interest in human space exploration, the NASA-led Artemis program is seeking to establish a sustained human presence on the Moon <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">smith2020artemis</span>]</cite>.
These efforts are targeting the Moon’s south pole region, where strategically important resources, such as water ice, are believed to be preserved at the bottom of permanently shadowed craters. Yet, with the lunar libration causing the polar region to move out of the Earth’s view periodically, recurring communication blackouts will be unavoidable, limiting support from Earth <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">weber2021artemis</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">horneck2003humex</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">braly_augmented_2019</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">haney2020apollo</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">landgraf_lunar_2021</span>]</cite>.
Hence, a small space station, referred to as <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Lunar Gateway</span>, will be constructed in orbit around the Moon. The Lunar Gateway should facilitate crewed missions on the lunar surface by functioning as a communication relay to Earth. Moreover, the station will feature docking ports for spacecraft and be able to accommodate several astronauts, enabling the monitoring of lunar surface activities while also facilitating research experiments in lunar orbit <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">johnson2021gateway</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To ensure the efficiency and safety of operations, Gateway will incorporate standardized procedures and protocols, leveraging decades of operational experience from the International Space Station (ISS).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These standardized procedures generally include workflow guidance for assembling, servicing, and maintaining of equipment, scientific experiments, and emergency procedures among others.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Yet historically, astronauts performing procedures in space (e.g., onboard ISS or during the Apollo missions) relied on relatively rudimentary tools, such as manual checklists, which often left them dependent on continuous supervision from mission control centers on Earth, which will not be possible during future missions to the Moon and beyond <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">apollo11_1969</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mindell2011digital</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">hersch2009checklist</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">marshburn2003independent</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kintz2016communication</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Therefore, to ease the work of astronauts during procedure completion, intelligent personal assistants (IPA) are attracting growing interest in the human spaceflight domain for their ability to facilitate natural, hands-free, and unobtrusive support for procedural tasks, thereby reducing the astronaut’s need for direct supervision from Earth, through guiding the astronaut through a given procedure.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this regard, a key milestone was the 2018 ISS deployment of CIMON, a Watson-powered IPA <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">DLR2018CIMON</span>]</cite>. Its successor, the CIMON-2, launched in 2019 and included enhanced features, such as visual recognition and emotion analysis, but required internet connection and lacked autonomous learning <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">schmitz2020towards</span>]</cite>. Similarly, in 2022, Amazon’s Alexa was integrated into the Artemis I lunar mission <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">AlexaArtemisI</span>]</cite>. Yet both IPAs are limited by their rule-based nature, meaning that they can only respond to a pre-defined set of questions regarding procedure steps. Meanwhile, the AI4U assistant, developed using a reinforcement learning approach, is being tested under simulated Mars conditions during a Mars analog study with the goal of eventually being deployed and further assessed aboard the ISS <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shashkovaa2022study</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">navarro2023eclss</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">spoon_ai4u</span>]</cite>. Unlike rule-based IPAs like CIMON and Alexa, AI4U learns new skills during operation but requires extensive training sessions.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Consequently, contemporary IPAs designed to support astronauts suffer from important limitations, including the lack of offline availability, limited flexibility regarding their range of answers, and delays in updating procedures (due to their rule-based nature or the requirement of extensive training).</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Moreover, these IPAs provide relevant procedure information primarily in the form of text-based and/or spoken instructions. Although they sometimes make use of visual aids, including pictures and videos, they are currently incapable of delivering more complex and spatially intuitive visual information, such as 3D representations of procedure steps or elements.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">In the past, this increased the workload for astronauts, as they had to switch their gaze between the checklist and the current task, while requiring complex mental rotations to interpret visual information from 2D pictures or videos, causing cognitive dissonance between procedure instructions and the real task <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">braly_augmented_2019</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">To address these shortcomings, we propose a novel IPA called <span class="ltx_text ltx_font_italic" id="S1.p10.1.1">CORE</span>, specifically designed to support astronauts during upcoming human spaceflight missions. CORE is designed to be modular, allowing for the integration of open-source and offline deployable components. These components include Generative Pre-Trained Transformers (GPT), Retrieval-Augmented Generation (RAG), Knowledge Graphs (KG), speech recognition, and speech synthesis. This unique combination enables CORE to effectively emulate human conversation, while the astronaut is visually supported using augmented reality (AR) elements displayed in a head-up display (HUD).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="372" id="S1.F1.g1" src="extracted/5870378/images/CPR.png" width="284"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example ISS procedure containing specific steps for a CPR, featuring pictures and location-depended information about the placement of electrodes on the body of the patient <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">NASA2016ISS</span>]</cite>.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>IPAs for Future Space Missions</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In the following sections, we elaborate four key criteria that should be met by a future IPA for astronaut support: (1) reliability, (2) flexibility, (3) offline availability, and (4) the integration of visual 3D information. We furthermore describe how these criteria can be fulfilled using state-of-the-art technology. Drawing on this analysis, we present our IPA prototype <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">CORE</span>, tailored to assist astronauts during procedure completion. We conclude by presenting an overview of our planned future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Reliability:</span>
Reliability is a critical requirement for IPAs supporting astronauts during procedure completion, as it directly affects crew safety and the success of the mission. Firstly, any misinformation or errors in the system regarding procedure steps could potentially lead to fatal consequences. Secondly, unreliability in conversational agents often leads to user disengagement, distrust, and frustration <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">tulshan2019survey</span>]</cite>. Hence, it can be assumed that the slightest unreliability during procedure support would lead to the disengagement of the astronauts and a preference for the usage of traditional tools.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">To significantly improve the reliability of astronautical procedure support, we propose the integration of GPTs with advanced data retrieval technologies.
</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">GPT models, recognized for their ability to generate human-like responses without explicit instructions, play an important role in modern AI systems <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">bubeck2023GPT4GeneralKnowledge</span>]</cite>. However, their reliance on training datasets poses challenges, as the information generated may be inaccurate or misleading, especially if it falls outside the scope of their training data <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">WhyChatGPTHallucinations</span>]</cite>. While model fine-tuning can adapt GPTs to specific domains by incorporating domain-specific knowledge, this process is computationally intensive and lacks transparency, as it provides no sources for the generated responses <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">WhyChatGPTHallucinations</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">To address these limitations and enhance reliability, we propose the integration of GPT models with RAG and KGs. This approach leverages the generative capabilities of GPTs alongside a robust retrieval system that utilizes a structured knowledge database <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">he2024gretriever</span>]</cite>. By interlinking documents, metadata, and multimodal data within a KG, connected by references, topics, keywords, or related company or person information, such a system ensures that all information retrieved and utilized by the AI is accurate and up-to-date. User queries initiate a vector search within this database to identify relevant information, which is added to the GPT query <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lewis2020RAGPaper</span>]</cite>. This approach not only reduces the likelihood of erroneous outputs (hallucinations) but also improves the system’s trustworthiness by allowing astronauts to verify the answer through the linked information used for its generation. Additionally, we suggest the enhancement of system transparency and user trust through the Graph of Thoughts (GoT) paradigm, which can be used to visualize the reasoning process of AI, enabling users to interact with and adjust the AI’s thought process in real-time <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">besta2023GoT</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">WuAiChains2022CHI</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Furthermore, to further improve the astronauts understanding of the systems capabilities, we propose displaying confidence scores next to retrieved data during RAG, indicating the reliability of the information retrieved <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rechkemmer2022confidence</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2020effect</span>]</cite>. This method, complemented by using KGs linked with established ontologies like schema.org <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">guha2016schema</span>]</cite>, ensures astronauts have access to rich, well-organized, and easily navigable information, crucial for decision-making in mission-critical scenarios <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">Auer2007DBPedia</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">Vrande2012Wikidata</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">Zhu_2022MMKG</span>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold" id="S2.p7.1.1">Flexibility:</span>
Importantly, the assistant’s adaptability to new situations and requirements, ranging from routine procedures to unexpected emergencies, is a critical factor. Furthermore, the assistant should be flexible enough to accommodate the varied personal styles of astronauts during procedure completion. For instance, one astronaut might prefer interacting with the assistant in a voice-based manner, while another may opt for displaying written procedure steps. Additionally, preferences in interaction styles may differ among astronauts, as one may prefer an assistant that explains just the necessary steps, while another might favor an assistant with its "own character" that can engage in casual conversations, similar to a colleague. Lastly, one crucial aspect for such a system is that procedures can be flexibly adapted due to updates in the procedures themselves.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">Regarding the need for flexible response styles, such as shifting between casual and formal tones, GPT-based assistants exhibit a level of versatility, unlike classic IPAs limited to predefined responses. GPT models can be steered continuously depending on requirements and preferences using steering vectors <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">konen2024style</span>]</cite>. The latest GPT models are capable of document writing and editing and are evaluated to modify, add, or remove components of Knowledge Graphs or document databases through natural language conversations <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">10387715</span>]</cite>. Furthermore, RAG enables such a system to access the latest updated information directly after indexing into a vector database, without the need for retraining of the GPT model <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lewis2020RAGPaper</span>]</cite>. Linked information in a mission KG can be added or edited in a natural conversation without changing the approved procedure’s content.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p9">
<p class="ltx_p" id="S2.p9.1"><span class="ltx_text ltx_font_bold" id="S2.p9.1.1">Offline Availability:</span>
Given the risk of radio blackouts and communication delays inherent in human spaceflight (and lunar south pole operations in particular), one additional crucial aspect is the need for offline availability when developing future IPA concepts to support astronauts during procedure completion.</p>
</div>
<div class="ltx_para" id="S2.p10">
<p class="ltx_p" id="S2.p10.1">While KG databases are widely available for offline deployment, leading GPT models like Gemini 1.5 or GPT-4 and leading speech synthesis modules are only available online <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">geminiteam2024gemini</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">openai2024gpt4</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">WhisperPaper</span>]</cite> due to their extensive computational requirements and intellectual property protection. However, published open-source GPT models like Mistral’s Mixtral 8x7b <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">touvron2023llama</span>]</cite> are narrowing the performance gap while also reducing model size <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">jiang2024mixtral</span>]</cite> and, consequently, hardware requirements. Some models, like Meta’s LLaMA 3.1 405B model <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">llama2024herd</span>]</cite>, are even achieving a similar performance. These models can benefit from further optimization methods such as low-rank adaptation (LoRa) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hu2021lora</span>]</cite> and low-bit quantization <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhao2023atomQuantization</span>]</cite>, which enable small models like Microsoft’s Phi-3 to run on mobile devices without the requirement of a dedicated GPU <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hu2021lora</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">carreira2023GPTMobile</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">abdin2024phi3</span>]</cite>. This approach allows for a streamlined and efficient AI assistant setup in astronaut suits for deep space missions, where critical functionalities like speech recognition are embedded in the suit, while more computationally intensive tasks are handled by the larger GPT model and database located at a nearby base, connected through reliable and high-speed radio communication links.</p>
</div>
<div class="ltx_para" id="S2.p11">
<p class="ltx_p" id="S2.p11.1">Overall, we propose that an offline IPA based on a GPT model with an RAG architecture querying a KG for astronauts with low hardware requirements is already feasible. The quality of answers generated by open-source GPT models like Llama 3.1 can be compared to GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">llama2024herd</span>]</cite>. Although, the quality of voices generated by open-source Text-To-Speech models has yet to match that of online available models like OpenAI’s <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">OpenAITTS</span>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p12">
<p class="ltx_p" id="S2.p12.1"><span class="ltx_text ltx_font_bold" id="S2.p12.1.1">Combining Voice, Text, and Visual 3D Information:</span>
Text and picture-based procedures pose the risk of significantly increasing the workload of astronauts due to 1) the need to switch attention back and forth between the visual spaces of the task and the procedure instructions and 2) the mental effort of mapping instruction steps to the real equipment <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">braly_augmented_2019</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">yeh1998effects</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p13">
<p class="ltx_p" id="S2.p13.1">To address these issues, past research has demonstrated that HUD AR technology can ease the work of astronauts during procedural tasks onboard the ISS by superimposing computer-generated instructions directly onto real equipment <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">braly_augmented_2019</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">markov2013pilot</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">helin2018user</span>]</cite>. Additionally, the combination of AR and voice commands offers hands-free interaction without requiring astronauts to divert their gaze to another display <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cardenas2021reducing</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">rometsch2022design</span>]</cite>. Moreover, recent research has shown that combining LLMs and AR HUDs is beneficial for the completion of procedural maintenance tasks <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2024augmented</span>]</cite>. Therefore, we suggest combining the advantages of AR technology and LLMs to allow for intuitive interaction between astronauts and procedure elements. We furthermore propose that graphically displaying relational information from the KG could further aid astronauts in understanding linked concepts, components, documents, or images during critical situations, enhancing the assistant’s explainability <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">nararatwong2020knowledge</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">serrano2022vowlexplain</span>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results - CORE Prototype</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In our initial experiments, we indexed ten publicly available procedures <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">NASA2016ISS</span>]</cite> into a vector database and combined the documents with metadata such as the edit date and related images in the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">ArangoDB</span> graph database. Subsequently, we implemented a RAG system based on the GPT-4 model and indexed textual information into the Elasticsearch vector database to achieve the best possible results during our experiments, while keeping the CORE system modular for easy replacement with newly released offline models, such as the Llama 3.1 405B model, which offer similar performance <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">llama2024herd</span>]</cite>. A speech user query is first converted into text using <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Whisper</span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">WhisperPaper</span>]</cite>. Relevant procedure information is retrieved from the vector database and added to the user query, along with linked information in the graph, using the following prompt template:</p>
</div>
<div class="ltx_para" id="S3.p2">
<blockquote class="ltx_quote" id="S3.p2.1">
<p class="ltx_p" id="S3.p2.1.1">You will be presented with a matching procedure enclosed by three quotation marks (”’). If a question is asked, respond with either the next step or only the first step. Specify the relevant step and repeat the text of the procedure step verbatim. If the information does not correspond to the question or if information is missing, state that this is the case.
”’<span class="ltx_text ltx_font_italic" id="S3.p2.1.1.1">[Procedure] [Graph information]</span>
”’</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Furthermore, using the following system-prompt, we directed the GPT-model to respond exclusively to procedure-related questions and return the current step as well as information about linked images in a separate format to be displayed to the user via the interface if necessary:</p>
</div>
<div class="ltx_para" id="S3.p4">
<blockquote class="ltx_quote" id="S3.p4.1">
<p class="ltx_p" id="S3.p4.1.1">You are a helpful assistant for astronauts, answering questions about provided procedures. If asked a question, respond with either the next step or the first step only. Name the corresponding step as &lt;&lt;STEP [NUMBER]&gt;&gt; and repeat the text of the procedure step word for word. If a figure or other data is referenced, include &lt;&lt;SHOW FIGURE [NUMBER]&gt;&gt; in your answer.</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Our first tests have shown that depending on the system-prompt, the GPT-4 model is able to interpret the given procedures and answer related user questions correctly.
For example procedure steps were returned verbatim for the user question:</p>
</div>
<div class="ltx_para" id="S3.p6">
<blockquote class="ltx_quote" id="S3.p6.1">
<p class="ltx_p" id="S3.p6.1.1">Hi, I have a person that is not breathing. I have already requested PMC. What was the fourth step of the ISS CPR procedure?</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">The system returns the corresponding step word by word and adds additional information to correctly display the step and the linked image.</p>
</div>
<div class="ltx_para" id="S3.p8">
<blockquote class="ltx_quote" id="S3.p8.1">
<p class="ltx_p" id="S3.p8.1.1">&lt;&lt;STEP 4&gt;&gt; - DEPLOY AED:
Connect AED electrodes to patient’s chest. (See Figure 1)
AED ON (green) → Press
Follow verbal prompts.
If verbal prompts inaudible, read prompts on screen.
Continue with "Step 5" &lt;&lt;SHOW FIGURE 1&gt;&gt;</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">When the system is asked for graph data that is not added as a full sentence, such as "Last update: 09 April 2015" the system formulates a full sentence. For instance, if the user asks:</p>
</div>
<div class="ltx_para" id="S3.p10">
<blockquote class="ltx_quote" id="S3.p10.1">
<p class="ltx_p" id="S3.p10.1.1">When was the procedure last updated?</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1">the system replies:</p>
</div>
<div class="ltx_para" id="S3.p12">
<blockquote class="ltx_quote" id="S3.p12.1">
<p class="ltx_p" id="S3.p12.1.1">The CPR procedure on the ISS was last updated on 09 April 2015.</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p13">
<p class="ltx_p" id="S3.p13.1">Moreover, unrelated questions regarding other topics than procedure-related information are not answered by CORE.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our work highlights the significance of IPA technologies for future ISS, Gateway missions, and missions beyond. We propose a solution based on state-of-the-art natural language processing methods. Our primary focus is to develop a versatile IPA that seamlessly adapts to a range of tasks and interaction styles through the use of GPT models. These models surpass current IPAs, being capable of handling unforeseen situations with enhanced reasoning, creativity, and task-solving abilities. Reliability is another crucial factor, as it directly impacts astronauts’ trust and engagement with the system. We recommend the integration of RAG and KG technology with GPT models, ensuring accurate information retrieval and enhancing trustworthiness, which is vital for high-stakes space operations. Additionally, we emphasize the need for offline availability of these systems, considering the communication limitations in space. Recent technological advancements suggest the feasibility of efficient, offline IPAs that require minimal hardware resources. Finally, we propose a combination of voice-based and visual information, utilizing AR cues to enhance the intuitive understanding of complex spatial information such as checklists, navigation, or KG relations.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The second version of CORE runs completely offline and is based on Llama 3.1 and integrates AR HUD features into our prototype. We will test it in a user study at the European Astronaut Centre (ESA EAC) to assess user experience and performance metrics.</p>
</div>
<div class="ltx_para" id="S4.p3">
<span class="ltx_ERROR undefined" id="S4.p3.1">\printbibliography</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 17:18:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
