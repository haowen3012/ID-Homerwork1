<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation</title>
<!--Generated on Wed Oct  2 23:10:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03771v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03771v1#S1" title="In SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03771v1#S2" title="In SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03771v1#S3" title="In SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03771v1#S4" title="In SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03771v1#S5" title="In SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Melody Yu 
<br class="ltx_break"/>Sage Hill School
<br class="ltx_break"/>Newport Coast, CA 92657 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">ocmelodyu@gmail.com</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">The project information and code is available at https://github.com/cskitty/seesay
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">In this paper, we present SeeSay, an assistive device designed for individuals with visual impairments. This system leverages large language models (LLMs) for speech recognition and visual querying. It effectively identifies, records, and responds to the user’s environment by providing audio guidance using retrieval-augmented generation (RAG). Our experiments demonstrate the system’s capability to recognize its surroundings and respond to queries with audio feedback in diverse settings. We hope that the SeeSay system will facilitate users’ comprehension and recollection of their surroundings, thereby enhancing their environmental perception, improving navigational capabilities, and boosting overall independence.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Visual impairment is a global health issue that affects over 253 million people, with 217 million experiencing moderate to severe vision loss, including 36 million who are blind [1]. Visual impairment severely hinders daily activities such as navigating surroundings, accessing information, and performing daily tasks. While canes help detect physical obstacles and navigate through environments, they cannot identify specific objects, read signs, or provide visual details that are farther away. Similarly, guide dogs require extensive training and care, face navigational limitations, and may not suit everyone due to health, legal, or personal reasons. There is an increasing need for more sophisticated solutions that offer accurate, real-time data about surroundings.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Assistive technology that leverages deep learning models offers significant potential for addressing the challenges visually impaired individuals face. This technology provides real-time environmental information by utilizing speech recognition and image description techniques. Additionally, the capability to run large models on affordable, readily available devices, such as Raspberry Pis, presents a cost-effective approach to developing assistive devices for the visually impaired. This affordability makes these advanced technologies more accessible to a wider audience, including those with restricted financial resources.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this study, we introduce SeeSay, an assistive device for the visually impaired. It features low-cost components and leverages large language models to process images captured by a Bluetooth camera which can be installed on any glasses. The system uses a Raspberry Pi to deliver real-time audio feedback, enhancing the user’s ability to navigate unfamiliar environments with greater confidence and independence. The performance of the SeeSay platform has been tested through various experiments, the details of which are discussed in subsequent sections of this paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Assistive technology for the visually impaired is rapidly growing with diverse solutions available:</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Assistive Devices.</span> The studies in [2,3] describe systems employing sonar devices to detect obstacles within a short range and communicate this data to the user through real-time alerts, aiming to enhance user safety and facilitate more effective environmental navigation. The study detailed in [4] employed the Microsoft Kinect sensor alongside object recognition technology to offer real-time feedback to users about their surroundings. In [5], the authors described the development of a new augmented-reality application that can run on the Google Glass device. This application enhanced the real-world images by applying an edge enhancement filter, making the boundaries of objects clearer and easier to see. The authors state that this application can help people with low vision to see more clearly and thus improve their quality of life. The paper [6] described the design and implementation of a new navigation aid that integrates a camera, ultrasonic sensors, and a microcontroller to alert the user to obstacles and provide guidance on the best path to take. The paper [7] focused on the development and demonstration of a novel approach to control iOS’s VoiceOver through a tactile button input system, making it easier for visually impaired individuals to use their mobile devices. In [8], the authors proposed a deep learning-based assistive system integrating an RGBD camera, earphones, and a smartphone interface to improve environmental perception for visually impaired individuals by providing walkable instructions and 3D spatial understanding through semantic mapping and touch interactions. The paper [9] proposed an Open Scene Understanding (OpenSU) system that generates pixel-wise dense segmentation masks of entities involved in grounded situation recognition tasks, built on top of a pure transformer backbone with efficient segment modeling to improve scene understanding and facilitate independent mobility for visually impaired individuals.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Mobile Apps.</span> Microsoft’s Seeing AI [10], is a free mobile app leveraging computer vision and machine learning, that narrates the surroundings for blind and visually impaired users, assisting with tasks such as reading text and identifying objects. Be My AI App [11] is a mobile application that can use visual querying models to provide accessibility guidance for blind or low-vision users with a success rate of 90%.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Current visual querying devices for the visually impaired lack the ability to use past observations when providing new information, limiting their effectiveness in locating previously seen objects. This paper addresses this limitation by proposing a novel method that combines large language models and retrieval augmented generation to answer user queries using both current and past visual information.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The SeeSay system consists of two primary components: a glasses attachment and a processing unit. First, the attachment comprises a compact 3D-printed enclosure designed to attach to the side of any pair of glasses, which houses an ESP32 board, a camera, and a battery. This assembly is engineered to capture photos and record audio, subsequently transmitting this data to the processing unit via Bluetooth. The second component, the processing unit, powered by a Raspberry Pi 5B, is responsible for analyzing both image and voice data and provides auditory feedback to the user.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To assist visually impaired users in navigating their environments, the SeeSay system persistently records their surroundings. It later utilizes Retrieval-Augmented Generation (RAG) to access and process this stored information, which enhancing its ability to provide more contextually relevant responses and support daily activities. SeeSay is open to a wide range of user inquiries:</p>
</div>
<div class="ltx_para" id="S3.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Scene Description:</span> Verbally describe the scene to the user and answer the user’s questions, aiding in spatial awareness and environmental context. "Describe what you see."</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Navigation Assistance:</span> Assist users in navigating unfamiliar settings. "Which direction should I take to find the restroom?"</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Locate Items:</span> Help the user to locate items from their past locations, answering questions such as "Where did I leave my phone?"</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Optical Character Recognition (OCR):</span> Read and convert printed text into spoken words, allowing users to easily access written information in books, signs, and screens. "What is the first item on this menu?"</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="294" id="S3.F1.g1" src="extracted/5897299/seesay.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the SeeSay platform using LLM-based Retrieval-Augmented Generation</figcaption>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The system utilizes a hybrid architecture to run large language models (LLMs). It runs the Whisper, Phi-2, and Piper TTS models locally on Raspberry Pi, handling tasks such as speech recognition, query answering, and speech synthesis, and delivering audio feedback to the user. For image description tasks, the system compensates for the Raspberry Pi’s limited computational power by offloading the processing to a cloud-based LLM service such as ChatGPT. This involves transmitting the images and question prompts to the cloud for answering.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">A control center orchestrates the system’s control flow, while a mosquitto MQTT broker facilitates the publication and subscription of messages within the system. Upon receiving images and voice data from the glasses through the Bluetooth connection, the data is immediately published to the MQTT broker. Subsequently, the data is relayed to the relevant components for processing, establishing a continuous interaction loop that allows the user to pose sequential questions. The following is a detailed explanation of a user’s question-and-answer flow within this system.</p>
</div>
<div class="ltx_para" id="S3.p6">
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.1.1.1">1.</span></span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">The glasses capture a new image every 30 seconds and transmit it via Bluetooth to a Raspberry Pi for storage.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.1.1.1">2.</span></span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">The Raspberry Pi transmits the images to a cloud-based language model ChatGPT4 to obtain textual descriptions of the images. Image descriptions are transformed into vectors via embedding and stored locally with their corresponding images.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.1.1.1">3.</span></span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">The user issues a voice command and then the glasses transmit the audio recording to the Raspberry Pi, where the Whisper LLM processes the recording into a textual command.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.1.1.1">4.</span></span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">The control center executes multiple iterations of question-and-answer sessions with a local LLM (Dolphin 2.6 Phi 2) to process the query.</p>
<ul class="ltx_itemize" id="S3.I2.i4.I1">
<li class="ltx_item" id="S3.I2.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.I1.i1.p1">
<p class="ltx_p" id="S3.I2.i4.I1.i1.p1.1">For simple questions, the local LLM directly returns the answer.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.I1.i2.p1">
<p class="ltx_p" id="S3.I2.i4.I1.i2.p1.1">For inquiries related to the current environment, as detected by the local LLM model, the control center acquires the latest image description and combines it with the original question to re-query the local LLM for a response.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.I1.i3.p1">
<p class="ltx_p" id="S3.I2.i4.I1.i3.p1.1">For inquiries necessitating historical images, the local LLM creates a query string, and the control center retrieves the most relevant description (cosine similarity) from the database. This description is subsequently utilized as context to re-query the local LLM for an answer.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.I1.i4.p1">
<p class="ltx_p" id="S3.I2.i4.I1.i4.p1.1">If the response from the local LLM is inadequate, the user can request further assistance, and the control center will use ChatGPT to address the query, using the previously retrieved image as context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.I1.i5.p1">
<p class="ltx_p" id="S3.I2.i4.I1.i5.p1.1">The user may issue a spoken command to request the inclusion of supplementary information to the latest image descriptions, thereby enhancing the contextual richness of the stored image description, "Remember this person as Mary."</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i5.1.1.1">5.</span></span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">The answer derived from the previous step is converted to speech using Piper TTS and relayed through Bluetooth-connected speakers or headphones for the user to hear.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance Metrics for Visual Assistive Device Test Cases</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">Test Case</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.2">Accuracy (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">Usability (1-5)</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">Response Time (s)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">Simple Question</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.2">86</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">4.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">9.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">Scene Description</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.3.2.2">86</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">4.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.3.2.4">10.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">Indoor Navigation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.4.3.2">70</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">3.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.4.3.4">18.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">Street Navigation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.5.4.2">70</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">3.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.5.4.4">16.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">Image Inquiries</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.6.5.2">80</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">3.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.6.5.4">17.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.7.6.1">Recognize Person</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.7.6.2">85</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.3">4.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.7.6.4">16.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">Item Locator</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.8.7.2">80</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.3">4.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.8.7.4">18.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.8.1">Printed OCR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.9.8.2">85</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.3">4.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.9.8.4">10.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.10.9.1">Handwriting OCR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.10.9.2">75</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.10.9.3">4.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.1.10.9.4">11.5</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To evaluate the SeeSay system, we tested the prototype design across various test scenarios, with each focusing on essential functionalities for the visually impaired. The device exhibited commendable performance in simpler tasks such as answering direct questions and describing scenes, achieving high accuracy and usability scores of 86% and 4.5, respectively, with response times of 9.2 and 10.5 seconds. These metrics suggest that the device is effective in providing information retrieval and enhancing environmental awareness, essential for real-time decision-making.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">However, more complex navigational tasks like indoor and street navigation presented some challenges, evidenced by lower accuracy of 70%, and relatively long response times of 18.5 seconds. While the device performed reasonably well in recognizing individuals and processing image inquiries, the latter’s longer response time (17.5 seconds) reflected multiple rounds of question and answer with the local LLM. Tasks such as item location and optical character recognition for printed text also demonstrated higher usability and satisfactory response times, highlighting the device’s utility in aiding users to locate objects and read printed material efficiently. Handwriting OCR also performed adequately, though improvements could enhance its correctness and responsiveness. The experimental results reveal that the SeeSay system effectively supports visually impaired users in simple tasks and scene descriptions, but also suggest that improvements in navigation tasks and handwriting OCR could increase overall usability.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The prototype evaluations of the SeeSay system reveal its capability to deliver usable and efficient speech outputs. Nonetheless, the response times are delayed and fluctuate based on the intricacy of the question and answer interactions. These outcomes highlight the system’s capability to support various assistive tasks, demonstrating its potential as a viable tool for visually impaired users.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In this paper, we propose continuous capturing and indexing of users’ visual observations to enrich the contextual data used by LLMs, thus extending the functionalities of visually assistive devices. This mechanism serves as a visual memory aid for the visually impaired, thus enhancing the system’s capacity to respond to user queries according to previously observed information. Through the SeeSay prototype, we have shown that integrating Retrieval Augmented Generation with Large Language Models can create a user-friendly, natural language-based assistance device. This integration improves the accuracy and relevance of interactions for users of visual assistance devices who rely on such technology for visual information.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Given the constrained processing capabilities and memory of the Raspberry Pi, tasks related to image description have to be delegated to cloud-based LLMs, thus elevating the operational costs. Future enhancements to the SeeSay system will involve integrating a more powerful computing platform capable of locally executing vision LLM models. This upgrade aims to reduce dependence on cloud-based LLM services and boost user privacy by enabling on-device LLM functionality.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">References</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">[1] Ackland, P., Resnikoff, S. &amp; Bourne R. (2017) World blindness and visual impairment: despite many successes, the problem is growing. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.1">Community Eye Health </span>. 2017;30(100):71-73.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">[2] Dunai, L., Fajarnes, G. P., Praderas, V. S., Garcia, B. D. and Lengua, I. L. (2010) "Real-time assistance prototype — A new navigation aid for blind people," IECON 2010 - 36th Annual Conference on IEEE Industrial Electronics Society, Glendale, AZ, USA, 2010, pp. 1173-1178</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">[3] Kamaludin, M.H., Mahmood, N.H., Ahmad, A.H. and Omar C.(2015), “Sonar assistive device for visually impaired people” in Jurnal Teknologi 73.6 .</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">[4] Takizawa, H, Yamaguchi,S., Aoyagi, M., Ezaki, N and Mizuno, S (2012) “Kinect cane: An assistive system for the visually impaired based on the concept of object recognition aid” in Personal and Ubiquitous Computing 19: pp. 955-965.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">[5] Hwang, A. D., and Peli, E. (2014) "An augmented-reality edge enhancement application for Google Glass." Optometry and vision science 91, no. 8: 1021-1030.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">[6] Mustapha, B., Zayegh, A. and Begg, R. K. (2013) "Wireless obstacle detection system for the elderly and visually impaired people." In 2013 IEEE International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA), pp. 1-5.</p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">[7] Batterman, J. M., Martin, V.F., Yeung, D., and Walker, B.N (2018) "Connected cane: tactile button input for controlling gestures of iOS voiceover embedded in a white cane." Assistive Technology 30, no. 2: pp. 91-99.</p>
</div>
<div class="ltx_para" id="Sx1.p8">
<p class="ltx_p" id="Sx1.p8.1">[8] Lin, Y., Wang, K., Yi, W. and Lian, S. (2019) “Deep learning based wearable
assistive system for visually impaired people,” in Proceedings of the
IEEE/CVF international conference on computer vision workshops</p>
</div>
<div class="ltx_para" id="Sx1.p9">
<p class="ltx_p" id="Sx1.p9.1">[9] Liu, R., Zhang, J. , Peng, K. , Zheng, J. , Cao, K. , Chen, Y., Yang, K. and Stiefelhagen, R. (2023) “Open scene understanding: Grounded situation
recognition meets segment anything for helping people with visual
impairments,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 1857–1867</p>
</div>
<div class="ltx_para" id="Sx1.p10">
<p class="ltx_p" id="Sx1.p10.1">[10] Microsoft, Seeing AI: New Technology Research to Support the Blind and Visually Impaired Community, https://blogs.microsoft.com/accessibility/seeing-ai/</p>
</div>
<div class="ltx_para" id="Sx1.p11">
<p class="ltx_p" id="Sx1.p11.1">[11] Be My AI https://www.bemyeyes.com/blog/introducing-be-my-ai</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 23:10:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
