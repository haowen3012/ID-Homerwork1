<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems</title>
<!--Generated on Mon Aug 12 17:06:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.09040v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S1" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>The RAGGED Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2.SS1" title="In 2 The RAGGED Framework ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Framework Overview</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2.SS1.SSS0.Px1" title="In 2.1 Framework Overview ‣ 2 The RAGGED Framework ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Effective Number of Context Passages (§<span class="ltx_text ltx_ref_tag">5</span>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2.SS1.SSS0.Px2" title="In 2.1 Framework Overview ‣ 2 The RAGGED Framework ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Context Utilization Behaviors (§<span class="ltx_text ltx_ref_tag">6</span>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2.SS1.SSS0.Px3" title="In 2.1 Framework Overview ‣ 2 The RAGGED Framework ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Influence of Retriever Quality (§<span class="ltx_text ltx_ref_tag">7</span>)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S2.SS2" title="In 2 The RAGGED Framework ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimenting with RAG Systems</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS1" title="In 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Retriever</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS1.SSS0.Px1" title="In 3.1 Retriever ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">BM25</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS1.SSS0.Px2" title="In 3.1 Retriever ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">ColBERT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS2" title="In 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Reader</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS2.SSS0.Px1" title="In 3.2 Reader ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">Flan</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS2.SSS0.Px2" title="In 3.2 Reader ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">LLaMa</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS2.SSS0.Px3" title="In 3.2 Reader ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">GPT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S3.SS2.SSS0.Px4" title="In 3.2 Reader ‣ 3 Experimenting with RAG Systems ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">Claude</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Datasets and Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS1" title="In 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS1.SSS0.Px1" title="In 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Natural Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS1.SSS0.Px2" title="In 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">HotpotQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS1.SSS0.Px3" title="In 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">BioASQ</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS2" title="In 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS2.SSS0.Px1" title="In 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Retriever Metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S4.SS2.SSS0.Px2" title="In 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Reader Metric</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Are More Contexts Always Better?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS0.SSS0.Px1" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">What to Vary:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS0.SSS0.Px2" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Behaviors to Expect:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS0.SSS0.Px3" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Model Behavior Implications:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS1" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Reader Trends with Scaling Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS2" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparing Optimal <math alttext="k^{*}" class="ltx_Math" display="inline"><semantics><msup><mi>k</mi><mo>∗</mo></msup><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">superscript</csymbol><ci>𝑘</ci><times></times></apply></annotation-xml><annotation encoding="application/x-tex">k^{*}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> Performances</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS3" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>RAG vs. No-context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS4" title="In 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>The Effect of Context Limit</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Reader Robustness in the Presence and Absence of Gold Passages</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS0.SSS0.Px1" title="In 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">What to Vary:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS0.SSS0.Px2" title="In 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Behaviors to Expect:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS0.SSS0.Px3" title="In 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Implications of Behaviors:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS1" title="In 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>With Gold Passages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS2" title="In 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Without Gold Passages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Impact of Retriever Choice</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px1" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">What to Vary:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px2" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Behaviors to Expect:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px3" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Implications of Behaviors:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px4" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Open Domain, Single-Hop</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px5" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Open-Domain, Multi-Hop</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7.SS0.SSS0.Px6" title="In 7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Biomedical Domain</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S8" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S8.SS0.SSS0.Px1" title="In 8 Related Work ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Context Limit and Processing Capacity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S8.SS0.SSS0.Px2" title="In 8 Related Work ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Domain Influence on Downstream Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S8.SS0.SSS0.Px3" title="In 8 Related Work ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Robustness to Noisy Contexts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S9" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A1" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A1.SS0.SSS0.Px1" title="In Appendix A Implementation Details ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title">Reader model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A2" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Dataset Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A3" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Retriever Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A4" title="In RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Reader Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAGGED: Towards Informed Design of 
<br class="ltx_break"/>Retrieval Augmented Generation Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer Hsia  Afreen Shaikh<sup class="ltx_sup" id="id3.3.id1"><span class="ltx_text ltx_font_italic" id="id3.3.id1.1">∗</span></sup>  Zhiruo Wang  Graham Neubig 
<br class="ltx_break"/>Carnegie Mellon University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id2">{jhsia2,afreens,zhiruow,gneubig}@cs.cmu.edu</span>
</span><span class="ltx_author_notes">   Equal contribution.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Retrieval-augmented generation (RAG) can significantly improve the performance of language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA).
However, the effectiveness of RAG is highly dependent on its configuration.
To systematically find the optimal configuration, we introduce RAGGED, a framework for analyzing RAG configurations across various DBQA tasks.
Using the framework, we discover distinct LM behaviors
in response to varying context quantities, context qualities, and retrievers.
For instance, while some models
are robust to noisy contexts, monotonically performing better with more contexts, others are more noise-sensitive and can effectively use only a few contexts before declining in performance.
This framework also provides a deeper analysis of these differences by evaluating the LMs’ sensitivity to signal and noise under specific context quality conditions.
Using RAGGED, researchers and practitioners can derive actionable insights about how to optimally configure their RAG systems for their specific question-answering tasks.
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code/data for the RAGGED framework are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/neulab/ragged" title="">https://github.com/neulab/ragged</a></span></span></span></p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\addauthor</span>
<p class="ltx_p" id="p1.2">gnred
<span class="ltx_ERROR undefined" id="p1.2.1">\addauthor</span>zworange


















</p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<div class="ltx_block ltx_align_bottom" id="p2.2">
<p class="ltx_p" id="p2.2.3"><span class="ltx_text ltx_font_bold" id="p2.2.3.1">RAGGED: Towards Informed Design of 
<br class="ltx_break"/>Retrieval Augmented Generation Systems</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.2.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p2.2.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.2.2.2.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.2.2.2.2.2">
<span class="ltx_td ltx_align_center" id="p2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="p2.2.2.2.2.2.2.2">Jennifer Hsia<span class="ltx_note ltx_role_thanks" id="p2.1.1.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>   Equal contribution.</span></span></span>  Afreen Shaikh<sup class="ltx_sup" id="p2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p2.2.2.2.2.2.2.2.2.1">∗</span></sup>  Zhiruo Wang  Graham Neubig</span></span></span>
<span class="ltx_tr" id="p2.2.2.2.2.3.1">
<span class="ltx_td ltx_align_center" id="p2.2.2.2.2.3.1.1">Carnegie Mellon University</span></span>
<span class="ltx_tr" id="p2.2.2.2.2.4.2">
<span class="ltx_td ltx_align_center" id="p2.2.2.2.2.4.2.1"><span class="ltx_text ltx_font_typewriter" id="p2.2.2.2.2.4.2.1.1">{jhsia2,afreens,zhiruow,gneubig}@cs.cmu.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib6" title="">2017</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib15" title="">2020</a>)</cite> is a technique widely applied to enhance the performance of top-performing LMs on knowledge-intensive generation tasks like document-based question answering <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib12" title="">2020</a>)</cite>.
Given a question,
the technique includes using a
<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">retriever</em> model to obtain multiple relevant passages (i.e. paragraphs) across potentially different documents, then inputting these passages to a <em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">reader</em> model as additional contexts for generating an answer.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="283" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of our RAGGED framework.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="286" id="S1.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example insight from using RAGGED: <span class="ltx_text ltx_font_smallcaps" id="S1.F2.5.1">LLaMa</span> and <span class="ltx_text ltx_font_smallcaps" id="S1.F2.6.2">Claude</span> models are more sensitive to noise in context, while <span class="ltx_text ltx_font_smallcaps" id="S1.F2.7.3">Flan</span> and <span class="ltx_text ltx_font_smallcaps" id="S1.F2.8.4">GPT</span> models are more robust to noise in context and can effectively use a larger number of context passages.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, using RAG effectively is not straightforward, and existing literature provides mixed, even contradictory, suggestions for configuring RAG. While different LMs have different context length limits <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib8" title="">2022</a>; Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib24" title="">2023</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib26" title="">2023b</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib16" title="">2023</a>)</cite>, these limits alone do not simply determine the optimal number of passages to provide in context. Early works suggest that providing more retrieved passages results in strictly better outputs <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib11" title="">2021</a>)</cite>. In contrast, other works find that providing a selected set of passages <cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib1" title="">2022</a>)</cite>, sentences <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib27" title="">2023</a>)</cite>, or tokens <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib3" title="">2023</a>)</cite> outperforms providing the full set, presumably due to increased context relevance and quality.
In addition to context quality and quantity,
how robust a reader model is to noisy context also determines downstream performance.
For example, <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib10" title="">2022</a>)</cite> train models on high-quality data, while <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib32" title="">2023</a>)</cite> intentionally inject noisy content into input context during model training, and observe increased robustness of these models to low-quality contexts.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To provide more concrete suggestions of the <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">best practices</span> under various cases,
we introduce an analysis framework, RAGGED,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For “retrieval augmented generation generalized evaluation device”.</span></span></span>
to test RAG combinations on a suite of representative document-based question answering (DBQA) tasks, including open-domain datasets like Natural Questions <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib14" title="">2019</a>)</cite> and HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib30" title="">2018</a>)</cite>, which respectively focus on single-hop and multi-hop questions, as well as BioASQ,
which targets the specialized, biomedical domain.
To ensure a comprehensive evaluation,
we incorporate both classic sparse and dense retrievers, BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib21" title="">2009</a>)</cite> and ColBERT <cite class="ltx_cite ltx_citemacro_citep">(Santhanam et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib22" title="">2021</a>)</cite>, and top-performing reader model families
<span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.2">GPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib5" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.3">Claude</span> <cite class="ltx_cite ltx_citemacro_citep">(Enis and Hopkins, <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib9" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.4">Flan</span> <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib8" title="">2022</a>; Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib24" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.5">LLaMa</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib26" title="">2023b</a>)</cite> families, respectively.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We begin by exploring <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">“How many contexts can readers benefit from?”</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5" title="5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>).
Our analysis identifies the optimal context quantity for different readers.
We find that while some readers’ performance improves monotonically as context quantity increases,
other models’ performance peaks early and starts deteriorating, sometimes even below their no-context performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To understand these differences in more detail, we ask
<span class="ltx_text ltx_font_italic" id="S1.p5.1.1">"How robust are reader models to irrelevant context when gold passages are present versus absent?"</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6" title="6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a>).
We analyze the readers under these data slices,
and find that even with sufficient information, all readers still get distracted by the irrelevant context and degrade in performance as the number of contexts increases.
The difference is that some models’ performance drops below their no-context baseline, while others are more robust and remain above the baseline.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Although it is useful to know model behavior under the presence and absence of signal,
a practitioner may not have such detailed information at deployment time.
Hence, we also examine reader sensitivity to context quality at the coarser level of retriever choice, and ask
<span class="ltx_text ltx_font_italic" id="S1.p6.1.1">“How does the retriever choice impact reader performance?”</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7" title="7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">7</span></a>)
For Wikipedia-based questions, we find that using a neural retriever offers a big advantage over using a lexical retriever, but this advantage only translates well for single-hop questions and not for multi-hop ones.
For special-domain questions, we find that using a neural retriever only offers a minimal advantage over using a lexical retriever, but this advantage is amplified in reader performance.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In summary, we demonstrate how RAGGED enables us to derive actionable insights about the conditions under which state-of-the-art RAG components combine to excel. We introduce a reusable framework that can easily be used to analyze new retriever and reader models, as they evolve. We release our full dataset and code, aiming to provide the community with a deeper understanding of the nuanced interplay between context quantity, quality, and model architecture in RAG systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The RAGGED Framework</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Framework Overview</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We first explain the three aspects we vary in our analysis, then explain the three research questions we can answer with our analysis.
The three aspects we vary in our framework are:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">RAG system components. For example, we vary the choice of the retriever (e.g., BM25, ColBERT), the reader family, (e.g., <span class="ltx_text ltx_font_smallcaps" id="S2.I1.i1.p1.1.1">GPT</span>, <span class="ltx_text ltx_font_smallcaps" id="S2.I1.i1.p1.1.2">Claude</span>, <span class="ltx_text ltx_font_smallcaps" id="S2.I1.i1.p1.1.3">Flan</span>, <span class="ltx_text ltx_font_smallcaps" id="S2.I1.i1.p1.1.4">LLaMa</span>), and the max input token length.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.3">The number of retrieved passages provided as context, denoted as <math alttext="k" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mi id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><ci id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_k</annotation></semantics></math>. We vary <math alttext="k" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><mi id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><ci id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_k</annotation></semantics></math> from 1 to 50, though we find that most insightful variations in behavior occur before <math alttext="k=30" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3.1"><semantics id="S2.I1.i2.p1.3.m3.1a"><mrow id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2" xref="S2.I1.i2.p1.3.m3.1.1.2.cmml">k</mi><mo id="S2.I1.i2.p1.3.m3.1.1.1" xref="S2.I1.i2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><eq id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.1"></eq><ci id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2">𝑘</ci><cn id="S2.I1.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.I1.i2.p1.3.m3.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">k=30</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.1d">italic_k = 30</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Slices of data to examine based on the quality of retrieved passages. By quality, we mean the presence of gold passages in the top-<math alttext="k" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mi id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><ci id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved passages.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS1.p1.2">By varying these,
we analyze the three following aspects of RAG system behavior:</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Effective Number of Context Passages (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5" title="5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>)</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.2">We observe how different model architectures and context limits respond to an increasing <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math>, number of context passages.
We observe two kinds of behaviors from different model families: some models’ performance monotonically improves as <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.2.m2.1d">italic_k</annotation></semantics></math> increases while others’ performance peaks early and deteriorates steadily.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Context Utilization Behaviors (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6" title="6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a>)</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">We analyze how effectively readers use context when the context quality varies.
Specifically, by context quality, we mean whether there is sufficient information, or signal, in the context to answer the question.
Whether a passage is considered a signal or noise is determined by whether it is marked as gold by human annotators.
Using these slices,
one can examine how sensitive a reader is to “signal” and how robust it is to the “noise”.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Influence of Retriever Quality (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S7" title="7 Impact of Retriever Choice ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">7</span></a>)</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">We vary the retriever choice
and analyze how much that affects reader performance
on questions of different domains (Wikipedia v. PubMed) and complexity (single-hop v. multi-hop).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">For all experiments, we use the following prompt:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.4">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S2.SS2.p2.4.4.4" style="width:433.6pt;">
<span class="ltx_p" id="S2.SS2.p2.4.4.4.5"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.4.4.4.5.1">Instruction:</span> Give simple short one phrase answers for the questions based on the context</span>
<span class="ltx_p" id="S2.SS2.p2.4.4.4.4"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.4.4.4.4.1">Context:</span> [passage<sub class="ltx_sub" id="S2.SS2.p2.4.4.4.4.2">1</sub>, passage<sub class="ltx_sub" id="S2.SS2.p2.4.4.4.4.3">2</sub>, <math alttext="\cdots" class="ltx_Math" display="inline" id="S2.SS2.p2.3.3.3.3.m3.1"><semantics id="S2.SS2.p2.3.3.3.3.m3.1a"><mi id="S2.SS2.p2.3.3.3.3.m3.1.1" mathvariant="normal" xref="S2.SS2.p2.3.3.3.3.m3.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.3.3.3.m3.1b"><ci id="S2.SS2.p2.3.3.3.3.m3.1.1.cmml" xref="S2.SS2.p2.3.3.3.3.m3.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.3.3.3.m3.1c">\cdots</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.3.3.3.m3.1d">⋯</annotation></semantics></math>, passage<sub class="ltx_sub" id="S2.SS2.p2.4.4.4.4.4"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.4.4.4.4.4.1">k</span></sub>]</span>
<span class="ltx_p" id="S2.SS2.p2.4.4.4.6"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.4.4.4.6.1">Question:</span> [the question of the current example]</span>
<span class="ltx_p" id="S2.SS2.p2.4.4.4.7"><span class="ltx_text ltx_font_italic" id="S2.SS2.p2.4.4.4.7.1">Answer:</span></span>
</span>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">We sample <math alttext="k\in\{1,2,5,10,20,50\}" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.6"><semantics id="S2.SS2.p3.1.m1.6a"><mrow id="S2.SS2.p3.1.m1.6.7" xref="S2.SS2.p3.1.m1.6.7.cmml"><mi id="S2.SS2.p3.1.m1.6.7.2" xref="S2.SS2.p3.1.m1.6.7.2.cmml">k</mi><mo id="S2.SS2.p3.1.m1.6.7.1" xref="S2.SS2.p3.1.m1.6.7.1.cmml">∈</mo><mrow id="S2.SS2.p3.1.m1.6.7.3.2" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml"><mo id="S2.SS2.p3.1.m1.6.7.3.2.1" stretchy="false" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">{</mo><mn id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">1</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.2" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.2.2" xref="S2.SS2.p3.1.m1.2.2.cmml">2</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.3" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.3.3" xref="S2.SS2.p3.1.m1.3.3.cmml">5</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.4" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.4.4" xref="S2.SS2.p3.1.m1.4.4.cmml">10</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.5" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.5.5" xref="S2.SS2.p3.1.m1.5.5.cmml">20</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.6" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.6.6" xref="S2.SS2.p3.1.m1.6.6.cmml">50</mn><mo id="S2.SS2.p3.1.m1.6.7.3.2.7" stretchy="false" xref="S2.SS2.p3.1.m1.6.7.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.6b"><apply id="S2.SS2.p3.1.m1.6.7.cmml" xref="S2.SS2.p3.1.m1.6.7"><in id="S2.SS2.p3.1.m1.6.7.1.cmml" xref="S2.SS2.p3.1.m1.6.7.1"></in><ci id="S2.SS2.p3.1.m1.6.7.2.cmml" xref="S2.SS2.p3.1.m1.6.7.2">𝑘</ci><set id="S2.SS2.p3.1.m1.6.7.3.1.cmml" xref="S2.SS2.p3.1.m1.6.7.3.2"><cn id="S2.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS2.p3.1.m1.1.1">1</cn><cn id="S2.SS2.p3.1.m1.2.2.cmml" type="integer" xref="S2.SS2.p3.1.m1.2.2">2</cn><cn id="S2.SS2.p3.1.m1.3.3.cmml" type="integer" xref="S2.SS2.p3.1.m1.3.3">5</cn><cn id="S2.SS2.p3.1.m1.4.4.cmml" type="integer" xref="S2.SS2.p3.1.m1.4.4">10</cn><cn id="S2.SS2.p3.1.m1.5.5.cmml" type="integer" xref="S2.SS2.p3.1.m1.5.5">20</cn><cn id="S2.SS2.p3.1.m1.6.6.cmml" type="integer" xref="S2.SS2.p3.1.m1.6.6">50</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.6c">k\in\{1,2,5,10,20,50\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.6d">italic_k ∈ { 1 , 2 , 5 , 10 , 20 , 50 }</annotation></semantics></math> to represent the reader behavior.
More details can be found in §<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#A1" title="Appendix A Implementation Details ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimenting with RAG Systems</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We introduce the retrievers and readers.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Retriever</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We employ two approaches: (1) a sparse retriever, based on lexical information, and (2) a dense retriever based on neural embeddings.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">BM25</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">BM25 is a probabilistic retrieval model <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib21" title="">2009</a>)</cite> that
estimates passage relevance via term weighting and passage length normalization.
BM25 relies on term-matching, and thus is supposed to be proficient at identifying lexical similarity especially in special domains.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">ColBERT</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">One of the best-performing neural-based retrievers is ColBERT <cite class="ltx_cite ltx_citemacro_citep">(Santhanam et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib22" title="">2021</a>)</cite>, i.e., contextualized late interaction over BERT.
ColBERT uses contextualized embeddings instead of term-matching as in BM25, thus is supposed to be better at identifying semantic similarities between queries and passages.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Reader</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We analyze closed-source models from the <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">GPT</span> and <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.2">Claude</span> families, and open-source models from the <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.3">Flan</span> and <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.4">LLaMa</span> families.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.1.1">Flan</span> </h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.1">Flan</span> models are encoder-decoder models.
We use the <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.2">FlanT5</span>-XXL <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib8" title="">2022</a>)</cite> with 11B parameters and <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.3">Flan</span>-UL2 <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib24" title="">2023</a>)</cite> with 20B parameters, both with a context length of 2<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> tokens.
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.4">FlanT5</span>-XXL is an instruction-tuned variant of the T5 model <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib20" title="">2023</a>)</cite>. <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.5">Flan</span>-UL2 <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib24" title="">2023</a>)</cite> is an upgraded T5-based model that is trained with Unifying Language Learning Paradigm, a pertaining process that uses a mixture-of-denoisers and mode switching to improve the model’s adaptability to different scenarios.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.1.1">LLaMa</span> </h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We use 7B and 70B <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.1">LLaMa2</span> models <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib25" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib26" title="">b</a>)</cite> and the 8B and 70B <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.2">LLaMa3</span> models.
The <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.3">LLaMa2</span> models have a context length of 4k tokens
while <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.4">LLaMa3</span> models have double the context length at 8k tokens.
The <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.5">LLaMa3</span> models have three major differences:
1) They use grouped query attention, which groups query heads to understand similar information better,
2) Their vocabulary is four times larger than that of <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.6">LLaMa2</span>,
3) They are trained on a seven times larger dataset than <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.7">LLaMa2</span>’s.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px3.1.1">GPT</span> </h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">We use <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px3.p1.1.1">GPT-3.5</span>-turbo model <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib5" title="">2020</a>)</cite>.
This model has a context length of 16k tokens, and is a closed source model, so further details about model size are unknown.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px4.1.1">Claude</span> </h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1">We use <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px4.p1.1.1">Claude Haiku</span>, which is Anthropic’s fastest and most compact model <cite class="ltx_cite ltx_citemacro_citep">(Enis and Hopkins, <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib9" title="">2024</a>)</cite>.
The context window of 200k tokens is the largest of all the models we compare in this paper, but the model size is unknown since the model is closed-source.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets and Evaluation Metrics</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We adopt three DBQA datasets from various domains (Wikipedia, biomedical) and of various complexity (single-hop, multi-hop). Details about the corpus of passages used for retrieval are in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A2.T2" title="Table 2 ‣ Appendix B Dataset Details ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Natural Questions</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We choose Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib14" title="">2019</a>)</cite>, a Wikipedia-based dataset, to examine how models perform on generic, open-domain, single-hop questions.
NQ questions are real user-search queries on Google.
We adopt the KILT version <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib19" title="">2021</a>)</cite> of the dataset, which provides one short phrase answer and at least one gold passage for each question.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">HotpotQA</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We choose HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib30" title="">2018</a>)</cite>, a multi-hop, Wikipedia-based dataset to examine how effectively models can identify multiple signal passages and reason over them together.
Each question requires reasoning over at least two passages to answer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">BioASQ</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">We choose BioASQ’s Task 11B <cite class="ltx_cite ltx_citemacro_citep">(Krithara et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib13" title="">2023</a>)</cite>, a PubMed-based dataset, with biomedical questions
to examine how models perform on special-domain questions.
Our evaluation dataset is a compilation of the BioASQ Task 11B training and golden enriched set.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retriever Metric</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We evaluate retrieval performance using the <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.1">recall@k</span> metric, following <cite class="ltx_cite ltx_citemacro_citet">Petroni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib19" title="">2021</a>)</cite>.
For a given query, recall@k measures the fraction of ground-truth passages that are among the top-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved passages.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Reader Metric</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.2">We use <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p1.1.1">unigram F<sub class="ltx_sub" id="S4.SS2.SSS0.Px2.p1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.SS2.SSS0.Px2.p1.1.1.1.1">1</span></sub></span>, which quantifies the overlap of unigrams in the reader output and gold answer<cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib19" title="">2021</a>)</cite>.
For each query, we compute the F<sub class="ltx_sub" id="S4.SS2.SSS0.Px2.p1.2.2">1</sub> score of the reader output against the list of gold answers and report the highest score.
We opt for this metric instead of exact match or substring match since many language models may respond correctly semantically but still fail these other metrics because of inexact wording or the tendency to answer in complete sentences instead of the short phrase format of the gold answers.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S4.F3.g1" src="x3.png" width="821"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Reader performance as we vary <math alttext="k" class="ltx_Math" display="inline" id="S4.F3.5.m1.1"><semantics id="S4.F3.5.m1.1b"><mi id="S4.F3.5.m1.1.1" xref="S4.F3.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F3.5.m1.1c"><ci id="S4.F3.5.m1.1.1.cmml" xref="S4.F3.5.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F3.5.m1.1e">italic_k</annotation></semantics></math>, the number of retrieved contexts provided by ColBERT, from 0 (no context) to 20. Colored circles mark the reader performance at optimal <math alttext="k^{*}" class="ltx_Math" display="inline" id="S4.F3.6.m2.1"><semantics id="S4.F3.6.m2.1b"><msup id="S4.F3.6.m2.1.1" xref="S4.F3.6.m2.1.1.cmml"><mi id="S4.F3.6.m2.1.1.2" xref="S4.F3.6.m2.1.1.2.cmml">k</mi><mo id="S4.F3.6.m2.1.1.3" xref="S4.F3.6.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F3.6.m2.1c"><apply id="S4.F3.6.m2.1.1.cmml" xref="S4.F3.6.m2.1.1"><csymbol cd="ambiguous" id="S4.F3.6.m2.1.1.1.cmml" xref="S4.F3.6.m2.1.1">superscript</csymbol><ci id="S4.F3.6.m2.1.1.2.cmml" xref="S4.F3.6.m2.1.1.2">𝑘</ci><times id="S4.F3.6.m2.1.1.3.cmml" xref="S4.F3.6.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m2.1d">k^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.m2.1e">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. Dashed lines indicate results when truncating <span class="ltx_text ltx_font_smallcaps" id="S4.F3.10.1">LLaMa2</span> inputs from 4<math alttext="k" class="ltx_Math" display="inline" id="S4.F3.7.m3.1"><semantics id="S4.F3.7.m3.1b"><mi id="S4.F3.7.m3.1.1" xref="S4.F3.7.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F3.7.m3.1c"><ci id="S4.F3.7.m3.1.1.cmml" xref="S4.F3.7.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m3.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F3.7.m3.1e">italic_k</annotation></semantics></math> to 2<math alttext="k" class="ltx_Math" display="inline" id="S4.F3.8.m4.1"><semantics id="S4.F3.8.m4.1b"><mi id="S4.F3.8.m4.1.1" xref="S4.F3.8.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F3.8.m4.1c"><ci id="S4.F3.8.m4.1.1.cmml" xref="S4.F3.8.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m4.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F3.8.m4.1e">italic_k</annotation></semantics></math> tokens for studying the effect of context limit.
</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Are More Contexts Always Better?</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we study how models perform with various amounts of context passages (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS1" title="5.1 Reader Trends with Scaling Contexts ‣ 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5.1</span></a>), and if these results relate to context limits (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S5.SS4" title="5.4 The Effect of Context Limit ‣ 5 Are More Contexts Always Better? ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5.4</span></a>). Our RAGGED analysis guidelines are:</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">What to Vary:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Vary the number of context passages provided to the reader model,
starting from <math alttext="k=1" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑘</ci><cn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">italic_k = 1</annotation></semantics></math> to 50,
instead of directly providing as many passages as the model’s context limit can fit.
This allows us to determine the point at which the model reaches peak performance, as many models do so before hitting their context limit.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Behaviors to Expect:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Initially, reader performance may improve with more contexts due to the increased availability of “signal”, or helpful information.
However, as the number of context passages increases,
the amount of “noise”, or irrelevant information, also increases.
As a result,
the reader performance may plateau or start degrading, sometimes even beyond the no-context performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Model Behavior Implications:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.2">Fitting as many retrieved contexts as possible under the reader’s context limit does not always guarantee optimal downstream performance.
Using these experiments,
one can find the optimal range of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.1.m1.1d">italic_k</annotation></semantics></math> in which a reader can most effectively sift for “signal” among “noise”.
This optimal range is often not a large <math alttext="k" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px3.p1.2.m2.1a"><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.2.m2.1b"><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.2.m2.1d">italic_k</annotation></semantics></math>, thus identifying this can help practitioners and researchers save time and cost.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Reader Trends with Scaling Contexts</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Results in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S4.F3" title="Figure 3 ‣ Reader Metric ‣ 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> reveal two typical trends in reader performance as we increase the number of ColBERT-retrieved context passages from <math alttext="k=1" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><eq id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></eq><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝑘</ci><cn id="S5.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_k = 1</annotation></semantics></math> to 20.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.4">In one trend, the reader model’s performance steadily improves with increasing <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_k</annotation></semantics></math> and then plateaus. This trend is observed in models such as the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p2.4.1">Flan</span> models and the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p2.4.2">GPT-3.5</span> model, which all peak at <math alttext="k\geq 10" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">≥</mo><mn id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><geq id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></geq><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">𝑘</ci><cn id="S5.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">k\geq 10</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_k ≥ 10</annotation></semantics></math> without any noticeable decline afterward. For these models, providing a larger number of contexts is beneficial.
However, note that for special domains, the exception is that <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p2.4.3">Flan</span> models peak earlier <math alttext="(k=1)" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.1.cmml"><mo id="S5.SS1.p2.3.m3.1.1.1.2" stretchy="false" xref="S5.SS1.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.p2.3.m3.1.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.1.1.2" xref="S5.SS1.p2.3.m3.1.1.1.1.2.cmml">k</mi><mo id="S5.SS1.p2.3.m3.1.1.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S5.SS1.p2.3.m3.1.1.1.1.3" xref="S5.SS1.p2.3.m3.1.1.1.1.3.cmml">1</mn></mrow><mo id="S5.SS1.p2.3.m3.1.1.1.3" stretchy="false" xref="S5.SS1.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1"><eq id="S5.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1.1.1"></eq><ci id="S5.SS1.p2.3.m3.1.1.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.1.1.2">𝑘</ci><cn id="S5.SS1.p2.3.m3.1.1.1.1.3.cmml" type="integer" xref="S5.SS1.p2.3.m3.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">(k=1)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">( italic_k = 1 )</annotation></semantics></math>, but still follow the trend of leveling off as <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><mi id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><ci id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">italic_k</annotation></semantics></math> increases.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.2">In the second trend, the reader model’s performance peaks early (at <math alttext="k&lt;5" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><lt id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></lt><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝑘</ci><cn id="S5.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">k&lt;5</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_k &lt; 5</annotation></semantics></math>) then steadily declines without leveling off as <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_k</annotation></semantics></math> increases. This pattern is seen in models such as the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p3.2.1">LLaMa</span> models and the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p3.2.2">Claude</span> model. For these models, it is safer to provide a smaller number of contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparing Optimal <math alttext="k^{*}" class="ltx_Math" display="inline" id="S5.SS2.1.m1.1"><semantics id="S5.SS2.1.m1.1b"><msup id="S5.SS2.1.m1.1.1" xref="S5.SS2.1.m1.1.1.cmml"><mi id="S5.SS2.1.m1.1.1.2" xref="S5.SS2.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS2.1.m1.1.1.3" xref="S5.SS2.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.1.m1.1c"><apply id="S5.SS2.1.m1.1.1.cmml" xref="S5.SS2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.1.m1.1.1.1.cmml" xref="S5.SS2.1.m1.1.1">superscript</csymbol><ci id="S5.SS2.1.m1.1.1.2.cmml" xref="S5.SS2.1.m1.1.1.2">𝑘</ci><times id="S5.SS2.1.m1.1.1.3.cmml" xref="S5.SS2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.1.m1.1d">k^{*}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.1.m1.1e">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> Performances</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare reader performances at <math alttext="k^{*}" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><msup id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑘</ci><times id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">k^{*}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, their optimal number of context passages. Regardless of the domain or the number of passage hops required to answer the question, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.1">GPT-3.5</span> consistently ranks near the top.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For Wikipedia-domain questions, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p2.1.1">Flan</span> models consistently rank high, with the larger <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p2.1.2">LLaMa</span> models performing either on par or close behind, followed by <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p2.1.3">Claude Haiku</span>, and then the smaller <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p2.1.4">LLaMa</span> models.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">For special-domain questions, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p3.1.1">Claude Haiku</span> rises to the second-best rank despite being one of the lower-ranked models for Wikipedia-domain questions. However, it’s important to note that <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p3.1.2">Claude Haiku</span> performs best when no context is provided, indicating that its high ranking is due to its extensive pre-trained knowledge rather than its ability to effectively use context. These observations underscore the necessity to carefully select <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">italic_k</annotation></semantics></math> to optimize reader performance.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">One potential reason that some models’ performance deteriorates as <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mi id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><ci id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">italic_k</annotation></semantics></math> increases is that some retrieved passages are irrelevant and unhelpful,
so the inclusion of more passages may also result in the inclusion of more “noise”.
In <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6" title="6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a>, we dig deeper into how readers perform under data slices of different informational qualities.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>RAG vs. No-context</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.2">We evaluate reader performance without context (<math alttext="k=0" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><eq id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></eq><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝑘</ci><cn id="S5.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">k=0</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_k = 0</annotation></semantics></math>) and compare it with the standard setting with top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mi id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><ci id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">italic_k</annotation></semantics></math> passages from ColBERT (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S4.F3" title="Figure 3 ‣ Reader Metric ‣ 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).
We find that <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p1.2.1">GPT</span> generally performs <em class="ltx_emph ltx_font_italic" id="S5.SS3.p1.2.2">better with context</em> than without, except on NQ where its pre-trained knowledge already allows it to achieve high performance without context.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p2.1.1">Claude Haiku</span>, on the other hand, quickly deteriorates and consistently performs <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.2">worse with context</em> than without. As a result, although <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p2.1.3">Claude Haiku</span> ranks high in no-context performance, it ranks lower when context is provided.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p3.1.1">Flan</span> models consistently perform <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.2">better with context</em> than without context.
Their ability to use context effectively allows them to make up for their lack of pretrained knowledge, evidenced by how they rank high in peak top-k performance but low in no-context performance.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.3"><span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p4.3.1">LLaMa</span> models perform better with context than without context only for smaller <math alttext="k" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mi id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><ci id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">italic_k</annotation></semantics></math> values (<math alttext="k\leq 5" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><mrow id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS3.p4.2.m2.1.1.1" xref="S5.SS3.p4.2.m2.1.1.1.cmml">≤</mo><mn id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><leq id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1.1"></leq><ci id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2">𝑘</ci><cn id="S5.SS3.p4.2.m2.1.1.3.cmml" type="integer" xref="S5.SS3.p4.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">k\leq 5</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">italic_k ≤ 5</annotation></semantics></math>).
Although <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p4.3.2">LLaMa3</span>’s no-context performance surpasses <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p4.3.3">LLaMa2</span>’s, likely due to its larger training set and vocabulary, <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p4.3.4">LLaMa3</span> struggles to use contexts as effectively and thus falls behind <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p4.3.5">LLaMa2</span> as <math alttext="k" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><mi id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><ci id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">italic_k</annotation></semantics></math> increases.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>The Effect of Context Limit</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Not all retrieved passages are helpful "signals" for answering the question; some may be "noise" that distracts the reader.
Hence as more passages are included, more “noise” can be included too.
The ability to sift for signal among noise is what determines the success of a reader, not merely the size of the reader’s context limit.
For example, <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.1">Claude Haiku</span> has the largest context limit among the readers, yet it struggles to effectively utilize a larger number of contexts and peaks at a <math alttext="k^{*}\leq 1" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><msup id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml"><mi id="S5.SS4.p1.1.m1.1.1.2.2" xref="S5.SS4.p1.1.m1.1.1.2.2.cmml">k</mi><mo id="S5.SS4.p1.1.m1.1.1.2.3" xref="S5.SS4.p1.1.m1.1.1.2.3.cmml">∗</mo></msup><mo id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">≤</mo><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><leq id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></leq><apply id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.1.1.2.1.cmml" xref="S5.SS4.p1.1.m1.1.1.2">superscript</csymbol><ci id="S5.SS4.p1.1.m1.1.1.2.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2">𝑘</ci><times id="S5.SS4.p1.1.m1.1.1.2.3.cmml" xref="S5.SS4.p1.1.m1.1.1.2.3"></times></apply><cn id="S5.SS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">k^{*}\leq 1</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ≤ 1</annotation></semantics></math> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S4.F3" title="Figure 3 ‣ Reader Metric ‣ 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Given that a large context limit may include more noise, we examine whether truncating the reader input can help exclude noise and improve reader performance.
We test this hypothesis by comparing <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p2.1.1">LLaMa2</span>’s performance under its original context length of 4k and a truncated length of <math alttext="2k" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">2</mn><mo id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><times id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"></times><cn id="S5.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.2">2</cn><ci id="S5.SS4.p2.1.m1.1.1.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">2k</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">2 italic_k</annotation></semantics></math>.
As shown by the dashed lines in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S4.F3" title="Figure 3 ‣ Reader Metric ‣ 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>,
early truncation has minimal effects on improving <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p2.1.2">LLaMa2</span>’s performance on Wikipedia-based questions, indicating that truncating the input does not provide a significant advantage.
These observations suggest that <em class="ltx_emph ltx_font_italic" id="S5.SS4.p2.1.3">context limit is not a key factor in configuring a RAG system.</em></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Reader Robustness in the Presence and Absence of Gold Passages</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We study how the readers respond to noise under contexts with gold context passages (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS1" title="6.1 With Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6.1</span></a>)
and without gold context passages (§<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS2" title="6.2 Without Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6.2</span></a>). Our RAGGED analysis guidelines are:</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">What to Vary:</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Analyze the reader model’s performance
on two slices of instances representing different qualities of retrieved contexts.
In §<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS1" title="6.1 With Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6.1</span></a>, we demonstrate analyzing the slice where the <math alttext="k" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved passages
include at least one gold passage.
This represents the scenario where there is sufficient context information to answer the question.
In this slice, we compare how the reader performs with a) the top-k passages, b) only the gold passages in the top-k passages (top-gold), and c) no context.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">In §<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#S6.SS2" title="6.2 Without Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6.2</span></a>, we demonstrate analyzing on the slice of instances, where none of the top-k retrieved passages include any gold passages.
This represents the scenario where there is insufficient context information to answer the question.
This setting is used to evaluate the model’s ability to ignore noise and fall back onto any helpful pre-trained knowledge.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Behaviors to Expect:</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Reader models provided with only gold passages often serve as an upper bound for their top-k performance. However, reader performance without context does not necessarily represent a lower bound for their top-k performance. This depends on the reader’s ability to distinguish between irrelevant information and helpful pre-trained knowledge.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Implications of Behaviors:</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">For practitioners, these analyses can help identify which readers are more robust to noise. A small gap between top-k and top-gold performance means the reader is highly robust to noise and is effective at identifying and using relevant information. Conversely, a significant gap between top-k and top-gold performance suggests the model struggles to discern signal from noise.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>With Gold Passages</h3>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="599" id="S6.F4.g1" src="x4.png" width="821"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>NQ results when there is sufficient information (at least one gold passage) in the top-k passages to answer the question. Top-gold means the context only includes the gold passages in the top-k passages.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.2">For NQ instances where gold passages are in the top-k retrieved passages,
<span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p1.2.1">GPT-3.5</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p1.2.2">Flan</span> models effectively identify and use relevant information, consistently performing better with context than without <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S6.F4" title="Figure 4 ‣ 6.1 With Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.
In contrast, the rest of the models struggle more with noise.
For example, <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p1.2.3">Claude Haiku</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p1.2.4">LLaMa2</span> models’ top-k performance falls below their no-context performance at <math alttext="k\leq 5" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">k</mi><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">≤</mo><mn id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><leq id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"></leq><ci id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">𝑘</ci><cn id="S6.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S6.SS1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">k\leq 5</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">italic_k ≤ 5</annotation></semantics></math> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p1.2.5">LLaMa3</span> models at <math alttext="k=10" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mi id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml">k</mi><mo id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.2.m2.1.1.3" xref="S6.SS1.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><eq id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1"></eq><ci id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2">𝑘</ci><cn id="S6.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S6.SS1.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">k=10</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">italic_k = 10</annotation></semantics></math>.
This highlights how the wrong configuration can result in using RAG being worse than not using RAG
<em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.2.6">even when there is sufficient context information</em>.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="326" id="S6.F5.g1" src="x5.png" width="821"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>HotpotQA results when there is sufficient information (all gold passages) in the top-k passages to answer the question. Top-gold means the context only includes the gold passages in the top-k passages</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">For HotpotQA and BioASQ, models generally follow similar trends to NQ regarding which models’ no-context performance serves as a strict lower bound. We provide a detailed discussion of the more interesting models below and include complete plots in
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A4" title="Appendix D Additional Reader Results ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.7">One difference between NQ and HotpotQA performance is that <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p3.7.1">LLaMa2</span> 7B’s top-k performance now dips below the no-context baseline much later at <math alttext="k=25" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mrow id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml"><mi id="S6.SS1.p3.1.m1.1.1.2" xref="S6.SS1.p3.1.m1.1.1.2.cmml">k</mi><mo id="S6.SS1.p3.1.m1.1.1.1" xref="S6.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS1.p3.1.m1.1.1.3" xref="S6.SS1.p3.1.m1.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><apply id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"><eq id="S6.SS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1.1"></eq><ci id="S6.SS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.p3.1.m1.1.1.2">𝑘</ci><cn id="S6.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S6.SS1.p3.1.m1.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">k=25</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_k = 25</annotation></semantics></math> instead of at <math alttext="k=15" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><mrow id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mi id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml">k</mi><mo id="S6.SS1.p3.2.m2.1.1.1" xref="S6.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><eq id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1.1"></eq><ci id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">𝑘</ci><cn id="S6.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S6.SS1.p3.2.m2.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">k=15</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">italic_k = 15</annotation></semantics></math>,
and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p3.7.2">LLaMa2</span> 70B dips below the no-context baseline at <math alttext="k&gt;50" class="ltx_Math" display="inline" id="S6.SS1.p3.3.m3.1"><semantics id="S6.SS1.p3.3.m3.1a"><mrow id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml"><mi id="S6.SS1.p3.3.m3.1.1.2" xref="S6.SS1.p3.3.m3.1.1.2.cmml">k</mi><mo id="S6.SS1.p3.3.m3.1.1.1" xref="S6.SS1.p3.3.m3.1.1.1.cmml">&gt;</mo><mn id="S6.SS1.p3.3.m3.1.1.3" xref="S6.SS1.p3.3.m3.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><apply id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"><gt id="S6.SS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1.1"></gt><ci id="S6.SS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.p3.3.m3.1.1.2">𝑘</ci><cn id="S6.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S6.SS1.p3.3.m3.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">k&gt;50</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.3.m3.1d">italic_k &gt; 50</annotation></semantics></math> instead of at <math alttext="k=25" class="ltx_Math" display="inline" id="S6.SS1.p3.4.m4.1"><semantics id="S6.SS1.p3.4.m4.1a"><mrow id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml"><mi id="S6.SS1.p3.4.m4.1.1.2" xref="S6.SS1.p3.4.m4.1.1.2.cmml">k</mi><mo id="S6.SS1.p3.4.m4.1.1.1" xref="S6.SS1.p3.4.m4.1.1.1.cmml">=</mo><mn id="S6.SS1.p3.4.m4.1.1.3" xref="S6.SS1.p3.4.m4.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><apply id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1"><eq id="S6.SS1.p3.4.m4.1.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1.1"></eq><ci id="S6.SS1.p3.4.m4.1.1.2.cmml" xref="S6.SS1.p3.4.m4.1.1.2">𝑘</ci><cn id="S6.SS1.p3.4.m4.1.1.3.cmml" type="integer" xref="S6.SS1.p3.4.m4.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">k=25</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.4.m4.1d">italic_k = 25</annotation></semantics></math> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S6.F5" title="Figure 5 ‣ 6.1 With Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>).
Similarly, <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p3.7.3">Claude</span> now dips below no-context baseline at <math alttext="5&lt;k&lt;10" class="ltx_Math" display="inline" id="S6.SS1.p3.5.m5.1"><semantics id="S6.SS1.p3.5.m5.1a"><mrow id="S6.SS1.p3.5.m5.1.1" xref="S6.SS1.p3.5.m5.1.1.cmml"><mn id="S6.SS1.p3.5.m5.1.1.2" xref="S6.SS1.p3.5.m5.1.1.2.cmml">5</mn><mo id="S6.SS1.p3.5.m5.1.1.3" xref="S6.SS1.p3.5.m5.1.1.3.cmml">&lt;</mo><mi id="S6.SS1.p3.5.m5.1.1.4" xref="S6.SS1.p3.5.m5.1.1.4.cmml">k</mi><mo id="S6.SS1.p3.5.m5.1.1.5" xref="S6.SS1.p3.5.m5.1.1.5.cmml">&lt;</mo><mn id="S6.SS1.p3.5.m5.1.1.6" xref="S6.SS1.p3.5.m5.1.1.6.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.5.m5.1b"><apply id="S6.SS1.p3.5.m5.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1"><and id="S6.SS1.p3.5.m5.1.1a.cmml" xref="S6.SS1.p3.5.m5.1.1"></and><apply id="S6.SS1.p3.5.m5.1.1b.cmml" xref="S6.SS1.p3.5.m5.1.1"><lt id="S6.SS1.p3.5.m5.1.1.3.cmml" xref="S6.SS1.p3.5.m5.1.1.3"></lt><cn id="S6.SS1.p3.5.m5.1.1.2.cmml" type="integer" xref="S6.SS1.p3.5.m5.1.1.2">5</cn><ci id="S6.SS1.p3.5.m5.1.1.4.cmml" xref="S6.SS1.p3.5.m5.1.1.4">𝑘</ci></apply><apply id="S6.SS1.p3.5.m5.1.1c.cmml" xref="S6.SS1.p3.5.m5.1.1"><lt id="S6.SS1.p3.5.m5.1.1.5.cmml" xref="S6.SS1.p3.5.m5.1.1.5"></lt><share href="https://arxiv.org/html/2403.09040v2#S6.SS1.p3.5.m5.1.1.4.cmml" id="S6.SS1.p3.5.m5.1.1d.cmml" xref="S6.SS1.p3.5.m5.1.1"></share><cn id="S6.SS1.p3.5.m5.1.1.6.cmml" type="integer" xref="S6.SS1.p3.5.m5.1.1.6">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.5.m5.1c">5&lt;k&lt;10</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.5.m5.1d">5 &lt; italic_k &lt; 10</annotation></semantics></math> instead of <math alttext="k\leq 5" class="ltx_Math" display="inline" id="S6.SS1.p3.6.m6.1"><semantics id="S6.SS1.p3.6.m6.1a"><mrow id="S6.SS1.p3.6.m6.1.1" xref="S6.SS1.p3.6.m6.1.1.cmml"><mi id="S6.SS1.p3.6.m6.1.1.2" xref="S6.SS1.p3.6.m6.1.1.2.cmml">k</mi><mo id="S6.SS1.p3.6.m6.1.1.1" xref="S6.SS1.p3.6.m6.1.1.1.cmml">≤</mo><mn id="S6.SS1.p3.6.m6.1.1.3" xref="S6.SS1.p3.6.m6.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.6.m6.1b"><apply id="S6.SS1.p3.6.m6.1.1.cmml" xref="S6.SS1.p3.6.m6.1.1"><leq id="S6.SS1.p3.6.m6.1.1.1.cmml" xref="S6.SS1.p3.6.m6.1.1.1"></leq><ci id="S6.SS1.p3.6.m6.1.1.2.cmml" xref="S6.SS1.p3.6.m6.1.1.2">𝑘</ci><cn id="S6.SS1.p3.6.m6.1.1.3.cmml" type="integer" xref="S6.SS1.p3.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.6.m6.1c">k\leq 5</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.6.m6.1d">italic_k ≤ 5</annotation></semantics></math>.
This could suggest that if the problem requires more than one signal passage and all such signals are present,
the reader may have more anchor points for the model to rely on before being distracted by the noise in increasing <math alttext="k" class="ltx_Math" display="inline" id="S6.SS1.p3.7.m7.1"><semantics id="S6.SS1.p3.7.m7.1a"><mi id="S6.SS1.p3.7.m7.1.1" xref="S6.SS1.p3.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.7.m7.1b"><ci id="S6.SS1.p3.7.m7.1.1.cmml" xref="S6.SS1.p3.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.7.m7.1d">italic_k</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Different from for NQ, the gap between top-pos and top-k for BioASQ is much smaller for all models (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A4.F10" title="Figure 10 ‣ Appendix D Additional Reader Results ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>).
One potential explanation is that the specialized domain jargon makes it easier for the reader to perform a second-step filtering for signal after the retriever’s first step of filtering.
We attribute this difference to the reader’s filtering ability instead of the retriever’s since ColBERT performs strictly worse on BioASQ than on NQ.
Another difference is that for <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p4.1.1">Claude Haiku</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS1.p4.1.2">LLaMa3</span> 70B, even the top-gold dips below the no-context baseline,
showing that they struggle much more with special domains.
A closer examination reveals that these readers tend to start returning gibberish as the number of contexts increases, even when all the contexts are gold passages.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Without Gold Passages</h3>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="661" id="S6.F6.g1" src="x6.png" width="823"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>NQ results with no gold passages.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">For NQ and HotpotQA, almost all models’ top-k performance is worse than their no-context performance (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S6.F6" title="Figure 6 ‣ 6.2 Without Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A4.F11" title="Figure 11 ‣ Appendix D Additional Reader Results ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>).
This general trend is not too surprising since the models are instructed to answer based on the context, and these slices of data do not include any gold passages.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">What is more surprising is the exception of the
<span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.1">Flan</span> models, which <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.2">benefit even from non-gold contexts</em> and consistently outperform its no-context baseline across all <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mi id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math>.
One potential reason this is possible is that even though the non-gold passages may provide insufficient information, they could still provide relevant keywords and information that act as clues for the reader.
<span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.3">Flan</span> models seem to be better at processing the information from these non-gold passages than other models.
As evidence, for NQ and k = 5, for the slice of data where there are no gold passages (paragraphs) but there are passages from the same Wikipedia pages as the gold passages, <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.4">Flan</span> models’ accuracy averages at 20%, which is much higher than <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.5">LLaMa2</span> models’ average accuracy of 8%, <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.6">LLaMa3</span> models’ at 4%, <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.7">GPT-3.5</span>’s at 10%, and <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p2.1.8">Claude Haiku</span>’s at 3%.</p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S6.F7.g1" src="x7.png" width="821"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>BioASQ results with no gold passages.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.2">For BioASQ (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S6.F7" title="Figure 7 ‣ 6.2 Without Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>), one key difference is that <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p3.2.1">GPT</span>’s top-k performance is above its no-context performance for <math alttext="k\geq 5" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><mrow id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml"><mi id="S6.SS2.p3.1.m1.1.1.2" xref="S6.SS2.p3.1.m1.1.1.2.cmml">k</mi><mo id="S6.SS2.p3.1.m1.1.1.1" xref="S6.SS2.p3.1.m1.1.1.1.cmml">≥</mo><mn id="S6.SS2.p3.1.m1.1.1.3" xref="S6.SS2.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><apply id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1"><geq id="S6.SS2.p3.1.m1.1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1.1"></geq><ci id="S6.SS2.p3.1.m1.1.1.2.cmml" xref="S6.SS2.p3.1.m1.1.1.2">𝑘</ci><cn id="S6.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S6.SS2.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">k\geq 5</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">italic_k ≥ 5</annotation></semantics></math> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS2.p3.2.2">LLaMa2</span> 7B’s too for all <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p3.2.m2.1"><semantics id="S6.SS2.p3.2.m2.1a"><mi id="S6.SS2.p3.2.m2.1.1" xref="S6.SS2.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.2.m2.1b"><ci id="S6.SS2.p3.2.m2.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.2.m2.1d">italic_k</annotation></semantics></math>,
as opposed to being consistently under their no-context baseline.
This indicates that for special-domain questions, these models may have stronger guardrails against misleading or irrelevant information, and can even fall back to using its pre-trained knowledge.
Full results for other models are in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A4.F12" title="Figure 12 ‣ Appendix D Additional Reader Results ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S6.F8.g1" src="x8.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Reader results on NQ, HotpotQA, and BioASQ using BM25 retrieved passages. Colored circles mark the reader performance at optimal <math alttext="k^{*}" class="ltx_Math" display="inline" id="S6.F8.2.m1.1"><semantics id="S6.F8.2.m1.1b"><msup id="S6.F8.2.m1.1.1" xref="S6.F8.2.m1.1.1.cmml"><mi id="S6.F8.2.m1.1.1.2" xref="S6.F8.2.m1.1.1.2.cmml">k</mi><mo id="S6.F8.2.m1.1.1.3" xref="S6.F8.2.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S6.F8.2.m1.1c"><apply id="S6.F8.2.m1.1.1.cmml" xref="S6.F8.2.m1.1.1"><csymbol cd="ambiguous" id="S6.F8.2.m1.1.1.1.cmml" xref="S6.F8.2.m1.1.1">superscript</csymbol><ci id="S6.F8.2.m1.1.1.2.cmml" xref="S6.F8.2.m1.1.1.2">𝑘</ci><times id="S6.F8.2.m1.1.1.3.cmml" xref="S6.F8.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F8.2.m1.1d">k^{*}</annotation><annotation encoding="application/x-llamapun" id="S6.F8.2.m1.1e">italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Impact of Retriever Choice</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We compare different retrievers
and study their influence on the reader model’s downstream performance. Our RAGGED analysis guidelines are:</p>
</div>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">What to Vary:</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1">Use different retrieval algorithms (varying in architecture, computational cost, and storage cost).
Here, we compare traditional, lexical and dense, neural retrievers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Behaviors to Expect:</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px2.p1.1">Neural retrievers generally perform better than lexical retrievers.
How much that advantage translates to a gain in reader performance highly depends on the domain and the complexity of the question (number of hops). We find that such advantages are retained for single-hop, open-domain questions, dampened for multi-hop, open-domain questions, and amplified for special-domain questions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Implications of Behaviors:</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px3.p1.1">These analyses inform which retrievers perform better and how much that translates to an improvement in reader performance.
This can help practitioners decide which retriever is worth investing in for their specific tasks.</p>
</div>
<div class="ltx_para" id="S7.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S7.SS0.SSS0.Px3.p2.1">We evaluate the BM25 and ColBERT performances in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A3.T3" title="Table 3 ‣ Appendix C Retriever Performance ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Table 3</span></a>, then compare their reader performances in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S4.F3" title="Figure 3 ‣ Reader Metric ‣ 4.2 Metrics ‣ 4 Datasets and Evaluation Metrics ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#S6.F8" title="Figure 8 ‣ 6.2 Without Gold Passages ‣ 6 Reader Robustness in the Presence and Absence of Gold Passages ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Open Domain, Single-Hop</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px4.p1.1">For NQ questions, using ColBERT offers substantial improvements over using BM25 (19-point recall@k gain).
Taking a look at the example at <math alttext="k=5" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S7.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S7.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1"><eq id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.1"></eq><ci id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.2">𝑘</ci><cn id="S7.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" type="integer" xref="S7.SS0.SSS0.Px4.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px4.p1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S7.SS0.SSS0.Px4.p1.1.m1.1d">italic_k = 5</annotation></semantics></math>, the reader performance paired with ColBERT is strictly better than when paired with BM25, and that retriever gain translates to a reader gain of 9 F1 points when averaged over all models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Open-Domain, Multi-Hop</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px5.p1.1">Although ColBERT outperforms BM25 significantly (15-point recall@k gain), its impact on reader performance is much smaller.
Taking a look at the example at <math alttext="k=5" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="S7.SS0.SSS0.Px5.p1.1.m1.1a"><mrow id="S7.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.1" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.3" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px5.p1.1.m1.1b"><apply id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1"><eq id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.1"></eq><ci id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.2">𝑘</ci><cn id="S7.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml" type="integer" xref="S7.SS0.SSS0.Px5.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px5.p1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S7.SS0.SSS0.Px5.p1.1.m1.1d">italic_k = 5</annotation></semantics></math>, the average reader performance with CoLBERT only offers a 4-point F1 gain.
This suggests that the challenge with multi-hop questions lies more with the reader’s multi-hop reasoning capability than with the retriever.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph">Biomedical Domain</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px6.p1.1">ColBERT only outperforms BM25 minimally on BioASQ (&lt;1 point),
yet still offers a 4-point F1 gain in reader performance.
This shows how a small retriever performance gain can be amplified in the reader performance under a special domain that the reader may have encountered less in pretraining.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Related Work</h2>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Context Limit and Processing Capacity</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px1.p1.2">LMs with longer context windows are applicable across various knowledge-intensive generation tasks <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib2" title="">2020</a>; Bertsch et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib4" title="">2023</a>; Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib23" title="">2024</a>)</cite>. However, it is unclear how performant these models are in processing long contexts. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib16" title="">2023</a>)</cite> study if LMs can be sensitive to the position of useful content within a long context, and struggle when it is in the middle.
Moreover, <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib29" title="">2024</a>)</cite> show that an LM with a smaller context window (4<math alttext="k" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S8.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S8.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math>) using RAG performs comparably with finetuning with a longer-window LM (16<math alttext="k" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S8.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S8.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.2.m2.1d">italic_k</annotation></semantics></math>).
Following this query, our work studies the effectiveness of LMs in utilizing long contexts, when they have different input capacities.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Domain Influence on Downstream Performance</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px2.p1.1">It is crucial to know when LMs benefit
from including retrieved passages in context.
<cite class="ltx_cite ltx_citemacro_citet">Mallen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib17" title="">2023</a>)</cite> find that retrieving contexts may be unnecessary and even detrimental when asking about common knowledge, but it benefits questions about rare knowledge.
In contrast,
we find that using RAG under the right configurations
still offers significant downstream performance boosts even for common, Wikipedia-based questions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Robustness to Noisy Contexts</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px3.p1.1">Feeding in noisy contexts deteriorates LM performance.
<cite class="ltx_cite ltx_citemacro_citet">Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib1" title="">2022</a>)</cite> propose to select documents with high evidentiality scores.
<cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib27" title="">2023</a>)</cite> learn a filter model to remove the noisy sentences, and <cite class="ltx_cite ltx_citemacro_citet">Berchansky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib3" title="">2023</a>)</cite> adopt a similar approach at the token level.
Further, <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib32" title="">2023</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib28" title="">2023</a>)</cite> use a neural summarizer model
to aid the LM in identifying relevant information in long retrieved passages.
Instead of reducing noise at test time, <cite class="ltx_cite ltx_citemacro_citet">Yoran et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib31" title="">2023</a>)</cite> train LMs to be robust to irrelevant content. Lastly, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib7" title="">2023</a>)</cite> build an evaluation benchmark to test LMs’ noise robustness. Our work similarly studies LMs’ responses to noisy content but is more fine-grained with varied noise ratios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">We propose RAGGED, a framework designed to assist researchers and practitioners in making informed decisions about designing RAG systems, focusing on three key aspects: the number of contexts, the reader model, and the retriever model. We demonstrate the framework’s utility in deriving insights about RAG behaviors in response to varied context volumes, document quality, and question domains.
We hope that our framework will be utilized by the community to deepen the understanding and customization of RAG systems.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Although this study provides valuable insights into RAG systems,
it has several limitations.
First, the RAGGED framework, although comprehensive,
focuses mainly on document-based question-answering tasks, which may not fully capture the nuances of other knowledge-intensive NLP tasks such as summarization, fact verification, and machine reading comprehension.
Second, our experiments were conducted with a specific set of models and datasets, which may limit the generalizability of our findings to other models, languages, or domains not covered in this study.
However, providing a comprehensive analysis is not the main motivation or contribution of the paper.
We encourage readers to use our framework to evaluate other models and datasets
and share the insights with the community.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Special thanks to Alex Cabrera, Alex Bäuerle, Jun Araki, Md Rizwan Parvez for providing Zeno support for analysis visualization.
Our appreciation extends to Hao Zhu, Jacob Springer, and Vijay Viswanathan for providing feedback for our paper.
This paper was supported in part by a gift from Bosch research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2022)</span>
<span class="ltx_bibblock">
Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.08688" title="">Evidentiality-guided generation for knowledge-intensive nlp tasks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.05150" title="">Longformer: The long-document transformer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berchansky et al. (2023)</span>
<span class="ltx_bibblock">
Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.13682" title="">Optimizing retrieval-augmented reader models via token elimination</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertsch et al. (2023)</span>
<span class="ltx_bibblock">
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=lJWUJWLCJo" title="">Unlimiformer: Long-range transformers with unlimited length input</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1171" title="">Reading Wikipedia to answer open-domain questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1870–1879, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.01431" title="">Benchmarking large language models in retrieval-augmented generation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Enis and Hopkins (2024)</span>
<span class="ltx_bibblock">
Maxim Enis and Mark Hopkins. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2404.13813" title="">From llm to nmt: Advancing low-resource machine translation with claude</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=iBBcRUlOAPR" title="">An empirical analysis of compute-optimal large language model training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2007.01282" title="">Leveraging passage retrieval with generative models for open domain question answering</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.04906" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krithara et al. (2023)</span>
<span class="ltx_bibblock">
Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41597-023-02068-4" title="">Bioasq-qa: A manually curated corpus for biomedical question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Scientific Data</em>, 10:170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/Q19-1026" title="">Natural questions: A benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Transactions of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.03172" title="">Lost in the middle: How language models use long contexts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et al. (2023)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.10511" title="">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">of Medicine (2023)</span>
<span class="ltx_bibblock">
National Library of Medicine. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lhncbc.nlm.nih.gov/ii/information/MBR.html" title="">Pubmed baseline 2023 repository</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2009.02252" title="">Kilt: a benchmark for knowledge intensive language tasks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2023)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1910.10683</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson, Hugo Zaragoza, et al. 2009.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et al. (2021)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock">Colbertv2: Effective and efficient retrieval via lightweight late interaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2112.01488</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Neurocomputing</em>, 568:127063.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2023)</span>
<span class="ltx_bibblock">
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023.

</span>
<span class="ltx_bibblock">Ul2: Unifying language learning paradigms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2205.05131</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2311.08377</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.04408" title="">Recomp: Improving retrieval-augmented lms with compression and selective augmentation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.03025" title="">Retrieval meets long context large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1809.09600" title="">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. (2023)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.01558" title="">Making retrieval-augmented language models robust to irrelevant context</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.09210" title="">Chain-of-note: Enhancing robustness in retrieval-augmented language models</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reader model</h4>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.3">We truncate the <span class="ltx_text ltx_font_italic" id="A1.SS0.SSS0.Px1.p1.3.1">Context</span> to make sure the the rest of the prompt still fits within a reader’s context limit.
Specifically, when using <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.3.2">FlanT5</span> and <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.3.3">FlanUL2</span> readers, we use T5Tokenizer to truncate sequences to up to 2<math alttext="k" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> tokens; when using <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.3.4">LLaMa</span> models, we apply the LlamaTokenizer and truncate sequences by 4<math alttext="k" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.2.m2.1d">italic_k</annotation></semantics></math> tokens for <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.3.5">LLaMa2</span> and 8<math alttext="k" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="A1.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="A1.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.3.m3.1d">italic_k</annotation></semantics></math> for <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.3.6">LLaMa3</span>.
For closed-source models, we spent around $300.
Subsequently, we incorporate a concise question-and-answer format that segments the query using "Question:" and cues the model’s response with "Answer:", ensuring precise and targeted answers.</p>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p2.1">For our reader decoding strategy, we used greedy decoding with a beam size of 1 and temperature of 1, selecting the most probable next word at each step without sampling.
The output generation was configured to produce responses with 10 tokens.
The experiments were conducted on NVIDIA A6000 GPUs, supported by an environment with 60GB RAM. The average response time was <math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="A1.SS0.SSS0.Px1.p2.1.m1.1a"><mo id="A1.SS0.SSS0.Px1.p2.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p2.1.m1.1b"><csymbol cd="latexml" id="A1.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p2.1.m1.1d">∼</annotation></semantics></math>1.1s per query when processing with a batch size of 50.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Dataset Details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">All corpus and datasets use English.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">For NQ and HotpotQA datasets in the open domain, we use the Wikipedia paragraphs corpus provided by the KILT benchmark
<cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib19" title="">2021</a>)</cite>.
For BioASQ, we use the PubMed Annual Baseline Repository for 2023 <cite class="ltx_cite ltx_citemacro_citep">(of Medicine, <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib18" title="">2023</a>)</cite>, where each passage is either a title or an abstract of PubMed papers.
Dataset sizes are in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.09040v2#A2.T2" title="Table 2 ‣ Appendix B Dataset Details ‣ RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">The Medline Corpus is from <cite class="ltx_cite ltx_citemacro_citet">of Medicine (<a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib18" title="">2023</a>)</cite> provided by the National Library of Medicine.</p>
</div>
<figure class="ltx_table" id="A2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.1.1.1">Corpus</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T1.1.1.1.2"># of par</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T1.1.1.1.3"># of doc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T1.1.1.1.4">Avg # of doc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T1.1.2.1.1">Wikipedia</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.1.2.1.2">111M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.1.2.1.3">5M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.1.2.1.4">18.9</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A2.T1.1.3.2.1">Medline</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T1.1.3.2.2">58M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T1.1.3.2.3">34M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T1.1.3.2.4">1.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Retrieval corpus information
</figcaption>
</figure>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">For NQ and HotpotQA, we use KILT’s dev set versions of the datasets, allowed under the MIT License <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib19" title="">2021</a>)</cite>.
For BioASQ <cite class="ltx_cite ltx_citemacro_citep">(Krithara et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09040v2#bib.bib13" title="">2023</a>)</cite>, we use Task 11B, distributed under <a class="ltx_ref ltx_href" href="http://participants-area.bioasq.org/datasets/" title="">CC BY 2.5 license</a>.</p>
</div>
<figure class="ltx_table" id="A2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A2.T2.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T2.1.1.1.2"># of Queries</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T2.1.2.1.1">NQ</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T2.1.2.1.2">2837</td>
</tr>
<tr class="ltx_tr" id="A2.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.1.3.2.1">HotpotQA</th>
<td class="ltx_td ltx_align_center" id="A2.T2.1.3.2.2">5600</td>
</tr>
<tr class="ltx_tr" id="A2.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T2.1.4.3.1">BioASQ</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T2.1.4.3.2">3837</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Dataset information</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Retriever Performance</h2>
<figure class="ltx_table" id="A3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T3.14" style="width:199.5pt;height:220.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.9pt,42.9pt) scale(0.719413057468663,0.719413057468663) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T3.14.14">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T3.14.14.15.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="A3.T3.14.14.15.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A3.T3.14.14.15.1.1.1">Retriever</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="A3.T3.14.14.15.1.2"><span class="ltx_text ltx_font_bold" id="A3.T3.14.14.15.1.2.1">Recall@k</span></td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.16.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.4">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.5">20</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.14.14.16.2.6">50</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.17.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="A3.T3.14.14.17.3.1" style="background-color:#E6EFC7;"><span class="ltx_text ltx_font_italic" id="A3.T3.14.14.17.3.1.1" style="background-color:#E6EFC7;">NQ</span></th>
</tr>
<tr class="ltx_tr" id="A3.T3.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.6.6.6.7" rowspan="2"><span class="ltx_text" id="A3.T3.6.6.6.7.1">BM25</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.1.1.1.1">  2.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.2.2.2.2">  4.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.3.3.3.3">  8.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.4.4.4.4">  11.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.5.5.5.5">  16.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.6.6.6.6">  22.8</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.18.4">
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.18.4.1">10.3</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.18.4.2">16.3</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.18.4.3">27.8</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.18.4.4">36.8</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.18.4.5">47.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.T3.14.14.18.4.6">53.2</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.19.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.14.14.19.5.1" rowspan="2"><span class="ltx_text" id="A3.T3.14.14.19.5.1.1">ColBERT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.2">12.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.3">18.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.4">25.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.5">32.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.6">38.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.14.14.19.5.7">41.8</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.20.6">
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.20.6.1">27.2</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.20.6.2">38.8</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.20.6.3">54.4</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.20.6.4">65.0</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.20.6.5">72.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.T3.14.14.20.6.6">77.2</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.21.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="A3.T3.14.14.21.7.1" style="background-color:#E6EFC7;"><span class="ltx_text ltx_font_italic" id="A3.T3.14.14.21.7.1.1" style="background-color:#E6EFC7;">HotpotQA</span></th>
</tr>
<tr class="ltx_tr" id="A3.T3.12.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.12.12.12.7" rowspan="2"><span class="ltx_text" id="A3.T3.12.12.12.7.1">BM25</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.7.7.7.1">  19.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.8.8.8.2">  25.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.9.9.9.3">  34.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.10.10.10.4">  41.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.11.11.11.5">  46.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.12.12.12.6">  54.2</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.22.8">
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.22.8.1">23.3</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.22.8.2">31.2</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.22.8.3">42.7</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.22.8.4">52.1</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.22.8.5">59.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.T3.14.14.22.8.6">62.8</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.23.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.14.14.23.9.1" rowspan="2"><span class="ltx_text" id="A3.T3.14.14.23.9.1.1">ColBERT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.2">31.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.3">40.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.4">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.5">56.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.6">61.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.14.14.23.9.7">64.9</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.24.10">
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.24.10.1">34.2</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.24.10.2">44.7</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.24.10.3">56.3</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.24.10.4">63.6</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.24.10.5">69.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.T3.14.14.24.10.6">73.1</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.25.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="A3.T3.14.14.25.11.1" style="background-color:#E6EFC7;"><span class="ltx_text ltx_font_italic" id="A3.T3.14.14.25.11.1.1" style="background-color:#E6EFC7;">BioASQ</span></th>
</tr>
<tr class="ltx_tr" id="A3.T3.13.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.13.13.13.2" rowspan="2"><span class="ltx_text" id="A3.T3.13.13.13.2.1">BM25</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.13.13.13.1">  8.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.13.13.13.3">12.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.13.13.13.4">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.13.13.13.5">25.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.13.13.13.6">33.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.13.13.13.7">37.8</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.26.12">
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.26.12.1">12.4</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.26.12.2">16.4</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.26.12.3">23.9</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.26.12.4">30.6</td>
<td class="ltx_td ltx_align_center" id="A3.T3.14.14.26.12.5">38.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.T3.14.14.26.12.6">43.6</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A3.T3.14.14.14.2" rowspan="2"><span class="ltx_text" id="A3.T3.14.14.14.2.1">ColBERT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.14.1">  8.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.14.3">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.14.4">20.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.14.5">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.14.14.14.6">34.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T3.14.14.14.7">38.6</td>
</tr>
<tr class="ltx_tr" id="A3.T3.14.14.27.13">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.1">14.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.2">18.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.3">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.4">32.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.5">39.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T3.14.14.27.13.6">44.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>For the Wikipedia-based dataset, the top row indicates recall@k at the retrieval unit of Wikipedia paragraph and the bottom row for the unit of Wikipedia page. For BioASQ, the top row indicates recall@k at the unit of title or abstract of a PubMed article and the bottom row at the unit of the article itself.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Reader Results</h2>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="599" id="A4.F9.g1" src="x9.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>HotpotQA results when there is sufficient information (all gold passages) included in the top-k passages to answer the question. For multi-hop questions, we select examples retrieved with all gold passages within the top-<math alttext="k" class="ltx_Math" display="inline" id="A4.F9.2.m1.1"><semantics id="A4.F9.2.m1.1b"><mi id="A4.F9.2.m1.1.1" xref="A4.F9.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.F9.2.m1.1c"><ci id="A4.F9.2.m1.1.1.cmml" xref="A4.F9.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.F9.2.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="A4.F9.2.m1.1e">italic_k</annotation></semantics></math> passages since all passages are necessary to answer the question.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="600" id="A4.F10.g1" src="x10.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>BioASQ results when there is sufficient information (at least one gold passage) included in the top-k passages to answer the question.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="661" id="A4.F11.g1" src="x11.png" width="823"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>HotpotQA results when there are no gold passages included in the top-k passages to answer the question.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="658" id="A4.F12.g1" src="x12.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>BioASQ results when there are no gold passages included in the top-k passages to answer the question.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 12 17:06:48 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
