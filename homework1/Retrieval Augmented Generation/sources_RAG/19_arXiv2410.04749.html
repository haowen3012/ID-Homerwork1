<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies</title>
<!--Generated on Mon Oct  7 04:57:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04749v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S1" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S2" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S2.SS1" title="In 2 Related Work ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Natural Language Explanation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S2.SS2" title="In 2 Related Work ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S2.SS3" title="In 2 Related Work ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Knowledge Graph</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S2.SS4" title="In 2 Related Work ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3.SS1" title="In 3 Methodology ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Pathology Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3.SS2" title="In 3 Methodology ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Knowledge Graph Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3.SS3" title="In 3 Methodology ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Vision Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3.SS4" title="In 3 Methodology ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S3.SS5" title="In 3 Methodology ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Projector</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S4" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S4.SS1" title="In 4 Experiment ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S4.SS2" title="In 4 Experiment ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS1" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison with Other Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS2" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison of different LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS3" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Impact of Different RAG Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS4" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Effect of the Number of Retrieved Knowledge Graph Triplets (K) on NLE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS5" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Comparison of Uni-modal and Cross-modal Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.SS6" title="In 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Qualitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S6" title="In LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ameer Hamza, Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim
<br class="ltx_break"/>Kyung Hee University, Republic of Korea 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Dr. S.T. Kim is the corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models’ insufficient domain-specific medical knowledge and privacy concerns associated with retrieval-based augmentation techniques. To address these issues, we propose a novel Vision-Language framework augmented with a Knowledge Graph (KG)-based datastore, which enhances the model’s understanding by incorporating additional domain-specific medical knowledge essential for generating accurate and informative NLEs. Our framework employs a KG-based retrieval mechanism that not only improves the precision of the generated explanations but also preserves data privacy by avoiding direct data retrieval. The KG datastore is designed as a plug-and-play module, allowing for seamless integration with various model architectures. We introduce and evaluate three distinct frameworks within this paradigm: KG-LLaVA, which integrates the pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are validated on the MIMIC-NLE dataset, where they achieve state-of-the-art results, underscoring the effectiveness of KG augmentation in generating high-quality NLEs for thoracic pathologies.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, natural language processing (NLP) has witnessed the development of numerous models trained on vast amounts of general domain data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>
While these models exhibit remarkable capabilities across various tasks, they often lack the specialized knowledge required for domain-specific applications, such as generating Natural Language Explanations (NLEs) for thoracic pathologies. This limitation is particularly pronounced in the medical domain, where accurate and contextually relevant explanations are crucial for diagnostic decision-making.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To bridge this gap, including Pre-Training, Fine-Tuning, and Retrieval-Augmented Generation (RAG) methods has been explored. One popular approach among these strategies is Pre-Training large models on medical data.
However, this strategy necessitates extensive data and substantial computational resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. In the medical domain, publicly available datasets are often limited, and concerns about data privacy, authenticity, and potential data leakage persist. Even when models are pre-trained on medical data, they frequently struggle with task-specific performance and may suffer from reduced factual accuracy. Moreover, pre-training these models is both expensive and time-consuming, and subsequent fine-tuning is often required to adapt them to specific downstream tasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Fine-tuning is another common strategy, where general domain models are adapted directly to medical tasks by training on specialized datasets. While this approach can be effective, it is hampered by the scarcity of high-quality medical datasets and the need to protect patient privacy and data security. Additionally, fine-tuned models can be prone to hallucination, generating explanations that lack factual correctness. Recent advancements in parameter-efficient fine-tuning, such as training low-rank adapters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, have aimed to reduce computational costs while maintaining model performance, yet challenges remain, particularly in maintaining the model’s generalization across diverse tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The third strategy, RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, has gained traction as a method for enhancing general domain models with domain-specific knowledge dynamically. In this approach, models are fine-tuned on task-specific data while being supplemented with relevant information retrieved from a datastore. RAG methods have shown promising results in maintaining factual accuracy and reducing hallucination risks. However, the effectiveness of RAG is highly dependent on the quality of the retrieval mechanism. Also, in the medical domain, concerns about data privacy are amplified, as retrieved information might still be traceable to individual patients, even after de-identification, thus posing a risk of data leakage.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To overcome these challenges, we propose a novel approach that combines the strengths of vision-language models with a Knowledge Graph (KG)-based retrieval system. Our method, KG-based Retrieval-Augmented Generation (KG-RAG), addresses privacy risks by abstracting patient-specific details and providing models with more relevant and factual information tailored to individual cases. The KG-based datastore serves as a robust source of domain-specific knowledge, enabling the generation of accurate and contextually appropriate NLEs for thoracic pathologies.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">KG-RAG emulates the cognitive process of radiologists, who rely on extensive experience and domain-specific knowledge to formulate diagnostic explanations. By leveraging a KG-based datastore tailored to each patient case, our approach significantly enhances the model’s performance in generating precise and informative explanations. To demonstrate the versatility and effectiveness of our method, we integrated KG-RAG into three distinct frameworks: KG-LLaVA, Med-XPT, and Bio-LLaVA.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">KG-LLaVA integrates the pre-trained LLaVA model with our KG-RAG module, fine-tuning it on our dataset to enrich its ability to generate detailed and accurate explanations by leveraging the CLIP ViT-L vision model. Med-XPT is a custom-built framework combining MedCLIP as the vision encoder, a transformer-based projector, and GPT-2 as the language model, trained from scratch on the MIMIC-NLE dataset to fully exploit the domain-specific knowledge provided by the KG-RAG module. Lastly, Bio-LLaVA adapts the LLaVA model by replacing the vision encoder with Bio-ViT-L, a model tailored for biomedical tasks, and modifying the projection layer to accommodate the unique feature dimensions of Bio-ViT-L. This framework was trained exclusively on the MIMIC-NLE dataset, showcasing its ability to generate precise NLEs without relying on pre-trained projector weights.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">This integration of advanced vision-language models with a domain-specific KG not only provides transparent and comprehensible reasoning for detected abnormalities but also elevates the model’s diagnostic accuracy. Our approach underscores the potential of combining state-of-the-art machine learning techniques with domain-specific knowledge to achieve expert-level reasoning, thereby improving the interpretability and accuracy of diagnostic outcomes in chest X-ray images.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Our main contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose the first KG retrieval-augmented Vision-Language Model (VLM) framework specifically designed for generating NLEs for thoracic pathologies. This approach integrates domain-specific medical knowledge into the explanation generation process, enhancing the accuracy and relevance of the outputs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our method addresses critical privacy concerns associated with medical data by abstracting patient-specific details through the use of a KG-based datastore. Furthermore, the proposed method is designed as a plug-and-play module, making it easily adaptable to existing radiology tasks and compatible with previous methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We validate the effectiveness of our approach by achieving state-of-the-art results on a benchmark dataset, MIMIC-NLE. Our method outperforms previous models, demonstrating the robustness and applicability of the KG-augmented framework in the medical domain.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="306" id="S1.F1.g1" src="x3.png" width="942"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Overview of the KG-LLaVA framework with integrated Knowledge Graph Retrieval Augmented Generation (KG-RAG) module. The framework combines a pre-trained LLaVA model with a CLIP ViT-L vision encoder to extract visual features, which are then projected into the language model’s embedding space. The KGR module uses MedCLIP to map input images to a shared latent space and retrieve relevant KG triplets via the FAISS library. These triplets provide domain-specific context that enhances the generation of accurate and informative NLEs for thoracic pathologies. The modular design allows for seamless integration with other architectures, such as Med-XPT and Bio-LLaVA, ensuring flexibility and adaptability across different vision-language tasks.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Natural Language Explanation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">NLEs provide textual interpretations of deep learning model predictions, aiming to offer accessible and comprehensible insights for users, particularly in complex domains like medical diagnostics. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">Hendricks et al.</span></a></cite> was the first to introduce the NLE task. This task was later extended to encompass the vision-language domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">Kayser et al.</span></a></cite> introduced the MIMIC-NLE dataset, derived from the MIMIC-CXR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, to advance interpretability and accessibility in the context of chest X-ray analysis. This dataset is currently the only publicly available resource specifically designed for generating NLEs related to chest X-rays.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In their work, <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">Kayser et al.</span></a></cite> also introduced benchmark methods for generating explanations, such as DPT (DenseNet-121 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> combined with GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>) and RATCHET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. While GPT-2 has demonstrated effective performance in general domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, it shows limitations when applied to medical NLE generation due to its reliance on commonsense knowledge, which is often insufficient for specialized medical contexts. The DPT model, in particular, struggled with generating accurate explanations for chest X-ray images due to its dependency on non-specialized knowledge sources. The MIMIC-NLE dataset provides explanations for predicted pathologies, making it a comprehensive resource for evaluating the quality of NLEs in medical imaging. Our approach demonstrates a substantial improvement over previous methods, underscoring the value of integrating domain-specific knowledge through KG augmentation in the generation of NLEs. Also, <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">Rio-Torto et al.</span></a></cite> have investigated parameter-efficient training techniques for the NLE task.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vision-Language Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The emergence of advanced Large Language Models (LLMs) like LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> has showcased significant improvements in text generation capabilities. Building on these developments, researchers have increasingly focused on extending these models to handle multimodal inputs, such as visual data. Despite these efforts, fully integrating visual and textual modalities remains a challenging endeavor, particularly in areas such as understanding spatial relationships, mathematical reasoning, and counting. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">Bordes et al.</span></a></cite> categorize VLMs into four primary categories, contrastive training, masking, pre-trained backbones, and generative vision-language models.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">VLMs in the pre-trained backbone category often utilize open-source LLMs, such as LLaMA/Viccuna, to learn mappings between a pre-trained image encoder and the LLM. This approach is computationally efficient, as it avoids training both text and image encoders from scratch, instead focusing on aligning the representations generated by these pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Our work falls within the category of VLMs based on pre-trained backbones. Specifically, we employ an LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> model which is based on pre-trained LLM alongside a pre-trained vision model, connected via a Projector that learns to map the visual information to the language model space. This approach leverages the strengths of pre-trained vision and language models.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Visual Instruction Tuning has been effectively employed in models like LLAVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, where a pre-trained LLM is trained on visual inputs provided by a vision model using carefully curated instruction datasets. This process typically involves mapping image features from pre-trained models like CLIP into the LLM’s embedding space. While more sophisticated projection methods, such as lightweight transformers in CLIPCap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> and the Bert-based Q-Former in BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, have been explored, the linear layer approach remains a popular choice due to its simplicity and efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Knowledge Graph</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">A KG represents relationships between a set of entities, offering a structured approach to capturing and utilizing interconnected information. In the radiology domain, the RadGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> method introduces a pioneering approach to constructing KGs from medical reports, providing a systematic means of extracting and representing clinically relevant information. RadGraph not only defines the methodology for creating these graphs but also supplies a publicly available dataset, which significantly aids research and development in this area.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Retrieval Augmented Generation</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> represents a significant advancement in enhancing language models beyond the capabilities achieved through traditional supervised fine-tuning. In RAG, external knowledge is dynamically retrieved and integrated into the language model, thereby augmenting its generative capabilities with new information that the model has not been explicitly trained on. This approach is particularly valuable when addressing dynamic or domain-specific knowledge, such as medical information, where up-to-date and specialized content is crucial.
However, as identified by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, the use of RAG in sensitive domains like healthcare raises substantial security concerns. Specifically, there is a risk that malicious actors could exploit RAG systems by prompting the model to retrieve and expose sensitive information from the retrieval datastore, potentially leading to privacy breaches. This vulnerability underscores the necessity for more secure implementations of RAG, particularly in the context of handling confidential medical data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this paper, we present a novel approach for generating NLEs via KG-RAG for thoracic pathologies. Our methodology utilizes medical vision models in conjunction with large language models, resulting in three distinct frameworks: KG-LLaVA, Med-XPT, and Bio-LLaVA, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">1</span></a>. Each framework is designed to leverage the strengths of KG-RAG in enhancing the accuracy and contextual relevance of NLEs in the medical domain.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pathology Classification</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For the task of pathology classification, which entails predicting the presence of pathologies in X-ray images along with their associated certainty levels, we adopted a methodology based on the approach outlined by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>. This method involves the prediction of 10 distinct pathologies, each categorized into three certainty levels—negative, uncertain, or positive—using the UMultiClass strategy introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>. The UMultiClass strategy is specifically designed to address the multi-label nature of pathology classification by effectively categorizing each pathology according to its likelihood of presence.
In our work, we used the method employed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>. Specifically, we process the visual features extracted from the medical vision model through a multi-layer perceptron (MLP). This modification is intended to enhance the model’s capability to interpret and classify the visual features of the X-ray images, thereby improving the accuracy of both the pathology predictions and their corresponding certainty levels.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Knowledge Graph Retrieval</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To address the risks highlighted in Section 2, we propose a KG-based RAG approach as a more secure alternative. Unlike traditional RAG systems, which may retrieve information that could be traced back to specific patient data, our approach employs a KG composed of general medical terms, entities, and their interrelationships. This structured representation of knowledge abstracts away direct patient-specific details, significantly reducing the risk of inadvertently exposing sensitive information.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In our framework, we augment the model with knowledge instances retrieved from a carefully constructed datastore, conditioning the model to generate responses based on this securely augmented information. This method not only enhances the model’s performance by providing relevant and contextual knowledge but also ensures the protection of sensitive medical data.
To enable effective knowledge retrieval, we constructed a datastore comprising KG triplets derived from the MIMIC-CXR training set <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> using RadGraph model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. We specifically utilized only the triplets with a ’suggestive_of’ relationship, as these triplets are more directly relevant to explaining the presence of pathologies. These triplet embeddings were generated using a medical CLIP model and are exclusively stored in the datastore, deliberately excluding any image features. This design facilitates a cross-modal retrieval process, where images can be used to query and retrieve relevant text-based knowledge.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For each query image, visual features are extracted using the medical CLIP model, which is designed to map both visual and triplet features into a unified feature space. This unified space allows for the effective retrieval of relevant triplets based on the visual features extracted from the datastore. The retrieval process is conducted by calculating the cosine similarity between the visual features of the query image and the stored triplet embeddings. This enables the selection of the top-k most similar triplets from the KG datastore, ensuring that the retrieved knowledge is closely aligned with the visual content of the query image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Vision Models</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In our proposed framework, we employ two distinct vision models tailored for different components of the system. First, we utilize the MedCLIP model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, which serves dual purposes. It is integrated with the DPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> framework for NLE generation and is also instrumental in constructing the KGR datastore. MedCLIP’s robust capabilities enable effective retrieval of relevant information based on image features, ensuring that the retrieved knowledge aligns with the visual content of medical images.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">For the KGR process within our LLaVA-based framework, we again utilize MedCLIP to perform the retrieval itself based on the image features. However, for the extraction of visual features and their subsequent projection into the language embedding space, we employ the ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> CLIP model as the vision encoder. The ViT-L CLIP model extracts visual features from input images, which are then projected into the language embedding space through a trainable projection matrix. Specifically, a simple linear layer is applied to convert the visual features into language embedding tokens, facilitating seamless integration with the language model. This dual-model approach allows us to leverage the specific strengths of both MedCLIP and ViT-L/14 within the framework, optimizing the generation of NLEs and ensuring accurate and contextually relevant retrieval of knowledge.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Language Models</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">For the decoding mechanism, we integrated language models such as GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> and LLaMA/Viccuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, known for their effectiveness in natural language generation (NLG) tasks. To address the dimensional incompatibility between the CLIP encoder outputs and the language model input requirements, we employed a projector.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Projector</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The GPT-2 based model uses a transformer-based projector, while the LLaMA/Viccuna-based model uses an MLP-based projector. The projector aligns the image embeddings with the features required by the Language Model (LM). We feed the LM with pathologies and their certainty levels (uncertain, positive), along with the retrieved knowledge. These elements are integrated into a structured prompt template. This prompt is then fed into the decoder, which generates NLEs conditioned on the image features, pathologies, and retrieved knowledge. This process ensures the generation of accurate and relevant NLEs tailored to each specific medical case, combining visual data and textual information effectively.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For our study, we utilized the MIMIC-NLE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, which is derived from MIMIC-CXR chest X-ray dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. This is currently the only publically available dataset for chest X-ray NLEs. The MIMIC-NLE dataset includes diagnoses, evidence labels, and corresponding NLEs for those diagnoses. For a detailed description of the dataset, please refer to the comprehensive overview provided in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>. The dataset consists of 38,003 NLEs and is divided into training, validation, and testing subsets, containing 37,016, 273, and 714 entries, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The visual instruction tuning approach is employed in both KG-LLaVA and Bio-LLaVA, where it enhances the model’s instruction-following abilities and generalization performance across various medical imaging tasks. However, instruction tuning was not applied to the Med-XPT framework, which focuses instead on leveraging its custom architecture for generating NLEs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For our study, we constructed an instruction-format dataset based on the LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> framework, with specific modifications to the prompts. These modifications were designed to tailor the questions to elicit detailed explanations from the model regarding the reasoning behind the occurrence of specific pathologies observed in the images. The following questions were incorporated into our prompts to guide the model in generating explanations:
<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">Which signs show that the patient has pathologies?,
Explain why these pathologies are present in the image?,
What evidence in the image indicates pathologies?,
How can you tell that the patient has pathologies from the image?,
What features suggest the presence of pathologies in this image?.</span>
These tailored prompts were crucial in enhancing the model’s ability to provide reasoning for the presence of specific pathologies based on the visual evidence present in the images.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our proposed frameworks, KG-LLaVA, Med-XPT, and Bio-LLaVA, each incorporate a KGR module to enhance their performance in generating NLEs for thoracic pathologies. KG-LLaVA builds upon the pre-trained LLaVA model, integrating the KGR module to leverage domain-specific knowledge derived from the input image. Med-XPT is trained from scratch, combining MedCLIP as the vision encoder with a transformer-based projector and GPT-2 as the language model, while incorporating the KG module to enrich the generation process. Bio-LLaVA adapts the LLaVA model by replacing the vision encoder with Bio-ViT-L and modifying the projection layer to accommodate the unique feature dimensions, also integrating the KG module to improve model performance. These frameworks were trained on the MIMIC-NLE dataset, allowing us to evaluate the effectiveness of the KGR module across different architectures.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The KGR process is powered by the MedCLIP model, which projects the input image into a shared latent space. This projected representation is then used to retrieve corresponding triplets from the KG using the FAISS library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, which employs a k-nearest neighbors algorithm to identify the most relevant triplets. These retrieved triplets serve as supplementary information, enriching the input to the language model and improving the model’s ability to generate accurate and contextually relevant explanations.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:405.0pt;height:172.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.3pt,12.9pt) scale(0.87,0.87) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.2">AUC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.3">B4</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.4">MET.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.5">R.L.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1.6">CIDEr</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.2.1.2.1.1">RATCHET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.2">66.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.3">4.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.1.2.1.4">14.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.5">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2.1.6">37.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.3.2.1">TieNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.3.2.2">64.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.3.2.3">3.5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.3.2.4">12.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.3.2.5">19.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.3.2.6">33.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.4.3.1">DPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.3.2">62.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.3.3">2.4</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.4.3.4">11.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.3.5">15.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.3.6">17.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.5.4.1">LoRA AE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.4.2">63.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.4.3">4.0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.5.4.4.1">15.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.4.5">20.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.4.6">24.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.6.5.1">Prompt AE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.5.2">61.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.5.3">3.7</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.6.5.4">14.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.5.5">19.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.5.6">23.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.7.6.1">Prefix AE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.6.2">65.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.6.3">3.7</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.7.6.4">14.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.6.5">19.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.7.6.6">21.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.8.7.1">LLaMA-Adapt AE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.8.7.2">63.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.8.7.3">4.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.8.7.4">14.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.8.7.5">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.8.7.6">29.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.9.8.1">+ multi-modal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.8.2">64.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.8.3">3.0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.9.8.4">14.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.8.5">18.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.9.8.6">18.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.1.10.9.1">+ MSE loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.9.2">62.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.9.3">2.0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.1.10.9.4">10.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.9.5">14.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.10.9.6">14.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.1.11.10.1">KG-LLaVA</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.11.10.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.11.10.2.1">83.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.11.10.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.11.10.3.1">7.2</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.2.1.11.10.4">15.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.11.10.5.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.11.10.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.11.10.6.1">62.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Comparison of our KG-LLaVA framework between other baselines on the MIMIC-NLE test set, focusing on NLG metrics. The metrics include Area Under the Curve (AUC), BLEU-4 (B4), METEOR (MET.), ROUGE-L (R.L.), and CIDEr scores. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Training Setup.</span>
We trained the LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> model with the integrated KGR module on the MIMIC-NLE dataset. The training process involved fine-tuning the model to maximize the likelihood of generating high-quality NLEs using the additional knowledge provided by the KG.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">The training was conducted with LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> (Low-Rank Adaptation) for training efficiency. We used the pre-trained LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> model as the baseline, with a learning rate of 2e-4, and employed the cosine learning rate scheduler. The training was performed over 5 epochs with a batch size of 8 per device and a gradient accumulation of 2 steps. The vision encoder used was CLIP ViT-L <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, and the multimodal projector was an MLP with two layers using the GELU activation function.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">We also employed techniques such as gradient checkpointing and mixed precision to optimize memory usage and training speed. The maximum sequence length for the model was set to 2048 tokens to accommodate the large inputs from both text and image modalities. The model was trained on the MIMIC-NLE dataset, and the training data was preprocessed lazily to streamline the process.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">In addition to our KG-LLaVA framework, we also applied our KGR module to Med-XPT architecture, which utilizes MedCLIP as the vision encoder, a transformer-based projector, and GPT-2 as the language model. We followed a similar structured training approach. We again used the FAISS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> library for retrieval based on image features. The training script included several key parameters: the visual encoder (medclip-vit) was used to retrieve captions stored in a JSON file, with the retrieval results incorporated into the input prompts. The attention mechanism within the model was tuned using a cross-attention size of 7, and we enabled the training of the decoder along with the attention mechanism. The training process spanned 15 epochs with a learning rate of 1e-4 and a batch size of 8, utilizing gradient accumulation with one step. The retrieved captions and templates were dynamically integrated into the training pipeline, which facilitated the model’s ability to generate accurate NLEs based on the retrieved knowledge.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:201.8pt;height:71.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.0pt,0.4pt) scale(0.99,0.99) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.2.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.2">B4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.3">MET.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.4">R.L.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.5">CIDEr</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.2.1.1">Bio-LLaVA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.2">5.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.1.2.1.3.1">14.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.1.2.1.4.1">23.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.5">46.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.3.2.1">Med-XPT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.1.3.2.2.1">7.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.3">11.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.4">22.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.3.2.5.1">62.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.2.1.4.3.1">KG-LLaVA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.4.3.2.1">7.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.4.3.3.1">15.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.4.3.4.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.4.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.1.4.3.5.1">62.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Performance comparison of our proposed frameworks—Bio-LLaVA, Med-XPT, and KG-LLaVA—on the MIMIC-NLE test set, focusing on NLG metrics. All frameworks incorporate KG-RAG module. Evaluation metrics include BLEU-4 (B4), METEOR (MET.), ROUGE-L (R.L.), and CIDEr, scores.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">Lastly, for the Bio-LLaVA framework, which incorporates the Bio-ViT-L model as the vision encoder, we customized the projection layer to handle the unique feature dimensions of Bio-ViT-L. The training of Bio-LLaVA followed the same structured methodology, emphasizing the integration of KG triplets to enhance model performance.</p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">Our training methodology across these frameworks—KG-LLaVA, Med-XPT, and Bio-LLaVA—underscores the flexibility and effectiveness of the KGR module, which consistently improves model performance by leveraging structured, domain-specific knowledge in both vision-language tasks and NLE generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussions</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with Other Methods</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this study, we evaluated the performance of our proposed KG-LLaVA framework against several well-established methods, including RATCHET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, TieNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, and DPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, using the MIMIC-NLE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> dataset. The results, as summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S4.T1" title="Table 1 ‣ 4.2 Training ‣ 4 Experiment ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">1</span></a>, clearly demonstrate that KG-LLaVA outperforms the previous methods across a range of evaluation metrics.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">KG-LLaVA achieves an AUC of 83.0, which is significantly higher than the AUC scores reported for RATCHET (66.4), TieNet (64.6), and DPT (62.5). This substantial improvement underscores the effectiveness of our approach in accurately classifying and generating relevant explanations for thoracic pathologies. Additionally, KG-LLaVA excels in key NLG metrics, including BLEU-4 (7.2), ROUGE-L (25.0) and CIDEr (62.2), highlighting its ability to generate high-quality, contextually accurate explanations.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Notably, while KG-LLaVA slightly outperforms RATCHET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> in METEOR (15.1 vs. 14.1), the overall superior performance of KG-LLaVA across the other metrics underscores the strength of incorporating KG-RAG module into a vision-language framework. These results suggest that KG-LLaVA has the potential to set a new benchmark for generating NLEs in medical imaging tasks, particularly for diagnosing thoracic pathologies.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">While <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> methods focused on optimizing model parameters, our approach leveraged the KG-RAG module and effective use of LoRA for fine-tuning, achieving superior performance without compromising the model’s complexity or parameter efficiency. This makes KG-LLaVA not only the best-performing model but also a robust and scalable solution for medical NLE generation.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.2" style="width:206.0pt;height:101.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.0pt,12.3pt) scale(0.804390151695281,0.804390151695281) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.2.1.1.1.1">Methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.1.2">RAG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.1.3">B4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.1.4">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.1.5">R-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.1.6">CIDEr</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.1.2.1.1">Med-XPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1.3">2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1.4">7.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1.5">12.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1.6">17.4</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.1.3.2.1">KG-LLaVA</th>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.3.2.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.3.2.3">7.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.3.2.4">15.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.3.2.5">24.4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.3.2.6">60.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.1.4.3.1">Med-XPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.4.3.2">NLE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.4.3.3">6.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.4.3.4">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.4.3.5">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.4.3.6">59.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.1.5.4.1">KG-LLaVA</th>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4.2">NLE</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4.3">6.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4.4">15.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4.5">24.6</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4.6">58.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.1.6.5.1">Med-XPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.6.5.2">KG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.6.5.3">7.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.6.5.4">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.6.5.5">22.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.5.6.1">62.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.2.1.7.6.1">KG-LLaVA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.7.6.2">KG</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.7.6.3.1">7.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.7.6.4.1">15.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.7.6.5.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.7.6.6">62.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Comparative analysis of the performance of Med-XPT, and KG-LLaVA across different RAG methods and without any RAG. The table includes results for NLG metrics such as BLEU-4 (B4), METEOR, ROUGE-L (R-L), and CIDEr. The "-" row shows results without any RAG integration, the "NLE" row represents results with natural language explanation-based RAG, and the "KG" row reflects the performance when the knowledge graph retrieval module is used.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison of different LLMs</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We further assessed the performance of our three proposed frameworks—Bio-LLaVA, Med-XPT, and KG-LLaVA all of which incorporate the KG-RAG module. The results, detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S4.T2" title="Table 2 ‣ 4.2 Training ‣ 4 Experiment ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">2</span></a>, provide insights into the comparative strengths of each framework in generating NLEs for thoracic pathologies.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">KG-LLaVA demonstrates the highest overall performance, leading in BLEU-4 (7.2), METEOR (15.1), and ROUGE-L (25.0). These results reflect its superior ability to generate accurate and contextually rich explanations. Med-XPT, on the other hand, performs exceptionally well in CIDEr (62.7), indicating its effectiveness in capturing the diversity and richness of language necessary for high-quality NLEs. Bio-LLaVA, while slightly behind in some metrics, still shows strong performance in METEOR (14.3) and ROUGE-L (23.0).
These findings underscore the flexibility and efficacy of the KG-RAG module across different architectures. The variability in performance across the frameworks suggests that the choice of architecture can be optimized based on specific aspects of the NLE task, such as accuracy, linguistic richness, or diversity in the generated explanations.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Impact of Different RAG Methods</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Finally, we conducted a detailed comparison of the two frameworks—Med-XPT, and KG-LLaVA—across various configurations: without any Retrieval Augmented Generation (RAG), with standard NLE, and with our proposed KG Retrieval module. The results, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.T3" title="Table 3 ‣ 5.1 Comparison with Other Methods ‣ 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">3</span></a>, illustrate the impact of different RAG methods on the performance of these frameworks in generating accurate and contextually rich NLEs for thoracic pathologies.
In the NLE configuration, where standard NLEs are generated without KG enhancement, both Med-XPT and KG-LLaVA exhibit strong performance, with KG-LLaVA slightly leading in most metrics. This indicates that while both frameworks leverage their respective architectures effectively, the pre-training knowledge embedded in KG-LLaVA likely contributes to its superior performance.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Despite the significantly smaller size and less extensive training of the GPT-2 language model compared to LLaVA, Med-XPT—which utilizes GPT-2—achieves competitive performance, particularly when augmented with the KG and NLE-based RAG methods. This suggests that the incorporation of additional domain-specific knowledge can effectively compensate for the limitations of smaller models, enabling them to generate high-quality explanations.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Notably, when comparing Med-XPT to the DPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> framework, which also uses GPT-2 but without the benefit of these advanced RAG methods, we observe a substantial improvement in performance. DPT’s lower scores across the board highlight the critical role that the KG and NLE-based RAG play in enhancing the model’s explanatory capabilities. This further emphasizes that even smaller models, when equipped with the right augmentation techniques, can perform on par with larger, more complex models like LLaVA.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">The most significant improvements are observed in the KG configuration, where the KGR module is employed. KG-LLaVA leads in BLEU-4, ROUGE-L, and METEOR metrics, while Med-XPT excels in CIDEr scores. This demonstrates the effectiveness of the KG-RAG module in enhancing the richness and contextual relevance of the generated explanations, particularly in KG-LLaVA.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">In addition to the strong performance metrics, it’s important to highlight that KG-LLaVA addresses critical privacy concerns by abstracting patient-specific details through a KG-based datastore. While KG-LLaVA may have slightly lower scores on some metrics compared to other models, its ability to safeguard data security and prevent data leakage makes it a valuable solution in medical AI applications, offering a robust balance between performance and privacy preservation.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="164" id="S5.F2.g1" src="x4.png" width="411"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S5.F2.3.2" style="font-size:90%;">Comparison of NLEs generated by different models—KG-LLaVA, Med-XPT, and Bio-LLaVA—against the ground truth (GT) for a specific thoracic pathology case. The image depicts a chest X-ray used as input, with the corresponding NLEs. KG-LLaVA accurately matches the GT by identifying the underlying abnormalities, while Bio-LLaVA and Med-XPT offer alternative interpretations, reflecting the models’ varying strengths and limitations in clinical reasoning.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Effect of the Number of Retrieved Knowledge Graph Triplets (K) on NLE</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We investigate the effect of the number of retrieved knowledge graph triplets (K) on NLE. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.T4" title="Table 4 ‣ 5.5 Comparison of Uni-modal and Cross-modal Retrieval ‣ 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">4</span></a>, as K increases from 1 to 7, the model demonstrates consistent performance across BLEU-4, METEOR, and ROUGE-L, with the highest CIDEr score observed at K=7. This suggests that retrieving more triplets enhances the richness and relevance of the generated explanations, particularly when the complexity of the input increases. However, the slight fluctuations in other metrics indicate that there is an optimal balance to be struck between the amount of retrieved knowledge and its utility in generating precise explanations.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Comparison of Uni-modal and Cross-modal Retrieval </h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.T5" title="Table 5 ‣ 5.5 Comparison of Uni-modal and Cross-modal Retrieval ‣ 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">5</span></a> shows the comparison of uni-modal retrieval and cross-modal retrieval (ours). The Cross-modal retrieval method demonstrates a clear advantage over the Uni-modal approach across all evaluation metrics, with a particularly notable improvement in CIDEr, where the score increases from 49.9 to 62.2. This substantial gain highlights the effectiveness of integrating both visual and textual modalities in the retrieval process, allowing the model to generate more contextually relevant and accurate Natural Language Explanations (NLEs). The Cross-modal approach enhances the model’s ability to interpret complex medical images, leading to higher-quality explanations that are more aligned with clinical expectations.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In contrast, the Uni-modal approach, which retrieves triplets by matching the input image with similar images in the datastore, shows comparatively lower performance. This image-to-image retrieval method, while effective, does not fully leverage the multimodal capabilities that Cross-modal retrieval offers. Moreover, storing images in the datastore raises significant privacy concerns, as it involves retaining patient-specific visual data, which could potentially be traced back to individual patients.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">Our method, by abstracting patient-specific details and operating directly within the latent space for image-to-text retrieval, significantly mitigates these privacy risks. This approach not only enhances performance but also aligns with stringent data privacy requirements, making it particularly suitable for clinical applications where data security is paramount. The ability to achieve superior results without compromising on privacy underscores the versatility and practicality of our KG-RAG framework, reinforcing its potential for broader adoption in healthcare settings. This experiment clearly validates the privacy-preserving design of our method, which is a critical contribution outlined in our work.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.3.2" style="font-size:90%;"> Effect of the Number of Retrieved Knowledge Graph Triplets (K) on NLE. We present the impact of varying the number of retrieved knowledge graph triplets (K) on the performance of the KG-LLaVA model in generating Natural Language Explanations (NLEs). The evaluation metrics include BLEU-4 (B4), METEOR, ROUGE-L (R-L), and CIDEr. </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T4.4" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.99,0.99) ;">
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S5.T4.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T4.5.1.1.1">K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.5.1.1.2">B4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.5.1.1.3">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.5.1.1.4">R-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.5.1.1.5">CIDEr</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.5.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.5.2.1.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.2.1.2">7.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.2.1.3">15.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.2.1.4">25.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.2.1.5">60.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.5.3.2.1">3</th>
<td class="ltx_td ltx_align_center" id="S5.T4.5.3.2.2">7.1</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.3.2.3">15.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.3.2.4">25.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.3.2.5">58.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.5.4.3.1">5</th>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.3.2">7.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.3.3">15.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.3.4">24.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.3.5">58.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T4.5.5.4.1">7</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.5.4.2">7.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.5.4.3">15.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.5.4.4">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.5.4.5">62.2</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.3.2" style="font-size:90%;">Comparison of Uni-modal and Cross-modal Retrieval on NLE Performance. We compare the performance of the KG-LLaVA model when using Uni-modal versus Cross-modal retrieval methods for generating Natural Language Explanations (NLEs).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.T5.4" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.99,0.99) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S5.T5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.5.1.1.1">K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.2">B4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.3">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.4">R-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.5">CIDEr</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.5.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.5.2.1.1">Uni-modal</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.2.1.2">5.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.2.1.3">14.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.2.1.4">23.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.2.1.5">49.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.5.3.2.1">Cross-modal</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.5.3.2.2">7.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.5.3.2.3">15.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.5.3.2.4">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.5.3.2.5">62.2</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Qualitative Results</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">The qualitative analysis of the generated NLEs from our proposed frameworks—KG-LLaVA, Bio-LLaVA, and Med-XPT—highlights distinct differences in their alignment with the ground truth (GT) as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04749v1#S5.F2" title="Figure 2 ‣ 5.3 Impact of Different RAG Methods ‣ 5 Results and Discussions ‣ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies"><span class="ltx_text ltx_ref_tag">2</span></a>. KG-LLaVA accurately replicates the GT by identifying the underlying infectious infiltrate, showcasing its strong alignment with expert annotations. In contrast, Bio-LLaVA introduces an alternative diagnosis, suggesting a new right lower lobe opacity possibly due to aspiration or pneumonia, which, while clinically plausible, diverges from the GT. Med-XPT incorrectly focuses on a right lower lobe opacity concerning consolidation, indicating challenges in precise localization and consistency. These findings underscore KG-LLaVA’s effectiveness in generating accurate NLEs, while also illustrating the flexibility and limitations of Bio-LLaVA and Med-XPT in clinical interpretation.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">Overall, these results highlight the significant impact of the KG-RAG module on improving model performance across different architectures. KG-LLaVA consistently shows strong results across all configurations, underscoring its potential as a leading framework for generating precise and contextually relevant NLEs in the medical imaging domain. The findings also suggest that the choice of RAG method plays a crucial role in determining the quality of NLEs, with KG-RAG offering the most substantial benefits including data security.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced a novel approach for generating NLEs for thoracic pathologies by integrating the KG-RAG module into vision-language models. Our KG-RAG framework effectively enhances the accuracy and contextual relevance of NLEs by incorporating domain-specific knowledge. Evaluated across three distinct frameworks—KG-LLaVA, Med-XPT, and Bio-LLaVA—our method consistently outperformed established models like RATCHET, TieNet, and DPT on the MIMIC-NLE dataset, highlighting the robustness and versatility of the KG-RAG approach.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Moreover, the inclusion of the KG-RAG module addresses critical privacy concerns by abstracting patient-specific details, thereby safeguarding data security and preventing data leakage. These findings underscore the critical role of integrating domain-specific knowledge in advancing vision-language models for medical imaging while ensuring the security and privacy of sensitive medical data. This approach sets a new benchmark for AI-driven diagnostics, paving the way for more transparent, accurate, and trustworthy healthcare systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Achiam et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">arXiv preprint arXiv:2303.08774</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Bordes et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">An introduction to vision-language modeling.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">arXiv preprint arXiv:2405.17247</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Brown et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 33:1877–1901, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Hendricks et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Generating visual explanations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.10.2" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14</em><span class="ltx_text" id="bib.bib4.11.3" style="font-size:90%;">, pages 3–19. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Hou et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Benjamin Hou, Georgios Kaissis, Ronald M Summers, and Bernhard Kainz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Ratchet: Medical transformer for chest x-ray diagnosis and reporting.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">MICCAI</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, pages 293–303, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Hu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Lora: Low-rank adaptation of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.9.1" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib6.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Huang et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Densely connected convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, pages 4700–4708, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Irvin et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">AAAI</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 590–597, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Jain et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Radgraph: Extracting clinical entities and relations from radiology reports.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Jiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.06825</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Johnson et al. [2019a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.9.1" style="font-size:90%;">arXiv:1901.07042</em><span class="ltx_text" id="bib.bib11.10.2" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Johnson et al. [2019b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Jeff Johnson, Matthijs Douze, and Hervé Jégou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Billion-scale similarity search with gpus.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">IEEE Transactions on Big Data</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 7(3):535–547, 2019b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Kayser et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">e-vil: A dataset and benchmark for natural language explanations in vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 1244–1254, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Kayser et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Maxime Kayser, Cornelius Emde, Oana-Maria Camburu, Guy Parsons, Bartlomiej Papiez, and Thomas Lukasiewicz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Explaining chest x-ray pathologies in natural language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.2" style="font-size:90%;">MICCAI</em><span class="ltx_text" id="bib.bib14.11.3" style="font-size:90%;">, pages 701–713, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Lewis et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Retrieval-augmented generation for knowledge-intensive nlp tasks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib15.10.2" style="font-size:90%;">, 33:9459–9474, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Li et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib16.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib17.11.3" style="font-size:90%;">, pages 19730–19742. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, pages 552–567, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Marasović et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A Smith, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">Findings of EMNLP</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Mokady et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Ron Mokady, Amir Hertz, and Amit H Bermano.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Clipcap: Clip prefix for image captioning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">arXiv preprint arXiv:2111.09734</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Park et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Multimodal explanations: Justifying decisions and pointing to the evidence.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib21.11.3" style="font-size:90%;">, pages 8779–8788, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Peng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Instruction tuning with gpt-4.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.03277</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Radford et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Language models are unsupervised multitask learners.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">OpenAI blog</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 1(8):9, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, pages 8748–8763, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Rio-Torto et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Isabel Rio-Torto, Jaime S Cardoso, and Luis Filipe Teixeira.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Parameter-efficient generation of natural language explanations for chest x-ray classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">Medical Imaging with Deep Learning</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Saab et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Capabilities of gemini models in medicine.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">arXiv preprint arXiv:2404.18416</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Touvron et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">arXiv preprint arXiv:2302.13971</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Wang et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, pages 9049–9058, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Medclip: Contrastive learning from unpaired medical images and text.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.1" style="font-size:90%;">EMNLP</em><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.4.4.1" style="font-size:90%;">Wu and Mooney [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.6.1" style="font-size:90%;">
Jialin Wu and Raymond J Mooney.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">Faithful multimodal explanation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.8.1" style="font-size:90%;">ACL BlackboxNLP workshop</em><span class="ltx_text" id="bib.bib30.9.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Zeng et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag).
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.16893</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Zhu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 04:57:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
