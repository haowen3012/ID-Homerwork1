<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation</title>
<!--Generated on Thu Aug  8 03:03:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.04187v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S1" title="In Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2" title="In Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1" title="In 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Medical Graph Construction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1.SSS0.Px1" title="In 2.1 Medical Graph Construction ‣ 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Semantic Document Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1.SSS0.Px2" title="In 2.1 Medical Graph Construction ‣ 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Element Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1.SSS0.Px3" title="In 2.1 Medical Graph Construction ‣ 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Hierarchy Linking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1.SSS0.Px4" title="In 2.1 Medical Graph Construction ‣ 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Relationship Linking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS1.SSS0.Px5" title="In 2.1 Medical Graph Construction ‣ 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Tags generation and merge the graphs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S2.SS2" title="In 2 Method ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieve from the graph</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3" title="In Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1" title="In 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS1" title="In 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>RAG data</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS1.Px1" title="In 3.1.1 RAG data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Top-level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS1.Px2" title="In 3.1.1 RAG data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Medium-level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS1.Px3" title="In 3.1.1 RAG data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Bottom-level</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS2" title="In 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>test data</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS2.Px1" title="In 3.1.2 test data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">PubMedQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS2.Px2" title="In 3.1.2 test data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">MedMCQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS1.SSS2.Px3" title="In 3.1.2 test data ‣ 3.1 Dataset ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">USMLE</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS2" title="In 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>LLM models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS2.SSS0.Px1" title="In 3.2 LLM models ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">LLAMA2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS2.SSS0.Px2" title="In 3.2 LLM models ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">LLAMA3</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS2.SSS0.Px3" title="In 3.2 LLM models ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">GPT-4</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS2.SSS0.Px4" title="In 3.2 LLM models ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Gemini</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS3" title="In 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS3.SSS1" title="In 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Medical Graph RAG Effect</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS3.SSS2" title="In 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Evidence-based response</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS3.SSS3" title="In 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Compare to SOTA Medical LLM Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.SS3.SSS4" title="In 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Ablation Study</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S4" title="In Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junde Wu 
<br class="ltx_break"/>University of Oxford 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">jundewu@ieee.org</span>
<br class="ltx_break"/>&amp;Jiayuan Zhu 
<br class="ltx_break"/>University of Oxford
&amp;Yunli Qi 
<br class="ltx_break"/>University of Oxford
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">We introduce a novel graph-based Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, called <span class="ltx_text ltx_font_bold" id="id2.id1.1">MedGraphRAG</span>, aimed at enhancing Large Language Model (LLM) capabilities and generating evidence-based results, thereby improving safety and reliability when handling private medical data. Our comprehensive pipeline begins with a hybrid static-semantic approach to document chunking, significantly improving context capture over traditional methods. Extracted entities are used to create a three-tier hierarchical graph structure, linking entities to foundational medical knowledge sourced from medical papers and dictionaries. These entities are then interconnected to form meta-graphs, which are merged based on semantic similarities to develop a comprehensive global graph. This structure supports precise information retrieval and response generation. The retrieval process employs a U-retrieve method to balance global awareness and indexing efficiency of the LLM. Our approach is validated through a comprehensive ablation study comparing various methods for document chunking, graph construction, and information retrieval. The results not only demonstrate that our hierarchical graph construction method consistently outperforms state-of-the-art models on multiple medical Q&amp;A benchmarks, but also confirms that the responses generated include source documentation, significantly enhancing the reliability of medical LLMs in practical applications.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancement of large language models (LLMs), such as OpenAI’s ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib11" title="">2023a</a>)</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib12" title="">2023b</a>)</cite>, has significantly transformed research in natural language processing and sparked numerous AI applications in everyday scenarios. However, these models still face limitations when applied to fields requiring specialized knowledge, such as finance, law, and medicine. There are two primary challenges: First, deploying trained LLMs for specific uses is complex due to their struggles with extremely long contexts and the high costs or impracticality of fine-tuning large models on specialized datasets. Second, in domains like medicine where precision is crucial, LLMs may produce hallucinations—outputs that seem accurate but lead to incorrect conclusions, which can be dangerous. Additionally, they sometimes provide overly simplistic answers without offering new insights or discoveries, which falls short in fields that demand high-level reasoning to derive correct answers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib8" title="">2021</a>)</cite> is a technique that answers user queries using specific and private datasets without requiring further training of the model. Originally designed for situations where the necessary answers are found within specific text regions, RAG sometimes struggles to synthesize new insights from disparate pieces of information linked by shared attributes. Additionally, it underperforms in tasks requiring a holistic understanding of summarized semantic concepts across large datasets or extensive documents. To address these limitations, the graph RAG <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib5" title="">2024</a>)</cite> method has been introduced. This approach leverages LLMs to create a knowledge graph from the private dataset, which, in conjunction with graph machine learning, enhances prompt augmentation during query processing. GraphRAG has demonstrated significant improvements, outperforming previous methods applied to private datasets by offering greater intelligence and mastery in information synthesis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we introduce a novel graph RAG method for applying LLMs to the medical domain, which we refer to as Medical Graph RAG (MedRAG). This technique improves LLM performance in the medical domain by response queries with grounded source citations and clear interpretations of medical terminology, boosting the transparency and interpretability of the results. This approach involves a three-tier hierarchical graph construction method. Initially, we use documents provided by users as our top-level source to extract entities. These entities are then linked to a second level consisting of more basic entities previously abstracted from credible medical books and papers. Subsequently, these entities are connected to a third level—the fundamental medical dictionary graph—that provides detailed explanations of each medical term and their semantic relationships. We then construct a comprehensive graph at the highest level by linking entities based on their content and hierarchical connections. This method ensures that the knowledge can be traced back to its sources and the results are factually accurate.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To respond to user queries, we implement a U-retrieve strategy that combines top-down retrieval with bottom-up response generation. The process begins by structuring the query using predefined medical tags and indexing them through the graphs in a top-down manner. The system then generates responses based on these queries, pulling from meta-graphs—nodes retrieved along with their TopK related nodes and relationships—and summarizing the information into a detailed response. This technique maintains a balance between global context awareness and the contextual limitations inherent in LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our medical graph RAG provides Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. The results provides the provenance, or source grounding information, as it generates each response, and demonstrates that an answer is grounded in the dataset. Having the cited source for each assertion readily available also enables a human user to quickly and accurately audit the LLM’s output directly against the original source material. It is super useful in the field of medicine that security is very important, and each of the reasoning should be evidence-based. By using such a method, we construct an evidence-based Medical LLM that the clinician could easiely check the source of the reasoning and calibrate the model response to ensure the safty usage of llm in the clinical senarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To evaluate our medical graph RAG, we implemented the method on several popular open and closed-source LLMs, including ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib11" title="">2023a</a>)</cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib16" title="">2023</a>)</cite>, testing them across mainstream medical Q&amp;A benchmarks such as PubMedQA <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib6" title="">2019</a>)</cite>, MedMCQA <cite class="ltx_cite ltx_citemacro_cite">Pal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib13" title="">2022</a>)</cite>, and USMLE <cite class="ltx_cite ltx_citemacro_cite">Kung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib7" title="">2023</a>)</cite>. For the RAG process, we supplied a comprehensive medical dictionary as the foundational knowledge layer, the UMLS medical knowledge graph <cite class="ltx_cite ltx_citemacro_cite">Lindberg et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib9" title="">1993</a>)</cite> as the foundamental layer detailing semantic relationships, and a curated MedC-K dataset <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib17" title="">2023</a>)</cite> —comprising the latest medical papers and books—as the intermediate level of data to simulate user-provided private data. Our experiments demonstrate that our model significantly enhances the performance of general-purpose LLMs on medical questions. Remarkably, it even surpasses many fine-tuned or specially trained LLMs on medical corpora, solely using the RAG approach without additional training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our contributions are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">1. We are pioneers in proposing a comprehensive pipeline for applying graph RAG specifically in the medical field.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">2. We have developed unique graph construction and data retrieval methods that enable LLMs to generate evidence-based responses utilizing holistic private data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">3. We conducted validation experiments across mainstream benchmarks, achieving state-of-the-art (SOTA) performance with various model variants.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">MedGraphRAG enhances Large Language Models (LLMs) with a medical graph RAG tailored for handling private medical data. It involves segmenting medical documents into chunks, extracting entities, and organizing them into a hierarchical graph structure across three levels—from user-provided documents to foundational medical information. These entities form meta-graphs, which are then merged based on content similarity into a comprehensive global graph. For user queries, the LLM retrieves and synthesizes information efficiently from the graph, enabling precise and contextually relevant medical responses.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Medical Graph Construction</h3>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Semantic Document Segmentation</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">Large medical documents often contain multiple themes or diverse content. To process these effectively, we first segment the document into data chunks that conform to the context limitations of Large Language Models (LLMs). Traditional methods such as chunking based on token size or fixed characters typically fail to detect subtle shifts in topics accurately. Consequently, these chunks may not fully capture the intended context, leading to a loss in the richness of meaning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p2.1">To enhance accuracy, we adopt a mixed method of character separation coupled with topic-based segmentation. Specifically, we utilize static characters (line break symbols) to isolate individual paragraphs within the document. Following this, we apply a derived form of the text for semantic chunking. Our approach includes the use of proposition transfer, which extracts standalone statements from a raw text <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib2" title="">2023</a>)</cite>. Through proposition transfer, each paragraph is transformed into self-sustaining statements. We then conduct a sequential analysis of the document to assess each proposition, deciding whether it should merge with an existing chunk or initiate a new one. This decision is made via a zero-shot approach by an LLM. To reduce noise generated by sequential processing, we implement a sliding window technique, managing five paragraphs at a time. We continuously adjust the window by removing the first paragraph and adding the next, maintaining focus on topic consistency. We set a hard threshold that the longest chunk cannot excess the context length limitation of LLM. After chunking the document, we construct graph on each individual of the data chunk.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Element Extraction</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">We then identify and extract instances of graph nodes from each chunk of source text. This is accomplished using a LLM prompt designed to recognize all relevant entities within the text. For each entity, the LLM is prompted to output the name, type, and a description. The name may either be the exact text from the document or a derivative term commonly used in medical contexts, carefully chosen to reflect professional medical terminology suitable for subsequent processing. The type is selected from a predefined table by the LLM, and the description is an LLM-generated explanation of the entity, contextualized within the document. To ensure the model’s effectiveness, we provide a few examples to guide the LLM in generating outputs as desired.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p2.1">For each entity data structure, we include a unique ID to trace its source document and paragraph. This identifier is crucial for retrieving information from the source, enabling the generation of evidence-based responses later.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p3.1">To enhance the quality of extraction and reduce noise and variance, we repeat the extraction process multiple times. This iterative approach encourages the LLM to detect any entities it may have initially overlooked. The decision to continue or halt the repetitive process is also determined by the LLM itself.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Hierarchy Linking</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">Medicine is a specialized field characterized by its consistent use of a precise terminology system and its foundation on numerous established truths, such as specific symptoms of diseases or side effects of drugs. In this domain, it is crucial that LLMs do not distort, modify, or add creative or random elements to the data, unlike their applications in other, less constrained contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p2.1">Recognizing this, we have developed a unique structure within the medical domain to link each entity to grounded medical knowledge and terms. This approach aims to provide credible source and profound definitions for each entity concept, thereby enhancing the authenticity of the responses and reducing the occurrence of hallucinations, a significant challenge when applying LLMs to medicine.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p3.1">Specifically, we construct a three-tiered RAG data structure to develop a comprehensive medical graph. The first level consists of user-provided documents, such as highly confidential medical reports from a specific hospital. After extracting entities from these documents as previously described, we link them to a more foundational level of commonly accepted information. The second level is constructed using medical textbooks and scholarly articles. We pre-construct a graph from these medical sources using the same methods outlined earlier, prior to receiving real user documents. Entities from the first level are linked to corresponding entities in the second level, based on relevance detected by LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p4">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p4.1">The entities of the second-level graph are then connected to a third level, which includes several well-defined medical terms and their knowledge relationships. This grounded information is sourced from reliable resources such as the Unified Medical Language System (UMLS), which integrates various health and biomedical vocabularies and their semantic relationships. For each entity, we compare the text embedding of its name with those of medical vocabularies in UMLS, selecting vocabularies where the cosine similarity falls below a specified threshold. Each linked vocabulary is further associated with its professional definitions and relationships in UMLS, and these relationships are translated into plain text, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Relationship Linking</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.1">We then instruct the LLM to identify all relationships between clearly-related entities. This decision is based on comprehensive information about each entity, including its name, description, definition, and associated lower-level medical foundation knowledge. The identified relationships specify the source and target entities, provide a description of their relationship, and include a score indicating the closeness of this relationship. To maintain order and precision in assessing relationship distance, we prompt the LLM to choose from a predefined list of descriptors—very related, related, medium, unrelated, very unrelated. After performing this analysis, we generate a weighted directed graph for each data chunk. These graphs serve as the fundamental building blocks in our system and are referred to as meta-graphs.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="358" id="S2.F1.1.g1" src="extracted/5780425/medgraphrag.png" width="548"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>MedGraphRAG framework.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Tags generation and merge the graphs</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px5.p1.1">After constructing the meta-graphs, our next step is to scan the data across each chunk to develop a global graph that links all meta-graphs together. The nodes in merged meta-graphs would link together based on the linking rule we used in the last paragraph. To achieve this, we calculate the distance between each pair of meta-graphs and sequentially merge the closest ones into larger entities. For efficient merging, we use the LLM to summarize the content of each meta-graph based on predefined medical categories, such as symptoms, patient history, body functions, and medications. The LLM generates a summary for each category derived from the meta-graph’s content, resulting in a list of tags that succinctly describe its main themes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px5.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px5.p2.1">Using these tags, the LLM calculates the similarity between two meta-graphs. Those with the highest similarity are considered for merging. The merged graph becomes a new graph, but retains its original meta-graphs and tags for easier indexing later. Subsequently, new summarized tag information is generated for the new graph, and its similarity to others is recalculated for potential further merging. This process can be repeated until a single global graph remains. However, as the summarized tag information accumulates, it loses detail, presenting a trade-off between merging effectiveness and efficiency. In practice, we limit the process to 24 iterations to prevent excessive loss of detail.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieve from the graph</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">After constructing the graph, the LLM efficiently retrieves information to respond to user queries through a strategy we called U-retrieve. We begin by generating summarized tag descriptions, similar to the previous step, and use these to identify the most relevant graph through a top-down matching process. This starts with one of the larger graphs, progressively indexing down to the smaller graphs it contains. This matching process is repeated until we reach the meta-graph layer and retrieve multiple relevant entities. Subsequently, we gather all pertinent content related to these activated entities and their TopK related entities. This includes the content of the entities themselves, their associated foundational medical knowledge, their relevance and relationships to other entities, and the content of any linked entities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Once the relevant content is identified, the LLM is prompted to generate an intermediate response using these information, presented in text form. This intermediate response is preserved and combined with the higher-level graph’s summarized tag information to formulate a more detailed or refined response. The LLM repeats this response generation process in a bottom-up manner until it reaches the highest level, generating a final response after scanning all the indexed graphs along the trajectory. This method allows the LLM to have a comprehensive overview, as it interacts with all the data in the graph, while also remaining efficient by accessing less relevant data in a summarized form.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>RAG data</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">In our RAG data structure, we design three distinct levels of data, each serving different roles in practice. The top-level data comprises private user information, such as medical reports in a hospital, which are confidential and not to be shared or exposed. This data is user-specific and subject to the highest frequency of updates or changes when using the LLM in a practical setting. The middle level consists of up-to-date, peer-reviewed, and credible medical books and papers. This layer provides users with the latest medical advancements and knowledge, ensuring they do not miss any cutting-edge discoveries. While these resources can be set as default data for different users, they can also be updated regularly by users or administrators to maintain currency. This data is updated at a medium frequency, typically annually. The bottom level includes data that define medical terms and their semantic relationships, primarily sourced from established vocabularies. This data is the most authoritative and serious and should be set as the default for every user intending to use the medical LLM. It is updated with the lowest frequency, approximately every five years or more.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Top-level</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.1">We employ MIMIC-IV, a publicly available electronic health record dataset, as our primary dataset. This dataset originates from Beth Israel Deaconess Medical Center and encompasses patient admissions spanning from 2008 to 2019. MIMIC-IV is designed to facilitate research and educational pursuits, encompassing a wide range of data including patient measurements, diagnoses, procedures, treatments, and anonymized clinical notes. This dataset is the product of a collaborative effort between the hospital and MIT, and is meticulously gathered, processed, and deidentified to comply with privacy standards. It is structured into three distinct modules—hospital, intensive care unit, and clinical notes—each specifically designed to meet various research requirements.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Medium-level</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">We utilized MedC-K, a substantial medical-specific corpus, as our medium-level data source. This corpus comprises 4.8 million biomedical academic papers and 30,000 textbooks. It includes the S2ORC dataset by Lo et al. (2020), which contains 81.1 million English-language academic papers. From this extensive collection, we have extracted 4.8 million papers related to biomedical studies from PubMed Central, amounting to over 75 billion tokens that encapsulate advanced medical knowledge. Additionally, we curated a collection of 30,000 medical textbooks from various libraries and publishers. After thorough cleaning and de-duplication processes, this collection provides approximately 4 billion tokens of essential medical knowledge.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Bottom-level</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px3.p1.1">We utilize the UMLS dataset as our foundational bottom-level data. The Unified Medical Language System (UMLS), developed by the U.S. National Library of Medicine, is an extensive dataset that unifies various medical vocabularies to enhance the interoperability of health information systems. It consists of three main components: the Metathesaurus, which amalgamates over 200 medical vocabularies including SNOMED CT and ICD-10; the Semantic Network, which organizes medical concepts and delineates their interrelationships; and the SPECIALIST Lexicon, which aids in natural language processing by providing detailed linguistic insights. UMLS is crucial for facilitating tasks such as electronic health record integration and clinical decision support, thereby improving the management and comprehension of medical data.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>test data</h4>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">PubMedQA</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">Developed by Jin et al. in 2019, PubMedQA is a biomedical question-answering dataset derived from PubMed abstracts. This dataset primarily focuses on addressing research questions through a multiple-choice format with options like yes, no, or maybe. It comprises three distinct parts: the PQA-L, which includes 1,000 manually labeled pairs used for testing; PQA-U, consisting of 61.2k unlabeled pairs which are not used; and PQA-A, featuring 211.3k artificially generated pairs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">MedMCQA</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.1">Introduced by Pal, Umapathi et al. in 2022, MedMCQA is a dataset of multiple-choice questions formulated from practice and previous examinations for Indian medical school entrance tests, specifically the AIIMS and NEET-PG. The dataset splits into a training set with 182,822 questions and a testing set containing 4,183 questions, each question offering four possible answers. This dataset serves as a significant resource for testing knowledge of medical school candidates.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">USMLE</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px3.p1.1">Created by Jin, Pan et al. in 2021, the USMLE dataset consists of multiple-choice questions from the United States Medical Licensing Exams, tailored to assess medical professionals’ readiness for board certification. This dataset is unique in its multilingual coverage, providing questions in English, Simplified Chinese, and Traditional Chinese. For the purpose of this description, only the English portion is considered, which includes 10,178 + 1,273 + 1,273 pieces of data.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LLM models</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">LLAMA2</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Building upon the original LLAMA dataset, LLAMA2 extends the evaluation framework by including more diverse and complex language tasks, potentially addressing the limitations and gaps identified in the initial version. Although specific details on LLAMA2 might be hypothetical or speculative in nature, one can expect that it would continue the focus on robust, comprehensive linguistic analysis, refining the tools and methods to better measure nuances in language understanding and generation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">LLAMA3</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">LLAMA3 is the latest iteration in the LLAMA series of large language models, developed to advance the capabilities of natural language understanding and generation. Building on the successes of its predecessors, LLAMA and LLAMA2, LLAMA3 incorporates even more sophisticated algorithms and a broader dataset to enhance its performance across a wide array of linguistic tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">GPT-4</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Developed by OpenAI, ChatGPT-4 is an iteration of the generative pre-trained transformer models that has been trained on a diverse range of internet text. As a more advanced version, ChatGPT-4 features improvements over its predecessors in terms of its ability to understand and generate human-like text, making it capable of engaging in more coherent and contextually relevant conversations. This model is designed to perform a wide range of tasks including but not limited to translation, question-answering, and content generation, showcasing significant advancements in handling complex dialogue scenarios and nuanced language subtleties.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Gemini</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1">Google’s Gemini is a cutting-edge language model designed to enhance the capabilities of conversational AI systems. Developed as part of Google’s ongoing efforts in natural language processing, Gemini aims to provide more nuanced and context-aware interactions than previous models. This model leverages deep learning techniques to understand and generate human-like responses, making it suitable for a wide range of applications including virtual assistants, customer support, and interactive applications.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Results</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Medical Graph RAG Effect</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">First, we conducted experiments to assess the impact of our Medical Graph RAG on various large language models, with the results presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.T1" title="Table 1 ‣ 3.3.1 Medical Graph RAG Effect ‣ 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. The data reveals that our MedGraphRAG significantly enhances the performance of LLMs on medical benchmarks. This improvement is attributed to the implementation of zero-shot RAG, which is more cost-effective, faster, and more convenient than fine-tuning or using adapters. Notably, MedGraphRAG yields more substantial improvements in smaller LLMs, such as LLaMA2-13B and LLaMA3-8B, which typically underperform on these benchmarks, thus broadening its applicability across a wider user base. MedGraphRAG also significantly boosts the performance of more powerful, closed-source LLMs like GPT and LLaMA3-70B, helping them achieve state-of-the-art (SOTA) results on multiple benchmarks. These outcomes surpass the accuracy of human experts, demonstrating the strong potential of AI to enhance clinical workflows.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="337" id="S3.F2.1.g1" src="extracted/5780425/medgraphrag_exp.png" width="329"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Compare to SOTA Medical LLM Models on MedQA benchmark.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The improvement of MedGraphRAG on various LLMs.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.2">Size</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.3">Open-sourced</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.4">MedQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.5">MedMCQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.6">PubMedQA</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.1">LLaMA2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.2">13B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.3">yes</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.4">42.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.5">37.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.6">68.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.1"><span class="ltx_text" id="S3.T1.1.3.3.1.1" style="background-color:#ECF4FF;">LLaMA2-MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.2"><span class="ltx_text" id="S3.T1.1.3.3.2.1" style="background-color:#ECF4FF;">13B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.3"><span class="ltx_text" id="S3.T1.1.3.3.3.1" style="background-color:#ECF4FF;">yes</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.4"><span class="ltx_text" id="S3.T1.1.3.3.4.1" style="background-color:#ECF4FF;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.5"><span class="ltx_text" id="S3.T1.1.3.3.5.1" style="background-color:#ECF4FF;">51.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.6"><span class="ltx_text" id="S3.T1.1.3.3.6.1" style="background-color:#ECF4FF;">73.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.1">LLaMA2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.2">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.3">yes</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.4">43.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.5">35.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.6">74.3</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.1"><span class="ltx_text" id="S3.T1.1.5.5.1.1" style="background-color:#ECF4FF;">LLaMA2-MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.2"><span class="ltx_text" id="S3.T1.1.5.5.2.1" style="background-color:#ECF4FF;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.3"><span class="ltx_text" id="S3.T1.1.5.5.3.1" style="background-color:#ECF4FF;">yes</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.4"><span class="ltx_text" id="S3.T1.1.5.5.4.1" style="background-color:#ECF4FF;">69.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.5"><span class="ltx_text" id="S3.T1.1.5.5.5.1" style="background-color:#ECF4FF;">58.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.6"><span class="ltx_text" id="S3.T1.1.5.5.6.1" style="background-color:#ECF4FF;">76.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.1">LLaMA3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.2">8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.3">yes</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.4">59.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.5">57.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.6">75.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.1"><span class="ltx_text" id="S3.T1.1.7.7.1.1" style="background-color:#ECF4FF;">LLaMA3-MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.2"><span class="ltx_text" id="S3.T1.1.7.7.2.1" style="background-color:#ECF4FF;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.3"><span class="ltx_text" id="S3.T1.1.7.7.3.1" style="background-color:#ECF4FF;">yes</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.4"><span class="ltx_text" id="S3.T1.1.7.7.4.1" style="background-color:#ECF4FF;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.5"><span class="ltx_text" id="S3.T1.1.7.7.5.1" style="background-color:#ECF4FF;">61.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.6"><span class="ltx_text" id="S3.T1.1.7.7.6.1" style="background-color:#ECF4FF;">77.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.1">LLaMA3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.2">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.3">yes</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.4">72.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.5">65.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.6">77.5</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.1"><span class="ltx_text" id="S3.T1.1.9.9.1.1" style="background-color:#ECF4FF;">LLaMA3-MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.2"><span class="ltx_text" id="S3.T1.1.9.9.2.1" style="background-color:#ECF4FF;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.3"><span class="ltx_text" id="S3.T1.1.9.9.3.1" style="background-color:#ECF4FF;">yes</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.4"><span class="ltx_text" id="S3.T1.1.9.9.4.1" style="background-color:#ECF4FF;">88.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.5"><span class="ltx_text" id="S3.T1.1.9.9.5.1" style="background-color:#ECF4FF;">79.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.9.9.6.1" style="background-color:#ECF4FF;">83.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.1">Gemini-pro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.3">no</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.4">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.5">54.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.10.6">69.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.1"><span class="ltx_text" id="S3.T1.1.11.11.1.1" style="background-color:#ECF4FF;">Gemini-MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.2"><span class="ltx_text" id="S3.T1.1.11.11.2.1" style="background-color:#ECF4FF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.3"><span class="ltx_text" id="S3.T1.1.11.11.3.1" style="background-color:#ECF4FF;">no</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.4"><span class="ltx_text" id="S3.T1.1.11.11.4.1" style="background-color:#ECF4FF;">72.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.5"><span class="ltx_text" id="S3.T1.1.11.11.5.1" style="background-color:#ECF4FF;">62.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.6"><span class="ltx_text" id="S3.T1.1.11.11.6.1" style="background-color:#ECF4FF;">76.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.2">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.3">no</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.4">81.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.5">72.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.6">75.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.1"><span class="ltx_text" id="S3.T1.1.13.13.1.1" style="background-color:#ECF4FF;">GPT-4 MedGraphRAG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.2"><span class="ltx_text" id="S3.T1.1.13.13.2.1" style="background-color:#ECF4FF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.3"><span class="ltx_text" id="S3.T1.1.13.13.3.1" style="background-color:#ECF4FF;">no</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.4.1" style="background-color:#ECF4FF;">91.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.5.1" style="background-color:#ECF4FF;">81.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.6"><span class="ltx_text" id="S3.T1.1.13.13.6.1" style="background-color:#ECF4FF;">83.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.1">Human (expert)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.4">87.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.5">90.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.14.14.6">78.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Evidence-based response</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Thanks to the graph linking mechanism in our MedGraphRAG, we can prompt LLMs to generate evidence-based responses to complex medical questions, enhancing both safety and explainability. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.F3" title="Figure 3 ‣ 3.3.4 Ablation Study ‣ 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the responses generated by GPT-4 alone and those enhanced by MedGraphRAG for a challenging medical diagnosis question. In this case, the patient exhibits symptoms commonly associated with Alzheimer’s—increasing forgetfulness and occasional episodes of sudden confusion and speech difficulty. However, a careful analysis by an experienced human expert would identify the condition as Vascular Dementia. The MedGraphRAG-enhanced response not only accurately identifies Vascular Dementia over Alzheimer’s but also provides detailed explanations supported by authentic citations. This ensures that each claim is verifiable, making the information trustworthy for clinicians. Additionally, the response includes simplified explanations of medical terms, making it accessible to users without a medical background. This evidence-based, user-friendly approach is crucial in clinical practice where safety is paramount.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Compare to SOTA Medical LLM Models</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">We also evaluated MedGraphRAG against a range of previous state-of-the-art (SOTA) models on these benchmarks, including both intensively fine-tuned models <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib4" title="">2022</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Yasunaga et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib18" title="">2022a</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Yasunaga et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib19" title="">2022b</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Bolton et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib1" title="">2022</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Singhal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib14" title="">2022</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Singhal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib15" title="">2023</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib17" title="">2023</a>)</cite> and non-fine-tuned models <cite class="ltx_cite ltx_citemacro_cite">Nori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib10" title="">2023</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib11" title="">2023a</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib12" title="">2023b</a>)</cite> on the MedQA benchmark. The results, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.F2" title="Figure 2 ‣ 3.3.1 Medical Graph RAG Effect ‣ 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, show that when applied to a powerful GPT-4 LLM, our MedGraphRAG surpasses the previous SOTA prompted model, Medprompt <cite class="ltx_cite ltx_citemacro_cite">Nori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib10" title="">2023</a>)</cite>, by a significant 1.1%. Even when compared with intensive fine-tuning methods on these medical datasets, MedGraphRAG outperforms all and achieves the SOTA. This superior performance stems from leveraging the inherent capabilities of the robust GPT-4 model. This further underscores the advantages of our non-fine-tuned MedGraphRAG approach: it inherits the strong capabilities of a closed-source model and outperforms many models that require costly and exhaustive fine-tuning.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Ablation Study</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">We conducted a comprehensive ablation study to validate the effectiveness of our proposed modules, the results of which are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#S3.T2" title="Table 2 ‣ 3.3.4 Ablation Study ‣ 3.3 Results ‣ 3 Experiment ‣ Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>. This study compares various methods for document chunking, hierarchy graph construction, and information retrieval. Specifically, for document chunking, we evaluated our hybrid static-semantic method against a purely static approach. For hierarchy graph construction, we contrasted our method with the basic construction approach used in LangChain. For information retrieval, we compared the summarized-based retrieval <cite class="ltx_cite ltx_citemacro_cite">Edge et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.04187v1#bib.bib3" title="">2024</a>)</cite> with our U-retrieve method. These methods were assessed across the three Q&amp;A benchmarks previously mentioned.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS4.p2">
<p class="ltx_p" id="S3.SS3.SSS4.p2.1">The results, as shown in the table, indicate that our hybrid semantic method significantly enhances performance over the vanilla model, underscoring the importance of sophisticated data chunking in all RAG pipelines. When comparing the base graph construction method with our proposed hierarchical approach, it’s clear that constructing the graph enhances RAG performance. Furthermore, our hierarchical graph construction technique yields the most significant improvements, surpassing the performance of most state-of-the-art (SOTA) methods. Additionally, replacing the summarized retrieval with our U-retrieve method further boosts performance, demonstrating the effectiveness of U-retrieve in enhancing retrieval accuracy and relevance.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="785" id="S3.F3.1.g1" src="extracted/5780425/medgraphrag_ex.png" width="494"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example case shows MedGraphRAG generating evidence-based responses with grounded citations and terminology explanations.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>An ablation study on MedGraphRAG. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:390.3pt;height:112.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.4pt,6.7pt) scale(0.893031412769254,0.893031412769254) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S3.T2.1.1.1.1.1">Doc. Chunking</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S3.T2.1.1.1.1.2">Graph Construction</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S3.T2.1.1.1.1.3">Retrieve</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.4">MedQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.5">MedMCQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.6">PubMedQA</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.1">Static</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.2">Hyb-Semantic</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.3">Base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.4">Hierarchy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.5">SumR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.6">UR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.7">(%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.8">(%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.9">(%)</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.1">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.3.7">83.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.3.8">74.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.3.9">75.8</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.4">
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.4.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.4.4.2">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.4.4.3"></td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.4.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.4.4.5">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.4.4.6"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.4.7">87.4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.4.8">77.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.4.9">77.9</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.5">
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.5.5.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.3">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.5.5.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.5">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.5.5.6"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.7">88.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.8">78.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.9">80.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.6">
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.6.6.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.6.6.2">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.6.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.6.6.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.6.6.5">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.6.6.6"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.6.7">90.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.6.8">80.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.6.6.9">82.5</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.7">
<td class="ltx_td ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.1"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.4">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.5"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.7.7.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.7.1">91.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.7.7.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.8.1">81.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.7.7.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.9.1">83.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In conclusion, this paper introduces MedGraphRAG, a novel graph-based Retrieval-Augmented Generation (RAG) framework for the medical domain, enhancing the capabilities of Large Language Models (LLMs). Our method combines advanced document chunking with a hierarchical graph structure, significantly improving data organization and retrieval accuracy. Our ablation studies confirm superior performance over state-of-the-art models on medical Q&amp;A benchmarks and provide credible, source-linked responses essential for medical applications. Moving forward, we aim to expand this framework to include more diverse datasets and explore its potential in real-time clinical settings.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolton et al. (2022)</span>
<span class="ltx_bibblock">
Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang.

</span>
<span class="ltx_bibblock">Biomedlm, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://crfm.stanford.edu/2022/12/15/biomedlm.html" title="">https://crfm.stanford.edu/2022/12/15/biomedlm.html</a>.

</span>
<span class="ltx_bibblock">Stanford Center for Research on Foundation Models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu.

</span>
<span class="ltx_bibblock">Dense X Retrieval: What Retrieval Granularity Should We Use?, December 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.06648" title="">http://arxiv.org/abs/2312.06648</a>.

</span>
<span class="ltx_bibblock">arXiv:2312.06648 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edge et al. (2024)</span>
<span class="ltx_bibblock">
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.

</span>
<span class="ltx_bibblock">From Local to Global: A Graph RAG Approach to Query-Focused Summarization, April 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.16130" title="">http://arxiv.org/abs/2404.16130</a>.

</span>
<span class="ltx_bibblock">arXiv:2404.16130 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.

</span>
<span class="ltx_bibblock">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ACM Transactions on Computing for Healthcare</em>, 3(1):1–23, January 2022.

</span>
<span class="ltx_bibblock">ISSN 2691-1957, 2637-8051.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3458754</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2007.15779" title="">http://arxiv.org/abs/2007.15779</a>.

</span>
<span class="ltx_bibblock">arXiv:2007.15779 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024)</span>
<span class="ltx_bibblock">
Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao.

</span>
<span class="ltx_bibblock">GRAG: Graph Retrieval-Augmented Generation, May 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.16506" title="">http://arxiv.org/abs/2405.16506</a>.

</span>
<span class="ltx_bibblock">arXiv:2405.16506 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2019)</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu.

</span>
<span class="ltx_bibblock">PubMedQA: A Dataset for Biomedical Research Question Answering, September 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.06146" title="">http://arxiv.org/abs/1909.06146</a>.

</span>
<span class="ltx_bibblock">arXiv:1909.06146 [cs, q-bio].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kung et al. (2023)</span>
<span class="ltx_bibblock">
Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng.

</span>
<span class="ltx_bibblock">Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">PLOS Digital Health</em>, 2(2):e0000198, February 2023.

</span>
<span class="ltx_bibblock">ISSN 2767-3170.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1371/journal.pdig.0000198</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931230/" title="">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931230/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, April 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2005.11401" title="">http://arxiv.org/abs/2005.11401</a>.

</span>
<span class="ltx_bibblock">arXiv:2005.11401 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindberg et al. (1993)</span>
<span class="ltx_bibblock">
D. A. Lindberg, B. L. Humphreys, and A. T. McCray.

</span>
<span class="ltx_bibblock">The Unified Medical Language System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Methods of Information in Medicine</em>, 32(4):281–291, August 1993.

</span>
<span class="ltx_bibblock">ISSN 0026-1270.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1055/s-0038-1634945</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nori et al. (2023)</span>
<span class="ltx_bibblock">
Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz.

</span>
<span class="ltx_bibblock">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine, November 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.16452" title="">http://arxiv.org/abs/2311.16452</a>.

</span>
<span class="ltx_bibblock">arXiv:2311.16452 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Openai. introducing chatgpt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt/" title="">https://openai.com/blog/chatgpt/</a>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al. (2022)</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering, March 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.14371" title="">http://arxiv.org/abs/2203.14371</a>.

</span>
<span class="ltx_bibblock">arXiv:2203.14371 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. (2022)</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2212.13138</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. (2023)</span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Towards expert-level medical question answering with large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models, February 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.13971" title="">http://arxiv.org/abs/2302.13971</a>.

</span>
<span class="ltx_bibblock">arXiv:2302.13971 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">PMC-LLaMA: Towards Building Open-source Language Models for Medicine, August 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.14454" title="">http://arxiv.org/abs/2304.14454</a>.

</span>
<span class="ltx_bibblock">arXiv:2304.14454 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al. (2022a)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, and Jure Leskovec.

</span>
<span class="ltx_bibblock">Deep Bidirectional Language-Knowledge Graph Pretraining, October 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.09338" title="">http://arxiv.org/abs/2210.09338</a>.

</span>
<span class="ltx_bibblock">arXiv:2210.09338 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al. (2022b)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Jure Leskovec, and Percy Liang.

</span>
<span class="ltx_bibblock">LinkBERT: Pretraining Language Models with Document Links, March 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.15827" title="">http://arxiv.org/abs/2203.15827</a>.

</span>
<span class="ltx_bibblock">arXiv:2203.15827 [cs].

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug  8 03:03:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
